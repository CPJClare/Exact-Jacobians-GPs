{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 3b__DixonPrice.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQo_XsusPbm"
      },
      "source": [
        "Dixon-Price synthetic function:\r\n",
        "\r\n",
        "GP EI: Newton-CG (exact GP EI gradients + exact GP EI Hessian) vs. L-BFGS-B (exact GP EI gradients, without Hessian)\r\n",
        "\r\n",
        "https://www.sfu.ca/~ssurjano/dixonpr.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhgucjJgsYfm",
        "outputId": "8a407e6f-7fa6-4b83-8a8b-52d867dc413a"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyGPGO\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/87/a113c91ba014708114f7635d5c0f6a5e5c773480c5f0a537b257a02d180d/pyGPGO-0.4.0.dev1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n",
            "Building wheels for collected packages: pyGPGO\n",
            "  Building wheel for pyGPGO (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyGPGO: filename=pyGPGO-0.4.0.dev1-cp37-none-any.whl size=19866 sha256=fba7ad3bacf9a5099bafe5fcc310040df579ebb953327047c42fb1cc6605d00b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/27/04/c4fa3bfe194d36e3cd51542132f43415a6813114a5e8301acb\n",
            "Successfully built pyGPGO\n",
            "Installing collected packages: pyGPGO\n",
            "Successfully installed pyGPGO-0.4.0.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E99qMZverfrP"
      },
      "source": [
        "### Import modules:\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import scipy as sp\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import random\r\n",
        "import warnings\r\n",
        "import time\r\n",
        "\r\n",
        "from pyGPGO.logger import EventLogger\r\n",
        "from pyGPGO.GPGO import GPGO\r\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\r\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess, logpdf\r\n",
        "from pyGPGO.acquisition import Acquisition\r\n",
        "from pyGPGO.covfunc import squaredExponential\r\n",
        "\r\n",
        "from collections import OrderedDict\r\n",
        "from joblib import Parallel, delayed\r\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\r\n",
        "from scipy.optimize import minimize\r\n",
        "from scipy.spatial.distance import cdist\r\n",
        "from scipy.special import gamma\r\n",
        "from scipy.stats import norm, t\r\n",
        "from matplotlib.pyplot import rc\r\n",
        "\r\n",
        "rc('text', usetex=False)\r\n",
        "plt.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\r\n",
        "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\r\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzPUbCs3s9nu"
      },
      "source": [
        "### Inputs:\r\n",
        "\r\n",
        "n_start_AcqFunc = 100\r\n",
        "\r\n",
        "obj_func = 'DixonPrice'\r\n",
        "n_test = n_start_AcqFunc # test points\r\n",
        "df = 3 # nu\r\n",
        "\r\n",
        "util_loser = 'dEI_GP'\r\n",
        "util_winner = 'dEI_GP'\r\n",
        "n_init = 5 # random initialisations"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AMqjmpbtAb1"
      },
      "source": [
        "### Objective function:\r\n",
        "\r\n",
        "if obj_func == 'DixonPrice': # 2-D\r\n",
        "            \r\n",
        "    def f_syn_polarity(x1_training, x2_training):\r\n",
        "        return  operator * ((x1_training - 1)**2\r\n",
        "                            + 2 * (2 * x2_training ** 2 - x1_training)**2            \r\n",
        "                           )\r\n",
        "\r\n",
        "# Constraints:\r\n",
        "    lb = -10 \r\n",
        "    ub = +10\r\n",
        "    \r\n",
        "# Input array dimension(s):\r\n",
        "    dim = 2\r\n",
        "    max_iter = 100\r\n",
        "\r\n",
        "# 2-D inputs' parameter bounds:\r\n",
        "    param = {'x1_training': ('cont', [lb, ub]),\r\n",
        "             'x2_training': ('cont', [lb, ub]),\r\n",
        "             }\r\n",
        "\r\n",
        "    \r\n",
        "# True y bounds:\r\n",
        "    y_lb = 0.00000\r\n",
        "    operator = -1 # targets global minimum \r\n",
        "    y_global_orig = y_lb * operator # targets global minimum\r\n",
        "\r\n",
        "# Test data:\r\n",
        "    x1_test = np.linspace(lb, ub, max_iter) \r\n",
        "    x2_test = np.linspace(lb, ub, max_iter)\r\n",
        "    Xstar_d = np.column_stack((x1_test,x2_test))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyQX-59DtC7-"
      },
      "source": [
        "### Cumulative Regret Calculator:\r\n",
        "\r\n",
        "def min_max_array(x):\r\n",
        "    new_list = []\r\n",
        "    for i, num in enumerate(x):\r\n",
        "            new_list.append(np.min(x[0:i+1]))\r\n",
        "    return new_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE_I_YSntHxl"
      },
      "source": [
        "### Set-seeds:\r\n",
        "\r\n",
        "run_num_1 = 111\r\n",
        "run_num_2 = 113\r\n",
        "run_num_3 = 3333\r\n",
        "run_num_4 = 444\r\n",
        "run_num_5 = 5555\r\n",
        "run_num_6 = 6\r\n",
        "run_num_7 = 777\r\n",
        "run_num_8 = 887\r\n",
        "run_num_9 = 99\r\n",
        "run_num_10 = 1000\r\n",
        "run_num_11 = 1113\r\n",
        "run_num_12 = 1234\r\n",
        "run_num_13 = 2345\r\n",
        "run_num_14 = 88\r\n",
        "run_num_15 = 1556\r\n",
        "run_num_16 = 1666\r\n",
        "run_num_17 = 717\r\n",
        "run_num_18 = 8\r\n",
        "run_num_19 = 1998\r\n",
        "run_num_20 = 2000"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe29v5N5tKjF"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\r\n",
        "\r\n",
        "def l2norm_(X, Xstar):\r\n",
        "    \r\n",
        "    return cdist(X, Xstar)\r\n",
        "\r\n",
        "def kronDelta(X, Xstar):\r\n",
        "\r\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\r\n",
        "\r\n",
        "class squaredExponentialDeriv(squaredExponential):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1\r\n",
        "    sigman = 1e-6\r\n",
        "\r\n",
        "    def K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\r\n",
        "        return K\r\n",
        "    \r\n",
        "    def dK(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\r\n",
        "        return dK\r\n",
        "    \r\n",
        "    def d2K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (r **2 - 1)\r\n",
        "        return d2K\r\n",
        "    \r\n",
        "cov_func = squaredExponentialDeriv()\r\n",
        "d_cov_func = squaredExponentialDeriv()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEBX5l7TtNSN"
      },
      "source": [
        "### Acquisition function derivatives:\r\n",
        "\r\n",
        "class Acquisition_new(Acquisition):    \r\n",
        "    def __init__(self, mode, eps=1e-08, **params):\r\n",
        "        \r\n",
        "        self.params = params\r\n",
        "        self.eps = eps\r\n",
        "\r\n",
        "        mode_dict = {\r\n",
        "            'dEI_GP': self.dEI_GP\r\n",
        "        }\r\n",
        "\r\n",
        "        self.f = mode_dict[mode]\r\n",
        "    \r\n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\r\n",
        "        \r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\r\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\r\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\r\n",
        "        \r\n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\r\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\r\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\r\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\r\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\r\n",
        "            \r\n",
        "        return f, df, d2f\r\n",
        "    \r\n",
        "    def _eval(self, tau, mean, std):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, **self.params)\r\n",
        "    \r\n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, **self.params)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SdY8rHwtRJ_"
      },
      "source": [
        "### Surrogate derivatives: \r\n",
        "\r\n",
        "from scipy.linalg import cholesky, solve\r\n",
        "\r\n",
        "class dGaussianProcess(GaussianProcess):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1\r\n",
        "    sigman = 1e-6\r\n",
        "\r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\r\n",
        "        L = cholesky(K).T\r\n",
        "        alpha = solve(L.T, solve(L, self.y))\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(self.L, Kstar.T)\r\n",
        "        dv = solve(self.L, dKstar.T)\r\n",
        "        d2v = solve(self.L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -2 * np.dot(dv.T, v)\r\n",
        "        dvdv = np.dot(dv.T, dv)\r\n",
        "        d2s = -2 * (dvdv + d2v)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha)\r\n",
        "        d2m = np.dot(d2Kstar, alpha)\r\n",
        "        return ds, dm, dvdv, d2v, d2m"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNv4Z0EqtUi2"
      },
      "source": [
        "class dGPGO(GPGO):  \r\n",
        "    n_start = n_start_AcqFunc\r\n",
        "    eps = 1e-08\r\n",
        "        \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expMSaSUtcB-"
      },
      "source": [
        "### d2GPGO - BayesOpt class: Exact Hessian\r\n",
        "\r\n",
        "class d2GPGO(GPGO):  \r\n",
        "    n_start = n_start_AcqFunc\r\n",
        "    p = np.full((n_start,1),1)*0 + 1\r\n",
        "    eps = 1e-08\r\n",
        "    \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "    \r\n",
        "    def hessp_nonzero(self, xnew, p):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        df2 = np.empty((self.n_start,))\r\n",
        "        df2 = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[2]\r\n",
        "        H2 = np.empty((self.n_start,))\r\n",
        "        df2 = np.asarray(df2)\r\n",
        "        p = np.asarray(p)\r\n",
        "        H2 = np.multiply(df2,p)\r\n",
        "        return H2\r\n",
        "\r\n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\r\n",
        "        \r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\r\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\r\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\r\n",
        "        \r\n",
        "        d2f = (z * norm.cdf(z) + norm.pdf(z)[0]) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\r\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\r\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx\r\n",
        "\r\n",
        "        return d2f\r\n",
        "\r\n",
        "    def hessp_nonzero1(self, xnew, p, *args):\r\n",
        "      new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "      new_std = np.sqrt(new_var + 1e-6)\r\n",
        "      ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "      df2 = np.empty((self.n_start,))\r\n",
        "      df2 = -self.dEI_GP(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0] * p\r\n",
        "      return df2\r\n",
        "\r\n",
        "    def d_optimizeAcq(self, method='Newton-CG', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,                  \r\n",
        "                                                                 hessp = self.hessp_nonzero1,                      \r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        \r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "\r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):    \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umKPgDuEtcxH",
        "outputId": "2c4bad40-2b19-44f8-ba52-2aa2b086d7e1"
      },
      "source": [
        "start_lose = time.time()\r\n",
        "start_lose"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1616065902.827723"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfEuNIqvtdSv",
        "outputId": "e6c138c1-197a-4164-8691-2136bf5d9c4c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_loser_1 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_1 = d2GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 2.24340351 -6.61860491]. \t  -14577.093458365041 \t -540.0726783817402\n",
            "init   \t [-1.27881961  5.38524945]. \t  -7033.582264098692 \t -540.0726783817402\n",
            "init   \t [-4.09349391 -7.01674086]. \t  -21064.19892235138 \t -540.0726783817402\n",
            "init   \t [-9.55043351 -1.59551015]. \t  -540.0726783817402 \t -540.0726783817402\n",
            "init   \t [-5.22635718 -3.24687616]. \t  -1423.2804229542423 \t -540.0726783817402\n",
            "1      \t [8.37416725 6.27688756]. \t  -9973.589615047737 \t -540.0726783817402\n",
            "2      \t [-9.76713775  9.96269922]. \t  -86875.28945668103 \t -540.0726783817402\n",
            "3      \t [ 9.98926049 -9.3463478 ]. \t  -54345.61921377557 \t -540.0726783817402\n",
            "4      \t [ 9.44490266 -0.94685252]. \t  \u001b[92m-188.41779181630653\u001b[0m \t -188.41779181630653\n",
            "5      \t [ -5.71524748 -13.96461029]. \t  -313258.9453086423 \t -188.41779181630653\n",
            "6      \t [-10.10913729 -20.06167194]. \t  -1328738.0445942972 \t -188.41779181630653\n",
            "7      \t [-12.38891977  -7.55737348]. \t  -32242.844494624373 \t -188.41779181630653\n",
            "8      \t [3.45685175 0.29515637]. \t  \u001b[92m-27.587271653981432\u001b[0m \t -27.587271653981432\n",
            "9      \t [-7.29911972  3.709041  ]. \t  -2492.777864737907 \t -27.587271653981432\n",
            "10     \t [  1.37566486 -13.1409446 ]. \t  -236662.76419149013 \t -27.587271653981432\n",
            "11     \t [-71.39705254 -75.51792945]. \t  -263462640.68493322 \t -27.587271653981432\n",
            "12     \t [-0.62072552 -0.79850955]. \t  \u001b[92m-9.81608378188993\u001b[0m \t -9.81608378188993\n",
            "13     \t [3.37484376 8.90502033]. \t  -48194.5769897518 \t -9.81608378188993\n",
            "14     \t [ 6.92233961 -4.46391467]. \t  -2203.9398923755048 \t -9.81608378188993\n",
            "15     \t [4.07770053 3.69301877]. \t  -1085.8663725931776 \t -9.81608378188993\n",
            "16     \t [-2.97259508  0.66277349]. \t  -45.443954498313985 \t -9.81608378188993\n",
            "17     \t [-35.39287151 -36.68907683]. \t  -14880596.167843673 \t -9.81608378188993\n",
            "18     \t [-0.9062252  -3.11263535]. \t  -826.4519509478366 \t -9.81608378188993\n",
            "19     \t [0.56454101 1.24982823]. \t  -13.292730763065736 \t -9.81608378188993\n",
            "20     \t [ 0.97902319 -0.15668085]. \t  \u001b[92m-1.7299625847814593\u001b[0m \t -1.7299625847814593\n",
            "21     \t [-3.93330011  8.95461175]. \t  -54015.580312945465 \t -1.7299625847814593\n",
            "22     \t [ 2.10667478 -1.40697019]. \t  -8.087911610910195 \t -1.7299625847814593\n",
            "23     \t [ 0.47070545 -0.19685938]. \t  \u001b[92m-0.5893623930947876\u001b[0m \t -0.5893623930947876\n",
            "24     \t [-0.24582787  0.26776761]. \t  -1.8550821588236268 \t -0.5893623930947876\n",
            "25     \t [6.99377468 1.23487141]. \t  -67.0349740057245 \t -0.5893623930947876\n",
            "26     \t [ 4.22932274 -1.4679427 ]. \t  -10.44145007047248 \t -0.5893623930947876\n",
            "27     \t [-0.8115279  -0.25321924]. \t  -5.047960650296771 \t -0.5893623930947876\n",
            "28     \t [-8.51833569 -4.8138452 ]. \t  -6110.83572388887 \t -0.5893623930947876\n",
            "29     \t [-6.71666219 -0.28074889]. \t  -154.05893299924375 \t -0.5893623930947876\n",
            "30     \t [-0.9954933  1.4494201]. \t  -58.00232597608417 \t -0.5893623930947876\n",
            "31     \t [9.67797817 1.64318147]. \t  -111.9079476716168 \t -0.5893623930947876\n",
            "32     \t [-0.45940608 -0.22987155]. \t  -2.7685148573582254 \t -0.5893623930947876\n",
            "33     \t [ 4.11952878 -2.21211234]. \t  -73.96924555251553 \t -0.5893623930947876\n",
            "34     \t [ 5.31832938 -0.89814586]. \t  -46.101980164629545 \t -0.5893623930947876\n",
            "35     \t [-8.07337685 -9.88184673]. \t  -82805.24270582151 \t -0.5893623930947876\n",
            "36     \t [ 3.55953023 -0.39807718]. \t  -27.580096161751513 \t -0.5893623930947876\n",
            "37     \t [1.74675114 1.18253548]. \t  -2.7627598092038275 \t -0.5893623930947876\n",
            "38     \t [-9.8545825   1.34674201]. \t  -481.3511805040524 \t -0.5893623930947876\n",
            "39     \t [ 5.70091754 -8.14733325]. \t  -32309.142731195105 \t -0.5893623930947876\n",
            "40     \t [0.74961508 0.18998971]. \t  -0.9804961633716365 \t -0.5893623930947876\n",
            "41     \t [18.61213103 17.98936427]. \t  -790642.2364853162 \t -0.5893623930947876\n",
            "42     \t [-4.88905439  1.65517437]. \t  -249.68254157856282 \t -0.5893623930947876\n",
            "43     \t [ 1.42646113 -1.55525242]. \t  -23.453880797636348 \t -0.5893623930947876\n",
            "44     \t [1.54194991 0.37716152]. \t  -3.4560620365163213 \t -0.5893623930947876\n",
            "45     \t [ 4.04534893 -0.93426908]. \t  -19.850760381658137 \t -0.5893623930947876\n",
            "46     \t [-3.30910877 -0.66231956]. \t  -53.62103126413878 \t -0.5893623930947876\n",
            "47     \t [ 2.75282019 -0.86907665]. \t  -6.158658066190721 \t -0.5893623930947876\n",
            "48     \t [-1.29426966 -0.67473396]. \t  -14.98597242488901 \t -0.5893623930947876\n",
            "49     \t [ 1.96596964 -0.56138311]. \t  -4.501113499485708 \t -0.5893623930947876\n",
            "50     \t [2.63992793 0.18523005]. \t  -15.912608980644505 \t -0.5893623930947876\n",
            "51     \t [0.10739441 0.27421282]. \t  -0.8004411893953339 \t -0.5893623930947876\n",
            "52     \t [9.09015916 9.62156367]. \t  -62058.86386351431 \t -0.5893623930947876\n",
            "53     \t [1.04414074 0.77201658]. \t  \u001b[92m-0.04568447350163922\u001b[0m \t -0.04568447350163922\n",
            "54     \t [ 2.05926448 -6.1821805 ]. \t  -11065.72629953515 \t -0.04568447350163922\n",
            "55     \t [ 2.60426264 -3.59796671]. \t  -1087.0926778505834 \t -0.04568447350163922\n",
            "56     \t [ 0.90438136 -0.6183934 ]. \t  -0.04809723355965922 \t -0.04568447350163922\n",
            "57     \t [1.35287358 0.98018969]. \t  -0.7712910919359555 \t -0.04568447350163922\n",
            "58     \t [ 9.89374551 -4.55284232]. \t  -2071.544755196085 \t -0.04568447350163922\n",
            "59     \t [1.87578473 0.8398646 ]. \t  -1.1995226063309452 \t -0.04568447350163922\n",
            "60     \t [4.99270134 8.41236709]. \t  -37303.99922392121 \t -0.04568447350163922\n",
            "61     \t [ 1.33361977 -0.75382377]. \t  -0.18901412516178046 \t -0.04568447350163922\n",
            "62     \t [ 7.17256087 -0.9033448 ]. \t  -99.49472631670415 \t -0.04568447350163922\n",
            "63     \t [-5.51046832  6.32629507]. \t  -14681.526646814731 \t -0.04568447350163922\n",
            "64     \t [ 2.17849603 -0.87463367]. \t  -2.230029811969959 \t -0.04568447350163922\n",
            "65     \t [-1.05056018  9.72447146]. \t  -72342.02090492527 \t -0.04568447350163922\n",
            "66     \t [-0.28079526 -1.18962417]. \t  -20.999649416284967 \t -0.04568447350163922\n",
            "67     \t [-1.80628276 -0.28973827]. \t  -15.669990723368109 \t -0.04568447350163922\n",
            "68     \t [-9.52462087 -4.31583999]. \t  -4487.0498646714095 \t -0.04568447350163922\n",
            "69     \t [ 0.66852413 -0.85686487]. \t  -1.38959045539405 \t -0.04568447350163922\n",
            "70     \t [ 0.48527097 -0.77956298]. \t  -1.3312304645847437 \t -0.04568447350163922\n",
            "71     \t [6.22407215 1.35304227]. \t  -40.42502717286698 \t -0.04568447350163922\n",
            "72     \t [4.81479802 0.54934477]. \t  -50.02174628725366 \t -0.04568447350163922\n",
            "73     \t [2.37932605 8.30118572]. \t  -36689.91287944836 \t -0.04568447350163922\n",
            "74     \t [-1.34219568 -9.43450538]. \t  -64346.925071400525 \t -0.04568447350163922\n",
            "75     \t [7.05797547 2.79815697]. \t  -184.66686542810012 \t -0.04568447350163922\n",
            "76     \t [-2.54790021 -0.34899218]. \t  -28.172443201245002 \t -0.04568447350163922\n",
            "77     \t [ 2.96475059 -1.30916547]. \t  -4.2891270744889365 \t -0.04568447350163922\n",
            "78     \t [ 0.6548571  -4.73899305]. \t  -3918.236193193717 \t -0.04568447350163922\n",
            "79     \t [-2.11542918 -0.94077489]. \t  -39.90080283256739 \t -0.04568447350163922\n",
            "80     \t [-2.53249873 -8.90909798]. \t  -52032.73112440396 \t -0.04568447350163922\n",
            "81     \t [ 3.2481393  -1.14003583]. \t  -5.895950675392666 \t -0.04568447350163922\n",
            "82     \t [4.40914038 2.89442519]. \t  -316.4822145066299 \t -0.04568447350163922\n",
            "83     \t [4.45233326 1.97971428]. \t  -34.85135964807685 \t -0.04568447350163922\n",
            "84     \t [0.86796068 0.72127877]. \t  -0.0769644466380927 \t -0.04568447350163922\n",
            "85     \t [-8.89096135  3.26803045]. \t  -1928.0780127427365 \t -0.04568447350163922\n",
            "86     \t [ 3.62652117 -8.97584202]. \t  -49622.52268386854 \t -0.04568447350163922\n",
            "87     \t [-1.94956536  6.35408321]. \t  -13686.721634452053 \t -0.04568447350163922\n",
            "88     \t [-5.28205775  0.69650269]. \t  -117.64650369098928 \t -0.04568447350163922\n",
            "89     \t [4.32847395 3.52225331]. \t  -850.2723996448592 \t -0.04568447350163922\n",
            "90     \t [ 5.95507955 -0.58533502]. \t  -80.09535385780426 \t -0.04568447350163922\n",
            "91     \t [1.27144605 1.61000405]. \t  -30.693378009561993 \t -0.04568447350163922\n",
            "92     \t [ 7.16829238 -3.33687695]. \t  -494.14050371366375 \t -0.04568447350163922\n",
            "93     \t [ 6.52661423 -2.56713644]. \t  -119.08863668164818 \t -0.04568447350163922\n",
            "94     \t [-5.71866905  3.75727751]. \t  -2350.7444695066465 \t -0.04568447350163922\n",
            "95     \t [ 8.07429825 -8.26933835]. \t  -33172.07978023832 \t -0.04568447350163922\n",
            "96     \t [4.65582967 2.22391992]. \t  -68.19250159634333 \t -0.04568447350163922\n",
            "97     \t [ 7.54668457 -2.59018668]. \t  -111.80691749774971 \t -0.04568447350163922\n",
            "98     \t [ 8.40402256 -4.17626348]. \t  -1457.0235509451786 \t -0.04568447350163922\n",
            "99     \t [1.67328896 1.86655206]. \t  -56.52195071756115 \t -0.04568447350163922\n",
            "100    \t [-5.34906297  1.65464129]. \t  -274.660530146399 \t -0.04568447350163922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j6UBhkdtdVe",
        "outputId": "b1fd03e2-4ded-46a5-eed6-aba4efa1918e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_loser_2 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_2 = d2GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.03970985 -8.52192802]. \t  -38238.719775621794 \t -57.443300302345605\n",
            "init   \t [ 7.89863525 -1.27012905]. \t  -91.24969381804144 \t -57.443300302345605\n",
            "init   \t [-7.44644532  1.51715749]. \t  -361.74642651372136 \t -57.443300302345605\n",
            "init   \t [ 6.80941839 -1.29758898]. \t  -57.443300302345605 \t -57.443300302345605\n",
            "init   \t [3.91821121 3.69276194]. \t  -1099.406544633519 \t -57.443300302345605\n",
            "1      \t [-3.35617937 -7.20418892]. \t  -22984.1595726879 \t -57.443300302345605\n",
            "2      \t [-2.87227721  9.90072912]. \t  -79154.24352053565 \t -57.443300302345605\n",
            "3      \t [9.76713458 9.30216525]. \t  -53406.34593257343 \t -57.443300302345605\n",
            "4      \t [-11.24015889  -4.90044022]. \t  -7175.399844674069 \t -57.443300302345605\n",
            "5      \t [-39.91235773 -49.94188246]. \t  -50569187.196252905 \t -57.443300302345605\n",
            "6      \t [-9.85635154  8.2084324 ]. \t  -41943.744160795155 \t -57.443300302345605\n",
            "7      \t [-12.32549179 -11.78582636]. \t  -168536.40755964673 \t -57.443300302345605\n",
            "8      \t [-0.96925372 -0.61007647]. \t  \u001b[92m-9.751086512664951\u001b[0m \t -9.751086512664951\n",
            "9      \t [-30.46313542 -28.91055927]. \t  -5795306.053677734 \t -9.751086512664951\n",
            "10     \t [ -1.04885896 -13.83872226]. \t  -295022.6934158594 \t -9.751086512664951\n",
            "11     \t [ 2.41067748 -3.9724009 ]. \t  -1701.3487582891503 \t -9.751086512664951\n",
            "12     \t [-62.53830551 -61.78584915]. \t  -118507684.39279555 \t -9.751086512664951\n",
            "13     \t [-2.49942915  3.66946082]. \t  -1744.4151199711505 \t -9.751086512664951\n",
            "14     \t [2.97258899 9.37888261]. \t  -59830.263246461305 \t -9.751086512664951\n",
            "15     \t [9.93364434 3.1610923 ]. \t  -281.86986791399926 \t -9.751086512664951\n",
            "16     \t [-17.21463036 -17.32511726]. \t  -763028.2719602239 \t -9.751086512664951\n",
            "17     \t [-4.36703327 -1.91510992]. \t  -302.69388055048165 \t -9.751086512664951\n",
            "18     \t [-12.9633468    0.85095954]. \t  -610.3641209958093 \t -9.751086512664951\n",
            "19     \t [ 2.36215305 -0.03432663]. \t  -12.992739196644045 \t -9.751086512664951\n",
            "20     \t [ 1.6965072  -8.90268805]. \t  -49185.009340831726 \t -9.751086512664951\n",
            "21     \t [ 9.78932172 -3.79351762]. \t  -798.6617852489943 \t -9.751086512664951\n",
            "22     \t [-7.36295185 -9.92860218]. \t  -83724.53731446597 \t -9.751086512664951\n",
            "23     \t [ 4.50123433 -0.03194624]. \t  -52.744120766750456 \t -9.751086512664951\n",
            "24     \t [-6.75397157 -4.29470109]. \t  -3869.5271448611848 \t -9.751086512664951\n",
            "25     \t [-1.19848282 -2.91446412]. \t  -666.344355168578 \t -9.751086512664951\n",
            "26     \t [0.84426983 1.01439373]. \t  \u001b[92m-2.970481683261152\u001b[0m \t -2.970481683261152\n",
            "27     \t [7.4322608 5.624308 ]. \t  -6276.115572689569 \t -2.970481683261152\n",
            "28     \t [-1.14439597  0.35568025]. \t  -8.503959360211185 \t -2.970481683261152\n",
            "29     \t [0.16060788 0.09041752]. \t  \u001b[92m-0.7461994299339245\u001b[0m \t -0.7461994299339245\n",
            "30     \t [-5.92526949  5.27255267]. \t  -7618.58423470874 \t -0.7461994299339245\n",
            "31     \t [-8.91358749 -0.79821554]. \t  -305.8651612090258 \t -0.7461994299339245\n",
            "32     \t [ 0.0508366  -0.03585975]. \t  -0.905570134789214 \t -0.7461994299339245\n",
            "33     \t [-0.62673533 -0.21716182]. \t  -3.6861051043431754 \t -0.7461994299339245\n",
            "34     \t [0.23272385 0.11518293]. \t  \u001b[92m-0.6737410248369122\u001b[0m \t -0.6737410248369122\n",
            "35     \t [ 5.8550793  -4.45603839]. \t  -2316.2292802856286 \t -0.6737410248369122\n",
            "36     \t [-0.49446034  6.17490108]. \t  -11784.361449314862 \t -0.6737410248369122\n",
            "37     \t [6.34963568 0.53972791]. \t  -95.1357160247394 \t -0.6737410248369122\n",
            "38     \t [-1.83422304 -0.20584179]. \t  -15.397671516899365 \t -0.6737410248369122\n",
            "39     \t [0.98387592 1.40484486]. \t  -17.562580023112382 \t -0.6737410248369122\n",
            "40     \t [0.16308031 2.00231073]. \t  -124.1155613071918 \t -0.6737410248369122\n",
            "41     \t [1.70144483 0.29624375]. \t  -5.148913665659565 \t -0.6737410248369122\n",
            "42     \t [1.30859837 0.91852788]. \t  \u001b[92m-0.3821945218466912\u001b[0m \t -0.3821945218466912\n",
            "43     \t [-0.09649101 -0.46175283]. \t  -1.749188122894419 \t -0.3821945218466912\n",
            "44     \t [0.8390783  0.64851188]. \t  \u001b[92m-0.025904257113482494\u001b[0m \t -0.025904257113482494\n",
            "45     \t [8.47542836 0.86464123]. \t  -153.32895620076943 \t -0.025904257113482494\n",
            "46     \t [-9.12987395  3.04892046]. \t  -1639.600308085574 \t -0.025904257113482494\n",
            "47     \t [-9.39681078  9.86068833]. \t  -83228.4691757223 \t -0.025904257113482494\n",
            "48     \t [ 0.90766099 -0.37734296]. \t  -0.784499367355668 \t -0.025904257113482494\n",
            "49     \t [0.89594701 0.78362719]. \t  -0.23153556869723002 \t -0.025904257113482494\n",
            "50     \t [-8.28999214 -2.10020643]. \t  -671.9263016290016 \t -0.025904257113482494\n",
            "51     \t [2.86550724 0.72834129]. \t  -9.992883851980888 \t -0.025904257113482494\n",
            "52     \t [ 9.98374981 -0.30160668]. \t  -272.8589791020866 \t -0.025904257113482494\n",
            "53     \t [9.76705851 8.21340072]. \t  -31403.330122896183 \t -0.025904257113482494\n",
            "54     \t [-0.03419426  0.8540656 ]. \t  -5.527956398992371 \t -0.025904257113482494\n",
            "55     \t [ 1.38265893 -0.18571954]. \t  -3.5979141742767697 \t -0.025904257113482494\n",
            "56     \t [6.41619118 2.27686027]. \t  -60.57164316360735 \t -0.025904257113482494\n",
            "57     \t [-5.35735181  1.19083881]. \t  -174.68431316623503 \t -0.025904257113482494\n",
            "58     \t [-9.42952584 -4.1515827 ]. \t  -3963.336127203735 \t -0.025904257113482494\n",
            "59     \t [2.45550249 0.77554451]. \t  -5.256320277166324 \t -0.025904257113482494\n",
            "60     \t [1.390785   7.89766594]. \t  -30433.296516509945 \t -0.025904257113482494\n",
            "61     \t [ 3.76049883 -1.15330976]. \t  -10.041462799760914 \t -0.025904257113482494\n",
            "62     \t [2.1678619  1.47347977]. \t  -10.820135442011312 \t -0.025904257113482494\n",
            "63     \t [-2.83471316 -2.43994193]. \t  -449.31961196714644 \t -0.025904257113482494\n",
            "64     \t [-2.39191873  0.372648  ]. \t  -25.75919399676671 \t -0.025904257113482494\n",
            "65     \t [-0.54379441  6.45592031]. \t  -14081.343119217587 \t -0.025904257113482494\n",
            "66     \t [-0.1896273   4.22494727]. \t  -2577.597424934544 \t -0.025904257113482494\n",
            "67     \t [ 0.32557563 -0.61495873]. \t  -0.8259787340296165 \t -0.025904257113482494\n",
            "68     \t [1.75681562 0.88938156]. \t  -0.6338915115745557 \t -0.025904257113482494\n",
            "69     \t [-5.594978  -5.0845089]. \t  -6609.945274306271 \t -0.025904257113482494\n",
            "70     \t [ 1.33605182 -1.02827291]. \t  -1.3254867767738763 \t -0.025904257113482494\n",
            "71     \t [ 0.82300292 -0.73197445]. \t  -0.15490231521035006 \t -0.025904257113482494\n",
            "72     \t [ 7.70530973 -4.0913636 ]. \t  -1373.4771047270685 \t -0.025904257113482494\n",
            "73     \t [ 1.25910626 -0.77446087]. \t  -0.07422297869913888 \t -0.025904257113482494\n",
            "74     \t [-0.31603546 -0.55825071]. \t  -3.4966053616034607 \t -0.025904257113482494\n",
            "75     \t [ 2.22809454 -0.84007332]. \t  -2.8420446459060957 \t -0.025904257113482494\n",
            "76     \t [ 3.23658576 -0.45542641]. \t  -20.92696738143664 \t -0.025904257113482494\n",
            "77     \t [-5.7912885   4.92505611]. \t  -5943.893075288378 \t -0.025904257113482494\n",
            "78     \t [-5.41625765  2.95579491]. \t  -1089.0455306742315 \t -0.025904257113482494\n",
            "79     \t [0.82424761 0.42705916]. \t  -0.4531483648225632 \t -0.025904257113482494\n",
            "80     \t [ 1.38126724 -0.85602978]. \t  -0.15957995887680337 \t -0.025904257113482494\n",
            "81     \t [-5.6520449  -0.54908606]. \t  -122.50064975718254 \t -0.025904257113482494\n",
            "82     \t [ 4.7673075  -1.44449855]. \t  -14.898647045675359 \t -0.025904257113482494\n",
            "83     \t [-4.09708912  0.73311384]. \t  -79.47950472911278 \t -0.025904257113482494\n",
            "84     \t [ 7.96364668 -3.79670713]. \t  -919.2992796888569 \t -0.025904257113482494\n",
            "85     \t [-0.42287358 -3.42135285]. \t  -1138.1612419423348 \t -0.025904257113482494\n",
            "86     \t [ 7.89604122 -2.16448552]. \t  -51.90046479188232 \t -0.025904257113482494\n",
            "87     \t [9.22419433 1.02904472]. \t  -168.63717468394677 \t -0.025904257113482494\n",
            "88     \t [-1.6844257  -7.01140427]. \t  -20008.808295517654 \t -0.025904257113482494\n",
            "89     \t [1.3270714  0.74694865]. \t  -0.19619236803955004 \t -0.025904257113482494\n",
            "90     \t [8.17555757 7.6307228 ]. \t  -23500.736860312965 \t -0.025904257113482494\n",
            "91     \t [-4.78552334  8.83038623]. \t  -51706.21634292386 \t -0.025904257113482494\n",
            "92     \t [-1.39136264 -7.11112906]. \t  -21029.567136461766 \t -0.025904257113482494\n",
            "93     \t [-1.75019318  1.96533379]. \t  -187.12493048878747 \t -0.025904257113482494\n",
            "94     \t [-7.84633632  8.56911237]. \t  -47945.970974422846 \t -0.025904257113482494\n",
            "95     \t [ 6.33238924 -5.55543991]. \t  -6165.2998444628665 \t -0.025904257113482494\n",
            "96     \t [5.71707915 4.2191094 ]. \t  -1808.44012269075 \t -0.025904257113482494\n",
            "97     \t [7.18033546 2.18284709]. \t  -49.23503654820095 \t -0.025904257113482494\n",
            "98     \t [ 5.28574755 -2.76259469]. \t  -217.4930416817421 \t -0.025904257113482494\n",
            "99     \t [6.29326671 1.34097759]. \t  -42.56440207309509 \t -0.025904257113482494\n",
            "100    \t [-3.29500331  4.82568198]. \t  -4992.364448817138 \t -0.025904257113482494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWnL1rwztdYG",
        "outputId": "512a33ed-db3d-47e4-e5cd-a369d9134ecd"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_loser_3 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_3 = d2GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.03151214 -7.81497449]. \t  -27448.6338140421 \t -1.7863168283775226\n",
            "init   \t [-0.27757442 -0.00337645]. \t  -1.7863168283775226 \t -1.7863168283775226\n",
            "init   \t [ 3.14219998 -5.28230578]. \t  -5551.437171353593 \t -1.7863168283775226\n",
            "init   \t [ 2.25589781 -7.60695193]. \t  -25754.9725373955 \t -1.7863168283775226\n",
            "init   \t [ 4.24404593 -2.54878916]. \t  -163.60006264419064 \t -1.7863168283775226\n",
            "1      \t [-5.61872775  6.96371276]. \t  -21099.51033227109 \t -1.7863168283775226\n",
            "2      \t [-9.6736302  -6.79831075]. \t  -20965.88636571433 \t -1.7863168283775226\n",
            "3      \t [5.95256263 7.26277879]. \t  -19842.284445028028 \t -1.7863168283775226\n",
            "4      \t [-9.5655754   1.04731115]. \t  -388.1935005848898 \t -1.7863168283775226\n",
            "5      \t [ -3.39252592 -11.90986646]. \t  -164851.86165944263 \t -1.7863168283775226\n",
            "6      \t [9.04845932 0.80638493]. \t  -184.83903204189994 \t -1.7863168283775226\n",
            "7      \t [-10.7263977  -13.72893042]. \t  -300749.9438566742 \t -1.7863168283775226\n",
            "8      \t [-15.75961716  -0.86138634]. \t  -875.5675604993338 \t -1.7863168283775226\n",
            "9      \t [-3.69056568 -3.79143409]. \t  -2126.7748637629666 \t -1.7863168283775226\n",
            "10     \t [0.18037714 9.89102847]. \t  -76429.06005447076 \t -1.7863168283775226\n",
            "11     \t [2.0717869  2.78702019]. \t  -363.6629536782101 \t -1.7863168283775226\n",
            "12     \t [-4.33305876  1.48107275]. \t  -180.52570116503853 \t -1.7863168283775226\n",
            "13     \t [ 9.97923216 -7.68882125]. \t  -23519.65817380143 \t -1.7863168283775226\n",
            "14     \t [ 7.90047976 -3.02246877]. \t  -262.69685628133414 \t -1.7863168283775226\n",
            "15     \t [-1.14314458  0.24951479]. \t  -7.806991674529126 \t -1.7863168283775226\n",
            "16     \t [ 0.69227414 -0.99806236]. \t  -3.474605827688598 \t -1.7863168283775226\n",
            "17     \t [-12.5921011    6.52940568]. \t  -19337.269540221525 \t -1.7863168283775226\n",
            "18     \t [9.8019244  9.90399252]. \t  -69549.65804729251 \t -1.7863168283775226\n",
            "19     \t [9.80185147 4.3301693 ]. \t  -1611.9285965474517 \t -1.7863168283775226\n",
            "20     \t [-1.34479405  4.14566972]. \t  -2557.0418088519914 \t -1.7863168283775226\n",
            "21     \t [5.32106081 1.0509611 ]. \t  -38.040932364880355 \t -1.7863168283775226\n",
            "22     \t [-7.68133572 -1.96381899]. \t  -549.3472080301431 \t -1.7863168283775226\n",
            "23     \t [ 0.69875183 -1.62871832]. \t  -42.53402639558832 \t -1.7863168283775226\n",
            "24     \t [ 1.48186512 -0.55010331]. \t  \u001b[92m-1.7691817080656447\u001b[0m \t -1.7691817080656447\n",
            "25     \t [-0.16112697 -0.43879   ]. \t  -1.944885381909589 \t -1.7691817080656447\n",
            "26     \t [-9.63444314  9.87604258]. \t  -83922.90247770057 \t -1.7691817080656447\n",
            "27     \t [ 0.5963662  -0.47838431]. \t  \u001b[92m-0.20137515636604097\u001b[0m \t -0.20137515636604097\n",
            "28     \t [-2.42502644 -7.25623395]. \t  -23223.619547857204 \t -0.20137515636604097\n",
            "29     \t [-6.28498014  2.79376569]. \t  -1011.873444488562 \t -0.20137515636604097\n",
            "30     \t [ 0.59503592 -0.66592894]. \t  -0.3343917077048179 \t -0.20137515636604097\n",
            "31     \t [0.83198626 0.2603903 ]. \t  -0.9981189288438952 \t -0.20137515636604097\n",
            "32     \t [ 1.05030748 -0.71487019]. \t  \u001b[92m-0.004124564057247064\u001b[0m \t -0.004124564057247064\n",
            "33     \t [-2.0920441  -0.39531208]. \t  -21.124817727872376 \t -0.004124564057247064\n",
            "34     \t [6.21132536 2.77450206]. \t  -195.8642463605286 \t -0.004124564057247064\n",
            "35     \t [ 5.96975159 -0.75963327]. \t  -71.07971261515861 \t -0.004124564057247064\n",
            "36     \t [0.17617301 0.31773115]. \t  -0.6800152980414986 \t -0.004124564057247064\n",
            "37     \t [1.82114459 5.62065139]. \t  -7531.337902276627 \t -0.004124564057247064\n",
            "38     \t [ 3.10151141 -0.22910713]. \t  -22.374749363394404 \t -0.004124564057247064\n",
            "39     \t [3.95667341 0.59968589]. \t  -29.70378586018728 \t -0.004124564057247064\n",
            "40     \t [-9.65660582  4.08851787]. \t  -3826.8116994960005 \t -0.004124564057247064\n",
            "41     \t [ 2.04512649 -0.70979254]. \t  -3.2451666129390726 \t -0.004124564057247064\n",
            "42     \t [2.43983052 6.74300894]. \t  -15665.337155333325 \t -0.004124564057247064\n",
            "43     \t [-0.57495815 -0.81106748]. \t  -9.629373911630637 \t -0.004124564057247064\n",
            "44     \t [-7.28936904 -9.8136836 ]. \t  -79993.64368395755 \t -0.004124564057247064\n",
            "45     \t [ 1.85954583 -0.92218021]. \t  -0.789198762757316 \t -0.004124564057247064\n",
            "46     \t [ 1.66295637 -0.82982573]. \t  -0.6028000162250124 \t -0.004124564057247064\n",
            "47     \t [ 9.94701812 -1.14531616]. \t  -187.31702100437585 \t -0.004124564057247064\n",
            "48     \t [ 1.3802462  -0.93461756]. \t  -0.4136331421561933 \t -0.004124564057247064\n",
            "49     \t [-9.40807885  0.81353567]. \t  -338.6694263984088 \t -0.004124564057247064\n",
            "50     \t [-6.99203415  8.21392252]. \t  -40351.60044043994 \t -0.004124564057247064\n",
            "51     \t [ 0.90859854 -0.12564998]. \t  -1.5466918835682992 \t -0.004124564057247064\n",
            "52     \t [-4.79538118 -0.90116801]. \t  -116.00868270749399 \t -0.004124564057247064\n",
            "53     \t [ 0.13867182 -0.29013308]. \t  -0.7436483448271808 \t -0.004124564057247064\n",
            "54     \t [6.7520481  4.14200034]. \t  -1552.2247321637162 \t -0.004124564057247064\n",
            "55     \t [-1.23181738 -0.109923  ]. \t  -8.135998004064485 \t -0.004124564057247064\n",
            "56     \t [6.7402457  0.60696178]. \t  -105.03301593549293 \t -0.004124564057247064\n",
            "57     \t [5.73119113 1.7566636 ]. \t  -22.772325636036644 \t -0.004124564057247064\n",
            "58     \t [4.6067698  2.27490683]. \t  -78.9874132939235 \t -0.004124564057247064\n",
            "59     \t [-5.50862208  0.93055068]. \t  -147.21100865555368 \t -0.004124564057247064\n",
            "60     \t [-4.60172925  0.73352925]. \t  -95.85554907913139 \t -0.004124564057247064\n",
            "61     \t [ 5.03056314 -0.10477405]. \t  -66.41774620184364 \t -0.004124564057247064\n",
            "62     \t [-1.97006732 -0.32444149]. \t  -18.331261772302526 \t -0.004124564057247064\n",
            "63     \t [-8.44339496  0.08126659]. \t  -232.20599304231348 \t -0.004124564057247064\n",
            "64     \t [ 3.03252856 -4.64622883]. \t  -3226.9486716463393 \t -0.004124564057247064\n",
            "65     \t [-2.48531623  0.6862654 ]. \t  -35.63933098076464 \t -0.004124564057247064\n",
            "66     \t [ 0.56816815 -0.02748304]. \t  -0.8286802273916385 \t -0.004124564057247064\n",
            "67     \t [ 2.57030794 -2.14737887]. \t  -90.968441299027 \t -0.004124564057247064\n",
            "68     \t [ 0.97628404 -6.80955698]. \t  -16841.20832557118 \t -0.004124564057247064\n",
            "69     \t [4.73898245 3.74417699]. \t  -1099.64257608463 \t -0.004124564057247064\n",
            "70     \t [4.26813386 1.630557  ]. \t  -12.88275330151906 \t -0.004124564057247064\n",
            "71     \t [-1.62358223 -1.94224028]. \t  -174.9941159860788 \t -0.004124564057247064\n",
            "72     \t [4.92267066 1.44979391]. \t  -16.420881447194066 \t -0.004124564057247064\n",
            "73     \t [5.89575926 2.11179948]. \t  -42.25319438508458 \t -0.004124564057247064\n",
            "74     \t [2.70013162 0.51728886]. \t  -12.264517209071222 \t -0.004124564057247064\n",
            "75     \t [ 6.79081091 -9.25907725]. \t  -54266.09092018643 \t -0.004124564057247064\n",
            "76     \t [-0.68860913  0.79039872]. \t  -10.363626937930775 \t -0.004124564057247064\n",
            "77     \t [-5.53208922 -3.06693246]. \t  -1227.9521036431554 \t -0.004124564057247064\n",
            "78     \t [-0.7973397 -0.612444 ]. \t  -8.020035398703488 \t -0.004124564057247064\n",
            "79     \t [5.50025847 2.631525  ]. \t  -159.68360540471554 \t -0.004124564057247064\n",
            "80     \t [ 7.7878485  -0.83363241]. \t  -127.94273632673325 \t -0.004124564057247064\n",
            "81     \t [0.33716028 0.69442895]. \t  -1.226374231584093 \t -0.004124564057247064\n",
            "82     \t [3.33980119 3.6619608 ]. \t  -1108.1060419131188 \t -0.004124564057247064\n",
            "83     \t [4.95962777 1.75251714]. \t  -18.47765320683075 \t -0.004124564057247064\n",
            "84     \t [1.25084981 0.03965129]. \t  -3.176462954511787 \t -0.004124564057247064\n",
            "85     \t [-8.65102928  9.910253  ]. \t  -84206.5061584383 \t -0.004124564057247064\n",
            "86     \t [ 4.57737539 -0.97959551]. \t  -26.929250813281612 \t -0.004124564057247064\n",
            "87     \t [ 0.65459451 -1.51311626]. \t  -30.92187508731874 \t -0.004124564057247064\n",
            "88     \t [-9.65611648  3.01445109]. \t  -1662.5647470123702 \t -0.004124564057247064\n",
            "89     \t [-0.21351966 -3.97757932]. \t  -2031.0558726958338 \t -0.004124564057247064\n",
            "90     \t [1.93608887 0.9600842 ]. \t  -0.8933991243481535 \t -0.004124564057247064\n",
            "91     \t [1.07126881 0.95117134]. \t  -1.094913543298511 \t -0.004124564057247064\n",
            "92     \t [ 2.78182392 -1.27265521]. \t  -3.593469907187097 \t -0.004124564057247064\n",
            "93     \t [ 3.47587984 -4.77822747]. \t  -3565.6219455686582 \t -0.004124564057247064\n",
            "94     \t [-3.33938631 -3.56105429]. \t  -1666.394147176159 \t -0.004124564057247064\n",
            "95     \t [3.73924683 5.81216801]. \t  -8154.342748064939 \t -0.004124564057247064\n",
            "96     \t [ 3.02335012 -1.1616632 ]. \t  -4.304451937194685 \t -0.004124564057247064\n",
            "97     \t [-6.11691153  5.8954905 ]. \t  -11490.603962595726 \t -0.004124564057247064\n",
            "98     \t [-8.88127832 -9.83888317]. \t  -82100.84304076748 \t -0.004124564057247064\n",
            "99     \t [1.39639806 5.72636826]. \t  -8239.890787569202 \t -0.004124564057247064\n",
            "100    \t [1.563685   0.77653893]. \t  -0.5735815402211539 \t -0.004124564057247064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zciLjgNltda2",
        "outputId": "42915841-cc02-40d0-abbd-6c3cfc36ac71"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_loser_4 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_4 = d2GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [6.78722095 6.4258993 ]. \t  -11523.905614569041 \t -765.9755148718126\n",
            "init   \t [2.8441433  3.34445237]. \t  -765.9755148718126 \t -765.9755148718126\n",
            "init   \t [-9.21720189 -8.68655223]. \t  -51387.35679532384 \t -765.9755148718126\n",
            "init   \t [-4.4699872  -3.67278993]. \t  -2007.9696367442386 \t -765.9755148718126\n",
            "init   \t [-6.12815176 -6.05814236]. \t  -12700.957148627067 \t -765.9755148718126\n",
            "1      \t [ 5.15020416 -5.71066603]. \t  -7234.8050437839365 \t -765.9755148718126\n",
            "2      \t [-3.97305893  9.21389704]. \t  -60413.181901672484 \t -765.9755148718126\n",
            "3      \t [-8.51899843  2.33131307]. \t  -842.4810529068852 \t -765.9755148718126\n",
            "4      \t [  1.17903038 -11.46989784]. \t  -137223.15007446375 \t -765.9755148718126\n",
            "5      \t [-69.72587698 -67.86198471]. \t  -172250110.28562367 \t -765.9755148718126\n",
            "6      \t [-13.74774616  -1.80552776]. \t  -1039.0478168891748 \t -765.9755148718126\n",
            "7      \t [-2.83803863  2.67646344]. \t  \u001b[92m-604.0016793287856\u001b[0m \t -604.0016793287856\n",
            "8      \t [-15.77348496 -11.71707459]. \t  -168891.2632674945 \t -604.0016793287856\n",
            "9      \t [ 8.97539946 -0.27131946]. \t  \u001b[92m-219.4802014122361\u001b[0m \t -219.4802014122361\n",
            "10     \t [ 0.91647747 -1.68350789]. \t  \u001b[92m-45.16846668192221\u001b[0m \t -45.16846668192221\n",
            "11     \t [  7.62383649 -10.88715893]. \t  -105326.20289873917 \t -45.16846668192221\n",
            "12     \t [ -9.00673327 -15.81401879]. \t  -518614.6541383693 \t -45.16846668192221\n",
            "13     \t [-10.15357019   7.64051398]. \t  -32335.925643605668 \t -45.16846668192221\n",
            "14     \t [-35.19286831 -37.22705085]. \t  -15758679.517484464 \t -45.16846668192221\n",
            "15     \t [1.85662458 8.42140482]. \t  -39191.49075064337 \t -45.16846668192221\n",
            "16     \t [ 0.34392676 -5.94518556]. \t  -9897.700686983879 \t -45.16846668192221\n",
            "17     \t [ -4.48794825 -11.01884562]. \t  -122362.37126124551 \t -45.16846668192221\n",
            "18     \t [-9.18108282 -1.919588  ]. \t  -651.5070438095488 \t -45.16846668192221\n",
            "19     \t [ 4.7111189  -0.56457836]. \t  -46.961186884792355 \t -45.16846668192221\n",
            "20     \t [-1.33383904 -0.79170411]. \t  \u001b[92m-18.8363908155254\u001b[0m \t -18.8363908155254\n",
            "21     \t [ 8.8265108  -4.11307361]. \t  -1312.078224120276 \t -18.8363908155254\n",
            "22     \t [6.13145361 1.66761882]. \t  -26.98058724585679 \t -18.8363908155254\n",
            "23     \t [9.53651703 3.34482853]. \t  -402.5642253885253 \t -18.8363908155254\n",
            "24     \t [0.78996349 0.5646075 ]. \t  \u001b[92m-0.090567001291792\u001b[0m \t -0.090567001291792\n",
            "25     \t [8.57973321 9.98456116]. \t  -72869.16695759939 \t -0.090567001291792\n",
            "26     \t [-5.55999398  0.15955791]. \t  -105.99817546653372 \t -0.090567001291792\n",
            "27     \t [-6.62352408  4.9130127 ]. \t  -6085.899140443135 \t -0.090567001291792\n",
            "28     \t [0.10783759 0.16183711]. \t  -0.8021042933517469 \t -0.090567001291792\n",
            "29     \t [0.28729995 0.16287836]. \t  -0.6176792684089881 \t -0.090567001291792\n",
            "30     \t [-1.49093561 -2.22100886]. \t  -264.1538630160063 \t -0.090567001291792\n",
            "31     \t [1.34693447 0.09164139]. \t  -3.658898676206393 \t -0.090567001291792\n",
            "32     \t [-1.31316384 -0.21393406]. \t  -9.297087794163076 \t -0.090567001291792\n",
            "33     \t [ 2.83658525 -2.40746082]. \t  -156.67834561625094 \t -0.090567001291792\n",
            "34     \t [0.65872821 0.45157139]. \t  -0.2423627962740756 \t -0.090567001291792\n",
            "35     \t [-0.43199815 -0.42367312]. \t  -3.301967937980036 \t -0.090567001291792\n",
            "36     \t [-1.46996565  5.29625253]. \t  -6634.836575577645 \t -0.090567001291792\n",
            "37     \t [6.681024 0.464195]. \t  -110.40078377600108 \t -0.090567001291792\n",
            "38     \t [1.56415682 0.63942284]. \t  -1.4325993920996853 \t -0.090567001291792\n",
            "39     \t [-2.68420732 -0.58160048]. \t  -36.16233598285577 \t -0.090567001291792\n",
            "40     \t [6.09202015 2.78638739]. \t  -204.00067974463605 \t -0.090567001291792\n",
            "41     \t [5.42932659 1.15853574]. \t  -34.688067098823886 \t -0.090567001291792\n",
            "42     \t [0.7881499  0.65508796]. \t  \u001b[92m-0.05471705757883294\u001b[0m \t -0.05471705757883294\n",
            "43     \t [-0.24532764  0.94987878]. \t  -9.95475067659605 \t -0.05471705757883294\n",
            "44     \t [ 5.57734152 -1.78229998]. \t  -22.155925992111435 \t -0.05471705757883294\n",
            "45     \t [ 6.46670087 -1.95032191]. \t  -32.487714493105415 \t -0.05471705757883294\n",
            "46     \t [ 9.60576125 -7.20033915]. \t  -17777.659722130218 \t -0.05471705757883294\n",
            "47     \t [1.91299589 0.49054712]. \t  -4.933222636279261 \t -0.05471705757883294\n",
            "48     \t [1.28504352 0.77939902]. \t  -0.09108283136151642 \t -0.05471705757883294\n",
            "49     \t [3.56905532 0.54776267]. \t  -24.2295804521989 \t -0.05471705757883294\n",
            "50     \t [-9.91387411 -4.76925574]. \t  -6258.645914932864 \t -0.05471705757883294\n",
            "51     \t [8.81606978 0.28697194]. \t  -210.7831447781824 \t -0.05471705757883294\n",
            "52     \t [0.02389677 1.98620116]. \t  -124.7036246345092 \t -0.05471705757883294\n",
            "53     \t [ 1.27648713 -0.63292458]. \t  -0.5282654422747914 \t -0.05471705757883294\n",
            "54     \t [ 6.49196718 -7.21609816]. \t  -19102.05751112033 \t -0.05471705757883294\n",
            "55     \t [ 1.72456471 -0.23824666]. \t  -5.715905177417754 \t -0.05471705757883294\n",
            "56     \t [ 0.75567059 -0.85408622]. \t  -1.048834726082826 \t -0.05471705757883294\n",
            "57     \t [0.01242725 5.36685212]. \t  -6635.061400079478 \t -0.05471705757883294\n",
            "58     \t [-5.54828522 -2.83642507]. \t  -979.3639882971961 \t -0.05471705757883294\n",
            "59     \t [-9.31011341  2.2251477 ]. \t  -844.5524321080262 \t -0.05471705757883294\n",
            "60     \t [ 0.78374146 -0.25038619]. \t  -0.9136302895674375 \t -0.05471705757883294\n",
            "61     \t [ 7.7589623  -0.11838766]. \t  -165.21816050757536 \t -0.05471705757883294\n",
            "62     \t [0.24305877 0.67589176]. \t  -1.4723702896995015 \t -0.05471705757883294\n",
            "63     \t [-3.36428627 -4.54178341]. \t  -4000.9142702923155 \t -0.05471705757883294\n",
            "64     \t [-9.52691162  7.81135867]. \t  -34727.70400980378 \t -0.05471705757883294\n",
            "65     \t [0.70007031 2.81598637]. \t  -459.71012085825157 \t -0.05471705757883294\n",
            "66     \t [-3.66699095 -1.76817563]. \t  -218.58888550221934 \t -0.05471705757883294\n",
            "67     \t [ 3.74941644 -7.90499041]. \t  -29400.175506072163 \t -0.05471705757883294\n",
            "68     \t [0.56436426 4.51406788]. \t  -3230.541957069111 \t -0.05471705757883294\n",
            "69     \t [-9.71312871 -4.81043275]. \t  -6385.34998145005 \t -0.05471705757883294\n",
            "70     \t [0.29754546 0.39250371]. \t  -0.4936659581205672 \t -0.05471705757883294\n",
            "71     \t [-2.95393482  4.31677071]. \t  -3251.4055782804458 \t -0.05471705757883294\n",
            "72     \t [-4.14324067  0.79371649]. \t  -84.84233316701318 \t -0.05471705757883294\n",
            "73     \t [-1.43321797 -0.44400218]. \t  -12.60001720740824 \t -0.05471705757883294\n",
            "74     \t [-4.11355789 -8.05623696]. \t  -35894.995196675634 \t -0.05471705757883294\n",
            "75     \t [ 7.85982694 -4.90624431]. \t  -3292.4393334292486 \t -0.05471705757883294\n",
            "76     \t [-2.42025704  1.13470729]. \t  -61.605767203668044 \t -0.05471705757883294\n",
            "77     \t [7.03050385 8.26918204]. \t  -33695.19910931697 \t -0.05471705757883294\n",
            "78     \t [8.53890693 8.20984019]. \t  -31942.053167629954 \t -0.05471705757883294\n",
            "79     \t [-0.62588663 -0.08256257]. \t  -3.4614784443388897 \t -0.05471705757883294\n",
            "80     \t [-8.01751757 -8.31904695]. \t  -42965.158021264026 \t -0.05471705757883294\n",
            "81     \t [3.76477698 3.76714555]. \t  -1219.7352679061078 \t -0.05471705757883294\n",
            "82     \t [1.78056241 1.11117737]. \t  -1.558355654716626 \t -0.05471705757883294\n",
            "83     \t [ 0.77818021 -6.51871063]. \t  -14182.359299973818 \t -0.05471705757883294\n",
            "84     \t [7.23246528 9.31484792]. \t  -55350.43524845048 \t -0.05471705757883294\n",
            "85     \t [8.73485241 4.70702164]. \t  -2591.309797284475 \t -0.05471705757883294\n",
            "86     \t [ 3.03157594 -9.76296096]. \t  -70391.07523319387 \t -0.05471705757883294\n",
            "87     \t [9.6329446  2.37617339]. \t  -80.03531643505009 \t -0.05471705757883294\n",
            "88     \t [ 9.77528004 -4.79792497]. \t  -2707.289737859233 \t -0.05471705757883294\n",
            "89     \t [ 9.69860606 -1.93611298]. \t  -85.35929646652941 \t -0.05471705757883294\n",
            "90     \t [ 6.15494206 -2.46099085]. \t  -97.56919143620553 \t -0.05471705757883294\n",
            "91     \t [ 6.18894526 -1.26474752]. \t  -44.80263421598132 \t -0.05471705757883294\n",
            "92     \t [ 5.37857082 -2.58028248]. \t  -145.16840990953483 \t -0.05471705757883294\n",
            "93     \t [5.45426873 5.76928074]. \t  -7489.910291650112 \t -0.05471705757883294\n",
            "94     \t [ 8.79753823 -2.35236101]. \t  -71.1043735607819 \t -0.05471705757883294\n",
            "95     \t [-1.29255613 -2.49606527]. \t  -383.55902441051194 \t -0.05471705757883294\n",
            "96     \t [ 6.62625188 -0.43495995]. \t  -109.72650818812865 \t -0.05471705757883294\n",
            "97     \t [3.86956944 0.02692883]. \t  -38.15911986727106 \t -0.05471705757883294\n",
            "98     \t [-5.98245003  7.30174925]. \t  -25412.370855398018 \t -0.05471705757883294\n",
            "99     \t [-1.28302942  5.55932414]. \t  -7967.22056861976 \t -0.05471705757883294\n",
            "100    \t [ 6.29792024 -4.77555238]. \t  -3119.230411100237 \t -0.05471705757883294\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH76hlE2tddm",
        "outputId": "147f3a85-0d53-4368-c99b-b22fe4e46420"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_loser_5 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_5 = d2GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.14706549 0.83521255]. \t  -0.14473002518929054 \t -0.14473002518929054\n",
            "init   \t [ 1.20847567 -2.35903813]. \t  -196.92158721117866 \t -0.14473002518929054\n",
            "init   \t [-7.61811642  6.91998231]. \t  -21453.432632023236 \t -0.14473002518929054\n",
            "init   \t [4.79483225 8.64715129]. \t  -41920.46290057033 \t -0.14473002518929054\n",
            "init   \t [7.18656013 4.16209252]. \t  -1546.3183076435794 \t -0.14473002518929054\n",
            "1      \t [-1.28443039  2.56483835]. \t  -422.31640133461167 \t -0.14473002518929054\n",
            "2      \t [2.20317475 0.72550179]. \t  -4.094787565175626 \t -0.14473002518929054\n",
            "3      \t [ 9.14614046 -9.26496158]. \t  -52900.23738128742 \t -0.14473002518929054\n",
            "4      \t [-8.79367401 -8.68508685]. \t  -51075.444313405846 \t -0.14473002518929054\n",
            "5      \t [-6.8891595  -1.20388848]. \t  -253.8430259429188 \t -0.14473002518929054\n",
            "6      \t [  1.74505468 -11.71884734]. \t  -148968.7092954128 \t -0.14473002518929054\n",
            "7      \t [ 8.14263038 -2.29282184]. \t  -62.264562994729815 \t -0.14473002518929054\n",
            "8      \t [-13.13734325  -0.64805355]. \t  -590.5937492821065 \t -0.14473002518929054\n",
            "9      \t [-3.20221526 -5.27215521]. \t  -6930.999622999035 \t -0.14473002518929054\n",
            "10     \t [-1.84291699  8.9668659 ]. \t  -52919.61385971303 \t -0.14473002518929054\n",
            "11     \t [ 4.10901338 -6.66589508]. \t  -14377.944182595398 \t -0.14473002518929054\n",
            "12     \t [-9.81401238  1.90982282]. \t  -702.3689615573313 \t -0.14473002518929054\n",
            "13     \t [ 5.23864402 -0.33234002]. \t  -68.32161774936593 \t -0.14473002518929054\n",
            "14     \t [-17.16060535  -9.38189827]. \t  -74982.77872573878 \t -0.14473002518929054\n",
            "15     \t [ -3.80227615 -10.64504562]. \t  -106224.80965589442 \t -0.14473002518929054\n",
            "16     \t [9.8423579  8.93336417]. \t  -44938.875045174995 \t -0.14473002518929054\n",
            "17     \t [9.98706881 0.45188222]. \t  -264.2693865262832 \t -0.14473002518929054\n",
            "18     \t [2.62424849 3.58085415]. \t  -1062.5512154154485 \t -0.14473002518929054\n",
            "19     \t [-4.82790206  1.44681468]. \t  -196.4849690295227 \t -0.14473002518929054\n",
            "20     \t [2.84013853 0.29764507]. \t  -17.568750935389215 \t -0.14473002518929054\n",
            "21     \t [-3.13659983 -0.8973509 ]. \t  -62.18094052921577 \t -0.14473002518929054\n",
            "22     \t [ 9.71329142 -4.8427496 ]. \t  -2842.285613658635 \t -0.14473002518929054\n",
            "23     \t [-12.89652347  -5.66127577]. \t  -12050.069324518998 \t -0.14473002518929054\n",
            "24     \t [1.55391271 0.36776819]. \t  -3.6010802944409313 \t -0.14473002518929054\n",
            "25     \t [ 4.45406992 -2.4730354 ]. \t  -132.91702384913037 \t -0.14473002518929054\n",
            "26     \t [1.29197572 0.60157165]. \t  -0.7309496076958966 \t -0.14473002518929054\n",
            "27     \t [1.55685076 0.70052217]. \t  -0.9722257568793669 \t -0.14473002518929054\n",
            "28     \t [-8.49169949 -3.47181353]. \t  -2215.4399590369885 \t -0.14473002518929054\n",
            "29     \t [-1.43298827 -1.38114447]. \t  -61.00471510119176 \t -0.14473002518929054\n",
            "30     \t [0.6911461  0.41591498]. \t  -0.333683065651094 \t -0.14473002518929054\n",
            "31     \t [-3.97942325  4.13866992]. \t  -2948.869911722159 \t -0.14473002518929054\n",
            "32     \t [6.75277291 1.05588498]. \t  -74.00921392624667 \t -0.14473002518929054\n",
            "33     \t [ 7.00076435 -0.89459341]. \t  -94.33283657327536 \t -0.14473002518929054\n",
            "34     \t [ 0.45730514 -5.35960526]. \t  -6496.797064097895 \t -0.14473002518929054\n",
            "35     \t [0.44948625 0.34367089]. \t  -0.39403092200703094 \t -0.14473002518929054\n",
            "36     \t [-7.08024673  2.05958194]. \t  -549.7667216159996 \t -0.14473002518929054\n",
            "37     \t [0.03776338 0.52974466]. \t  -1.473994249019223 \t -0.14473002518929054\n",
            "38     \t [4.41059298 1.23477726]. \t  -15.338110603825989 \t -0.14473002518929054\n",
            "39     \t [-4.04270974 -8.11068269]. \t  -36805.06171120061 \t -0.14473002518929054\n",
            "40     \t [-0.37664613  5.33264668]. \t  -6557.223340876488 \t -0.14473002518929054\n",
            "41     \t [0.259762   0.73613382]. \t  -1.9059834191978755 \t -0.14473002518929054\n",
            "42     \t [1.21505183 0.67809029]. \t  -0.22081563759203085 \t -0.14473002518929054\n",
            "43     \t [-0.5537127   0.08392089]. \t  -3.0588125779110795 \t -0.14473002518929054\n",
            "44     \t [-7.13427636  8.61588551]. \t  -48489.65974397891 \t -0.14473002518929054\n",
            "45     \t [9.99091957 3.82021444]. \t  -817.8983239354657 \t -0.14473002518929054\n",
            "46     \t [-4.41761244 -1.65350377]. \t  -224.80709949856373 \t -0.14473002518929054\n",
            "47     \t [0.30512694 0.58399476]. \t  -0.7670655834092532 \t -0.14473002518929054\n",
            "48     \t [-5.00223563  4.62773213]. \t  -4612.218532478034 \t -0.14473002518929054\n",
            "49     \t [-0.5993517  5.5202509]. \t  -7578.302277915097 \t -0.14473002518929054\n",
            "50     \t [5.50806031 6.92633429]. \t  -16379.165169186766 \t -0.14473002518929054\n",
            "51     \t [-2.21384766  0.20785693]. \t  -20.91117833670712 \t -0.14473002518929054\n",
            "52     \t [ 0.99725083 -0.43475213]. \t  -0.7669040832315646 \t -0.14473002518929054\n",
            "53     \t [-0.00500506  0.02017868]. \t  -1.0101028960365688 \t -0.14473002518929054\n",
            "54     \t [5.09791047 1.46819062]. \t  -18.03079954411408 \t -0.14473002518929054\n",
            "55     \t [ 6.40466661 -3.12495411]. \t  -373.79468694818684 \t -0.14473002518929054\n",
            "56     \t [ 8.70979338 -1.79778323]. \t  -69.52764849066111 \t -0.14473002518929054\n",
            "57     \t [-9.12550271 -8.71878829]. \t  -52047.64348572759 \t -0.14473002518929054\n",
            "58     \t [-7.09555014 -1.87065862]. \t  -462.83524980013703 \t -0.14473002518929054\n",
            "59     \t [ 0.75868219 -0.73814022]. \t  -0.2773824700882223 \t -0.14473002518929054\n",
            "60     \t [ 7.55197615 -4.15741169]. \t  -1502.675032654747 \t -0.14473002518929054\n",
            "61     \t [ 8.19411257 -1.87895633]. \t  -54.32335346758279 \t -0.14473002518929054\n",
            "62     \t [0.57623195 0.57246729]. \t  -0.1921264278632733 \t -0.14473002518929054\n",
            "63     \t [-2.98420544 -5.26783978]. \t  -6856.742089050018 \t -0.14473002518929054\n",
            "64     \t [ 0.92926639 -0.42745446]. \t  -0.64081573713701 \t -0.14473002518929054\n",
            "65     \t [6.80905509 0.11733726]. \t  -125.72312142648643 \t -0.14473002518929054\n",
            "66     \t [4.83577875 1.65862836]. \t  -15.601156239351546 \t -0.14473002518929054\n",
            "67     \t [-2.01667832  4.6241505 ]. \t  -4019.993725988372 \t -0.14473002518929054\n",
            "68     \t [ 0.18942375 -0.7103234 ]. \t  -2.000833360687899 \t -0.14473002518929054\n",
            "69     \t [-6.24072439  0.690922  ]. \t  -155.97769002463605 \t -0.14473002518929054\n",
            "70     \t [-7.98074325 -9.01144947]. \t  -58148.32966529846 \t -0.14473002518929054\n",
            "71     \t [ 7.19505941 -2.2019923 ]. \t  -50.90358099568845 \t -0.14473002518929054\n",
            "72     \t [ 2.49053743 -3.98211312]. \t  -1710.2958083542276 \t -0.14473002518929054\n",
            "73     \t [-8.33248565  0.93367549]. \t  -290.14625559474524 \t -0.14473002518929054\n",
            "74     \t [ 7.03740317 -5.13546999]. \t  -4215.020903821437 \t -0.14473002518929054\n",
            "75     \t [-7.43781506 -0.14536251]. \t  -183.0997841788702 \t -0.14473002518929054\n",
            "76     \t [ 1.25417438 -8.4556558 ]. \t  -40181.68519036071 \t -0.14473002518929054\n",
            "77     \t [9.54647128 9.218395  ]. \t  -51536.510783866055 \t -0.14473002518929054\n",
            "78     \t [ 1.64567548 -0.79931802]. \t  -0.6875341970480675 \t -0.14473002518929054\n",
            "79     \t [-5.68758126  9.14632012]. \t  -59901.28760036906 \t -0.14473002518929054\n",
            "80     \t [-8.24874276 -6.05901984]. \t  -13426.231068029243 \t -0.14473002518929054\n",
            "81     \t [ 4.51812308 -2.96445056]. \t  -353.3900566599988 \t -0.14473002518929054\n",
            "82     \t [ 3.78276595 -0.85424074]. \t  -18.53933857539038 \t -0.14473002518929054\n",
            "83     \t [ 3.5932631  -1.31636857]. \t  -6.757582461633976 \t -0.14473002518929054\n",
            "84     \t [1.25943139 0.81670423]. \t  \u001b[92m-0.0784290628392909\u001b[0m \t -0.0784290628392909\n",
            "85     \t [ 4.36878359 -1.39793882]. \t  -11.772487649462366 \t -0.0784290628392909\n",
            "86     \t [2.56789062 4.34321879]. \t  -2474.798140262368 \t -0.0784290628392909\n",
            "87     \t [1.01842622 1.40811759]. \t  -17.37189189256229 \t -0.0784290628392909\n",
            "88     \t [ 1.57698761 -7.3710349 ]. \t  -22935.722571382918 \t -0.0784290628392909\n",
            "89     \t [2.22968594 2.45295873]. \t  -193.7617872452074 \t -0.0784290628392909\n",
            "90     \t [-7.14545072 -6.97458179]. \t  -21879.700625109097 \t -0.0784290628392909\n",
            "91     \t [ 3.03407927 -0.93957637]. \t  -7.355519630724508 \t -0.0784290628392909\n",
            "92     \t [ 8.64980484 -6.85356859]. \t  -14608.311187269232 \t -0.0784290628392909\n",
            "93     \t [4.36301875 2.32076921]. \t  -93.45842424711255 \t -0.0784290628392909\n",
            "94     \t [-5.34944394  6.9938714 ]. \t  -21331.681049501938 \t -0.0784290628392909\n",
            "95     \t [ 8.44278079 -7.19997955]. \t  -18195.42650662692 \t -0.0784290628392909\n",
            "96     \t [7.94956547 1.96494096]. \t  -48.40004505770033 \t -0.0784290628392909\n",
            "97     \t [ 9.17998145 -4.28924253]. \t  -1592.1130148597142 \t -0.0784290628392909\n",
            "98     \t [-8.86077216  7.52949317]. \t  -29986.042979117836 \t -0.0784290628392909\n",
            "99     \t [-3.64129716  4.08824078]. \t  -2769.7220190796297 \t -0.0784290628392909\n",
            "100    \t [1.15671967 0.73125358]. \t  \u001b[92m-0.03978830364345711\u001b[0m \t -0.03978830364345711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh_Amb8TtdgO",
        "outputId": "943a2cce-81b2-4c88-d181-4a383e6f5efb"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_loser_6 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_6 = d2GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.85720303 -3.36040389]. \t  -480.81536490368427 \t -43.91981950896418\n",
            "init   \t [ 6.42458246 -9.16606749]. \t  -52264.36633669738 \t -43.91981950896418\n",
            "init   \t [-7.8468664   1.90104128]. \t  -532.7651696660866 \t -43.91981950896418\n",
            "init   \t [ 0.59634724 -1.62385143]. \t  -43.91981950896418 \t -43.91981950896418\n",
            "init   \t [-3.29184301  2.45038864]. \t  -486.6398213674611 \t -43.91981950896418\n",
            "1      \t [2.90710192 9.80448543]. \t  -71709.54822485936 \t -43.91981950896418\n",
            "2      \t [-9.22684845 -9.49630667]. \t  -71990.69239039435 \t -43.91981950896418\n",
            "3      \t [-7.69566798  9.86414506]. \t  -81924.88656432132 \t -43.91981950896418\n",
            "4      \t [-14.49082832  -1.61337326]. \t  -1015.8916436940622 \t -43.91981950896418\n",
            "5      \t [-1.42589422 -8.37210292]. \t  -40112.73076912865 \t -43.91981950896418\n",
            "6      \t [6.91801255 2.83109097]. \t  -201.085049944819 \t -43.91981950896418\n",
            "7      \t [-19.59825297 -20.37813544]. \t  -1445883.5322964664 \t -43.91981950896418\n",
            "8      \t [9.25715772 8.0152434 ]. \t  -28500.28911684498 \t -43.91981950896418\n",
            "9      \t [-4.90896655 -3.12913489]. \t  -1234.6258339881792 \t -43.91981950896418\n",
            "10     \t [2.01537901 3.50509075]. \t  -1018.5721581962224 \t -43.91981950896418\n",
            "11     \t [-1.82649604  6.25302689]. \t  -12816.690262854694 \t -43.91981950896418\n",
            "12     \t [-53.93633628 -56.45952   ]. \t  -82674586.97341676 \t -43.91981950896418\n",
            "13     \t [-9.69959485 -3.64327509]. \t  -2742.0990939491844 \t -43.91981950896418\n",
            "14     \t [ 2.88661417 -4.65553919]. \t  -3277.8214178889693 \t -43.91981950896418\n",
            "15     \t [-9.73766146  5.06611173]. \t  -7574.054369880718 \t -43.91981950896418\n",
            "16     \t [ 4.07882326 -0.20600831]. \t  \u001b[92m-41.38233652059989\u001b[0m \t -41.38233652059989\n",
            "17     \t [9.96878176 0.33386517]. \t  -270.402225297586 \t -41.38233652059989\n",
            "18     \t [9.88907131 3.42383547]. \t  -446.5581873408777 \t -41.38233652059989\n",
            "19     \t [-0.309037   0.3724757]. \t  \u001b[92m-2.401573527313572\u001b[0m \t -2.401573527313572\n",
            "20     \t [ 9.94339114 -5.83893736]. \t  -6864.475500501584 \t -2.401573527313572\n",
            "21     \t [-1.40789736 -3.87381463]. \t  -1980.3284163693127 \t -2.401573527313572\n",
            "22     \t [-5.52268268 -6.94329763]. \t  -20826.662449252213 \t -2.401573527313572\n",
            "23     \t [ 6.47365283 -0.10186634]. \t  -113.24069486906149 \t -2.401573527313572\n",
            "24     \t [-0.92096538  0.71057555]. \t  -11.146089933675487 \t -2.401573527313572\n",
            "25     \t [5.88507079 5.6322615 ]. \t  -6650.094762645206 \t -2.401573527313572\n",
            "26     \t [-5.61427526  0.08836312]. \t  -107.13999011846055 \t -2.401573527313572\n",
            "27     \t [-0.52729332 -0.33376025]. \t  -3.45788031331116 \t -2.401573527313572\n",
            "28     \t [-0.02112416 -0.0644817 ]. \t  \u001b[92m-1.0444279594933907\u001b[0m \t -1.0444279594933907\n",
            "29     \t [ 0.67886926 -0.48958899]. \t  \u001b[92m-0.18270511013132607\u001b[0m \t -0.18270511013132607\n",
            "30     \t [ 1.95165392 -0.92110338]. \t  -1.0354821205920615 \t -0.18270511013132607\n",
            "31     \t [2.22604554 0.04247199]. \t  -11.381647173006462 \t -0.18270511013132607\n",
            "32     \t [0.33569442 0.67160449]. \t  -1.0829442104911082 \t -0.18270511013132607\n",
            "33     \t [0.45427659 0.18684963]. \t  -0.5934192115956451 \t -0.18270511013132607\n",
            "34     \t [ 0.50455892 -0.37492265]. \t  -0.3452992605154044 \t -0.18270511013132607\n",
            "35     \t [-2.23936006 -0.75611641]. \t  -33.379910466131996 \t -0.18270511013132607\n",
            "36     \t [ 1.1367388  -0.27255539]. \t  -1.9716412782339923 \t -0.18270511013132607\n",
            "37     \t [ 1.49794773 -1.30191666]. \t  -7.407478668549623 \t -0.18270511013132607\n",
            "38     \t [-5.4178733  5.785831 ]. \t  -10515.893810317048 \t -0.18270511013132607\n",
            "39     \t [-0.44859786  1.71351337]. \t  -82.00482592882513 \t -0.18270511013132607\n",
            "40     \t [ 1.58969771 -0.77234865]. \t  -0.6624103433081502 \t -0.18270511013132607\n",
            "41     \t [-1.37569669 -0.11997504]. \t  -9.58908936230584 \t -0.18270511013132607\n",
            "42     \t [ 4.67892651 -1.93448782]. \t  -29.276831331822052 \t -0.18270511013132607\n",
            "43     \t [-2.06431944  2.74454778]. \t  -596.2230506718907 \t -0.18270511013132607\n",
            "44     \t [ 7.74852284 -9.71120688]. \t  -65470.96414442118 \t -0.18270511013132607\n",
            "45     \t [-0.3398488   1.92037519]. \t  -120.85401780007317 \t -0.18270511013132607\n",
            "46     \t [-9.70531014 -0.41807056]. \t  -316.8047327360364 \t -0.18270511013132607\n",
            "47     \t [5.79106982 3.37893875]. \t  -603.908252005958 \t -0.18270511013132607\n",
            "48     \t [ 2.4933684  -8.68377221]. \t  -44001.31737096439 \t -0.18270511013132607\n",
            "49     \t [ 3.36906149 -0.59688918]. \t  -19.726523072711384 \t -0.18270511013132607\n",
            "50     \t [0.29509901 0.63965042]. \t  -1.0443751101793834 \t -0.18270511013132607\n",
            "51     \t [-3.79388167  7.61725185]. \t  -28745.721149029167 \t -0.18270511013132607\n",
            "52     \t [0.52246869 0.61092236]. \t  -0.32837344430799603 \t -0.18270511013132607\n",
            "53     \t [0.72380351 0.45289169]. \t  -0.27295152970918507 \t -0.18270511013132607\n",
            "54     \t [ 8.02164712 -2.67265804]. \t  -127.7928233851124 \t -0.18270511013132607\n",
            "55     \t [ 6.92146635 -2.07353916]. \t  -40.69286965924218 \t -0.18270511013132607\n",
            "56     \t [ 9.40375787 -1.45236495]. \t  -124.392218513257 \t -0.18270511013132607\n",
            "57     \t [0.92604478 0.63613875]. \t  \u001b[92m-0.03270704534357148\u001b[0m \t -0.03270704534357148\n",
            "58     \t [4.55449115 1.57638214]. \t  -12.979638220860418 \t -0.03270704534357148\n",
            "59     \t [ 1.48185466 -0.86420782]. \t  -0.23246502420954696 \t -0.03270704534357148\n",
            "60     \t [3.04362669 0.72259344]. \t  -12.171163950885502 \t -0.03270704534357148\n",
            "61     \t [ 7.48668909 -2.11199074]. \t  -46.19168681964074 \t -0.03270704534357148\n",
            "62     \t [ 8.96861704 -2.04719604]. \t  -64.18704220289794 \t -0.03270704534357148\n",
            "63     \t [ 5.89567966 -2.16578406]. \t  -48.26595766914302 \t -0.03270704534357148\n",
            "64     \t [-9.71885462  3.02903771]. \t  -1690.6306426150086 \t -0.03270704534357148\n",
            "65     \t [-9.37022401 -5.04390326]. \t  -7368.183318605593 \t -0.03270704534357148\n",
            "66     \t [7.18587785 1.24681412]. \t  -71.5054688868222 \t -0.03270704534357148\n",
            "67     \t [ 2.94638521 -1.49718838]. \t  -8.511683431768233 \t -0.03270704534357148\n",
            "68     \t [-2.69311824  1.32085629]. \t  -90.08427328645001 \t -0.03270704534357148\n",
            "69     \t [-2.64260059  1.02280298]. \t  -58.10619491706118 \t -0.03270704534357148\n",
            "70     \t [8.73397685 7.35943592]. \t  -19895.599784253176 \t -0.03270704534357148\n",
            "71     \t [-7.45020871 -5.3634945 ]. \t  -8517.338851062503 \t -0.03270704534357148\n",
            "72     \t [8.54124136 6.9162167 ]. \t  -15239.051709360609 \t -0.03270704534357148\n",
            "73     \t [ 1.31820967 -0.04303052]. \t  -3.5571116527695414 \t -0.03270704534357148\n",
            "74     \t [ 1.05122142 -0.74109144]. \t  \u001b[92m-0.00708151029528118\u001b[0m \t -0.00708151029528118\n",
            "75     \t [-5.54358842  5.51304305]. \t  -8842.3887107496 \t -0.00708151029528118\n",
            "76     \t [-1.09631494 -0.62532173]. \t  -11.451080009705871 \t -0.00708151029528118\n",
            "77     \t [4.66219434 9.80236653]. \t  -70333.85057301067 \t -0.00708151029528118\n",
            "78     \t [ 2.24003725 -8.95614477]. \t  -50046.53950081282 \t -0.00708151029528118\n",
            "79     \t [2.08556743 0.6680644 ]. \t  -4.024703356773264 \t -0.00708151029528118\n",
            "80     \t [-0.70520444  6.38911094]. \t  -13564.859667105267 \t -0.00708151029528118\n",
            "81     \t [ 0.76413099 -0.43252571]. \t  -0.3597936617961455 \t -0.00708151029528118\n",
            "82     \t [-8.11969226  2.93973397]. \t  -1373.8712307481312 \t -0.00708151029528118\n",
            "83     \t [ 3.7895797  -1.06406981]. \t  -12.433557590511274 \t -0.00708151029528118\n",
            "84     \t [-8.6698848  -1.14240882]. \t  -347.9871088865486 \t -0.00708151029528118\n",
            "85     \t [-1.97214442 -6.10811556]. \t  -11740.98045602341 \t -0.00708151029528118\n",
            "86     \t [ 3.03353766 -1.29445199]. \t  -4.337109278540841 \t -0.00708151029528118\n",
            "87     \t [8.90445694 8.69235288]. \t  -40509.58705670207 \t -0.00708151029528118\n",
            "88     \t [-3.62428043  3.07093999]. \t  -1032.5906525321786 \t -0.00708151029528118\n",
            "89     \t [-4.53268014  0.81560817]. \t  -99.36274943872641 \t -0.00708151029528118\n",
            "90     \t [-3.02485719 -0.02365569]. \t  -34.51254144719541 \t -0.00708151029528118\n",
            "91     \t [-3.43540112  1.15080945]. \t  -93.70595834481217 \t -0.00708151029528118\n",
            "92     \t [ 7.61381756 -5.9672286 ]. \t  -8134.124752290816 \t -0.00708151029528118\n",
            "93     \t [-2.09951624  0.1038914 ]. \t  -18.605157588421953 \t -0.00708151029528118\n",
            "94     \t [-0.05444533  5.13783057]. \t  -5587.156302415063 \t -0.00708151029528118\n",
            "95     \t [6.57010489 8.77670648]. \t  -43538.24471272127 \t -0.00708151029528118\n",
            "96     \t [-8.81909582 -6.38636263]. \t  -16437.245746249155 \t -0.00708151029528118\n",
            "97     \t [-7.38079581 -5.17206868]. \t  -7483.323761270523 \t -0.00708151029528118\n",
            "98     \t [ 1.30913725 -0.76022489]. \t  -0.1425391002847861 \t -0.00708151029528118\n",
            "99     \t [-3.06810703  6.60137068]. \t  -16297.396964435744 \t -0.00708151029528118\n",
            "100    \t [-7.12382562 -8.06620731]. \t  -37741.79479062148 \t -0.00708151029528118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNmoDTuUtdi3",
        "outputId": "52bc1946-1e8f-4d4c-be9f-c475ed126623"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_loser_7 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_7 = d2GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-6.9467253  -3.95286781]. \t  -2981.1801714787616 \t -297.17705346549894\n",
            "init   \t [-8.75927171 -0.80279317]. \t  -297.17705346549894 \t -297.17705346549894\n",
            "init   \t [6.70506769 8.53994095]. \t  -38761.39667089972 \t -297.17705346549894\n",
            "init   \t [4.53977969 5.36992444]. \t  -5658.63331787327 \t -297.17705346549894\n",
            "init   \t [-4.61589867  2.88058584]. \t  -931.3893916140455 \t -297.17705346549894\n",
            "1      \t [ 2.44677649 -4.93762112]. \t  -4291.960403637314 \t -297.17705346549894\n",
            "2      \t [-16.04639688  -7.48820171]. \t  -33157.32991792821 \t -297.17705346549894\n",
            "3      \t [ 9.83790456 -9.57130198]. \t  -60200.563023412855 \t -297.17705346549894\n",
            "4      \t [-7.89992531  9.8439324 ]. \t  -81449.7775076618 \t -297.17705346549894\n",
            "5      \t [ 9.0936247  -0.78669084]. \t  \u001b[92m-188.93582542124247\u001b[0m \t -188.93582542124247\n",
            "6      \t [-28.95089568 -20.19941257]. \t  -1428891.0578332487 \t -188.93582542124247\n",
            "7      \t [ -0.25006192 -11.32647782]. \t  -131923.07441803082 \t -188.93582542124247\n",
            "8      \t [ -7.36104312 -13.27926308]. \t  -259325.63694486077 \t -188.93582542124247\n",
            "9      \t [0.42210597 0.91487768]. \t  \u001b[92m-3.4684505922616853\u001b[0m \t -3.4684505922616853\n",
            "10     \t [-1.254296   8.6667351]. \t  -45896.790383750355 \t -3.4684505922616853\n",
            "11     \t [-17.33653492 -15.49694963]. \t  -495642.1586824317 \t -3.4684505922616853\n",
            "12     \t [-1.9100026  -2.01948093]. \t  -211.14135246069847 \t -3.4684505922616853\n",
            "13     \t [-9.80709009  3.90771204]. \t  -3372.637438441402 \t -3.4684505922616853\n",
            "14     \t [ 4.71326897 -0.3376098 ]. \t  -54.024346075689095 \t -3.4684505922616853\n",
            "15     \t [-10.32453847  -8.04481479]. \t  -39195.42980045439 \t -3.4684505922616853\n",
            "16     \t [-2.27043489 -6.22179853]. \t  -12712.321926758015 \t -3.4684505922616853\n",
            "17     \t [ 6.98250393 -4.18484421]. \t  -1608.6479900034708 \t -3.4684505922616853\n",
            "18     \t [9.2308028  3.85823094]. \t  -911.6187989663807 \t -3.4684505922616853\n",
            "19     \t [  4.89317314 -10.50413319]. \t  -93137.56648231776 \t -3.4684505922616853\n",
            "20     \t [-0.77337085  1.7188456 ]. \t  -92.44927319836273 \t -3.4684505922616853\n",
            "21     \t [1.17449585 1.72972581]. \t  -46.29123898808672 \t -3.4684505922616853\n",
            "22     \t [-0.28270835  0.18519308]. \t  \u001b[92m-1.8921659111897258\u001b[0m \t -1.8921659111897258\n",
            "23     \t [0.50977244 0.19575997]. \t  \u001b[92m-0.6155236750725124\u001b[0m \t -0.6155236750725124\n",
            "24     \t [6.60263427 0.83155523]. \t  -85.87933860546708 \t -0.6155236750725124\n",
            "25     \t [0.2556143  4.44312131]. \t  -3078.07540073286 \t -0.6155236750725124\n",
            "26     \t [2.83021787 0.11147651]. \t  -19.089830108221747 \t -0.6155236750725124\n",
            "27     \t [2.17675963 1.73632202]. \t  -31.073957966626836 \t -0.6155236750725124\n",
            "28     \t [ 0.37922296 -0.50519886]. \t  \u001b[92m-0.4198061371296233\u001b[0m \t -0.4198061371296233\n",
            "29     \t [ 0.67228202 -1.066556  ]. \t  -5.245343610391255 \t -0.4198061371296233\n",
            "30     \t [-6.25798902 -8.33121493]. \t  -42146.928669870766 \t -0.4198061371296233\n",
            "31     \t [-6.29681339  5.3741027 ]. \t  -8260.296298236426 \t -0.4198061371296233\n",
            "32     \t [ 0.0161191  -0.11549953]. \t  -0.968244709594219 \t -0.4198061371296233\n",
            "33     \t [-4.21983528 -0.97236269]. \t  -101.93076801903568 \t -0.4198061371296233\n",
            "34     \t [ 9.9914595  -5.30792187]. \t  -4378.7168431587115 \t -0.4198061371296233\n",
            "35     \t [ 1.33632763 -0.07436351]. \t  -3.625785579881615 \t -0.4198061371296233\n",
            "36     \t [ 1.06265215 -0.75733626]. \t  \u001b[92m-0.018193713930163783\u001b[0m \t -0.018193713930163783\n",
            "37     \t [ 0.68679032 -0.117311  ]. \t  -0.9673651500676966 \t -0.018193713930163783\n",
            "38     \t [ 2.03404561 -0.73545391]. \t  -2.882851291403675 \t -0.018193713930163783\n",
            "39     \t [ 1.23186749 -0.74096685]. \t  -0.089569407601569 \t -0.018193713930163783\n",
            "40     \t [0.92306378 1.08478666]. \t  -4.09835311959922 \t -0.018193713930163783\n",
            "41     \t [3.5532121  2.09062985]. \t  -60.35485630164188 \t -0.018193713930163783\n",
            "42     \t [-2.58398152  0.49445503]. \t  -31.73100475602193 \t -0.018193713930163783\n",
            "43     \t [ 3.0514052  -1.40033879]. \t  -5.723776894473894 \t -0.018193713930163783\n",
            "44     \t [ 4.51089508 -2.26283513]. \t  -77.99105069792797 \t -0.018193713930163783\n",
            "45     \t [ 1.05224331 -0.49900088]. \t  -0.6170923306322798 \t -0.018193713930163783\n",
            "46     \t [-7.14913901  0.49635082]. \t  -183.20473778959644 \t -0.018193713930163783\n",
            "47     \t [ 4.09436519 -1.66270756]. \t  -13.692556912772787 \t -0.018193713930163783\n",
            "48     \t [ 0.54012536 -0.22009734]. \t  -0.6044075147402664 \t -0.018193713930163783\n",
            "49     \t [-0.86282025 -0.02065943]. \t  -4.96196440325069 \t -0.018193713930163783\n",
            "50     \t [ 1.75308595 -1.44238467]. \t  -12.162729024487641 \t -0.018193713930163783\n",
            "51     \t [2.70702473 9.13587249]. \t  -53940.189687606355 \t -0.018193713930163783\n",
            "52     \t [ 0.70468157 -0.497099  ]. \t  -0.1758054681011272 \t -0.018193713930163783\n",
            "53     \t [ 3.94567893 -1.31045478]. \t  -9.199461528723564 \t -0.018193713930163783\n",
            "54     \t [ 1.21790954 -0.79675347]. \t  -0.052835028882927494 \t -0.018193713930163783\n",
            "55     \t [ 1.97969685 -8.40183591]. \t  -38755.35146785771 \t -0.018193713930163783\n",
            "56     \t [ 4.72560139 -3.81075416]. \t  -1196.619410042751 \t -0.018193713930163783\n",
            "57     \t [ 4.61962336 -1.88464164]. \t  -25.443426037925917 \t -0.018193713930163783\n",
            "58     \t [ 3.65113881 -1.22708075]. \t  -7.84692942393984 \t -0.018193713930163783\n",
            "59     \t [-6.05459709 -2.74044759]. \t  -938.0535616460793 \t -0.018193713930163783\n",
            "60     \t [-2.37426724  7.82643393]. \t  -31201.6159004122 \t -0.018193713930163783\n",
            "61     \t [ 8.3389707  -1.14735903]. \t  -118.9797646176284 \t -0.018193713930163783\n",
            "62     \t [9.45672564 4.78440042]. \t  -2710.4183504109697 \t -0.018193713930163783\n",
            "63     \t [ 9.65168305 -3.44245725]. \t  -469.61957102685653 \t -0.018193713930163783\n",
            "64     \t [8.95735451 2.72156887]. \t  -131.9171380345195 \t -0.018193713930163783\n",
            "65     \t [ 9.65399849 -2.64245281]. \t  -112.06311789377975 \t -0.018193713930163783\n",
            "66     \t [-1.46270822  0.50283556]. \t  -13.814092868752613 \t -0.018193713930163783\n",
            "67     \t [-9.98911439 -3.7319014 ]. \t  -2984.9899047190547 \t -0.018193713930163783\n",
            "68     \t [-1.73689293 -2.46917106]. \t  -395.6084880472298 \t -0.018193713930163783\n",
            "69     \t [-0.01296347 -9.350029  ]. \t  -61152.39954670551 \t -0.018193713930163783\n",
            "70     \t [-2.92670956 -0.51002889]. \t  -39.18223157807998 \t -0.018193713930163783\n",
            "71     \t [ 2.12837204 -1.28109444]. \t  -3.9368119637349457 \t -0.018193713930163783\n",
            "72     \t [-0.60794976 -6.81148879]. \t  -17449.970281161743 \t -0.018193713930163783\n",
            "73     \t [-4.54631698  7.84714993]. \t  -32646.281973896577 \t -0.018193713930163783\n",
            "74     \t [ 0.87445829 -0.61735948]. \t  -0.04093518822293769 \t -0.018193713930163783\n",
            "75     \t [-1.67836209 -0.38892624]. \t  -15.021468529265658 \t -0.018193713930163783\n",
            "76     \t [0.5117976 0.4381675]. \t  -0.2710154883400123 \t -0.018193713930163783\n",
            "77     \t [1.28463349 0.65014381]. \t  -0.4669141093128888 \t -0.018193713930163783\n",
            "78     \t [1.07329673 0.74406266]. \t  \u001b[92m-0.007679210655349976\u001b[0m \t -0.007679210655349976\n",
            "79     \t [5.61940086 1.89146486]. \t  -26.05670548141057 \t -0.007679210655349976\n",
            "80     \t [ 5.13145893 -6.98450545]. \t  -17105.591663517112 \t -0.007679210655349976\n",
            "81     \t [0.99500481 3.75849543]. \t  -1485.9503694431025 \t -0.007679210655349976\n",
            "82     \t [-6.86179769 -6.00034947]. \t  -12502.820114758297 \t -0.007679210655349976\n",
            "83     \t [9.01949008 0.36589373]. \t  -217.49790444167454 \t -0.007679210655349976\n",
            "84     \t [-1.87594435 -0.71639782]. \t  -25.118861073358822 \t -0.007679210655349976\n",
            "85     \t [4.32363851 5.40727383]. \t  -5876.267471595092 \t -0.007679210655349976\n",
            "86     \t [2.33340893 0.85823742]. \t  -3.2580945782997146 \t -0.007679210655349976\n",
            "87     \t [-2.10379631 -5.07196877]. \t  -5745.594840224066 \t -0.007679210655349976\n",
            "88     \t [7.14575166 3.60301174]. \t  -745.9770755526986 \t -0.007679210655349976\n",
            "89     \t [5.12178132 1.61066334]. \t  -16.997976753067213 \t -0.007679210655349976\n",
            "90     \t [-9.46301857  0.66459733]. \t  -323.5706524987455 \t -0.007679210655349976\n",
            "91     \t [ 4.74676856 -2.01197137]. \t  -36.473748104095634 \t -0.007679210655349976\n",
            "92     \t [ 0.71210475 -0.51965567]. \t  -0.14206593561364175 \t -0.007679210655349976\n",
            "93     \t [0.08800363 9.77315282]. \t  -72917.78441875978 \t -0.007679210655349976\n",
            "94     \t [ 0.75676517 -4.31883246]. \t  -2671.551655887892 \t -0.007679210655349976\n",
            "95     \t [-2.55040769 -1.69504259]. \t  -150.27739847390453 \t -0.007679210655349976\n",
            "96     \t [2.12746572 0.54465512]. \t  -5.978517671234961 \t -0.007679210655349976\n",
            "97     \t [ 1.37175544 -1.3771187 ]. \t  -11.862198666920797 \t -0.007679210655349976\n",
            "98     \t [ 5.61987587 -7.25406049]. \t  -19870.792210242922 \t -0.007679210655349976\n",
            "99     \t [-6.33179118  5.3289402 ]. \t  -8023.791023469346 \t -0.007679210655349976\n",
            "100    \t [-2.64019465  8.15832871]. \t  -36873.09521062235 \t -0.007679210655349976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_21yVprtdlu",
        "outputId": "6311644b-f5cc-4120-a386-b1214d371544"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_loser_8 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_8 = d2GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-7.8100025  -9.15569971]. \t  -61652.61989589963 \t -123.0699213709516\n",
            "init   \t [ 1.9938517 -4.0025859]. \t  -1806.696586806874 \t -123.0699213709516\n",
            "init   \t [ 5.93223564 -2.6760775 ]. \t  -165.12946982275756 \t -123.0699213709516\n",
            "init   \t [ 7.61216688 -4.04316733]. \t  -1301.9580077172761 \t -123.0699213709516\n",
            "init   \t [-3.41793472  1.37433832]. \t  -123.0699213709516 \t -123.0699213709516\n",
            "1      \t [-98.81463622 -92.74238158]. \t  -598667007.0549719 \t -123.0699213709516\n",
            "2      \t [7.33725011 6.34690223]. \t  -10765.158736519235 \t -123.0699213709516\n",
            "3      \t [-6.99148152  9.31430956]. \t  -65227.40102136099 \t -123.0699213709516\n",
            "4      \t [0.47559161 8.55656073]. \t  -42605.34024544054 \t -123.0699213709516\n",
            "5      \t [  1.48689111 -10.76449601]. \t  -106041.28301651844 \t -123.0699213709516\n",
            "6      \t [-9.721135    0.63451534]. \t  -336.55101039217925 \t -123.0699213709516\n",
            "7      \t [ 7.86685851 -9.65405122]. \t  -63796.325713701866 \t -123.0699213709516\n",
            "8      \t [-15.02212417  -9.80071859]. \t  -86062.62570077169 \t -123.0699213709516\n",
            "9      \t [-3.84702547 -4.02290713]. \t  -2646.4865780007854 \t -123.0699213709516\n",
            "10     \t [2.55766053 2.09985064]. \t  \u001b[92m-80.8286747929862\u001b[0m \t -80.8286747929862\n",
            "11     \t [-157.47204314 -154.00426315]. \t  -4530040731.798599 \t -80.8286747929862\n",
            "12     \t [-11.76815469  -4.3178455 ]. \t  -4975.955908370858 \t -80.8286747929862\n",
            "13     \t [9.32063891 1.10185427]. \t  -164.2454074862331 \t -80.8286747929862\n",
            "14     \t [-10.93490528   5.15056325]. \t  -8332.268364131181 \t -80.8286747929862\n",
            "15     \t [-5.93291083  4.05033361]. \t  -3050.154259880464 \t -80.8286747929862\n",
            "16     \t [-0.67603058  3.45860831]. \t  -1213.126473428566 \t -80.8286747929862\n",
            "17     \t [-2.97296382 -8.17278991]. \t  -37314.11878500217 \t -80.8286747929862\n",
            "18     \t [5.81963696 1.69341152]. \t  \u001b[92m-23.24313088311752\u001b[0m \t -23.24313088311752\n",
            "19     \t [-7.81452295 -3.02346961]. \t  -1439.8297642178889 \t -23.24313088311752\n",
            "20     \t [ 3.45191722 -0.2698732 ]. \t  -27.874530699759976 \t -23.24313088311752\n",
            "21     \t [5.28988091 9.87566309]. \t  -72041.8584116926 \t -23.24313088311752\n",
            "22     \t [-0.86780242 -1.67731768]. \t  -87.84812617121251 \t -23.24313088311752\n",
            "23     \t [-5.70559701 -0.1133256 ]. \t  -110.66022693567565 \t -23.24313088311752\n",
            "24     \t [ 6.01091204 -0.06423932]. \t  -97.17306129123519 \t -23.24313088311752\n",
            "25     \t [9.96127562 9.88691337]. \t  -68931.10822656454 \t -23.24313088311752\n",
            "26     \t [6.56689966 2.27800672]. \t  -60.04893605475835 \t -23.24313088311752\n",
            "27     \t [ 4.31035597 -6.45856125]. \t  -12529.537238076215 \t -23.24313088311752\n",
            "28     \t [4.51189193 3.22305507]. \t  -541.3867332844394 \t -23.24313088311752\n",
            "29     \t [ 1.21617242 -0.28224862]. \t  \u001b[92m-2.280568347954156\u001b[0m \t -2.280568347954156\n",
            "30     \t [-0.3596398   0.21727254]. \t  \u001b[92m-2.260951379250973\u001b[0m \t -2.260951379250973\n",
            "31     \t [1.28727522 0.13986241]. \t  -3.1982950231029417 \t -2.260951379250973\n",
            "32     \t [-0.8861657   0.18598234]. \t  -5.382987519614326 \t -2.260951379250973\n",
            "33     \t [5.60323033 1.17893089]. \t  -37.13374314994377 \t -2.260951379250973\n",
            "34     \t [9.67357193 3.56869834]. \t  -574.3619453946935 \t -2.260951379250973\n",
            "35     \t [-2.64936754  5.75183021]. \t  -9484.730710461623 \t -2.260951379250973\n",
            "36     \t [-0.11224753  0.1229503 ]. \t  \u001b[92m-1.277696284017129\u001b[0m \t -1.277696284017129\n",
            "37     \t [-1.75997782  0.11864031]. \t  -14.012287256721972 \t -1.277696284017129\n",
            "38     \t [ 9.58787667 -1.49820867]. \t  -125.74344140498607 \t -1.277696284017129\n",
            "39     \t [ 1.63582599 -0.160976  ]. \t  -5.422383185662185 \t -1.277696284017129\n",
            "40     \t [ 9.7669401  -5.86685853]. \t  -7056.125746915859 \t -1.277696284017129\n",
            "41     \t [ 4.72147872 -1.72425438]. \t  -16.84882935049343 \t -1.277696284017129\n",
            "42     \t [ 3.13745005 -0.87479751]. \t  -9.733003784786302 \t -1.277696284017129\n",
            "43     \t [ 0.93529965 -0.25051774]. \t  -1.3156778895830032 \t -1.277696284017129\n",
            "44     \t [0.26388089 0.44404726]. \t  \u001b[92m-0.5759188141122675\u001b[0m \t -0.5759188141122675\n",
            "45     \t [0.19549425 0.15967268]. \t  -0.6889920372295295 \t -0.5759188141122675\n",
            "46     \t [ 0.56341775 -1.07579026]. \t  -6.3242283104089605 \t -0.5759188141122675\n",
            "47     \t [ 0.33371736 -0.56558523]. \t  -0.631273038861805 \t -0.5759188141122675\n",
            "48     \t [ 1.25359233 -0.54627356]. \t  -0.9269836002471781 \t -0.5759188141122675\n",
            "49     \t [2.69184983 5.03424066]. \t  -4609.961593003638 \t -0.5759188141122675\n",
            "50     \t [-3.23428689 -0.57425663]. \t  -48.25298214641066 \t -0.5759188141122675\n",
            "51     \t [5.06834609 5.96825386]. \t  -8773.955635900966 \t -0.5759188141122675\n",
            "52     \t [2.45394383 1.13319004]. \t  -2.140079578398679 \t -0.5759188141122675\n",
            "53     \t [1.00927738 0.78270075]. \t  \u001b[92m-0.09336657669638086\u001b[0m \t -0.09336657669638086\n",
            "54     \t [1.20506414 0.52160345]. \t  -0.9156918883402045 \t -0.09336657669638086\n",
            "55     \t [ 6.13223941 -8.32988771]. \t  -35214.04422492559 \t -0.09336657669638086\n",
            "56     \t [-6.13893734 -1.59182807]. \t  -302.14783823643666 \t -0.09336657669638086\n",
            "57     \t [ 0.46456789 -0.3258581 ]. \t  -0.41389811919616726 \t -0.09336657669638086\n",
            "58     \t [0.39177332 0.47859436]. \t  -0.37873951816249485 \t -0.09336657669638086\n",
            "59     \t [-6.16955622  0.44482737]. \t  -137.60883826786048 \t -0.09336657669638086\n",
            "60     \t [3.21586681 1.25857388]. \t  -4.914645044663584 \t -0.09336657669638086\n",
            "61     \t [ 3.72617784 -1.24365415]. \t  -8.232984507080612 \t -0.09336657669638086\n",
            "62     \t [-7.01656824  4.39875358]. \t  -4243.920950143667 \t -0.09336657669638086\n",
            "63     \t [-0.23611372  2.32528763]. \t  -245.73469272243707 \t -0.09336657669638086\n",
            "64     \t [-0.10455221  1.37739503]. \t  -31.62422486459417 \t -0.09336657669638086\n",
            "65     \t [9.06425658 7.71589085]. \t  -24267.53443867601 \t -0.09336657669638086\n",
            "66     \t [2.41021695 1.00190576]. \t  -2.312863857404778 \t -0.09336657669638086\n",
            "67     \t [-6.69536845  2.63555047]. \t  -906.9189730934511 \t -0.09336657669638086\n",
            "68     \t [-5.39998576 -6.84540931]. \t  -19690.208465023108 \t -0.09336657669638086\n",
            "69     \t [4.31523028 4.55266855]. \t  -2769.505070887002 \t -0.09336657669638086\n",
            "70     \t [ 1.0094345  -0.76651103]. \t  \u001b[92m-0.05496475915320519\u001b[0m \t -0.05496475915320519\n",
            "71     \t [2.63980749 1.18030711]. \t  -2.7318592802003865 \t -0.05496475915320519\n",
            "72     \t [-2.21950213  4.78261049]. \t  -4611.883322732063 \t -0.05496475915320519\n",
            "73     \t [ 2.78685118 -0.15378793]. \t  -18.203102605247672 \t -0.05496475915320519\n",
            "74     \t [1.00373523 1.87651118]. \t  -72.93550957059833 \t -0.05496475915320519\n",
            "75     \t [-4.09492747 -2.35923518]. \t  -489.6757853579257 \t -0.05496475915320519\n",
            "76     \t [ 2.04095505 -0.91198357]. \t  -1.3686406872156445 \t -0.05496475915320519\n",
            "77     \t [3.66025196 0.74747573]. \t  -20.008726306035815 \t -0.05496475915320519\n",
            "78     \t [ 9.87354977 -8.45660868]. \t  -35539.20050149259 \t -0.05496475915320519\n",
            "79     \t [-0.61430943 -9.6560787 ]. \t  -70010.95138703822 \t -0.05496475915320519\n",
            "80     \t [ 0.45204368 -0.60065918]. \t  -0.4455589139961583 \t -0.05496475915320519\n",
            "81     \t [1.13377697 0.69766944]. \t  -0.06928311330863457 \t -0.05496475915320519\n",
            "82     \t [-5.28630706  7.33387952]. \t  -25513.321947840155 \t -0.05496475915320519\n",
            "83     \t [ 7.09723595 -3.04258965]. \t  -297.89342045477264 \t -0.05496475915320519\n",
            "84     \t [ 7.18884261 -1.81663248]. \t  -38.99452072330875 \t -0.05496475915320519\n",
            "85     \t [-1.53502454 -5.06789235]. \t  -5603.688678125964 \t -0.05496475915320519\n",
            "86     \t [ 0.93232909 -0.2382274 ]. \t  -1.3455264914714893 \t -0.05496475915320519\n",
            "87     \t [-4.37081265 -0.80027575]. \t  -92.72894595951811 \t -0.05496475915320519\n",
            "88     \t [1.96352415 0.98137928]. \t  -0.9311633886251407 \t -0.05496475915320519\n",
            "89     \t [ 1.52017907 -1.78452586]. \t  -47.29397676328524 \t -0.05496475915320519\n",
            "90     \t [ 0.66501211 -3.19880746]. \t  -784.1705345699172 \t -0.05496475915320519\n",
            "91     \t [-6.54058379  8.44174   ]. \t  -44498.522389758466 \t -0.05496475915320519\n",
            "92     \t [-0.39591866  9.01209183]. \t  -53030.154493987946 \t -0.05496475915320519\n",
            "93     \t [ 7.5702067  -6.56973839]. \t  -12447.148877440855 \t -0.05496475915320519\n",
            "94     \t [3.22035406 7.00700377]. \t  -18045.75399055018 \t -0.05496475915320519\n",
            "95     \t [-4.18805224  5.0527217 ]. \t  -6131.608945360351 \t -0.05496475915320519\n",
            "96     \t [-9.842156   -3.27093426]. \t  -2069.4497773451208 \t -0.05496475915320519\n",
            "97     \t [ 9.86423471 -7.70892022]. \t  -23836.50570445349 \t -0.05496475915320519\n",
            "98     \t [ 2.08086811 -1.21197818]. \t  -2.636879467086902 \t -0.05496475915320519\n",
            "99     \t [-0.2734736  -5.50592384]. \t  -7420.184072100605 \t -0.05496475915320519\n",
            "100    \t [ 1.36835905 -1.26158467]. \t  -6.722923782042761 \t -0.05496475915320519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkq32q1utdoO",
        "outputId": "559ae129-092f-454c-dc13-73ae07d049da"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_loser_9 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_9 = d2GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.44557117 -0.23843202]. \t  -28.18355402366269 \t -28.18355402366269\n",
            "init   \t [ 6.50990348 -9.37107225]. \t  -57236.27240949214 \t -28.18355402366269\n",
            "init   \t [6.16099927 1.31234839]. \t  -41.39446954533859 \t -28.18355402366269\n",
            "init   \t [-4.04755003 -9.06608559]. \t  -56766.42271365435 \t -28.18355402366269\n",
            "init   \t [ 9.81254799 -9.86348534]. \t  -68353.23444364112 \t -28.18355402366269\n",
            "1      \t [-8.12773813  6.26616826]. \t  -15102.348898507695 \t -28.18355402366269\n",
            "2      \t [3.58702664 9.74831565]. \t  -69550.52203005194 \t -28.18355402366269\n",
            "3      \t [-9.43477705 -1.75785015]. \t  -596.5317353689182 \t -28.18355402366269\n",
            "4      \t [-10.3959775  -14.11947711]. \t  -334880.43578743684 \t -28.18355402366269\n",
            "5      \t [-25.12802344 -28.96611193]. \t  -5802458.414603443 \t -28.18355402366269\n",
            "6      \t [-2.43872094 -1.89096098]. \t  -195.7682062944888 \t -28.18355402366269\n",
            "7      \t [-19.88480669 -22.42372734]. \t  -2103867.5398021336 \t -28.18355402366269\n",
            "8      \t [-1.40064521  4.59220821]. \t  -3803.7422899005496 \t -28.18355402366269\n",
            "9      \t [-12.44384631  -7.30331812]. \t  -28560.24665118541 \t -28.18355402366269\n",
            "10     \t [ 9.75128696 -3.02170619]. \t  -221.4296353439725 \t -28.18355402366269\n",
            "11     \t [ 1.73519405 -5.07561161]. \t  -4958.324483215351 \t -28.18355402366269\n",
            "12     \t [9.42812131 5.7332353 ]. \t  -6413.082723731119 \t -28.18355402366269\n",
            "13     \t [-3.07662798  9.01497684]. \t  -54874.09859199814 \t -28.18355402366269\n",
            "14     \t [-5.7676168   1.41660456]. \t  -237.14257591454188 \t -28.18355402366269\n",
            "15     \t [ 6.09954382 -2.89800788]. \t  -254.87217861802378 \t -28.18355402366269\n",
            "16     \t [3.50714316 4.17023192]. \t  -1962.47764474379 \t -28.18355402366269\n",
            "17     \t [  1.07573112 -10.65694261]. \t  -102210.88318237649 \t -28.18355402366269\n",
            "18     \t [9.31253201 0.86788036]. \t  -190.96856354894862 \t -28.18355402366269\n",
            "19     \t [-5.79459426 -4.0505999 ]. \t  -3027.5246691072184 \t -28.18355402366269\n",
            "20     \t [ 0.33480144 -0.28157673]. \t  \u001b[92m-0.5046035264236911\u001b[0m \t -0.5046035264236911\n",
            "21     \t [1.5959279  0.65238208]. \t  -1.4643551669147068 \t -0.5046035264236911\n",
            "22     \t [ 0.85060929 -0.11635805]. \t  -1.3787238663949732 \t -0.5046035264236911\n",
            "23     \t [0.83672598 0.72697099]. \t  \u001b[92m-0.12367647103295268\u001b[0m \t -0.12367647103295268\n",
            "24     \t [0.94648725 0.90220458]. \t  -0.931636257694127 \t -0.12367647103295268\n",
            "25     \t [-1.99377345  1.00092489]. \t  -40.92228735256559 \t -0.12367647103295268\n",
            "26     \t [-9.60473065  2.21670449]. \t  -867.6879617370829 \t -0.12367647103295268\n",
            "27     \t [1.06348619 0.22995532]. \t  -1.838513644398887 \t -0.12367647103295268\n",
            "28     \t [0.78674205 0.45759508]. \t  -0.31626149785962837 \t -0.12367647103295268\n",
            "29     \t [-0.14794848  0.42494163]. \t  -1.8361498491509356 \t -0.12367647103295268\n",
            "30     \t [-8.56128461 -8.9134474 ]. \t  -56177.37173959819 \t -0.12367647103295268\n",
            "31     \t [ 0.64558148 -0.95083686]. \t  -2.8288898956337603 \t -0.12367647103295268\n",
            "32     \t [0.34811035 0.79545804]. \t  -2.108193295002079 \t -0.12367647103295268\n",
            "33     \t [8.63124405 9.41658793]. \t  -56986.396973058334 \t -0.12367647103295268\n",
            "34     \t [ 1.99128511 -0.59617389]. \t  -4.261691560773536 \t -0.12367647103295268\n",
            "35     \t [2.69193457 0.81412992]. \t  -6.596300611156385 \t -0.12367647103295268\n",
            "36     \t [ 7.7534389  -4.87385121]. \t  -3206.5945659035438 \t -0.12367647103295268\n",
            "37     \t [ 2.09538573 -1.23213011]. \t  -2.970468638494177 \t -0.12367647103295268\n",
            "38     \t [ 6.21004765 -0.33326829]. \t  -98.85478029240848 \t -0.12367647103295268\n",
            "39     \t [ 2.6868747  -1.19432516]. \t  -2.9006253887634355 \t -0.12367647103295268\n",
            "40     \t [7.4142927  2.17243145]. \t  -49.34135647191449 \t -0.12367647103295268\n",
            "41     \t [-1.74714322 -4.10257865]. \t  -2515.2045779119944 \t -0.12367647103295268\n",
            "42     \t [ 0.99251198 -0.64088396]. \t  \u001b[92m-0.05857055433931272\u001b[0m \t -0.05857055433931272\n",
            "43     \t [-0.11073318 -0.01054058]. \t  -1.258350394263938 \t -0.05857055433931272\n",
            "44     \t [-5.26032605  3.72159654]. \t  -2212.031920249907 \t -0.05857055433931272\n",
            "45     \t [-1.40742607 -0.19889224]. \t  -10.21531636805608 \t -0.05857055433931272\n",
            "46     \t [-3.7522252  -0.51040316]. \t  -59.10494071270438 \t -0.05857055433931272\n",
            "47     \t [ 3.52470214 -1.92862941]. \t  -37.021064453977985 \t -0.05857055433931272\n",
            "48     \t [4.7869921  1.40198461]. \t  -15.806337438195051 \t -0.05857055433931272\n",
            "49     \t [0.98498249 0.79425871]. \t  -0.15336383505731177 \t -0.05857055433931272\n",
            "50     \t [-9.68754667  9.79829972]. \t  -81480.73107563582 \t -0.05857055433931272\n",
            "51     \t [ 2.40196759 -1.11106934]. \t  -1.9744864412390422 \t -0.05857055433931272\n",
            "52     \t [-1.2689238  -1.77401796]. \t  -119.55210110591288 \t -0.05857055433931272\n",
            "53     \t [-7.54571784  4.77818483]. \t  -5735.1731847284855 \t -0.05857055433931272\n",
            "54     \t [-0.81638481 -0.2880134 ]. \t  -5.229034166546592 \t -0.05857055433931272\n",
            "55     \t [-6.17192749 -0.29887605]. \t  -132.09630872203184 \t -0.05857055433931272\n",
            "56     \t [-0.8944667  -0.79780662]. \t  -12.984748011240098 \t -0.05857055433931272\n",
            "57     \t [-9.51984013 -4.71799834]. \t  -5951.06311065501 \t -0.05857055433931272\n",
            "58     \t [-7.30446077 -4.45298308]. \t  -4479.931410554043 \t -0.05857055433931272\n",
            "59     \t [3.50919049 0.79610868]. \t  -16.345689504051435 \t -0.05857055433931272\n",
            "60     \t [1.49521071 3.85263359]. \t  -1589.6380746331806 \t -0.05857055433931272\n",
            "61     \t [ 9.01563964 -8.25265235]. \t  -32422.354289695904 \t -0.05857055433931272\n",
            "62     \t [0.46431107 0.21764948]. \t  -0.5601243458933476 \t -0.05857055433931272\n",
            "63     \t [2.28876159 9.60659356]. \t  -66456.95186784181 \t -0.05857055433931272\n",
            "64     \t [ 8.06318215 -1.34429787]. \t  -89.47411813080859 \t -0.05857055433931272\n",
            "65     \t [7.82567942 8.46717967]. \t  -36799.93362102439 \t -0.05857055433931272\n",
            "66     \t [0.00685211 6.65830377]. \t  -15721.881791517571 \t -0.05857055433931272\n",
            "67     \t [ 1.32890142 -1.14151099]. \t  -3.3706212636673056 \t -0.05857055433931272\n",
            "68     \t [-8.75020178 -5.68207233]. \t  -10847.325259422847 \t -0.05857055433931272\n",
            "69     \t [-2.72271128 -6.53641866]. \t  -15562.549628196019 \t -0.05857055433931272\n",
            "70     \t [ 6.33822363 -6.12437893]. \t  -9461.780082440946 \t -0.05857055433931272\n",
            "71     \t [-9.01697013 -4.22873828]. \t  -4111.091740220923 \t -0.05857055433931272\n",
            "72     \t [1.72739661 2.71616151]. \t  -339.96948038768585 \t -0.05857055433931272\n",
            "73     \t [2.53221122 1.54358743]. \t  -12.3212592951994 \t -0.05857055433931272\n",
            "74     \t [ 0.57639002 -0.38361329]. \t  -0.3385743156929134 \t -0.05857055433931272\n",
            "75     \t [ 0.67615036 -0.89514612]. \t  -1.8213969141604853 \t -0.05857055433931272\n",
            "76     \t [-4.31190326  4.05092865]. \t  -2785.779603738259 \t -0.05857055433931272\n",
            "77     \t [ 2.51854328 -1.65411194]. \t  -19.753826243047307 \t -0.05857055433931272\n",
            "78     \t [-3.40110708 -5.96485396]. \t  -11137.778595501333 \t -0.05857055433931272\n",
            "79     \t [-2.28071149 -6.41422133]. \t  -14313.303102479305 \t -0.05857055433931272\n",
            "80     \t [ 0.267685   -6.28865747]. \t  -12427.84636356615 \t -0.05857055433931272\n",
            "81     \t [ 1.05834657 -0.66554439]. \t  -0.06288087702130596 \t -0.05857055433931272\n",
            "82     \t [-9.04863361  1.2583201 ]. \t  -399.40569125181196 \t -0.05857055433931272\n",
            "83     \t [ 8.26840316 -2.99018816]. \t  -237.68949699227193 \t -0.05857055433931272\n",
            "84     \t [-3.18441793 -4.2986316 ]. \t  -3240.090681636895 \t -0.05857055433931272\n",
            "85     \t [-2.56593163  1.55682023]. \t  -122.63020144000545 \t -0.05857055433931272\n",
            "86     \t [8.73188655 3.77820009]. \t  -845.2649612317308 \t -0.05857055433931272\n",
            "87     \t [1.9901395  1.41484557]. \t  -9.088229006468957 \t -0.05857055433931272\n",
            "88     \t [-6.99691523  3.70409887]. \t  -2435.848917227575 \t -0.05857055433931272\n",
            "89     \t [-4.54889154 -3.53520937]. \t  -1776.5217755378185 \t -0.05857055433931272\n",
            "90     \t [5.57518348 7.26651834]. \t  -20032.691716494577 \t -0.05857055433931272\n",
            "91     \t [-0.72336016  1.23215094]. \t  -31.24144084408041 \t -0.05857055433931272\n",
            "92     \t [-5.34625143 -7.38849223]. \t  -26272.63213176849 \t -0.05857055433931272\n",
            "93     \t [ 4.34379926 -2.21067563]. \t  -70.158921569656 \t -0.05857055433931272\n",
            "94     \t [-6.10772408 -4.44348195]. \t  -4208.653841123318 \t -0.05857055433931272\n",
            "95     \t [-5.83204081 -5.12141147]. \t  -6842.067098723277 \t -0.05857055433931272\n",
            "96     \t [ 9.69587589 -7.31644498]. \t  -19035.44955913957 \t -0.05857055433931272\n",
            "97     \t [-5.07031387 -4.65553508]. \t  -4725.5175973484165 \t -0.05857055433931272\n",
            "98     \t [-3.20025038  3.94954699]. \t  -2384.099734581828 \t -0.05857055433931272\n",
            "99     \t [-2.21002255  4.40918558]. \t  -3387.3857635547797 \t -0.05857055433931272\n",
            "100    \t [ 5.85237815 -0.64502844]. \t  -73.95148943738262 \t -0.05857055433931272\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Aur-aRtdq-",
        "outputId": "ba8680c1-f54d-475d-c47f-2906680fee0b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_loser_10 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_10 = d2GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.07179171 -7.69986114]. \t  -26686.60864226137 \t -217.28346804397978\n",
            "init   \t [ 9.00565729 -0.35617197]. \t  -217.28346804397978 \t -217.28346804397978\n",
            "init   \t [ 7.4494907  -5.75334638]. \t  -6945.3080829060855 \t -217.28346804397978\n",
            "init   \t [-9.1858075  -2.05611077]. \t  -726.1597680698058 \t -217.28346804397978\n",
            "init   \t [-5.33735605  6.83481449]. \t  -19549.897628464034 \t -217.28346804397978\n",
            "1      \t [7.70674409 9.05288798]. \t  -48843.620887164754 \t -217.28346804397978\n",
            "2      \t [-15.25611937 -23.05652361]. \t  -2326427.769537942 \t -217.28346804397978\n",
            "3      \t [1.36988239 1.21701407]. \t  \u001b[92m-5.2080597150791546\u001b[0m \t -5.2080597150791546\n",
            "4      \t [-6.09332056 -9.94318576]. \t  -83141.37562406182 \t -5.2080597150791546\n",
            "5      \t [-13.31372023 -10.92037738]. \t  -127034.53367053602 \t -5.2080597150791546\n",
            "6      \t [-12.32578431   5.3295012 ]. \t  -9736.306173457677 \t -5.2080597150791546\n",
            "7      \t [-2.54930645 -3.07434066]. \t  -933.0125798751727 \t -5.2080597150791546\n",
            "8      \t [1.24965186 7.03532041]. \t  -19106.987039786436 \t -5.2080597150791546\n",
            "9      \t [  3.33057403 -14.79113224]. \t  -377107.4589717389 \t -5.2080597150791546\n",
            "10     \t [-5.23559953  1.25325459]. \t  -179.22739744816857 \t -5.2080597150791546\n",
            "11     \t [-24.81991314 -23.82318355]. \t  -2691440.0907384963 \t -5.2080597150791546\n",
            "12     \t [ 4.22877326 -1.43039514]. \t  -10.462357531639375 \t -5.2080597150791546\n",
            "13     \t [5.51544221 3.75025185]. \t  -1043.115120657744 \t -5.2080597150791546\n",
            "14     \t [9.54236899 3.4799621 ]. \t  -503.85377059445585 \t -5.2080597150791546\n",
            "15     \t [  8.15883745 -10.95854986]. \t  -107718.54550968278 \t -5.2080597150791546\n",
            "16     \t [-10.58671083  -6.27140216]. \t  -16064.548947643812 \t -5.2080597150791546\n",
            "17     \t [-9.94821397  9.81261826]. \t  -82151.13071974964 \t -5.2080597150791546\n",
            "18     \t [ 1.6836885  -2.42491622]. \t  -203.54916287883756 \t -5.2080597150791546\n",
            "19     \t [-1.61008651 -8.12689208]. \t  -35759.709458312296 \t -5.2080597150791546\n",
            "20     \t [-1.07723556  1.77179767]. \t  -112.52959781343904 \t -5.2080597150791546\n",
            "21     \t [-8.86272635  1.69591828]. \t  -524.4700543909037 \t -5.2080597150791546\n",
            "22     \t [1.47436007 2.08092455]. \t  -103.50605803636999 \t -5.2080597150791546\n",
            "23     \t [1.48498074 0.10928219]. \t  \u001b[92m-4.5048067157798375\u001b[0m \t -4.5048067157798375\n",
            "24     \t [1.31647724 0.60330654]. \t  \u001b[92m-0.7928686392780588\u001b[0m \t -0.7928686392780588\n",
            "25     \t [5.92758943 0.03200877]. \t  -94.50519339523548 \t -0.7928686392780588\n",
            "26     \t [3.14639543 0.8992353 ]. \t  -9.283595521489193 \t -0.7928686392780588\n",
            "27     \t [1.94145525 0.59178404]. \t  -3.9666913147840033 \t -0.7928686392780588\n",
            "28     \t [-5.93047362 -4.08043286]. \t  -3126.0703549650893 \t -0.7928686392780588\n",
            "29     \t [-2.4411987   9.96635689]. \t  -80892.44248158827 \t -0.7928686392780588\n",
            "30     \t [ 4.37461865 -3.04172198]. \t  -410.67509251468584 \t -0.7928686392780588\n",
            "31     \t [ 4.10201641 -0.5376862 ]. \t  -34.45688821842011 \t -0.7928686392780588\n",
            "32     \t [-1.55242068  4.02672546]. \t  -2315.993619530783 \t -0.7928686392780588\n",
            "33     \t [-2.10660267  0.72098866]. \t  -29.44878295538978 \t -0.7928686392780588\n",
            "34     \t [0.88073479 0.94285716]. \t  -1.624247685324456 \t -0.7928686392780588\n",
            "35     \t [-0.50780535 -0.26290934]. \t  -3.1082329756069855 \t -0.7928686392780588\n",
            "36     \t [-0.64239201  0.3027251 ]. \t  -4.060936518602019 \t -0.7928686392780588\n",
            "37     \t [-0.54113752  0.57541392]. \t  -5.271158655725608 \t -0.7928686392780588\n",
            "38     \t [-1.20376593  0.77635104]. \t  -16.46514859501741 \t -0.7928686392780588\n",
            "39     \t [3.24178211 1.61151147]. \t  -12.647415456204175 \t -0.7928686392780588\n",
            "40     \t [-1.30140785 -0.34567534]. \t  -10.042085244814903 \t -0.7928686392780588\n",
            "41     \t [ 9.8356841  -3.07346653]. \t  -242.11726431745427 \t -0.7928686392780588\n",
            "42     \t [ 3.13526747 -0.79955969]. \t  -11.453859276511249 \t -0.7928686392780588\n",
            "43     \t [ 6.54051464 -1.86523814]. \t  -31.046269103610765 \t -0.7928686392780588\n",
            "44     \t [ 0.37173045 -0.33724089]. \t  \u001b[92m-0.4363489168057352\u001b[0m \t -0.4363489168057352\n",
            "45     \t [ 0.60710758 -0.22506819]. \t  -0.6660240443850638 \t -0.4363489168057352\n",
            "46     \t [-8.4873731  4.8505468]. \t  -6260.055858579166 \t -0.4363489168057352\n",
            "47     \t [-0.3634558   0.08098012]. \t  -2.1426237164615056 \t -0.4363489168057352\n",
            "48     \t [ 1.01870563 -0.71130785]. \t  \u001b[92m-0.000442052499338034\u001b[0m \t -0.000442052499338034\n",
            "49     \t [ 1.65413147 -0.56809754]. \t  -2.4626854038665797 \t -0.000442052499338034\n",
            "50     \t [5.48219774 3.88996803]. \t  -1248.3360607294771 \t -0.000442052499338034\n",
            "51     \t [4.51493231 2.3815716 ]. \t  -105.62070416922616 \t -0.000442052499338034\n",
            "52     \t [5.80760198 1.38966903]. \t  -30.680969393564517 \t -0.000442052499338034\n",
            "53     \t [5.99859689 2.31612087]. \t  -69.73621458104004 \t -0.000442052499338034\n",
            "54     \t [ 0.72766745 -0.28826936]. \t  -0.7046598830045853 \t -0.000442052499338034\n",
            "55     \t [2.80077054 1.12389109]. \t  -3.393484006632173 \t -0.000442052499338034\n",
            "56     \t [-3.76757267  0.39468033]. \t  -56.00815295654134 \t -0.000442052499338034\n",
            "57     \t [ 6.76050866 -2.05206792]. \t  -38.70433769107767 \t -0.000442052499338034\n",
            "58     \t [4.84023392 1.31613956]. \t  -18.532977520979344 \t -0.000442052499338034\n",
            "59     \t [ 0.3249369  -4.31804827]. \t  -2733.447425327223 \t -0.000442052499338034\n",
            "60     \t [-6.31918975 -0.51484084]. \t  -147.39668670073516 \t -0.000442052499338034\n",
            "61     \t [-8.11865691 -7.58792165]. \t  -30475.006856139753 \t -0.000442052499338034\n",
            "62     \t [1.5259366  0.89554043]. \t  -0.2887925157654918 \t -0.000442052499338034\n",
            "63     \t [ 1.51171163 -0.78200951]. \t  -0.4284678478953032 \t -0.000442052499338034\n",
            "64     \t [2.30669218 5.53075925]. \t  -6933.509486673679 \t -0.000442052499338034\n",
            "65     \t [ 2.642255   -1.44058112]. \t  -7.246896560299004 \t -0.000442052499338034\n",
            "66     \t [ 3.6629799  -1.07712952]. \t  -10.696417574329054 \t -0.000442052499338034\n",
            "67     \t [ 1.36710345 -0.84054689]. \t  -0.13898493565453612 \t -0.000442052499338034\n",
            "68     \t [ 2.3753554  -1.17126276]. \t  -2.1629769994058146 \t -0.000442052499338034\n",
            "69     \t [ 7.38047154 -9.25721189]. \t  -53840.23648129998 \t -0.000442052499338034\n",
            "70     \t [ 5.53014797 -1.52113567]. \t  -22.15103832611234 \t -0.000442052499338034\n",
            "71     \t [1.27292744 0.82576303]. \t  -0.09099382248244976 \t -0.000442052499338034\n",
            "72     \t [ 1.50812225 -1.30782685]. \t  -7.575030105411535 \t -0.000442052499338034\n",
            "73     \t [ 4.79010638 -1.18871613]. \t  -22.079610760707066 \t -0.000442052499338034\n",
            "74     \t [9.94231219 3.27114541]. \t  -342.55812679072415 \t -0.000442052499338034\n",
            "75     \t [-8.46680573  8.44266127]. \t  -45706.03566664887 \t -0.000442052499338034\n",
            "76     \t [ 3.25293074 -5.22260964]. \t  -5268.121662610138 \t -0.000442052499338034\n",
            "77     \t [-9.60702549  8.65089734]. \t  -50854.704943960656 \t -0.000442052499338034\n",
            "78     \t [ 1.43472543 -8.73045771]. \t  -45606.45220107105 \t -0.000442052499338034\n",
            "79     \t [5.34550126 0.93888375]. \t  -44.551934510489104 \t -0.000442052499338034\n",
            "80     \t [-9.36547476  2.02581118]. \t  -725.0845732068685 \t -0.000442052499338034\n",
            "81     \t [ 0.20326007 -0.8050308 ]. \t  -3.0236077481688683 \t -0.000442052499338034\n",
            "82     \t [6.82596773 5.53661569]. \t  -5970.574953084083 \t -0.000442052499338034\n",
            "83     \t [ 2.00780911 -1.00237165]. \t  -1.0156849059458342 \t -0.000442052499338034\n",
            "84     \t [-9.84186936 -0.80085221]. \t  -365.05948714640203 \t -0.000442052499338034\n",
            "85     \t [ 8.13288047 -6.31727247]. \t  -10327.7779569444 \t -0.000442052499338034\n",
            "86     \t [ 1.09998098 -8.34432714]. \t  -38173.95512799832 \t -0.000442052499338034\n",
            "87     \t [6.84783286 1.11760082]. \t  -72.03814129887033 \t -0.000442052499338034\n",
            "88     \t [ 7.11878012 -1.20910938]. \t  -72.63365938492225 \t -0.000442052499338034\n",
            "89     \t [-8.17943794 -0.93216136]. \t  -280.967265407088 \t -0.000442052499338034\n",
            "90     \t [3.69377315 9.72531409]. \t  -68805.27204374927 \t -0.000442052499338034\n",
            "91     \t [ 2.25504466 -3.58192906]. \t  -1097.1994125105232 \t -0.000442052499338034\n",
            "92     \t [3.84302816 0.29679479]. \t  -34.97444363718047 \t -0.000442052499338034\n",
            "93     \t [5.81368138 7.99447649]. \t  -29795.869878511236 \t -0.000442052499338034\n",
            "94     \t [ 2.1995583  -9.43301258]. \t  -61787.33215146634 \t -0.000442052499338034\n",
            "95     \t [-1.41958965 -0.15751026]. \t  -10.171562020910802 \t -0.000442052499338034\n",
            "96     \t [-3.95788396  5.24743984]. \t  -6993.457889775188 \t -0.000442052499338034\n",
            "97     \t [-3.87570309  8.8357695 ]. \t  -51234.876069122845 \t -0.000442052499338034\n",
            "98     \t [-3.18917536 -1.71868298]. \t  -183.05701438345855 \t -0.000442052499338034\n",
            "99     \t [-2.92348034 -0.27284857]. \t  -34.2726500274852 \t -0.000442052499338034\n",
            "100    \t [-4.26573344 -0.88641983]. \t  -95.87407536021895 \t -0.000442052499338034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMu7U9NNtdtn",
        "outputId": "191ac504-ddeb-4377-ea80-d9916b8815c3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_loser_11 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_11 = d2GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [9.62071325 6.42495699]. \t  -10714.638013792866 \t -9.740000060755142\n",
            "init   \t [ 2.91787228 -1.52623973]. \t  -9.740000060755142 \t -9.740000060755142\n",
            "init   \t [-5.95378042 -0.16186459]. \t  -120.5034756901465 \t -9.740000060755142\n",
            "init   \t [-7.22883337 -0.94957929]. \t  -230.87623811313406 \t -9.740000060755142\n",
            "init   \t [-7.72530745 -9.94151023]. \t  -84448.33746856163 \t -9.740000060755142\n",
            "1      \t [ 9.71879818 -6.86089495]. \t  -14331.165445255861 \t -9.740000060755142\n",
            "2      \t [-0.41525252  9.69422331]. \t  -70969.40865155176 \t -9.740000060755142\n",
            "3      \t [-9.5194484  9.1420568]. \t  -62537.95985021731 \t -9.740000060755142\n",
            "4      \t [ 0.94955298 -9.31626642]. \t  -59606.43954696483 \t -9.740000060755142\n",
            "5      \t [-13.42939903  -2.71608082]. \t  -1796.8369371408419 \t -9.740000060755142\n",
            "6      \t [-30.4257063 -34.970082 ]. \t  -12264506.515856808 \t -9.740000060755142\n",
            "7      \t [ 8.49048764 -0.19399542]. \t  -197.73923329838743 \t -9.740000060755142\n",
            "8      \t [-0.93565485  2.80807474]. \t  -561.942488462207 \t -9.740000060755142\n",
            "9      \t [-11.44062611   2.84834442]. \t  -1685.6681675814116 \t -9.740000060755142\n",
            "10     \t [-1.83453863 -3.36281919]. \t  -1203.800439457558 \t -9.740000060755142\n",
            "11     \t [3.80487189 5.14344458]. \t  -4830.503211479563 \t -9.740000060755142\n",
            "12     \t [ 4.86315383 -5.28179033]. \t  -5202.952094226745 \t -9.740000060755142\n",
            "13     \t [-4.90683956  5.94357573]. \t  -11453.223121939483 \t -9.740000060755142\n",
            "14     \t [-9.24242595 -5.07926204]. \t  -7507.97487803886 \t -9.740000060755142\n",
            "15     \t [-15.28890667 -18.61717135]. \t  -1004173.1143073527 \t -9.740000060755142\n",
            "16     \t [4.93743793 0.47992033]. \t  -55.586730611777675 \t -9.740000060755142\n",
            "17     \t [5.24640175 9.84084534]. \t  -71035.84612395246 \t -9.740000060755142\n",
            "18     \t [ 1.24247459 -0.14679882]. \t  \u001b[92m-2.935793768536829\u001b[0m \t -2.935793768536829\n",
            "19     \t [  6.0795186  -10.39082813]. \t  -88107.48525261377 \t -2.935793768536829\n",
            "20     \t [-4.99105109 -5.25851406]. \t  -7306.864663801201 \t -2.935793768536829\n",
            "21     \t [1.69220352 0.61311038]. \t  \u001b[92m-2.247830625816612\u001b[0m \t -2.247830625816612\n",
            "22     \t [-3.5787386   0.06074801]. \t  -46.685349674750675 \t -2.247830625816612\n",
            "23     \t [6.73920919 2.57971789]. \t  -119.2861815459801 \t -2.247830625816612\n",
            "24     \t [-8.09405974  3.165073  ]. \t  -1665.2320345594542 \t -2.247830625816612\n",
            "25     \t [1.80824395 0.86910591]. \t  \u001b[92m-0.830334799634687\u001b[0m \t -0.830334799634687\n",
            "26     \t [1.87079499 1.19411038]. \t  -2.683022385928261 \t -0.830334799634687\n",
            "27     \t [2.41978629 0.35417106]. \t  -11.424151773197686 \t -0.830334799634687\n",
            "28     \t [ 2.11103866 -1.799357  ]. \t  -39.329204245967176 \t -0.830334799634687\n",
            "29     \t [ 5.14182964 -1.39003463]. \t  -20.418443676855386 \t -0.830334799634687\n",
            "30     \t [ 0.11492233 -0.19005244]. \t  \u001b[92m-0.7870060760640734\u001b[0m \t -0.7870060760640734\n",
            "31     \t [-9.94522097 -0.65158699]. \t  -352.83394027924084 \t -0.7870060760640734\n",
            "32     \t [ 3.33727079 -1.36661083]. \t  -5.779610152409278 \t -0.7870060760640734\n",
            "33     \t [-4.41086062  1.67943638]. \t  -231.3577428847595 \t -0.7870060760640734\n",
            "34     \t [ 0.20107465 -0.59627987]. \t  -1.1585321581818193 \t -0.7870060760640734\n",
            "35     \t [ 9.36615136 -3.14755016]. \t  -288.3135879525186 \t -0.7870060760640734\n",
            "36     \t [-3.42738622 -8.83248316]. \t  -50870.05885384415 \t -0.7870060760640734\n",
            "37     \t [-0.34840984 -0.39820313]. \t  -2.7040995724777024 \t -0.7870060760640734\n",
            "38     \t [2.25145657 1.19414518]. \t  -2.2873652842691294 \t -0.7870060760640734\n",
            "39     \t [-5.25358867  9.68630861]. \t  -74462.0396231826 \t -0.7870060760640734\n",
            "40     \t [ 4.07954436 -1.59769352]. \t  -11.58773412206113 \t -0.7870060760640734\n",
            "41     \t [-2.53182558 -0.79020428]. \t  -41.06074099944064 \t -0.7870060760640734\n",
            "42     \t [-4.54756724 -1.76204974]. \t  -262.2104568959321 \t -0.7870060760640734\n",
            "43     \t [ 6.4025268  -1.69787445]. \t  -29.998761195228397 \t -0.7870060760640734\n",
            "44     \t [ 0.47490937 -0.36091606]. \t  \u001b[92m-0.3676450862009029\u001b[0m \t -0.3676450862009029\n",
            "45     \t [-1.7687581  0.5638483]. \t  -19.230299812700427 \t -0.3676450862009029\n",
            "46     \t [ 0.38019515 -0.75350278]. \t  -1.5252282444465597 \t -0.3676450862009029\n",
            "47     \t [ 0.67758561 -0.50797742]. \t  \u001b[92m-0.15611780076293735\u001b[0m \t -0.15611780076293735\n",
            "48     \t [-2.82695935 -5.75623563]. \t  -9563.009858686777 \t -0.15611780076293735\n",
            "49     \t [9.47038186 2.22423336]. \t  -72.10699928438305 \t -0.15611780076293735\n",
            "50     \t [-0.38202811 -0.08194416]. \t  -2.222775432715206 \t -0.15611780076293735\n",
            "51     \t [ 1.26913823 -3.74176847]. \t  -1429.3281577514304 \t -0.15611780076293735\n",
            "52     \t [ 1.53432015 -1.01035666]. \t  -0.8002472121981175 \t -0.15611780076293735\n",
            "53     \t [-2.36433395 -0.2277325 ]. \t  -23.501364856204273 \t -0.15611780076293735\n",
            "54     \t [ 4.31002081 -1.08189392]. \t  -18.710411026093894 \t -0.15611780076293735\n",
            "55     \t [ 5.53284339 -8.2539833 ]. \t  -34197.885286768586 \t -0.15611780076293735\n",
            "56     \t [-0.23000497 -8.42508835]. \t  -40439.91226306904 \t -0.15611780076293735\n",
            "57     \t [-0.88657591  2.10122396]. \t  -192.39391201053718 \t -0.15611780076293735\n",
            "58     \t [4.09868526 1.58938879]. \t  -11.420663851959533 \t -0.15611780076293735\n",
            "59     \t [0.67519118 0.69616347]. \t  -0.27848565286067795 \t -0.15611780076293735\n",
            "60     \t [0.16913564 1.24693191]. \t  -17.983918447586262 \t -0.15611780076293735\n",
            "61     \t [-7.87187674 -4.59603992]. \t  -5102.547071025583 \t -0.15611780076293735\n",
            "62     \t [-0.50260382  0.36668725]. \t  -3.4483133808556765 \t -0.15611780076293735\n",
            "63     \t [3.22732899 1.01580301]. \t  -7.669005702117132 \t -0.15611780076293735\n",
            "64     \t [-1.18879403  1.0196727 ]. \t  -26.153851426825078 \t -0.15611780076293735\n",
            "65     \t [ 0.93973681 -0.12912926]. \t  -1.6467103361946283 \t -0.15611780076293735\n",
            "66     \t [1.97432265 2.53632665]. \t  -238.20247058643878 \t -0.15611780076293735\n",
            "67     \t [0.16099158 0.89012971]. \t  -4.757608938674867 \t -0.15611780076293735\n",
            "68     \t [ 2.10734891 -0.67162117]. \t  -4.13123039575514 \t -0.15611780076293735\n",
            "69     \t [ 3.35980611 -1.0829861 ]. \t  -7.625435078763864 \t -0.15611780076293735\n",
            "70     \t [1.0003115  0.90210071]. \t  -0.7869100675756836 \t -0.15611780076293735\n",
            "71     \t [-1.2800157  -0.45151021]. \t  -10.89539509195281 \t -0.15611780076293735\n",
            "72     \t [1.26688474 0.82203192]. \t  \u001b[92m-0.08553779603923299\u001b[0m \t -0.08553779603923299\n",
            "73     \t [ 2.32782963 -5.9831748 ]. \t  -9598.132944240877 \t -0.08553779603923299\n",
            "74     \t [4.63518173 0.35127548]. \t  -51.730529059530966 \t -0.08553779603923299\n",
            "75     \t [ 2.0286068  -1.08649317]. \t  -1.278915759527782 \t -0.08553779603923299\n",
            "76     \t [-0.05913795  0.47833897]. \t  -1.6558431734022214 \t -0.08553779603923299\n",
            "77     \t [ 9.67852138 -2.76893952]. \t  -139.28678989564332 \t -0.08553779603923299\n",
            "78     \t [ 1.05483998 -0.73713101]. \t  \u001b[92m-0.005040638158741799\u001b[0m \t -0.005040638158741799\n",
            "79     \t [-0.69810267 -7.36598808]. \t  -23858.13215841738 \t -0.005040638158741799\n",
            "80     \t [ 9.96461037 -1.9507269 ]. \t  -91.44630152846665 \t -0.005040638158741799\n",
            "81     \t [ 1.39442669 -4.26455993]. \t  -2447.1487452458628 \t -0.005040638158741799\n",
            "82     \t [2.74498008 6.90529164]. \t  -17160.390579779105 \t -0.005040638158741799\n",
            "83     \t [8.75404465 3.15419183]. \t  -308.4941042225309 \t -0.005040638158741799\n",
            "84     \t [ 8.32326456 -5.33167961]. \t  -4764.020014189273 \t -0.005040638158741799\n",
            "85     \t [0.38153504 0.26450757]. \t  -0.4992463373231544 \t -0.005040638158741799\n",
            "86     \t [9.92326249 1.50576109]. \t  -137.69927087117713 \t -0.005040638158741799\n",
            "87     \t [-9.81419467  4.16506142]. \t  -4079.167887251405 \t -0.005040638158741799\n",
            "88     \t [-4.3145491  -3.64523851]. \t  -1936.638108318239 \t -0.005040638158741799\n",
            "89     \t [9.49820215 9.93517295]. \t  -70697.90824820579 \t -0.005040638158741799\n",
            "90     \t [-2.07678822  8.62162833]. \t  -45455.60988660681 \t -0.005040638158741799\n",
            "91     \t [-1.13533016 -4.2962465 ]. \t  -2900.286107739129 \t -0.005040638158741799\n",
            "92     \t [-1.36834628  4.22089697]. \t  -2743.651788682231 \t -0.005040638158741799\n",
            "93     \t [ 3.08294269 -8.08402201]. \t  -32578.006703472063 \t -0.005040638158741799\n",
            "94     \t [7.61503009 3.48124151]. \t  -596.4105271155338 \t -0.005040638158741799\n",
            "95     \t [5.56094173 2.21288963]. \t  -56.63570762012374 \t -0.005040638158741799\n",
            "96     \t [3.4338222  1.40468789]. \t  -6.448749544123398 \t -0.005040638158741799\n",
            "97     \t [ 7.47631853 -0.76898239]. \t  -121.16277987722734 \t -0.005040638158741799\n",
            "98     \t [-0.21372332  5.24303817]. \t  -6093.924112011358 \t -0.005040638158741799\n",
            "99     \t [6.84978975 1.46795829]. \t  -47.12310505709362 \t -0.005040638158741799\n",
            "100    \t [ 9.75240598 -3.36757363]. \t  -410.9070903804599 \t -0.005040638158741799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjKllHqtdwW",
        "outputId": "ad9ebe57-2cd6-4f40-b169-f0966697fe6e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_loser_12 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_12 = d2GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-6.16961099  2.44217542]. \t  -706.482335023616 \t -706.482335023616\n",
            "init   \t [-1.24544522  5.70717167]. \t  -8820.05719953671 \t -706.482335023616\n",
            "init   \t [ 5.59951616 -4.54814789]. \t  -2580.3963775443385 \t -706.482335023616\n",
            "init   \t [-4.4707149   6.03744355]. \t  -12002.832292900235 \t -706.482335023616\n",
            "init   \t [9.16278707 7.51865269]. \t  -21656.00779050222 \t -706.482335023616\n",
            "1      \t [-8.80381554 -6.31425832]. \t  -15776.028882530063 \t -706.482335023616\n",
            "2      \t [-2.10622833 -9.37900803]. \t  -63404.573438939275 \t -706.482335023616\n",
            "3      \t [-83.49822185 -73.61332398]. \t  -238558525.23009148 \t -706.482335023616\n",
            "4      \t [-21.65008419 -14.3878703 ]. \t  -380132.7002112589 \t -706.482335023616\n",
            "5      \t [-12.50100416   0.61250738]. \t  \u001b[92m-533.4728585449686\u001b[0m \t -533.4728585449686\n",
            "6      \t [-0.91777906 -1.83475346]. \t  \u001b[92m-120.73571628803775\u001b[0m \t -120.73571628803775\n",
            "7      \t [-11.94113053 -12.7024045 ]. \t  -224139.73656201683 \t -120.73571628803775\n",
            "8      \t [  7.39047826 -10.97607442]. \t  -109139.4620138441 \t -120.73571628803775\n",
            "9      \t [-21.27263264 -22.73633954]. \t  -2227199.812784498 \t -120.73571628803775\n",
            "10     \t [-11.1942074   7.6712936]. \t  -33374.83851113447 \t -120.73571628803775\n",
            "11     \t [4.56993219 2.3320827 ]. \t  \u001b[92m-92.30816127407111\u001b[0m \t -92.30816127407111\n",
            "12     \t [8.90595546 0.41689246]. \t  -208.99507419761974 \t -92.30816127407111\n",
            "13     \t [3.81060723 7.05653583]. \t  -18355.05216300961 \t -92.30816127407111\n",
            "14     \t [-5.01962349 -2.86264476]. \t  -952.9344376453503 \t -92.30816127407111\n",
            "15     \t [-34.27656754 -35.59135544]. \t  -13188086.138494762 \t -92.30816127407111\n",
            "16     \t [-16.69940899 -18.57535964]. \t  -999410.0716183715 \t -92.30816127407111\n",
            "17     \t [-27.16754368 -27.74758715]. \t  -4911926.41165045 \t -92.30816127407111\n",
            "18     \t [-15.94555069  -6.49930666]. \t  -20458.527171087047 \t -92.30816127407111\n",
            "19     \t [ 2.59951443 -1.15942121]. \t  \u001b[92m-2.574288631529033\u001b[0m \t -2.574288631529033\n",
            "20     \t [ 0.83524952 -4.82263016]. \t  -4173.401985650753 \t -2.574288631529033\n",
            "21     \t [ 9.80385012 -3.66204039]. \t  -656.6799799228362 \t -2.574288631529033\n",
            "22     \t [0.41164028 1.40764708]. \t  -25.569634792920606 \t -2.574288631529033\n",
            "23     \t [ 3.02736634 -8.60907648]. \t  -42173.11120502375 \t -2.574288631529033\n",
            "24     \t [-8.76053262 -1.24693435]. \t  -377.0723940853243 \t -2.574288631529033\n",
            "25     \t [-2.79976767  1.32577054]. \t  -94.1992792109429 \t -2.574288631529033\n",
            "26     \t [-9.71378829  3.36471435]. \t  -2208.657680014171 \t -2.574288631529033\n",
            "27     \t [ 3.84635456 -0.75894838]. \t  -22.62077042786802 \t -2.574288631529033\n",
            "28     \t [-2.1550919   9.79142082]. \t  -75203.54889502253 \t -2.574288631529033\n",
            "29     \t [ 1.30303054 -0.68242502]. \t  \u001b[92m-0.36803440147714755\u001b[0m \t -0.36803440147714755\n",
            "30     \t [8.30726276 3.35485216]. \t  -456.835330498602 \t -0.36803440147714755\n",
            "31     \t [2.38560175 0.61238765]. \t  -7.2700345066352465 \t -0.36803440147714755\n",
            "32     \t [0.47081769 0.28791741]. \t  -0.4661142043144742 \t -0.36803440147714755\n",
            "33     \t [6.27237399 0.34585011]. \t  -100.59571092523811 \t -0.36803440147714755\n",
            "34     \t [1.55684435 2.61619308]. \t  -294.6851990140005 \t -0.36803440147714755\n",
            "35     \t [-4.55889339 -5.94489775]. \t  -11353.772515528482 \t -0.36803440147714755\n",
            "36     \t [1.51397997 0.05864757]. \t  -4.806881629888764 \t -0.36803440147714755\n",
            "37     \t [3.2611797  0.44444286]. \t  -21.54223029871821 \t -0.36803440147714755\n",
            "38     \t [-0.69103619  0.67198608]. \t  -7.942338781930578 \t -0.36803440147714755\n",
            "39     \t [-6.86405958  9.58526159]. \t  -72732.69823547796 \t -0.36803440147714755\n",
            "40     \t [ 8.3255585  -6.62808577]. \t  -12706.086385646855 \t -0.36803440147714755\n",
            "41     \t [ 2.03023734 -1.24810203]. \t  -3.4170544277389445 \t -0.36803440147714755\n",
            "42     \t [-6.82062611 -9.65198533]. \t  -74669.03325393294 \t -0.36803440147714755\n",
            "43     \t [-0.4269311   0.93074995]. \t  -11.36320310339419 \t -0.36803440147714755\n",
            "44     \t [ 9.6233783  -6.74246078]. \t  -13293.14962611658 \t -0.36803440147714755\n",
            "45     \t [ 6.38818665 -1.39574055]. \t  -41.45271613405559 \t -0.36803440147714755\n",
            "46     \t [ 0.16106039 -0.32524712]. \t  -0.7089223925176974 \t -0.36803440147714755\n",
            "47     \t [ 0.39359025 -0.50753292]. \t  -0.3973005863171114 \t -0.36803440147714755\n",
            "48     \t [ 1.23966921 -0.6307238 ]. \t  -0.45179180458815715 \t -0.36803440147714755\n",
            "49     \t [ 3.72603451 -1.53019535]. \t  -9.26281331271517 \t -0.36803440147714755\n",
            "50     \t [ 0.42410574 -0.16121444]. \t  -0.608609047374506 \t -0.36803440147714755\n",
            "51     \t [-2.30560633 -0.39849111]. \t  -24.689354688555653 \t -0.36803440147714755\n",
            "52     \t [ 3.12362364 -1.40164354]. \t  -5.8077136376483365 \t -0.36803440147714755\n",
            "53     \t [-0.24281543  0.08831361]. \t  -1.678145801874099 \t -0.36803440147714755\n",
            "54     \t [ 2.1335826  -1.06749967]. \t  -1.32736659080495 \t -0.36803440147714755\n",
            "55     \t [2.58485884 0.17125506]. \t  -15.275173187265553 \t -0.36803440147714755\n",
            "56     \t [ 8.87043805 -6.82608291]. \t  -14281.795986853876 \t -0.36803440147714755\n",
            "57     \t [1.5756336 0.5361869]. \t  -2.33391809291586 \t -0.36803440147714755\n",
            "58     \t [-0.8996596  -0.41278593]. \t  -6.686109513706473 \t -0.36803440147714755\n",
            "59     \t [-5.13610495  4.50565333]. \t  -4221.568063430106 \t -0.36803440147714755\n",
            "60     \t [-4.98991934 -0.07850062]. \t  -85.924024481949 \t -0.36803440147714755\n",
            "61     \t [-1.24885828  0.27865609]. \t  -9.000675746779773 \t -0.36803440147714755\n",
            "62     \t [3.3416869  8.35157673]. \t  -37082.38566533463 \t -0.36803440147714755\n",
            "63     \t [ 4.79446834 -1.39027191]. \t  -16.123166796405208 \t -0.36803440147714755\n",
            "64     \t [ 6.85791138 -6.5587892 ]. \t  -12572.4745327583 \t -0.36803440147714755\n",
            "65     \t [0.42545809 0.84415819]. \t  -2.329090627837269 \t -0.36803440147714755\n",
            "66     \t [ 0.65681684 -0.64620393]. \t  \u001b[92m-0.1813865615295248\u001b[0m \t -0.1813865615295248\n",
            "67     \t [0.99390865 0.40569914]. \t  -0.8837559307466968 \t -0.1813865615295248\n",
            "68     \t [-3.81265908 -4.99503952]. \t  -5793.441585001492 \t -0.1813865615295248\n",
            "69     \t [5.75408802 3.95036   ]. \t  -1318.6783370451064 \t -0.1813865615295248\n",
            "70     \t [3.28012838 3.6359738 ]. \t  -1078.0148941175532 \t -0.1813865615295248\n",
            "71     \t [5.05829476 1.21602719]. \t  -25.296901985951777 \t -0.1813865615295248\n",
            "72     \t [3.71758773 1.46561664]. \t  -8.05455327038622 \t -0.1813865615295248\n",
            "73     \t [-3.60580754  8.70584072]. \t  -48188.54724637447 \t -0.1813865615295248\n",
            "74     \t [4.38671998 1.52971845]. \t  -11.641989004998031 \t -0.1813865615295248\n",
            "75     \t [3.43072363 6.91320283]. \t  -16990.636376821025 \t -0.1813865615295248\n",
            "76     \t [6.63283614 3.72989125]. \t  -929.8747398348743 \t -0.1813865615295248\n",
            "77     \t [ 2.01271661 -6.01062813]. \t  -9869.066925274641 \t -0.1813865615295248\n",
            "78     \t [ 4.62301327 -5.15861912]. \t  -4736.985788552286 \t -0.1813865615295248\n",
            "79     \t [ 1.07596146 -0.63551883]. \t  \u001b[92m-0.1496252200584844\u001b[0m \t -0.1496252200584844\n",
            "80     \t [6.26398562 3.76637446]. \t  -1005.1632549001689 \t -0.1496252200584844\n",
            "81     \t [-0.28452549 -7.4087819 ]. \t  -24230.092896333244 \t -0.1496252200584844\n",
            "82     \t [3.05383398 0.86897819]. \t  -8.983560494324518 \t -0.1496252200584844\n",
            "83     \t [5.71979017 1.40724964]. \t  -28.46519373905457 \t -0.1496252200584844\n",
            "84     \t [ 1.17286731 -8.54177696]. \t  -41905.756991193644 \t -0.1496252200584844\n",
            "85     \t [5.37190763 1.68734372]. \t  -19.32139545757848 \t -0.1496252200584844\n",
            "86     \t [9.2733085  8.70085541]. \t  -40473.99489624034 \t -0.1496252200584844\n",
            "87     \t [ 6.6781321  -2.58651124]. \t  -122.07341521384448 \t -0.1496252200584844\n",
            "88     \t [-9.54284769 -3.55890536]. \t  -2543.6078971587644 \t -0.1496252200584844\n",
            "89     \t [9.88080675 3.8493994 ]. \t  -859.384405902526 \t -0.1496252200584844\n",
            "90     \t [-7.64447184  4.58158647]. \t  -5000.274323967172 \t -0.1496252200584844\n",
            "91     \t [-3.17673625 -2.51350093]. \t  -517.4907760011075 \t -0.1496252200584844\n",
            "92     \t [ 6.01484388 -3.34460012]. \t  -560.3075707518249 \t -0.1496252200584844\n",
            "93     \t [-2.84232082 -7.60664852]. \t  -28129.85622966966 \t -0.1496252200584844\n",
            "94     \t [ 8.14601407 -1.24864916]. \t  -101.62235127458007 \t -0.1496252200584844\n",
            "95     \t [ 7.38654134 -1.79812914]. \t  -42.48072689108267 \t -0.1496252200584844\n",
            "96     \t [ 5.11526129 -0.09034429]. \t  -68.93369481205232 \t -0.1496252200584844\n",
            "97     \t [4.90338108 9.29475615]. \t  -56383.70145988153 \t -0.1496252200584844\n",
            "98     \t [-6.1461776   5.99112053]. \t  -12198.24388708779 \t -0.1496252200584844\n",
            "99     \t [-4.9205641  -9.43264269]. \t  -66917.97104986853 \t -0.1496252200584844\n",
            "100    \t [-2.10756868 -2.72354809]. \t  -583.7872836272707 \t -0.1496252200584844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoONg4VEtdy-",
        "outputId": "e4017a0e-4b31-4370-f072-f7e13beb914b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_loser_13 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_13 = d2GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.21288618 -1.72798696]. \t  -20.120935450808645 \t -20.120935450808645\n",
            "init   \t [ 3.36205116 -5.40113156]. \t  -6051.706374022101 \t -20.120935450808645\n",
            "init   \t [6.15356672 2.73636929]. \t  -182.2099268583363 \t -20.120935450808645\n",
            "init   \t [-6.55612296 -4.79228264]. \t  -5567.089701718418 \t -20.120935450808645\n",
            "init   \t [ 8.30639981 -0.74368981]. \t  -157.07069920616067 \t -20.120935450808645\n",
            "1      \t [-7.5961809  7.6177101]. \t  -30655.10899619148 \t -20.120935450808645\n",
            "2      \t [9.21200838 9.90573549]. \t  -70031.75643065354 \t -20.120935450808645\n",
            "3      \t [-0.35482891  6.84457911]. \t  -17693.150381619864 \t -20.120935450808645\n",
            "4      \t [  1.95769685 -12.19506591]. \t  -174619.63161434853 \t -20.120935450808645\n",
            "5      \t [-11.9643305  -15.59332468]. \t  -496709.83311336255 \t -20.120935450808645\n",
            "6      \t [-22.77369066 -26.42302672]. \t  -4028404.509187324 \t -20.120935450808645\n",
            "7      \t [ 9.52302423 -8.71140556]. \t  -40545.127961554834 \t -20.120935450808645\n",
            "8      \t [-10.69102118   1.07360333]. \t  -474.48601337183226 \t -20.120935450808645\n",
            "9      \t [-13.35917962  -7.01077322]. \t  -25142.57149147708 \t -20.120935450808645\n",
            "10     \t [-2.31617433  0.30042063]. \t  -23.463828530537207 \t -20.120935450808645\n",
            "11     \t [ -5.50256985 -11.63794115]. \t  -152820.63484752472 \t -20.120935450808645\n",
            "12     \t [-5.93071029  2.22222066]. \t  -547.7723959592485 \t -20.120935450808645\n",
            "13     \t [-1.83352863 -6.66349998]. \t  -16438.520278525288 \t -20.120935450808645\n",
            "14     \t [1.97386237 1.54260487]. \t  \u001b[92m-16.465283083000184\u001b[0m \t -16.465283083000184\n",
            "15     \t [4.71638699 7.04000936]. \t  -17839.203779468306 \t -16.465283083000184\n",
            "16     \t [-10.46673522  -2.93656592]. \t  -1667.5703889123708 \t -16.465283083000184\n",
            "17     \t [ 0.19753514 -1.15056144]. \t  \u001b[92m-12.649421222072922\u001b[0m \t -12.649421222072922\n",
            "18     \t [-49.61973046 -48.75792787]. \t  -46164982.36181571 \t -12.649421222072922\n",
            "19     \t [-2.971555   -2.20495059]. \t  -338.1078904537743 \t -12.649421222072922\n",
            "20     \t [9.32525675 4.27502511]. \t  -1551.8660816935121 \t -12.649421222072922\n",
            "21     \t [ 7.34447518 -3.72975327]. \t  -878.9201542551558 \t -12.649421222072922\n",
            "22     \t [-0.99270955  2.1767774 ]. \t  -223.1887903552362 \t -12.649421222072922\n",
            "23     \t [ 3.28985042 -0.06806035]. \t  -26.7679039328027 \t -12.649421222072922\n",
            "24     \t [-10.44592626 -10.34925581]. \t  -101075.34640317387 \t -12.649421222072922\n",
            "25     \t [-7.74567573 -0.99745382]. \t  -266.0469191386042 \t -12.649421222072922\n",
            "26     \t [2.7548341  2.97582273]. \t  -450.455745326422 \t -12.649421222072922\n",
            "27     \t [1.01457089 0.87483281]. \t  \u001b[92m-0.5329183747012183\u001b[0m \t -0.5329183747012183\n",
            "28     \t [1.89130708 0.43928521]. \t  -5.326670366966644 \t -0.5329183747012183\n",
            "29     \t [-9.17852254  3.71331485]. \t  -2805.600104448162 \t -0.5329183747012183\n",
            "30     \t [-0.10149059 -2.95025057]. \t  -614.3747839984946 \t -0.5329183747012183\n",
            "31     \t [-0.249501   -0.74885867]. \t  -5.320971299908408 \t -0.5329183747012183\n",
            "32     \t [1.96575651 0.7240668 ]. \t  -2.6152379244249984 \t -0.5329183747012183\n",
            "33     \t [ 5.34822395 -0.19269434]. \t  -74.53639617323107 \t -0.5329183747012183\n",
            "34     \t [-3.41906934  3.59694844]. \t  -1735.9392767867666 \t -0.5329183747012183\n",
            "35     \t [ 2.06508456 -0.56175521]. \t  -5.246811586325608 \t -0.5329183747012183\n",
            "36     \t [ 5.63294844 -7.94344199]. \t  -29092.624121161647 \t -0.5329183747012183\n",
            "37     \t [ 1.36210559 -1.02819482]. \t  -1.2629214291826965 \t -0.5329183747012183\n",
            "38     \t [1.08516894 1.09124532]. \t  -3.3688904174500798 \t -0.5329183747012183\n",
            "39     \t [-2.7772763   9.65998633]. \t  -71765.00933891739 \t -0.5329183747012183\n",
            "40     \t [ 5.82415001 -9.03224803]. \t  -49534.30542867251 \t -0.5329183747012183\n",
            "41     \t [ 0.4830715  -0.18168467]. \t  -0.6150812447407004 \t -0.5329183747012183\n",
            "42     \t [ 1.58713924 -0.05125146]. \t  -5.349457986508344 \t -0.5329183747012183\n",
            "43     \t [ 0.46004222 -0.80472513]. \t  -1.686414736882948 \t -0.5329183747012183\n",
            "44     \t [1.49779498 0.70156839]. \t  -0.7749560014307908 \t -0.5329183747012183\n",
            "45     \t [ 2.3717882  -1.06308583]. \t  -1.9066607910579674 \t -0.5329183747012183\n",
            "46     \t [-5.13967359 -0.96374632]. \t  -135.61965735675838 \t -0.5329183747012183\n",
            "47     \t [1.27814783 0.45382512]. \t  -1.5780866676474714 \t -0.5329183747012183\n",
            "48     \t [9.3826568  0.90601465]. \t  -190.1129819863197 \t -0.5329183747012183\n",
            "49     \t [1.67255024 1.11552115]. \t  -1.7847691163162096 \t -0.5329183747012183\n",
            "50     \t [ 1.93084158 -1.36181104]. \t  -7.190577679575107 \t -0.5329183747012183\n",
            "51     \t [-3.03008776 -8.62582546]. \t  -46126.901882453436 \t -0.5329183747012183\n",
            "52     \t [-1.06968479 -9.60467806]. \t  -68876.2628835377 \t -0.5329183747012183\n",
            "53     \t [-0.19256959  0.6314155 ]. \t  -3.3821872186628372 \t -0.5329183747012183\n",
            "54     \t [3.61105275 0.61496235]. \t  -23.116167764375554 \t -0.5329183747012183\n",
            "55     \t [-0.28535136 -0.45466743]. \t  -2.6287606917904114 \t -0.5329183747012183\n",
            "56     \t [2.06155584 9.71293459]. \t  -69655.6541345046 \t -0.5329183747012183\n",
            "57     \t [-2.41865885 -4.25492326]. \t  -2995.8397045771317 \t -0.5329183747012183\n",
            "58     \t [-0.28680741  0.46609942]. \t  -2.6964343801871378 \t -0.5329183747012183\n",
            "59     \t [2.28988634 4.81890543]. \t  -3900.782175509248 \t -0.5329183747012183\n",
            "60     \t [4.48241068 2.44398572]. \t  -123.54146915409495 \t -0.5329183747012183\n",
            "61     \t [-9.9903605  -0.01517048]. \t  -320.42102403555316 \t -0.5329183747012183\n",
            "62     \t [0.63733264 0.65791808]. \t  \u001b[92m-0.23584225442787377\u001b[0m \t -0.23584225442787377\n",
            "63     \t [0.26448304 0.34802002]. \t  -0.5419750656223005 \t -0.23584225442787377\n",
            "64     \t [0.79923757 0.55169774]. \t  \u001b[92m-0.11288360462082157\u001b[0m \t -0.11288360462082157\n",
            "65     \t [ 0.77876368 -0.82187594]. \t  -0.7037630616891044 \t -0.11288360462082157\n",
            "66     \t [ 5.96303625 -8.00820816]. \t  -29939.094089609996 \t -0.11288360462082157\n",
            "67     \t [4.08981309 0.7279362 ]. \t  -27.909119138463204 \t -0.11288360462082157\n",
            "68     \t [4.80742649 1.22440502]. \t  -21.042118328385396 \t -0.11288360462082157\n",
            "69     \t [-1.18667684 -0.12297926]. \t  -7.74336669155783 \t -0.11288360462082157\n",
            "70     \t [ 9.99087551 -4.86280159]. \t  -2863.8340680637184 \t -0.11288360462082157\n",
            "71     \t [3.86563751 8.40336539]. \t  -37747.851242460165 \t -0.11288360462082157\n",
            "72     \t [-3.98859714  0.66553368]. \t  -72.40696445989835 \t -0.11288360462082157\n",
            "73     \t [6.47280141 0.89138348]. \t  -77.6520672146135 \t -0.11288360462082157\n",
            "74     \t [0.84942276 0.63727097]. \t  \u001b[92m-0.025440321298634487\u001b[0m \t -0.025440321298634487\n",
            "75     \t [-0.2850654  -2.28991978]. \t  -233.74622481526887 \t -0.025440321298634487\n",
            "76     \t [ 5.40287929 -3.75820845]. \t  -1063.208321874586 \t -0.025440321298634487\n",
            "77     \t [ 0.47597082 -0.58416281]. \t  -0.35990888928450027 \t -0.025440321298634487\n",
            "78     \t [ 4.00702346 -4.21814416]. \t  -2003.4400713894202 \t -0.025440321298634487\n",
            "79     \t [ 1.46228444 -6.62887934]. \t  -14937.668100017143 \t -0.025440321298634487\n",
            "80     \t [2.01928411 9.20045794]. \t  -55964.60885655887 \t -0.025440321298634487\n",
            "81     \t [ 5.67102364 -3.82147844]. \t  -1129.740002070439 \t -0.025440321298634487\n",
            "82     \t [ 1.64310826 -0.82825349]. \t  -0.5605792656315828 \t -0.025440321298634487\n",
            "83     \t [-7.57356718  6.86182203]. \t  -20776.68867790153 \t -0.025440321298634487\n",
            "84     \t [-1.10128033 -1.17135281]. \t  -33.98981420862948 \t -0.025440321298634487\n",
            "85     \t [-6.34781947  4.59890045]. \t  -4787.166495711154 \t -0.025440321298634487\n",
            "86     \t [-8.69033094  3.93211189]. \t  -3232.3357421883297 \t -0.025440321298634487\n",
            "87     \t [ 1.17039274 -0.73383188]. \t  -0.046471197960616606 \t -0.025440321298634487\n",
            "88     \t [-1.84270439  9.92074067]. \t  -78959.45602882205 \t -0.025440321298634487\n",
            "89     \t [ 5.35086292 -4.73006518]. \t  -3123.046485571771 \t -0.025440321298634487\n",
            "90     \t [ 5.51750874 -2.52415985]. \t  -124.81656827996554 \t -0.025440321298634487\n",
            "91     \t [0.82574087 0.64960229]. \t  -0.03103057252125801 \t -0.025440321298634487\n",
            "92     \t [ 8.76963296 -9.60728067]. \t  -61892.78078871723 \t -0.025440321298634487\n",
            "93     \t [ 4.04346237 -2.9327525 ]. \t  -355.5608081303744 \t -0.025440321298634487\n",
            "94     \t [ 2.71201342 -1.00824714]. \t  -3.852770028128274 \t -0.025440321298634487\n",
            "95     \t [-0.17249246  1.00137998]. \t  -10.862247659516049 \t -0.025440321298634487\n",
            "96     \t [-7.06018259  3.18491234]. \t  -1560.7397815242407 \t -0.025440321298634487\n",
            "97     \t [-4.1757566   7.11125473]. \t  -22209.55836836178 \t -0.025440321298634487\n",
            "98     \t [-5.39054826  7.86566842]. \t  -33388.93420504962 \t -0.025440321298634487\n",
            "99     \t [ 5.99666688 -0.69597377]. \t  -75.526395702937 \t -0.025440321298634487\n",
            "100    \t [9.84428029 6.27909718]. \t  -9602.938005260863 \t -0.025440321298634487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQHSe2Dtd1m",
        "outputId": "5908da5c-6b55-48c0-bf04-4955a0a942cc"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_loser_14 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_14 = d2GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [2.95102099 0.14299376]. \t  -20.744156010505872 \t -20.744156010505872\n",
            "init   \t [0.5668276  7.92570408]. \t  -31283.56791285067 \t -20.744156010505872\n",
            "init   \t [3.99982381 4.28594204]. \t  -2152.65365220131 \t -20.744156010505872\n",
            "init   \t [ 4.34676764 -5.54361076]. \t  -6535.783782469756 \t -20.744156010505872\n",
            "init   \t [-6.49690953 -0.86317023]. \t  -183.78911629570374 \t -20.744156010505872\n",
            "1      \t [-3.82909823 -9.9518425 ]. \t  -81556.54609796016 \t -20.744156010505872\n",
            "2      \t [-26.06651609 -28.6934826 ]. \t  -5596572.497420566 \t -20.744156010505872\n",
            "3      \t [-8.75957512  8.21832577]. \t  -41475.906975640784 \t -20.744156010505872\n",
            "4      \t [  3.90475719 -12.48795166]. \t  -189727.95697983992 \t -20.744156010505872\n",
            "5      \t [-10.97756517 -11.6266194 ]. \t  -158441.2390006013 \t -20.744156010505872\n",
            "6      \t [8.42986383 9.7389967 ]. \t  -65770.10932310983 \t -20.744156010505872\n",
            "7      \t [-12.08738063  -3.93567084]. \t  -3880.7075116888527 \t -20.744156010505872\n",
            "8      \t [8.47629598 0.03320735]. \t  -199.51542183217234 \t -20.744156010505872\n",
            "9      \t [ 9.93412348 -8.73957701]. \t  -40878.51622921913 \t -20.744156010505872\n",
            "10     \t [-1.51986379 -3.40109471]. \t  -1222.0635260913602 \t -20.744156010505872\n",
            "11     \t [-4.04855107  3.73708648]. \t  -2070.951930397764 \t -20.744156010505872\n",
            "12     \t [-11.34472164   2.22267455]. \t  -1053.4176824372353 \t -20.744156010505872\n",
            "13     \t [9.29077045 4.26738451]. \t  -1540.850903126652 \t -20.744156010505872\n",
            "14     \t [-7.75022868 -5.14349578]. \t  -7436.159541013263 \t -20.744156010505872\n",
            "15     \t [-0.27698467  1.07452841]. \t  \u001b[92m-15.007626072495281\u001b[0m \t -15.007626072495281\n",
            "16     \t [ 8.24794658 -3.29737402]. \t  -416.89257111012483 \t -15.007626072495281\n",
            "17     \t [-3.84518681  9.43716885]. \t  -66246.36120432989 \t -15.007626072495281\n",
            "18     \t [ 0.71976447 -7.80599006]. \t  -29353.365235377667 \t -15.007626072495281\n",
            "19     \t [ 4.93462122 -0.58925448]. \t  -51.43948921692635 \t -15.007626072495281\n",
            "20     \t [0.0206396  2.76110062]. \t  -464.6647646905044 \t -15.007626072495281\n",
            "21     \t [-1.17185112  0.59624905]. \t  \u001b[92m-11.80738873932642\u001b[0m \t -11.80738873932642\n",
            "22     \t [ 0.50131895 -0.47781854]. \t  \u001b[92m-0.2526785797869709\u001b[0m \t -0.2526785797869709\n",
            "23     \t [-0.88813303  0.23649444]. \t  -5.565015412361721 \t -0.2526785797869709\n",
            "24     \t [ 2.27968192 -1.77412231]. \t  -33.88346506846342 \t -0.2526785797869709\n",
            "25     \t [-7.94035663  1.18678472]. \t  -311.36780316751907 \t -0.2526785797869709\n",
            "26     \t [-2.86990943  0.33672408]. \t  -34.15499915685047 \t -0.2526785797869709\n",
            "27     \t [1.27794013 0.21785427]. \t  -2.8763193968011858 \t -0.2526785797869709\n",
            "28     \t [ 0.70801262 -0.05226983]. \t  -1.072404996191551 \t -0.2526785797869709\n",
            "29     \t [0.5283986 0.0404275]. \t  -0.7739305609058844 \t -0.2526785797869709\n",
            "30     \t [-7.72621745  4.29234338]. \t  -4049.9440807169954 \t -0.2526785797869709\n",
            "31     \t [ 0.58951108 -0.70364728]. \t  -0.4896668791270167 \t -0.2526785797869709\n",
            "32     \t [5.6873449  1.34849608]. \t  -30.379987455612657 \t -0.2526785797869709\n",
            "33     \t [ 1.35261235 -0.74064282]. \t  -0.25490492907314577 \t -0.2526785797869709\n",
            "34     \t [-4.12909203 -0.52282149]. \t  -70.03335985327645 \t -0.2526785797869709\n",
            "35     \t [ 5.34586013 -1.82885466]. \t  -22.496799744657586 \t -0.2526785797869709\n",
            "36     \t [ 0.62923186 -0.36561634]. \t  -0.39938507686093977 \t -0.2526785797869709\n",
            "37     \t [ 1.19424952 -0.31428534]. \t  -2.024550545279648 \t -0.2526785797869709\n",
            "38     \t [-0.06061799  0.49695564]. \t  -1.7399570405290783 \t -0.2526785797869709\n",
            "39     \t [2.21243124 0.62390212]. \t  -5.582262981047484 \t -0.2526785797869709\n",
            "40     \t [-1.08640402 -0.3534857 ]. \t  -7.924521510214021 \t -0.2526785797869709\n",
            "41     \t [-9.271154   -1.43618489]. \t  -464.42410367310896 \t -0.2526785797869709\n",
            "42     \t [7.00037581 1.56548397]. \t  -44.81523591915629 \t -0.2526785797869709\n",
            "43     \t [ 1.98427218 -0.50928043]. \t  -5.26440119480513 \t -0.2526785797869709\n",
            "44     \t [ 1.45304025 -1.47538326]. \t  -17.030712921107778 \t -0.2526785797869709\n",
            "45     \t [4.37574021 7.78749613]. \t  -27349.3736907906 \t -0.2526785797869709\n",
            "46     \t [-4.75306107 -2.73532456]. \t  -810.6227739528122 \t -0.2526785797869709\n",
            "47     \t [0.04416618 0.55856979]. \t  -1.5860337751806841 \t -0.2526785797869709\n",
            "48     \t [ 3.77285617 -1.8905101 ]. \t  -30.472691179992328 \t -0.2526785797869709\n",
            "49     \t [4.43027072 0.97124894]. \t  -24.706780058562657 \t -0.2526785797869709\n",
            "50     \t [0.77392253 0.55509967]. \t  \u001b[92m-0.10081884565326728\u001b[0m \t -0.10081884565326728\n",
            "51     \t [-9.78985189  3.04528552]. \t  -1722.435073549983 \t -0.10081884565326728\n",
            "52     \t [ 0.89472625 -0.61788589]. \t  \u001b[92m-0.045488607899963125\u001b[0m \t -0.045488607899963125\n",
            "53     \t [0.55612782 0.46089339]. \t  -0.2314926430225663 \t -0.045488607899963125\n",
            "54     \t [3.1979886  1.12182317]. \t  -5.758714471612511 \t -0.045488607899963125\n",
            "55     \t [-7.84198973 -5.30413911]. \t  -8298.306829346673 \t -0.045488607899963125\n",
            "56     \t [2.75543878 1.19495299]. \t  -3.1017202041493457 \t -0.045488607899963125\n",
            "57     \t [-7.76871008 -3.18398393]. \t  -1649.8465300153564 \t -0.045488607899963125\n",
            "58     \t [-4.2717033   5.34641832]. \t  -7577.557966132889 \t -0.045488607899963125\n",
            "59     \t [-4.85850499  0.88508561]. \t  -116.88996371597301 \t -0.045488607899963125\n",
            "60     \t [-3.70409777 -5.27977129]. \t  -7092.176487241333 \t -0.045488607899963125\n",
            "61     \t [-4.80405407 -9.43966596]. \t  -67025.33281313878 \t -0.045488607899963125\n",
            "62     \t [2.46348913 1.07851701]. \t  -2.1793884622273807 \t -0.045488607899963125\n",
            "63     \t [-8.14075251  2.79073766]. \t  -1208.563170401845 \t -0.045488607899963125\n",
            "64     \t [ 0.74047536 -0.6718284 ]. \t  -0.11999111469854173 \t -0.045488607899963125\n",
            "65     \t [8.5032172  2.33748767]. \t  -68.05447515116728 \t -0.045488607899963125\n",
            "66     \t [-6.75387576 -3.05541974]. \t  -1352.9894685659553 \t -0.045488607899963125\n",
            "67     \t [ 6.52116839 -9.49089424]. \t  -60327.305906501824 \t -0.045488607899963125\n",
            "68     \t [8.08369547 1.50633416]. \t  -75.32144567329101 \t -0.045488607899963125\n",
            "69     \t [-8.34277372 -3.1991881 ]. \t  -1747.5942162311192 \t -0.045488607899963125\n",
            "70     \t [6.77549093 2.35990157]. \t  -71.42399333431575 \t -0.045488607899963125\n",
            "71     \t [6.16516321 0.61811416]. \t  -85.0212252663612 \t -0.045488607899963125\n",
            "72     \t [9.96549504 1.77069083]. \t  -107.68323953330102 \t -0.045488607899963125\n",
            "73     \t [-9.8685891   8.29621224]. \t  -43644.11708412386 \t -0.045488607899963125\n",
            "74     \t [4.12031716 0.18517141]. \t  -42.56957734983289 \t -0.045488607899963125\n",
            "75     \t [-3.28487492 -7.62804324]. \t  -28654.8945934056 \t -0.045488607899963125\n",
            "76     \t [ 8.24492868 -8.27259918]. \t  -33142.230176738776 \t -0.045488607899963125\n",
            "77     \t [ 3.1867796 -9.2083516]. \t  -55383.167089094495 \t -0.045488607899963125\n",
            "78     \t [-8.28717958 -9.40295244]. \t  -68623.7541473987 \t -0.045488607899963125\n",
            "79     \t [ 6.74504647 -8.46425389]. \t  -37320.518206723325 \t -0.045488607899963125\n",
            "80     \t [-5.78481188  3.27956315]. \t  -1536.1638856835025 \t -0.045488607899963125\n",
            "81     \t [-7.29670082  2.55242434]. \t  -895.163642157233 \t -0.045488607899963125\n",
            "82     \t [-5.18300448 -5.94565128]. \t  -11555.15717335925 \t -0.045488607899963125\n",
            "83     \t [-2.35585868 -5.29245568]. \t  -6826.784218325494 \t -0.045488607899963125\n",
            "84     \t [ 6.79908003 -6.61180915]. \t  -13036.977935772078 \t -0.045488607899963125\n",
            "85     \t [ 3.3611301  -7.53643968]. \t  -24308.960123070992 \t -0.045488607899963125\n",
            "86     \t [-2.53790586  2.45106559]. \t  -436.1168249295706 \t -0.045488607899963125\n",
            "87     \t [-2.16686506  0.02720457]. \t  -19.432476406526817 \t -0.045488607899963125\n",
            "88     \t [8.16694511 2.96724987]. \t  -229.67532676975765 \t -0.045488607899963125\n",
            "89     \t [ 9.89443542 -1.46004536]. \t  -142.52663895626415 \t -0.045488607899963125\n",
            "90     \t [ 8.15236323 -4.41999009]. \t  -1963.2814553179464 \t -0.045488607899963125\n",
            "91     \t [ 1.41598313 -3.59687813]. \t  -1196.6663659639614 \t -0.045488607899963125\n",
            "92     \t [ 8.71660806 -2.36680358]. \t  -71.9154860123935 \t -0.045488607899963125\n",
            "93     \t [2.13956651 1.21966504]. \t  -2.695063632186856 \t -0.045488607899963125\n",
            "94     \t [ 6.48143849 -2.35955314]. \t  -73.3571027698803 \t -0.045488607899963125\n",
            "95     \t [-2.66782198 -0.61507657]. \t  -36.90677547748368 \t -0.045488607899963125\n",
            "96     \t [ 0.28359708 -0.42166557]. \t  -0.523603048363225 \t -0.045488607899963125\n",
            "97     \t [ 6.34341167 -5.35530953]. \t  -5233.667329281431 \t -0.045488607899963125\n",
            "98     \t [ 9.7014008  -1.18928938]. \t  -170.17915159284428 \t -0.045488607899963125\n",
            "99     \t [ 8.99303271 -1.29363549]. \t  -127.64426897721775 \t -0.045488607899963125\n",
            "100    \t [-3.29970925 -9.55423711]. \t  -69111.27565440805 \t -0.045488607899963125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq4J_FTytd4O",
        "outputId": "76ca799f-5481-4b57-e527-81f0902edf6e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_loser_15 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_15 = d2GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.55763717 -4.88997339]. \t  -4281.404311682752 \t -3267.0472724202245\n",
            "init   \t [-4.94946261  4.19808709]. \t  -3267.0472724202245 \t -3267.0472724202245\n",
            "init   \t [-1.04895274 -5.46114084]. \t  -7372.467127638806 \t -3267.0472724202245\n",
            "init   \t [-1.96580869  7.6451547 ]. \t  -28265.42637457811 \t -3267.0472724202245\n",
            "init   \t [-1.25573066  7.56850362]. \t  -26833.737231776184 \t -3267.0472724202245\n",
            "1      \t [-9.76533982 -4.68487047]. \t  -5874.976174155925 \t -3267.0472724202245\n",
            "2      \t [8.28335803 6.88744078]. \t  -15048.8154067911 \t -3267.0472724202245\n",
            "3      \t [ 9.8362843  -6.65190316]. \t  -12452.665647642647 \t -3267.0472724202245\n",
            "4      \t [ -7.51309763 -15.06647996]. \t  -426056.7889400076 \t -3267.0472724202245\n",
            "5      \t [  6.30126693 -12.74130618]. \t  -202760.38799762737 \t -3267.0472724202245\n",
            "6      \t [-11.43331086   2.11634209]. \t  \u001b[92m-986.1826473273671\u001b[0m \t -986.1826473273671\n",
            "7      \t [3.11660642 2.04785404]. \t  \u001b[92m-60.042812109163265\u001b[0m \t -60.042812109163265\n",
            "8      \t [ -0.99208407 -12.01114778]. \t  -167656.22909151207 \t -60.042812109163265\n",
            "9      \t [-394.874255   -394.96858932]. \t  -195181737862.25616 \t -60.042812109163265\n",
            "10     \t [9.34011063 0.13447336]. \t  -242.68421009840233 \t -60.042812109163265\n",
            "11     \t [-1.31331264  0.26183701]. \t  \u001b[92m-9.558909094313757\u001b[0m \t -9.558909094313757\n",
            "12     \t [-9.43319146  9.39489306]. \t  -69272.00418704003 \t -9.558909094313757\n",
            "13     \t [-18.78649583 -17.94034514]. \t  -878199.9330115292 \t -9.558909094313757\n",
            "14     \t [-5.1354557  -1.68794517]. \t  -272.3851117882557 \t -9.558909094313757\n",
            "15     \t [-6.21236802 -9.18113137]. \t  -61161.19930509341 \t -9.558909094313757\n",
            "16     \t [ 5.92511739 -2.82220197]. \t  -224.43804183091714 \t -9.558909094313757\n",
            "17     \t [3.41287497 6.92277872]. \t  -17094.966500815648 \t -9.558909094313757\n",
            "18     \t [ 5.01310713 -7.60769602]. \t  -24543.230092565893 \t -9.558909094313757\n",
            "19     \t [-0.42302743  2.88510736]. \t  -584.8437263443703 \t -9.558909094313757\n",
            "20     \t [-0.16470989 -1.10145798]. \t  -14.784450893194698 \t -9.558909094313757\n",
            "21     \t [6.05803867 1.05523586]. \t  -54.936773239936926 \t -9.558909094313757\n",
            "22     \t [-2.53318672  0.22537608]. \t  -26.367491999314574 \t -9.558909094313757\n",
            "23     \t [-1.2302025  -0.28339211]. \t  \u001b[92m-8.84258979830038\u001b[0m \t -8.84258979830038\n",
            "24     \t [-8.65061276  0.06284463]. \t  -243.07397492319214 \t -8.84258979830038\n",
            "25     \t [ 2.12406673 -0.93845658]. \t  \u001b[92m-1.5265781348713974\u001b[0m \t -1.5265781348713974\n",
            "26     \t [ 9.1944841  -2.44941832]. \t  -82.88355600745071 \t -1.5265781348713974\n",
            "27     \t [ 2.55959379 -0.87484459]. \t  -4.5495524754281735 \t -1.5265781348713974\n",
            "28     \t [-9.17200044  4.92120769]. \t  -6740.9644090024985 \t -1.5265781348713974\n",
            "29     \t [6.61800799 3.16753108]. \t  -393.2862247841722 \t -1.5265781348713974\n",
            "30     \t [ 4.76769694 -0.21789049]. \t  -57.864620777424264 \t -1.5265781348713974\n",
            "31     \t [ 1.77231288 -0.66430961]. \t  -2.1795935255634977 \t -1.5265781348713974\n",
            "32     \t [9.94034579 3.01432936]. \t  -215.46199536446963 \t -1.5265781348713974\n",
            "33     \t [ 1.72066799 -0.88136658]. \t  \u001b[92m-0.5751763577586422\u001b[0m \t -0.5751763577586422\n",
            "34     \t [-4.58915243 -4.97926089]. \t  -5901.1498158668055 \t -0.5751763577586422\n",
            "35     \t [-1.47488689 -1.75213823]. \t  -122.09736089145511 \t -0.5751763577586422\n",
            "36     \t [ 0.13088774 -0.3750274 ]. \t  -0.8005984719525697 \t -0.5751763577586422\n",
            "37     \t [ 0.07548293 -0.56950331]. \t  -1.5118141254390878 \t -0.5751763577586422\n",
            "38     \t [ 1.19764072 -1.00125792]. \t  -1.3428323483178457 \t -0.5751763577586422\n",
            "39     \t [-0.81077909 -0.01382482]. \t  -4.59488637802519 \t -0.5751763577586422\n",
            "40     \t [ 2.38815579 -1.26345171]. \t  -3.22130319108295 \t -0.5751763577586422\n",
            "41     \t [ 0.41462043 -0.60444439]. \t  \u001b[92m-0.5424894601756779\u001b[0m \t -0.5424894601756779\n",
            "42     \t [-3.55222476  1.31838855]. \t  -119.52297848892475 \t -0.5424894601756779\n",
            "43     \t [ 0.91848763 -0.65606032]. \t  \u001b[92m-0.013293005193297873\u001b[0m \t -0.013293005193297873\n",
            "44     \t [0.85033931 3.49628115]. \t  -1113.7181578937912 \t -0.013293005193297873\n",
            "45     \t [2.72532957 0.28393154]. \t  -16.12593184759468 \t -0.013293005193297873\n",
            "46     \t [5.50934152 9.97080643]. \t  -74769.15171027063 \t -0.013293005193297873\n",
            "47     \t [ 0.3794039  -0.42480457]. \t  -0.3858229843734759 \t -0.013293005193297873\n",
            "48     \t [ 6.33124059 -0.34756025]. \t  -102.58966238895482 \t -0.013293005193297873\n",
            "49     \t [4.43689229 1.87440423]. \t  -25.227290333061198 \t -0.013293005193297873\n",
            "50     \t [-5.04355946  9.7782733 ]. \t  -77082.56208637712 \t -0.013293005193297873\n",
            "51     \t [ 0.91774604 -0.55424122]. \t  -0.1908438035179484 \t -0.013293005193297873\n",
            "52     \t [-4.73836531 -2.7767814 ]. \t  -845.7312696634915 \t -0.013293005193297873\n",
            "53     \t [-5.19661439  0.6196679 ]. \t  -109.55072131728093 \t -0.013293005193297873\n",
            "54     \t [ 3.58843631 -1.90789745]. \t  -33.95743430081127 \t -0.013293005193297873\n",
            "55     \t [3.97051309 1.06646228]. \t  -14.575623335779952 \t -0.013293005193297873\n",
            "56     \t [2.43116934 5.18720372]. \t  -5282.4727117377015 \t -0.013293005193297873\n",
            "57     \t [-8.41074348 -8.70603717]. \t  -51289.14438183774 \t -0.013293005193297873\n",
            "58     \t [-2.07398088  5.0103825 ]. \t  -5476.232284473293 \t -0.013293005193297873\n",
            "59     \t [-4.38452908 -0.14070639]. \t  -68.13892913132634 \t -0.013293005193297873\n",
            "60     \t [0.6782251  0.33823138]. \t  -0.507503248292467 \t -0.013293005193297873\n",
            "61     \t [2.42559477 1.49682716]. \t  -10.481562535705507 \t -0.013293005193297873\n",
            "62     \t [1.37467832 0.57132965]. \t  -1.182499006742763 \t -0.013293005193297873\n",
            "63     \t [-9.07829232  3.54027274]. \t  -2433.382440195059 \t -0.013293005193297873\n",
            "64     \t [ 2.89586453 -3.25500391]. \t  -672.952472255595 \t -0.013293005193297873\n",
            "65     \t [1.89302818 1.1430674 ]. \t  -1.8348119343184734 \t -0.013293005193297873\n",
            "66     \t [3.33598621 2.31905593]. \t  -115.57125351130023 \t -0.013293005193297873\n",
            "67     \t [0.29177252 3.76797388]. \t  -1580.112963263459 \t -0.013293005193297873\n",
            "68     \t [-0.59337761  1.571409  ]. \t  -63.74556706935428 \t -0.013293005193297873\n",
            "69     \t [ 3.64714609 -1.32715244]. \t  -7.038372402690053 \t -0.013293005193297873\n",
            "70     \t [2.22014433 1.8529181 ]. \t  -44.668056289721186 \t -0.013293005193297873\n",
            "71     \t [2.69040706 0.80803451]. \t  -6.691530484075024 \t -0.013293005193297873\n",
            "72     \t [4.93270146 4.87445417]. \t  -3642.9255103394166 \t -0.013293005193297873\n",
            "73     \t [-4.33233879  7.85144038]. \t  -32603.475042977963 \t -0.013293005193297873\n",
            "74     \t [2.97409311 1.28107221]. \t  -4.087016700722935 \t -0.013293005193297873\n",
            "75     \t [ 2.75759941 -7.14404541]. \t  -19730.88974639189 \t -0.013293005193297873\n",
            "76     \t [ 4.01917162 -7.50134124]. \t  -23562.76031559984 \t -0.013293005193297873\n",
            "77     \t [5.15533938 1.51242473]. \t  -17.9407646245595 \t -0.013293005193297873\n",
            "78     \t [ 9.83384425 -0.69132109]. \t  -235.67437894663698 \t -0.013293005193297873\n",
            "79     \t [2.12235086 0.95782537]. \t  -1.4249747288496792 \t -0.013293005193297873\n",
            "80     \t [4.41412612 1.54129151]. \t  -11.88343948408625 \t -0.013293005193297873\n",
            "81     \t [7.74797399 9.81281296]. \t  -68373.2105317395 \t -0.013293005193297873\n",
            "82     \t [0.519504   5.28291595]. \t  -6116.167104381529 \t -0.013293005193297873\n",
            "83     \t [ 1.21280511 -0.77999727]. \t  -0.04531779701916971 \t -0.013293005193297873\n",
            "84     \t [3.34558955 0.86833311]. \t  -12.255225877529249 \t -0.013293005193297873\n",
            "85     \t [1.92851199 9.24974436]. \t  -57249.36396355148 \t -0.013293005193297873\n",
            "86     \t [0.82692303 1.02223155]. \t  -3.2202514883390627 \t -0.013293005193297873\n",
            "87     \t [-8.26180241 -0.01564831]. \t  -222.31192709882077 \t -0.013293005193297873\n",
            "88     \t [-3.35697144 -0.49094299]. \t  -48.45937206509758 \t -0.013293005193297873\n",
            "89     \t [-1.97101032 -4.06030242]. \t  -2450.870425270952 \t -0.013293005193297873\n",
            "90     \t [-2.89804816 -1.01231984]. \t  -64.15288597645016 \t -0.013293005193297873\n",
            "91     \t [6.47232451 0.25390567]. \t  -110.42348666866121 \t -0.013293005193297873\n",
            "92     \t [-4.06431762  0.37047618]. \t  -63.298079846082146 \t -0.013293005193297873\n",
            "93     \t [0.51746225 0.78366197]. \t  -1.243287302612685 \t -0.013293005193297873\n",
            "94     \t [0.05293803 0.23778476]. \t  -0.9041612632147503 \t -0.013293005193297873\n",
            "95     \t [5.21339778 7.16195836]. \t  -18981.104424719997 \t -0.013293005193297873\n",
            "96     \t [0.816233   0.49729744]. \t  -0.24065367201825308 \t -0.013293005193297873\n",
            "97     \t [-0.48793295  0.75684032]. \t  -7.550899215889365 \t -0.013293005193297873\n",
            "98     \t [0.86093962 0.76857634]. \t  -0.22475210817207011 \t -0.013293005193297873\n",
            "99     \t [-4.26426686 -2.52050317]. \t  -603.6837600478647 \t -0.013293005193297873\n",
            "100    \t [-4.95102259 -9.69287804]. \t  -74421.35628980433 \t -0.013293005193297873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tJ-ta9Gtd7H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36c48d32-0734-4ce8-8ec7-8c8dfba9a1fc"
      },
      "source": [
        "\r\n",
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_loser_16 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_16 = d2GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.27896664 -3.67267997]. \t  -815.4627708918207 \t -28.992408538517264\n",
            "init   \t [3.44301557 0.15823335]. \t  -28.992408538517264 \t -28.992408538517264\n",
            "init   \t [ 5.63320853 -4.32636643]. \t  -2044.16089755496 \t -28.992408538517264\n",
            "init   \t [-5.32582432  1.27599373]. \t  -187.32245577722023 \t -28.992408538517264\n",
            "init   \t [7.50048729 4.37885841]. \t  -1945.4933626881252 \t -28.992408538517264\n",
            "1      \t [-2.44633796 -8.71755625]. \t  -47714.014485289874 \t -28.992408538517264\n",
            "2      \t [-0.94408028  9.32615359]. \t  -61182.65342174163 \t -28.992408538517264\n",
            "3      \t [-9.58291173  9.55394704]. \t  -73946.57346618871 \t -28.992408538517264\n",
            "4      \t [-9.96784472 -3.20585558]. \t  -1983.5841560879148 \t -28.992408538517264\n",
            "5      \t [-12.07466105  -9.68777512]. \t  -79995.55058438538 \t -28.992408538517264\n",
            "6      \t [  5.36487823 -11.09395012]. \t  -115975.40671915698 \t -28.992408538517264\n",
            "7      \t [-1.00187217 -2.41726175]. \t  -325.987359586068 \t -28.992408538517264\n",
            "8      \t [-12.67901686 -16.99291113]. \t  -696852.2878803565 \t -28.992408538517264\n",
            "9      \t [-10.30945384   2.48615157]. \t  -1155.8842484877978 \t -28.992408538517264\n",
            "10     \t [2.61479957e-03 3.14594750e+00]. \t  -784.3903644241187 \t -28.992408538517264\n",
            "11     \t [8.87535711 9.17115881]. \t  -50843.647954551314 \t -28.992408538517264\n",
            "12     \t [ 9.83171549 -7.92230725]. \t  -26848.289891580538 \t -28.992408538517264\n",
            "13     \t [-5.4193772   5.71201839]. \t  -10030.748009177907 \t -28.992408538517264\n",
            "14     \t [-4.98143955 -3.92257851]. \t  -2592.572457088352 \t -28.992408538517264\n",
            "15     \t [7.90708084 0.0832154 ]. \t  -172.31396434180684 \t -28.992408538517264\n",
            "16     \t [2.96807012 6.08391022]. \t  -10102.87986903581 \t -28.992408538517264\n",
            "17     \t [ 1.58776498 -5.50892905]. \t  -6988.053884985168 \t -28.992408538517264\n",
            "18     \t [-7.21381515 -8.3774682 ]. \t  -43625.858420012475 \t -28.992408538517264\n",
            "19     \t [-2.39123278  0.39915672]. \t  \u001b[92m-26.187412693296885\u001b[0m \t -26.187412693296885\n",
            "20     \t [4.85166618 1.75768794]. \t  \u001b[92m-18.358610867762607\u001b[0m \t -18.358610867762607\n",
            "21     \t [ 2.40127997 -1.86557906]. \t  -43.54149213746919 \t -18.358610867762607\n",
            "22     \t [-6.8214451  -0.93193248]. \t  -207.66884082649068 \t -18.358610867762607\n",
            "23     \t [-3.17980515 -0.24943833]. \t  -39.306829323052 \t -18.358610867762607\n",
            "24     \t [ 9.94693951 -2.70869861]. \t  -124.73974959393753 \t -18.358610867762607\n",
            "25     \t [4.88743129 0.09542781]. \t  -62.53069680578899 \t -18.358610867762607\n",
            "26     \t [ 0.03095551 -0.10316497]. \t  \u001b[92m-0.9392342300294892\u001b[0m \t -0.9392342300294892\n",
            "27     \t [2.71800746 2.29317025]. \t  -124.60821707074462 \t -0.9392342300294892\n",
            "28     \t [-9.44126646  5.41888513]. \t  -9403.290179141928 \t -0.9392342300294892\n",
            "29     \t [ 1.001862   -0.83198532]. \t  \u001b[92m-0.2926728162280416\u001b[0m \t -0.2926728162280416\n",
            "30     \t [ 3.96180743 -1.29116039]. \t  -9.560109736461621 \t -0.2926728162280416\n",
            "31     \t [9.61926857 1.57518535]. \t  -117.66430894956987 \t -0.2926728162280416\n",
            "32     \t [-4.97834856  9.77045296]. \t  -76790.810880623 \t -0.2926728162280416\n",
            "33     \t [0.77393442 0.19706055]. \t  -1.020685854736246 \t -0.2926728162280416\n",
            "34     \t [-7.3547023   1.77997483]. \t  -444.705705149057 \t -0.2926728162280416\n",
            "35     \t [ 2.13926091 -0.81780946]. \t  -2.583156892710142 \t -0.2926728162280416\n",
            "36     \t [-3.43632514  1.34585004]. \t  -119.33852461529034 \t -0.2926728162280416\n",
            "37     \t [ 0.42556786 -0.1561308 ]. \t  -0.6139501756576893 \t -0.2926728162280416\n",
            "38     \t [ 0.62800132 -0.47460437]. \t  \u001b[92m-0.20139744124299974\u001b[0m \t -0.20139744124299974\n",
            "39     \t [ 1.32460687 -9.68401501]. \t  -69367.56720518795 \t -0.20139744124299974\n",
            "40     \t [ 0.36977173 -9.90508545]. \t  -76716.14558306814 \t -0.20139744124299974\n",
            "41     \t [-1.2770428   0.27378714]. \t  -9.25736274573276 \t -0.20139744124299974\n",
            "42     \t [7.39423722 9.80328097]. \t  -68353.61004999385 \t -0.20139744124299974\n",
            "43     \t [1.37489323 0.10534516]. \t  -3.8001287876282275 \t -0.20139744124299974\n",
            "44     \t [ 8.19858837 -9.66164772]. \t  -63773.66709109646 \t -0.20139744124299974\n",
            "45     \t [-2.99252903 -8.06234898]. \t  -35391.53099516104 \t -0.20139744124299974\n",
            "46     \t [0.82950982 0.01096577]. \t  -1.4044421387685726 \t -0.20139744124299974\n",
            "47     \t [5.60056357 1.828265  ]. \t  -23.51764895890401 \t -0.20139744124299974\n",
            "48     \t [ 2.43890832 -0.14598987]. \t  -13.554794344136067 \t -0.20139744124299974\n",
            "49     \t [4.67432013 1.10826181]. \t  -23.338183015419787 \t -0.20139744124299974\n",
            "50     \t [ 3.34524984 -0.67491752]. \t  -17.35107527644133 \t -0.20139744124299974\n",
            "51     \t [-8.03251586  4.12578886]. \t  -3622.4977333168395 \t -0.20139744124299974\n",
            "52     \t [ 3.83209313 -1.62122812]. \t  -12.08010992927644 \t -0.20139744124299974\n",
            "53     \t [-6.21234888 -9.10408776]. \t  -59207.06821301126 \t -0.20139744124299974\n",
            "54     \t [ 1.4172399  -0.75774293]. \t  -0.31869408914705477 \t -0.20139744124299974\n",
            "55     \t [ 0.71733069 -0.46317606]. \t  -0.24609716901289924 \t -0.20139744124299974\n",
            "56     \t [ 0.22004071 -0.03815501]. \t  -0.7026265894274512 \t -0.20139744124299974\n",
            "57     \t [-5.87599396  3.20777726]. \t  -1447.0835971801562 \t -0.20139744124299974\n",
            "58     \t [ 8.52377976 -9.26055523]. \t  -53189.38260389485 \t -0.20139744124299974\n",
            "59     \t [-1.78942802 -0.09691785]. \t  -14.320185603340352 \t -0.20139744124299974\n",
            "60     \t [ 1.54529583 -0.42017554]. \t  -3.1400333306013914 \t -0.20139744124299974\n",
            "61     \t [3.00913684 9.91331595]. \t  -74918.36128909086 \t -0.20139744124299974\n",
            "62     \t [ 6.61537771 -1.9690889 ]. \t  -34.12822279685744 \t -0.20139744124299974\n",
            "63     \t [ 5.9626285  -1.42803205]. \t  -31.727176942282654 \t -0.20139744124299974\n",
            "64     \t [ 7.61775892 -1.63593355]. \t  -54.05701102065142 \t -0.20139744124299974\n",
            "65     \t [-1.75567565  9.70653295]. \t  -72351.48731166038 \t -0.20139744124299974\n",
            "66     \t [-7.61918752 -8.44388807]. \t  -45204.992501737106 \t -0.20139744124299974\n",
            "67     \t [-2.53598717 -7.62877066]. \t  -28302.273204681238 \t -0.20139744124299974\n",
            "68     \t [ 7.17931782 -0.57421152]. \t  -123.20164106679526 \t -0.20139744124299974\n",
            "69     \t [6.33797836 5.1826123 ]. \t  -4518.405148097174 \t -0.20139744124299974\n",
            "70     \t [ 6.23554753 -0.27153034]. \t  -101.54063958936088 \t -0.20139744124299974\n",
            "71     \t [-0.0710583   7.88508369]. \t  -30961.893643346364 \t -0.20139744124299974\n",
            "72     \t [9.75480775 4.36809253]. \t  -1690.409359333059 \t -0.20139744124299974\n",
            "73     \t [3.25089107 3.65040972]. \t  -1100.1950384901293 \t -0.20139744124299974\n",
            "74     \t [3.73819659 1.41966354]. \t  -7.66905843336126 \t -0.20139744124299974\n",
            "75     \t [2.17539147 1.07904357]. \t  -1.4285337460124834 \t -0.20139744124299974\n",
            "76     \t [2.52932056 1.22033585]. \t  -2.7422364226939915 \t -0.20139744124299974\n",
            "77     \t [-6.22501894  3.52131694]. \t  -1977.2233309434237 \t -0.20139744124299974\n",
            "78     \t [-0.10241599  0.11671667]. \t  -1.2489452582909175 \t -0.20139744124299974\n",
            "79     \t [ 9.55122594 -1.83559808]. \t  -88.94248751324137 \t -0.20139744124299974\n",
            "80     \t [9.33855769 4.80413045]. \t  -2781.0713822845873 \t -0.20139744124299974\n",
            "81     \t [-9.64089881 -9.7796186 ]. \t  -80853.1547843824 \t -0.20139744124299974\n",
            "82     \t [ 6.88349109 -5.5330375 ]. \t  -5941.485893656064 \t -0.20139744124299974\n",
            "83     \t [ 8.16371685 -9.0944609 ]. \t  -49509.41347498307 \t -0.20139744124299974\n",
            "84     \t [9.72920925 2.42401936]. \t  -84.38035313543975 \t -0.20139744124299974\n",
            "85     \t [ 7.84801605 -2.83981008]. \t  -184.04612616242872 \t -0.20139744124299974\n",
            "86     \t [1.08954768 0.73596483]. \t  \u001b[92m-0.00809714241241969\u001b[0m \t -0.00809714241241969\n",
            "87     \t [ 2.54599126 -8.46775776]. \t  -39685.38564442981 \t -0.00809714241241969\n",
            "88     \t [-8.10679451 -3.97384878]. \t  -3233.485078639014 \t -0.00809714241241969\n",
            "89     \t [0.61124987 4.40812793]. \t  -2926.5720044905806 \t -0.00809714241241969\n",
            "90     \t [ 4.40378496 -1.63600701]. \t  -13.387914139576013 \t -0.00809714241241969\n",
            "91     \t [-2.05843435  2.31714723]. \t  -336.86901750125134 \t -0.00809714241241969\n",
            "92     \t [1.60378007 1.267584  ]. \t  -5.547193917927336 \t -0.00809714241241969\n",
            "93     \t [0.59222959 0.97291003]. \t  -3.55084523485797 \t -0.00809714241241969\n",
            "94     \t [-4.58912948  6.6451055 ]. \t  -17293.53875567509 \t -0.00809714241241969\n",
            "95     \t [-3.30205712  8.57469757]. \t  -45230.517092884154 \t -0.00809714241241969\n",
            "96     \t [ 3.00188445 -1.29398595]. \t  -4.248241124859602 \t -0.00809714241241969\n",
            "97     \t [-5.68991653  6.92891146]. \t  -20734.417945070247 \t -0.00809714241241969\n",
            "98     \t [5.05108621 5.33718012]. \t  -5407.76273916 \t -0.00809714241241969\n",
            "99     \t [-7.08194145  8.05431634]. \t  -37508.00702857726 \t -0.00809714241241969\n",
            "100    \t [-6.40214562  9.5152672 ]. \t  -70354.36649607628 \t -0.00809714241241969\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqjDLo0otd9-",
        "outputId": "b037bba1-1277-4217-899d-356da3582185"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_loser_17 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_17 = d2GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [3.03648056 4.61039249]. \t  -3120.691750998894 \t -133.8839693070206\n",
            "init   \t [-6.13265275  0.39354476]. \t  -133.8839693070206 \t -133.8839693070206\n",
            "init   \t [3.52766533 6.08889746]. \t  -9981.232217710998 \t -133.8839693070206\n",
            "init   \t [0.67025729 8.16068359]. \t  -35124.935571397844 \t -133.8839693070206\n",
            "init   \t [-1.13037742 -4.8805558 ]. \t  -4761.569630397088 \t -133.8839693070206\n",
            "1      \t [ 9.47756828 -1.18144623]. \t  -161.27269462973322 \t -133.8839693070206\n",
            "2      \t [-9.85290042 -8.55285713]. \t  -48886.9473889917 \t -133.8839693070206\n",
            "3      \t [-9.20794452  7.56905046]. \t  -30751.632331903027 \t -133.8839693070206\n",
            "4      \t [ 9.29095451 -9.9002349 ]. \t  -69811.16084951065 \t -133.8839693070206\n",
            "5      \t [9.65812984 9.94541442]. \t  -70886.65075415972 \t -133.8839693070206\n",
            "6      \t [  2.32802534 -12.12842359]. \t  -170377.1312406423 \t -133.8839693070206\n",
            "7      \t [-12.21855525  -1.35547702]. \t  -679.9172523504255 \t -133.8839693070206\n",
            "8      \t [ -4.50004644 -12.10420029]. \t  -177070.5685006512 \t -133.8839693070206\n",
            "9      \t [-24.69577122 -18.96020191]. \t  -1106763.1545213743 \t -133.8839693070206\n",
            "10     \t [ 3.73996695 -1.00135042]. \t  \u001b[92m-13.524827059253516\u001b[0m \t -13.524827059253516\n",
            "11     \t [-1.77704389  2.72980879]. \t  -564.2072089927992 \t -13.524827059253516\n",
            "12     \t [-12.08898324 -15.40062802]. \t  -473433.99252908037 \t -13.524827059253516\n",
            "13     \t [-48.06356377 -51.80764927]. \t  -58671305.4708505 \t -13.524827059253516\n",
            "14     \t [ 4.37958906 -5.03368884]. \t  -4298.145604370104 \t -13.524827059253516\n",
            "15     \t [9.10259516 3.76068583]. \t  -801.6209395632309 \t -13.524827059253516\n",
            "16     \t [-6.72742494 -3.83957523]. \t  -2682.3455894633826 \t -13.524827059253516\n",
            "17     \t [-4.14762975  6.09236623]. \t  -12313.8116109825 \t -13.524827059253516\n",
            "18     \t [-9.29176093  2.06018579]. \t  -738.2121256501493 \t -13.524827059253516\n",
            "19     \t [5.97783219 0.84276511]. \t  -66.31725600831446 \t -13.524827059253516\n",
            "20     \t [ 1.214519   -0.57448845]. \t  \u001b[92m-0.6608370322265632\u001b[0m \t -0.6608370322265632\n",
            "21     \t [2.09342659 0.88499568]. \t  -1.7510226016935926 \t -0.6608370322265632\n",
            "22     \t [1.05351068 0.02102328]. \t  -2.2189094119435655 \t -0.6608370322265632\n",
            "23     \t [1.44577925 1.67789363]. \t  -35.225073284875805 \t -0.6608370322265632\n",
            "24     \t [ 1.66809502 -0.35033549]. \t  -4.494075401198297 \t -0.6608370322265632\n",
            "25     \t [-2.95416191 -1.14883584]. \t  -78.21680514925553 \t -0.6608370322265632\n",
            "26     \t [ 8.36052857 -5.63741378]. \t  -6148.3323853774355 \t -0.6608370322265632\n",
            "27     \t [-0.15503682 -0.34806502]. \t  -1.6498608210348982 \t -0.6608370322265632\n",
            "28     \t [3.4151658  0.23670087]. \t  -27.654112970727507 \t -0.6608370322265632\n",
            "29     \t [ 6.54257869 -1.27232828]. \t  -52.56543751486487 \t -0.6608370322265632\n",
            "30     \t [ 0.78085351 -1.11597829]. \t  -5.89596239273167 \t -0.6608370322265632\n",
            "31     \t [ 1.75450306 -2.22842517]. \t  -134.30425328859306 \t -0.6608370322265632\n",
            "32     \t [1.94919333 0.37320184]. \t  -6.483004615041654 \t -0.6608370322265632\n",
            "33     \t [2.49488838 1.40223124]. \t  -6.3681739485166915 \t -0.6608370322265632\n",
            "34     \t [4.53817702 9.84643969]. \t  -71731.91582202556 \t -0.6608370322265632\n",
            "35     \t [-0.48462041 -8.5446659 ]. \t  -42930.953323449256 \t -0.6608370322265632\n",
            "36     \t [ 4.32130582 -1.42087353]. \t  -11.191865244224502 \t -0.6608370322265632\n",
            "37     \t [-4.23645762 -6.79406318]. \t  -18673.174550915657 \t -0.6608370322265632\n",
            "38     \t [0.00411651 0.12813953]. \t  -0.9934339480147786 \t -0.6608370322265632\n",
            "39     \t [-0.55255405  0.15964732]. \t  -3.138917589489545 \t -0.6608370322265632\n",
            "40     \t [ 0.79948955 -0.20138059]. \t  -1.07234771750924 \t -0.6608370322265632\n",
            "41     \t [4.21911297 1.80139989]. \t  -20.67729944878713 \t -0.6608370322265632\n",
            "42     \t [-9.09251394 -1.73514767]. \t  -558.7241520724862 \t -0.6608370322265632\n",
            "43     \t [-0.4526636  -1.14708337]. \t  -21.135601736152545 \t -0.6608370322265632\n",
            "44     \t [7.1318091 5.9497966]. \t  -8144.916701036996 \t -0.6608370322265632\n",
            "45     \t [-2.84303274  9.83692353]. \t  -77139.60634977453 \t -0.6608370322265632\n",
            "46     \t [-0.0747283  -0.08948061]. \t  -1.1715090907208028 \t -0.6608370322265632\n",
            "47     \t [ 4.79508243 -8.60485142]. \t  -41079.5131057878 \t -0.6608370322265632\n",
            "48     \t [0.926602   0.34816913]. \t  -0.9415330206969398 \t -0.6608370322265632\n",
            "49     \t [0.66694606 0.46070457]. \t  \u001b[92m-0.22848762858161603\u001b[0m \t -0.22848762858161603\n",
            "50     \t [6.18169791 2.12023797]. \t  -42.63230587658132 \t -0.22848762858161603\n",
            "51     \t [-6.93322465  3.21704821]. \t  -1589.9934468588062 \t -0.22848762858161603\n",
            "52     \t [-8.87122825 -6.11949031]. \t  -14131.446577500716 \t -0.22848762858161603\n",
            "53     \t [2.68085887 1.00126563]. \t  -3.7386792713057373 \t -0.22848762858161603\n",
            "54     \t [1.14493633 0.64073547]. \t  -0.23076734911195318 \t -0.22848762858161603\n",
            "55     \t [ 0.0769536  -0.59468342]. \t  -1.6466795796674982 \t -0.22848762858161603\n",
            "56     \t [ 4.78604696 -0.69940656]. \t  -43.33143017223823 \t -0.22848762858161603\n",
            "57     \t [7.35570631 0.57277674]. \t  -130.1632091411202 \t -0.22848762858161603\n",
            "58     \t [-1.01056436 -0.55111896]. \t  -9.278402021577415 \t -0.22848762858161603\n",
            "59     \t [ 0.03828497 -4.77979984]. \t  -4169.625990517897 \t -0.22848762858161603\n",
            "60     \t [-5.50634066  4.88851737]. \t  -5724.441837423282 \t -0.22848762858161603\n",
            "61     \t [8.76679784 4.29391193]. \t  -1640.5056011511847 \t -0.22848762858161603\n",
            "62     \t [ 0.53814252 -0.82267738]. \t  -1.543241617824508 \t -0.22848762858161603\n",
            "63     \t [-4.07708623  0.20798826]. \t  -60.44800878327176 \t -0.22848762858161603\n",
            "64     \t [ 2.45485905 -8.43743202]. \t  -39160.49674376253 \t -0.22848762858161603\n",
            "65     \t [0.34576108 0.29031239]. \t  -0.4908271938161843 \t -0.22848762858161603\n",
            "66     \t [9.18016933 2.20926011]. \t  -67.5914342215606 \t -0.22848762858161603\n",
            "67     \t [0.31687549 0.60476593]. \t  -0.8104589458355942 \t -0.22848762858161603\n",
            "68     \t [-9.0776245  -0.18304074]. \t  -268.8071160142377 \t -0.22848762858161603\n",
            "69     \t [0.7511544  0.65589036]. \t  \u001b[92m-0.08578648214106574\u001b[0m \t -0.08578648214106574\n",
            "70     \t [-0.182838    5.53966783]. \t  -7580.3405946686125 \t -0.08578648214106574\n",
            "71     \t [1.25853039 7.04421599]. \t  -19201.570925244465 \t -0.08578648214106574\n",
            "72     \t [8.06193148 8.60590626]. \t  -39284.21404271839 \t -0.08578648214106574\n",
            "73     \t [-4.05737843  1.88193186]. \t  -273.80806922211553 \t -0.08578648214106574\n",
            "74     \t [-4.86316402 -1.06300487]. \t  -135.85444704839995 \t -0.08578648214106574\n",
            "75     \t [-1.32087828 -0.11672028]. \t  -9.021360851319733 \t -0.08578648214106574\n",
            "76     \t [-9.21247784  5.10503346]. \t  -7628.3109577788855 \t -0.08578648214106574\n",
            "77     \t [-5.41577237  1.00350708]. \t  -151.56674301338865 \t -0.08578648214106574\n",
            "78     \t [-4.97692702  8.28742049]. \t  -40556.84917917206 \t -0.08578648214106574\n",
            "79     \t [9.37697898 0.9479332 ]. \t  -185.08124840416218 \t -0.08578648214106574\n",
            "80     \t [4.92905531 1.29978043]. \t  -20.243697047984963 \t -0.08578648214106574\n",
            "81     \t [1.55605664 5.36534893]. \t  -6276.315307365103 \t -0.08578648214106574\n",
            "82     \t [ 0.49368553 -0.45121304]. \t  -0.2713185369627945 \t -0.08578648214106574\n",
            "83     \t [5.04482816 2.88401065]. \t  -285.02641478571906 \t -0.08578648214106574\n",
            "84     \t [3.50735086 1.33896325]. \t  -6.299068342379327 \t -0.08578648214106574\n",
            "85     \t [9.48298339 8.28908303]. \t  -32806.59307027537 \t -0.08578648214106574\n",
            "86     \t [2.77266618 2.27223079]. \t  -117.25002781250015 \t -0.08578648214106574\n",
            "87     \t [-2.22392311  8.04670637]. \t  -34712.23316395252 \t -0.08578648214106574\n",
            "88     \t [-2.77815484 -3.57040748]. \t  -1613.0870410767286 \t -0.08578648214106574\n",
            "89     \t [6.22206129 1.86430453]. \t  -28.333393614984992 \t -0.08578648214106574\n",
            "90     \t [0.07531789 6.78340765]. \t  -16911.902094659115 \t -0.08578648214106574\n",
            "91     \t [ 3.26029884 -6.05289087]. \t  -9809.218906598351 \t -0.08578648214106574\n",
            "92     \t [-4.91703696 -6.17035295]. \t  -13177.608925696788 \t -0.08578648214106574\n",
            "93     \t [0.78199885 1.18467404]. \t  -8.24801580874012 \t -0.08578648214106574\n",
            "94     \t [-1.44728277  3.12767244]. \t  -888.9934981790411 \t -0.08578648214106574\n",
            "95     \t [ 3.71376938 -3.98894139]. \t  -1587.6567265140147 \t -0.08578648214106574\n",
            "96     \t [-9.33531504 -4.91257016]. \t  -6742.800976894743 \t -0.08578648214106574\n",
            "97     \t [-1.48768052  0.89607536]. \t  -25.329061398933813 \t -0.08578648214106574\n",
            "98     \t [9.91881346 2.85325587]. \t  -160.5290339656304 \t -0.08578648214106574\n",
            "99     \t [ 4.75492254 -2.11220385]. \t  -48.842018231321674 \t -0.08578648214106574\n",
            "100    \t [2.62822905 7.69920127]. \t  -26880.869212000907 \t -0.08578648214106574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOqNmgSIteAw",
        "outputId": "fd2cfb7f-b356-48ae-ed52-b779f8c325e6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_loser_18 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_18 = d2GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [7.46858806 9.37081326]. \t  -56594.51755971677 \t -0.3114572247523004\n",
            "init   \t [7.3838908  0.61711383]. \t  -128.46197097057788 \t -0.3114572247523004\n",
            "init   \t [-5.34543344 -9.77202391]. \t  -77131.46635795479 \t -0.3114572247523004\n",
            "init   \t [-1.39062363 -1.9529728 ]. \t  -168.39363959555394 \t -0.3114572247523004\n",
            "init   \t [ 0.45349343 -0.43216408]. \t  -0.3114572247523004 \t -0.3114572247523004\n",
            "1      \t [1.13550865 0.60170966]. \t  -0.3568618976266401 \t -0.3114572247523004\n",
            "2      \t [ 3.31812939 -1.0295427 ]. \t  -8.2451529455016 \t -0.3114572247523004\n",
            "3      \t [ 1.55677101 -0.46918709]. \t  -2.8031293078482102 \t -0.3114572247523004\n",
            "4      \t [0.21033772 1.30032928]. \t  -20.73880231778645 \t -0.3114572247523004\n",
            "5      \t [ 1.16777959 -0.15129036]. \t  -2.5459272005116174 \t -0.3114572247523004\n",
            "6      \t [2.47418429 2.01618701]. \t  -66.15017672068348 \t -0.3114572247523004\n",
            "7      \t [ 7.75378082 -9.0667772 ]. \t  -49129.77128708541 \t -0.3114572247523004\n",
            "8      \t [ 1.51924114 -1.78797001]. \t  -47.78979400321323 \t -0.3114572247523004\n",
            "9      \t [-7.62617408  6.75933541]. \t  -19677.75907934677 \t -0.3114572247523004\n",
            "10     \t [1.80849446 0.14477759]. \t  -6.895226115446554 \t -0.3114572247523004\n",
            "11     \t [-8.8857121  -1.85868522]. \t  -596.6997945649014 \t -0.3114572247523004\n",
            "12     \t [-12.563451    -9.79237446]. \t  -83697.43588765047 \t -0.3114572247523004\n",
            "13     \t [-0.89233545  8.68263527]. \t  -46010.33718604979 \t -0.3114572247523004\n",
            "14     \t [ 1.20311176 -8.23775477]. \t  -36190.27725568557 \t -0.3114572247523004\n",
            "15     \t [-4.97486204  1.13315472]. \t  -149.49090255246807 \t -0.3114572247523004\n",
            "16     \t [ 9.64534257 -3.26867649]. \t  -349.6064144297867 \t -0.3114572247523004\n",
            "17     \t [8.91827125 4.38311967]. \t  -1803.8132399047415 \t -0.3114572247523004\n",
            "18     \t [ 5.56274069 -4.38046667]. \t  -2174.3667392768994 \t -0.3114572247523004\n",
            "19     \t [-15.66957877  -2.56339516]. \t  -1938.0877840723028 \t -0.3114572247523004\n",
            "20     \t [ 0.01659496 -0.14973594]. \t  -0.9686812212706989 \t -0.3114572247523004\n",
            "21     \t [0.58763415 0.08320862]. \t  -0.8285081806874215 \t -0.3114572247523004\n",
            "22     \t [-5.27154561 -4.0510081 ]. \t  -2941.4671141840217 \t -0.3114572247523004\n",
            "23     \t [-2.35036867  3.71676354]. \t  -1808.7095734178135 \t -0.3114572247523004\n",
            "24     \t [-9.72325011  2.12185616]. \t  -816.4488745307899 \t -0.3114572247523004\n",
            "25     \t [3.81633004 5.39272946]. \t  -5915.066295079373 \t -0.3114572247523004\n",
            "26     \t [-9.32224177 -5.02565783]. \t  -7267.413617078394 \t -0.3114572247523004\n",
            "27     \t [ 4.65408046 -0.27111566]. \t  -53.9797189991604 \t -0.3114572247523004\n",
            "28     \t [-2.34368362 -5.41436337]. \t  -7446.9216999562595 \t -0.3114572247523004\n",
            "29     \t [ 2.2070049  -0.59857045]. \t  -5.899634380382098 \t -0.3114572247523004\n",
            "30     \t [-5.07611724  9.6193476 ]. \t  -72343.2166376359 \t -0.3114572247523004\n",
            "31     \t [ 1.53820266 -3.74958855]. \t  -1413.349018197135 \t -0.3114572247523004\n",
            "32     \t [-0.50881339  0.56400909]. \t  -4.8986864133896715 \t -0.3114572247523004\n",
            "33     \t [ 0.83978172 -0.30290454]. \t  -0.8870751760697538 \t -0.3114572247523004\n",
            "34     \t [ 0.47034676 -0.96131867]. \t  -4.077861806196292 \t -0.3114572247523004\n",
            "35     \t [9.7801873  0.77394804]. \t  -224.39987157760925 \t -0.3114572247523004\n",
            "36     \t [-6.82928239  2.46567943]. \t  -782.4187970264329 \t -0.3114572247523004\n",
            "37     \t [-1.44159435  0.29147986]. \t  -11.155346575845451 \t -0.3114572247523004\n",
            "38     \t [4.56005176 1.73262349]. \t  -16.843758511131934 \t -0.3114572247523004\n",
            "39     \t [3.22811239 9.7546806 ]. \t  -70002.42429157911 \t -0.3114572247523004\n",
            "40     \t [ 1.51388645 -1.29011615]. \t  -6.851896947195383 \t -0.3114572247523004\n",
            "41     \t [0.35858708 0.34939831]. \t  -0.4375984028170006 \t -0.3114572247523004\n",
            "42     \t [-1.45873764  3.37094123]. \t  -1175.8958193529193 \t -0.3114572247523004\n",
            "43     \t [ 1.13583175 -1.30228563]. \t  -10.198099674339856 \t -0.3114572247523004\n",
            "44     \t [ 2.38363385 -1.51792942]. \t  -11.812004860917737 \t -0.3114572247523004\n",
            "45     \t [1.29488692 2.03669629]. \t  -98.12550813143261 \t -0.3114572247523004\n",
            "46     \t [3.14615823 0.44296895]. \t  -19.771890457892486 \t -0.3114572247523004\n",
            "47     \t [-3.96348555 -0.80264161]. \t  -79.80220245638867 \t -0.3114572247523004\n",
            "48     \t [ 4.3295782 -1.3902185]. \t  -11.51698603824117 \t -0.3114572247523004\n",
            "49     \t [-9.99029967  9.72713166]. \t  -79501.57394069897 \t -0.3114572247523004\n",
            "50     \t [ 3.92571954 -1.22337911]. \t  -10.2985990501231 \t -0.3114572247523004\n",
            "51     \t [ 5.36164386 -1.11266445]. \t  -35.67730628127329 \t -0.3114572247523004\n",
            "52     \t [-0.56231739  0.3250666 ]. \t  -3.637916593236201 \t -0.3114572247523004\n",
            "53     \t [-9.47411852 -4.2405916 ]. \t  -4239.1767153471865 \t -0.3114572247523004\n",
            "54     \t [1.18019608 0.87848411]. \t  \u001b[92m-0.29640456113924124\u001b[0m \t -0.29640456113924124\n",
            "55     \t [2.50544651 0.48761377]. \t  -10.507455765258028 \t -0.29640456113924124\n",
            "56     \t [0.13205602 0.08168474]. \t  -0.781511468791801 \t -0.29640456113924124\n",
            "57     \t [ 0.50902042 -0.31144592]. \t  -0.43954029937797634 \t -0.29640456113924124\n",
            "58     \t [1.09198943 9.34927193]. \t  -60361.30298145525 \t -0.29640456113924124\n",
            "59     \t [6.01736486 1.94151523]. \t  -29.804470395611276 \t -0.29640456113924124\n",
            "60     \t [-6.35952935 -0.36455175]. \t  -141.95254316345068 \t -0.29640456113924124\n",
            "61     \t [1.1827683  0.69002812]. \t  \u001b[92m-0.13965617248442003\u001b[0m \t -0.13965617248442003\n",
            "62     \t [ 3.00439812 -0.20606656]. \t  -21.06423677740036 \t -0.13965617248442003\n",
            "63     \t [-6.16593836  8.44297235]. \t  -44294.66023866623 \t -0.13965617248442003\n",
            "64     \t [ 2.91741373 -6.11095378]. \t  -10305.570540281722 \t -0.13965617248442003\n",
            "65     \t [ 3.90927432 -1.68601516]. \t  -14.772370702381068 \t -0.13965617248442003\n",
            "66     \t [ 5.80697974 -3.99152321]. \t  -1381.0973566289983 \t -0.13965617248442003\n",
            "67     \t [ 7.65122881 -2.76088774]. \t  -159.56963396417095 \t -0.13965617248442003\n",
            "68     \t [ 5.66563522 -1.74182199]. \t  -22.091766105963604 \t -0.13965617248442003\n",
            "69     \t [-2.68325184  0.42643709]. \t  -32.13414096530475 \t -0.13965617248442003\n",
            "70     \t [-9.40595051 -9.81454609]. \t  -81762.00729734289 \t -0.13965617248442003\n",
            "71     \t [0.63921489 0.77483256]. \t  -0.7607665374321587 \t -0.13965617248442003\n",
            "72     \t [-4.11978008  9.49501985]. \t  -68055.49271965375 \t -0.13965617248442003\n",
            "73     \t [-9.00271179 -9.1175408 ]. \t  -61533.45992394255 \t -0.13965617248442003\n",
            "74     \t [ 6.42254531 -2.98170781]. \t  -287.44038370868657 \t -0.13965617248442003\n",
            "75     \t [1.61856633 4.40196963]. \t  -2758.5635071437023 \t -0.13965617248442003\n",
            "76     \t [2.4398866  0.90198281]. \t  -3.3943681171170725 \t -0.13965617248442003\n",
            "77     \t [ 4.87203415 -1.90860132]. \t  -26.642457297939035 \t -0.13965617248442003\n",
            "78     \t [-4.30895206  9.99717508]. \t  -83420.17417554371 \t -0.13965617248442003\n",
            "79     \t [1.77099676 0.9167528 ]. \t  -0.6106811551020479 \t -0.13965617248442003\n",
            "80     \t [4.92638197 0.89556046]. \t  -37.49216083441669 \t -0.13965617248442003\n",
            "81     \t [-0.58991652  7.26217391]. \t  -22503.477832359345 \t -0.13965617248442003\n",
            "82     \t [3.29021582 1.7579007 ]. \t  -21.95176131569489 \t -0.13965617248442003\n",
            "83     \t [0.18833227 9.67605813]. \t  -69986.44152893897 \t -0.13965617248442003\n",
            "84     \t [5.89672057 3.89720334]. \t  -1222.4860890315838 \t -0.13965617248442003\n",
            "85     \t [ 1.86863242 -6.05169292]. \t  -10190.204384489207 \t -0.13965617248442003\n",
            "86     \t [6.78639434 1.59289557]. \t  -39.34261612961099 \t -0.13965617248442003\n",
            "87     \t [ 8.45534118 -1.27291957]. \t  -109.96815249778564 \t -0.13965617248442003\n",
            "88     \t [5.89916839 1.46122112]. \t  -29.308051722625464 \t -0.13965617248442003\n",
            "89     \t [-2.56543486 -7.77172208]. \t  -30450.44183915338 \t -0.13965617248442003\n",
            "90     \t [-9.23238755 -6.67352048]. \t  -19432.10910463239 \t -0.13965617248442003\n",
            "91     \t [-1.41444092 -3.81254297]. \t  -1864.549814117909 \t -0.13965617248442003\n",
            "92     \t [-5.58080905 -2.34532776]. \t  -593.2280959541445 \t -0.13965617248442003\n",
            "93     \t [-4.25079556 -3.48029923]. \t  -1649.3087842062514 \t -0.13965617248442003\n",
            "94     \t [0.79344463 0.46348984]. \t  -0.3073644911053172 \t -0.13965617248442003\n",
            "95     \t [-1.46501658 -2.1326753 ]. \t  -229.1721286101595 \t -0.13965617248442003\n",
            "96     \t [ 7.96616065 -5.40487512]. \t  -5140.787007532487 \t -0.13965617248442003\n",
            "97     \t [-2.85345816 -9.83264323]. \t  -77015.66153958983 \t -0.13965617248442003\n",
            "98     \t [-4.75559923  8.75778337]. \t  -50057.95735549469 \t -0.13965617248442003\n",
            "99     \t [ 9.77428042 -6.17916373]. \t  -8945.403788429308 \t -0.13965617248442003\n",
            "100    \t [-0.98210894  5.46957316]. \t  -7400.7526049408825 \t -0.13965617248442003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4HnKuoqteDo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a928f80-88f8-4470-9c53-de9c06b04329"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_loser_19 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_19 = d2GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.18891153 5.22983981]. \t  -5727.433065247983 \t -79.36780179787098\n",
            "init   \t [-5.10936891  5.43378939]. \t  -8270.721382692287 \t -79.36780179787098\n",
            "init   \t [-2.91055837  5.93394799]. \t  -10771.054116716396 \t -79.36780179787098\n",
            "init   \t [-3.07999955 -1.12251129]. \t  -79.36780179787098 \t -79.36780179787098\n",
            "init   \t [-3.93392821 -1.1577216 ]. \t  -111.8486364354089 \t -79.36780179787098\n",
            "1      \t [ 1.46418724 -8.68671967]. \t  -44673.21769976243 \t -79.36780179787098\n",
            "2      \t [ 9.77192101 -1.83559751]. \t  -95.34580222090166 \t -79.36780179787098\n",
            "3      \t [-9.83688426 -8.54529444]. \t  -48715.20932349114 \t -79.36780179787098\n",
            "4      \t [9.60900479 6.31645299]. \t  -9926.306792942905 \t -79.36780179787098\n",
            "5      \t [-19.89378227 -17.68129152]. \t  -832873.3935472633 \t -79.36780179787098\n",
            "6      \t [-11.03676257   1.68012826]. \t  -701.4902303882396 \t -79.36780179787098\n",
            "7      \t [-221.40622812 -220.02527179]. \t  -18834988186.842484 \t -79.36780179787098\n",
            "8      \t [ 3.54230836 -1.67159161]. \t  \u001b[92m-14.836616950179952\u001b[0m \t -14.836616950179952\n",
            "9      \t [ 9.69261719 -9.77242937]. \t  -65820.84067993904 \t -14.836616950179952\n",
            "10     \t [6.1714239  1.74841076]. \t  -26.75024761279674 \t -14.836616950179952\n",
            "11     \t [-11.13575487   8.15137111]. \t  -41633.940943751186 \t -14.836616950179952\n",
            "12     \t [-10.8247456  -15.63695061]. \t  -499846.2558155469 \t -14.836616950179952\n",
            "13     \t [-4.0006694  -6.33080221]. \t  -14190.408102991003 \t -14.836616950179952\n",
            "14     \t [5.25069229 9.97986436]. \t  -75247.1563887796 \t -14.836616950179952\n",
            "15     \t [ 6.04018878 -4.75696195]. \t  -3101.3787215228717 \t -14.836616950179952\n",
            "16     \t [-9.71366175 -2.87887312]. \t  -1497.0563453997004 \t -14.836616950179952\n",
            "17     \t [-0.01867416 -2.7871405 ]. \t  -484.9524075499398 \t -14.836616950179952\n",
            "18     \t [3.28533039e-03 9.54204293e+00]. \t  -66320.270226604 \t -14.836616950179952\n",
            "19     \t [-6.07511758  9.87539755]. \t  -80950.23212547012 \t -14.836616950179952\n",
            "20     \t [1.87033353 0.88174238]. \t  \u001b[92m-0.9564275448657346\u001b[0m \t -0.9564275448657346\n",
            "21     \t [-1.99485964  1.54871836]. \t  -101.22945157110775 \t -0.9564275448657346\n",
            "22     \t [8.29922861 0.49047679]. \t  -175.5239149537315 \t -0.9564275448657346\n",
            "23     \t [5.32281973 4.50262384]. \t  -2500.2067375240285 \t -0.9564275448657346\n",
            "24     \t [-7.09003623  1.39325757]. \t  -306.2344721363766 \t -0.9564275448657346\n",
            "25     \t [ 5.87833288 -0.87175509]. \t  -61.789764091542935 \t -0.9564275448657346\n",
            "26     \t [1.66901144 0.5908698 ]. \t  -2.332315378665112 \t -0.9564275448657346\n",
            "27     \t [ 2.94846742 -0.54942807]. \t  -14.791996007475372 \t -0.9564275448657346\n",
            "28     \t [1.74455806 1.00922445]. \t  \u001b[92m-0.725490831216369\u001b[0m \t -0.725490831216369\n",
            "29     \t [3.36099777 1.23128568]. \t  -5.790619961472802 \t -0.725490831216369\n",
            "30     \t [4.07823914 0.5127498 ]. \t  -34.714852575284745 \t -0.725490831216369\n",
            "31     \t [1.19985657 1.0576507 ]. \t  -2.1923130056958544 \t -0.725490831216369\n",
            "32     \t [1.54039677 1.50735894]. \t  -18.338440491069356 \t -0.725490831216369\n",
            "33     \t [1.61226296 0.73707752]. \t  -0.9275793793917622 \t -0.725490831216369\n",
            "34     \t [ 2.79740563 -1.63587076]. \t  -16.28406693367883 \t -0.725490831216369\n",
            "35     \t [-6.1337001  -2.42919645]. \t  -694.2676943617337 \t -0.725490831216369\n",
            "36     \t [2.8662484  1.52275885]. \t  -9.75817822447908 \t -0.725490831216369\n",
            "37     \t [ 5.60982676 -8.11857633]. \t  -31880.560182608337 \t -0.725490831216369\n",
            "38     \t [ 1.68596507 -0.44724688]. \t  -3.7776541833733646 \t -0.725490831216369\n",
            "39     \t [1.28759636 0.63032908]. \t  \u001b[92m-0.5687443438123919\u001b[0m \t -0.5687443438123919\n",
            "40     \t [-0.18777685 -0.14018029]. \t  -1.5139425615306104 \t -0.5687443438123919\n",
            "41     \t [ 2.89175981 -4.27626718]. \t  -2272.420119245629 \t -0.5687443438123919\n",
            "42     \t [-5.49923748 -9.87419648]. \t  -80441.73748078574 \t -0.5687443438123919\n",
            "43     \t [-0.55451056 -0.09011807]. \t  -3.0680212516129677 \t -0.5687443438123919\n",
            "44     \t [6.06496491 1.23994435]. \t  -43.53455910851092 \t -0.5687443438123919\n",
            "45     \t [-0.18132266  0.64219402]. \t  -3.4201947638151076 \t -0.5687443438123919\n",
            "46     \t [ 3.15690964 -1.1965707 ]. \t  -4.824363825315868 \t -0.5687443438123919\n",
            "47     \t [9.71954047 2.46706392]. \t  -88.06743646545817 \t -0.5687443438123919\n",
            "48     \t [ 0.64782613 -0.20896758]. \t  -0.7523272766214226 \t -0.5687443438123919\n",
            "49     \t [-0.10943596  0.44571472]. \t  -1.744457910380689 \t -0.5687443438123919\n",
            "50     \t [0.04101544 0.51743679]. \t  -1.4086450926190945 \t -0.5687443438123919\n",
            "51     \t [1.05995838 0.3443026 ]. \t  -1.3578245007950012 \t -0.5687443438123919\n",
            "52     \t [-4.93788066  0.59444666]. \t  -98.98176829668556 \t -0.5687443438123919\n",
            "53     \t [ 9.74263245 -5.38350273]. \t  -4727.073427523028 \t -0.5687443438123919\n",
            "54     \t [-9.46079549  4.0904731 ]. \t  -3794.494128750665 \t -0.5687443438123919\n",
            "55     \t [ 2.49263756 -0.64160804]. \t  -7.80119752836785 \t -0.5687443438123919\n",
            "56     \t [-9.88305268  3.57345874]. \t  -2627.9148983338264 \t -0.5687443438123919\n",
            "57     \t [-0.13029439  7.62149116]. \t  -27054.77398453055 \t -0.5687443438123919\n",
            "58     \t [ 7.64504727 -1.64755856]. \t  -53.97928492671871 \t -0.5687443438123919\n",
            "59     \t [9.53325184 9.96976773]. \t  -71710.9705251735 \t -0.5687443438123919\n",
            "60     \t [0.6840267  0.89720559]. \t  -1.814528304778658 \t -0.5687443438123919\n",
            "61     \t [0.95205653 0.65289855]. \t  \u001b[92m-0.02210046413617886\u001b[0m \t -0.02210046413617886\n",
            "62     \t [ 3.72077027 -1.43675858]. \t  -7.735160207135614 \t -0.02210046413617886\n",
            "63     \t [-5.26775709  4.094441  ]. \t  -3049.6450899565607 \t -0.02210046413617886\n",
            "64     \t [0.1437718  0.00606477]. \t  -0.7744250973727614 \t -0.02210046413617886\n",
            "65     \t [-8.51581064 -6.35642805]. \t  -16048.163840611456 \t -0.02210046413617886\n",
            "66     \t [-0.12631791 -8.28057783]. \t  -37683.12253195882 \t -0.02210046413617886\n",
            "67     \t [6.83967979 1.89499509]. \t  -34.33624386773789 \t -0.02210046413617886\n",
            "68     \t [1.419442   4.07963932]. \t  -2031.2474747312667 \t -0.02210046413617886\n",
            "69     \t [ 3.78253068 -1.31721328]. \t  -7.937700772204609 \t -0.02210046413617886\n",
            "70     \t [-8.65575034 -5.5448383 ]. \t  -9934.216145768465 \t -0.02210046413617886\n",
            "71     \t [3.68954082 1.72482088]. \t  -17.45310921132101 \t -0.02210046413617886\n",
            "72     \t [2.32381192 9.31181879]. \t  -58549.521470673964 \t -0.02210046413617886\n",
            "73     \t [2.26427693 2.81049035]. \t  -367.9061270205067 \t -0.02210046413617886\n",
            "74     \t [0.49812806 2.25226803]. \t  -186.39245707661863 \t -0.02210046413617886\n",
            "75     \t [ 7.55133276 -4.59334745]. \t  -2443.658936169019 \t -0.02210046413617886\n",
            "76     \t [5.95833852 5.32796835]. \t  -5189.148320446915 \t -0.02210046413617886\n",
            "77     \t [4.68751362 2.33051026]. \t  -89.86005760374469 \t -0.02210046413617886\n",
            "78     \t [-1.24901293 -9.33659734]. \t  -61670.94055074521 \t -0.02210046413617886\n",
            "79     \t [-4.9410944  6.2398932]. \t  -13751.490771490682 \t -0.02210046413617886\n",
            "80     \t [-0.23602593  2.10356679]. \t  -166.63900281264824 \t -0.02210046413617886\n",
            "81     \t [0.71643498 4.29508963]. \t  -2617.942914090971 \t -0.02210046413617886\n",
            "82     \t [ 7.79848131 -4.21075841]. \t  -1576.6462240394658 \t -0.02210046413617886\n",
            "83     \t [ 8.7539227  -2.88569542]. \t  -184.96080419553866 \t -0.02210046413617886\n",
            "84     \t [-8.88001853  5.7413491 ]. \t  -11289.549130960664 \t -0.02210046413617886\n",
            "85     \t [2.81089834 1.15366833]. \t  -3.323753070715753 \t -0.02210046413617886\n",
            "86     \t [-5.96788264  6.81807644]. \t  -19626.883857233865 \t -0.02210046413617886\n",
            "87     \t [-4.4992704  -8.03524031]. \t  -35743.89855337572 \t -0.02210046413617886\n",
            "88     \t [ 1.8961735  -1.32638718]. \t  -6.067700625202037 \t -0.02210046413617886\n",
            "89     \t [ 2.27665772 -1.26445249]. \t  -3.3264198310195807 \t -0.02210046413617886\n",
            "90     \t [-2.5565048   1.26777645]. \t  -79.2580482941634 \t -0.02210046413617886\n",
            "91     \t [ 1.49281686 -0.97523396]. \t  -0.5779962492910728 \t -0.02210046413617886\n",
            "92     \t [8.88254343 7.4035404 ]. \t  -20360.147479058807 \t -0.02210046413617886\n",
            "93     \t [ 4.31696136 -1.52430922]. \t  -11.22013278578763 \t -0.02210046413617886\n",
            "94     \t [4.33586629 0.94515392]. \t  -24.125196097896414 \t -0.02210046413617886\n",
            "95     \t [-4.56624156 -3.7311091 ]. \t  -2131.6166846930196 \t -0.02210046413617886\n",
            "96     \t [-4.2956356  -9.65005052]. \t  -72640.99768931672 \t -0.02210046413617886\n",
            "97     \t [5.96686349 3.20976704]. \t  -453.2320428473832 \t -0.02210046413617886\n",
            "98     \t [-7.3883821   2.35828361]. \t  -755.707722767639 \t -0.02210046413617886\n",
            "99     \t [-9.88634809  7.60304415]. \t  -31618.46472055067 \t -0.02210046413617886\n",
            "100    \t [1.05089398 0.78511245]. \t  -0.06877206463151544 \t -0.02210046413617886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfiDmH_bteGg",
        "outputId": "18e645c9-8e86-4eb4-c801-f6158694e3f6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_loser_20 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_20 = d2GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.41034571 1.2905751 ]. \t  -7.547501684041513 \t -7.547501684041513\n",
            "init   \t [-0.23116335 -3.27044493]. \t  -936.6052904493877 \t -7.547501684041513\n",
            "init   \t [-2.48263644  0.64071738]. \t  -33.95727983513621 \t -7.547501684041513\n",
            "init   \t [-8.63787423  1.6905812 ]. \t  -504.96347244129606 \t -7.547501684041513\n",
            "init   \t [-5.24204473 -6.78486837]. \t  -18977.794322697024 \t -7.547501684041513\n",
            "1      \t [ 9.84240059 -0.83299926]. \t  -221.1494177066687 \t -7.547501684041513\n",
            "2      \t [6.73867561 8.36273465]. \t  -35481.183803304404 \t -7.547501684041513\n",
            "3      \t [ 8.38641873 -9.74808405]. \t  -66058.08004040587 \t -7.547501684041513\n",
            "4      \t [-3.83701174  9.3377774 ]. \t  -63551.82998472117 \t -7.547501684041513\n",
            "5      \t [-12.19903967  -8.2977456 ]. \t  -45116.755243920124 \t -7.547501684041513\n",
            "6      \t [  1.26673862 -11.47144427]. \t  -137205.63578357687 \t -7.547501684041513\n",
            "7      \t [-10.97478399   7.91175596]. \t  -37226.053497944995 \t -7.547501684041513\n",
            "8      \t [ 5.47512615 -4.02382947]. \t  -1468.0303063345998 \t -7.547501684041513\n",
            "9      \t [5.63829286 2.23576808]. \t  -59.515957614893054 \t -7.547501684041513\n",
            "10     \t [1.19303261 6.15434341]. \t  -11118.082403967923 \t -7.547501684041513\n",
            "11     \t [-9.54463926 -3.22235832]. \t  -1948.8029558851763 \t -7.547501684041513\n",
            "12     \t [-5.01121724 -1.68703234]. \t  -265.25914072042866 \t -7.547501684041513\n",
            "13     \t [-4.42201966  3.78243266]. \t  -2212.10178279139 \t -7.547501684041513\n",
            "14     \t [9.25668229 3.88152593]. \t  -939.7712605560133 \t -7.547501684041513\n",
            "15     \t [ 9.57479367 -4.59830407]. \t  -2213.9396015619786 \t -7.547501684041513\n",
            "16     \t [ 3.59151328 -0.2476054 ]. \t  -30.78242579367131 \t -7.547501684041513\n",
            "17     \t [ 3.13436832 -7.03540992]. \t  -18382.68788783232 \t -7.547501684041513\n",
            "18     \t [ 6.80318444 -0.80413179]. \t  -94.39557526986036 \t -7.547501684041513\n",
            "19     \t [0.07353678 1.37104359]. \t  -28.031294430127247 \t -7.547501684041513\n",
            "20     \t [2.24537612 1.87345321]. \t  -47.13841828913171 \t -7.547501684041513\n",
            "21     \t [ 1.56907163 -0.75434789]. \t  \u001b[92m-0.6953475680428638\u001b[0m \t -0.6953475680428638\n",
            "22     \t [-7.91645344  4.98784765]. \t  -6732.014903639481 \t -0.6953475680428638\n",
            "23     \t [0.97356652 0.45807815]. \t  \u001b[92m-0.6142988143004436\u001b[0m \t -0.6142988143004436\n",
            "24     \t [2.04126339 0.2563074 ]. \t  -8.379485400996032 \t -0.6142988143004436\n",
            "25     \t [2.89502442 9.87660257]. \t  -73884.91156089552 \t -0.6142988143004436\n",
            "26     \t [1.10851917 0.19428036]. \t  -2.1460761247010613 \t -0.6142988143004436\n",
            "27     \t [ 2.35187261 -0.88310616]. \t  -3.08246657976607 \t -0.6142988143004436\n",
            "28     \t [-1.51682124 -6.5001646 ]. \t  -14805.593989173782 \t -0.6142988143004436\n",
            "29     \t [4.75441337e+00 2.94046931e-03]. \t  -59.304183931531455 \t -0.6142988143004436\n",
            "30     \t [ 3.08937579 -1.91176107]. \t  -39.987102000861455 \t -0.6142988143004436\n",
            "31     \t [0.88675372 0.81948702]. \t  \u001b[92m-0.429361323646086\u001b[0m \t -0.429361323646086\n",
            "32     \t [1.08758492 1.09581677]. \t  -3.4610936757242396 \t -0.429361323646086\n",
            "33     \t [ 2.133047   -0.35171012]. \t  -8.395124681097528 \t -0.429361323646086\n",
            "34     \t [-0.46424221  0.38160145]. \t  -3.2855099476439045 \t -0.429361323646086\n",
            "35     \t [1.29907932 0.92614417]. \t  -0.4362375627798162 \t -0.429361323646086\n",
            "36     \t [-0.46140817 -0.15979036]. \t  -2.6609732203354115 \t -0.429361323646086\n",
            "37     \t [-0.31083107  0.17096781]. \t  -1.9910298573398262 \t -0.429361323646086\n",
            "38     \t [1.2794607  0.49242837]. \t  -1.3405248153936646 \t -0.429361323646086\n",
            "39     \t [4.15468728 3.76019348]. \t  -1173.8311036297891 \t -0.429361323646086\n",
            "40     \t [6.69241841 1.71294587]. \t  -33.761748337579775 \t -0.429361323646086\n",
            "41     \t [5.38047634 1.39580166]. \t  -23.592798871500698 \t -0.429361323646086\n",
            "42     \t [-8.46193236 -9.39442377]. \t  -68519.05264027971 \t -0.429361323646086\n",
            "43     \t [ 1.23830095 -0.64916763]. \t  \u001b[92m-0.3695704673718105\u001b[0m \t -0.3695704673718105\n",
            "44     \t [-0.78664609  0.09319311]. \t  -4.4849877300655745 \t -0.3695704673718105\n",
            "45     \t [-5.31984363  0.58723395]. \t  -112.16935051451406 \t -0.3695704673718105\n",
            "46     \t [-1.79830301  1.68982602]. \t  -120.61048256335266 \t -0.3695704673718105\n",
            "47     \t [ 0.04686235 -0.1308756 ]. \t  -0.9087891819257023 \t -0.3695704673718105\n",
            "48     \t [-1.07695296 -0.67179586]. \t  -12.151146752139825 \t -0.3695704673718105\n",
            "49     \t [1.11134397 0.72212149]. \t  \u001b[92m-0.021761456759101162\u001b[0m \t -0.021761456759101162\n",
            "50     \t [-7.40554879 -0.92130892]. \t  -236.3886239345398 \t -0.021761456759101162\n",
            "51     \t [-3.20948066 -0.82850565]. \t  -59.71511199630507 \t -0.021761456759101162\n",
            "52     \t [9.55322663 6.75971883]. \t  -13466.88613990843 \t -0.021761456759101162\n",
            "53     \t [ 8.36030492 -9.70579298]. \t  -64886.2395979656 \t -0.021761456759101162\n",
            "54     \t [ 7.54899001 -6.5777006 ]. \t  -12519.612770632544 \t -0.021761456759101162\n",
            "55     \t [ 5.62561432 -5.31808828]. \t  -5210.860236817998 \t -0.021761456759101162\n",
            "56     \t [ 5.02624    -1.52125245]. \t  -16.527133174138513 \t -0.021761456759101162\n",
            "57     \t [-1.70044178  7.3703767 ]. \t  -24359.483718936048 \t -0.021761456759101162\n",
            "58     \t [-8.13851557  3.27121202]. \t  -1828.7559536916694 \t -0.021761456759101162\n",
            "59     \t [-4.85954037  1.18420917]. \t  -151.8154572199726 \t -0.021761456759101162\n",
            "60     \t [ 0.98001672 -0.49114353]. \t  -0.49555669428749133 \t -0.021761456759101162\n",
            "61     \t [ 6.45334052 -2.34556267]. \t  -71.14370387407709 \t -0.021761456759101162\n",
            "62     \t [ 4.35537261 -3.04429801]. \t  -413.41059438586024 \t -0.021761456759101162\n",
            "63     \t [1.58271516 0.93359517]. \t  -0.3910676451479479 \t -0.021761456759101162\n",
            "64     \t [ 3.36340441 -0.82636672]. \t  -13.56681562067499 \t -0.021761456759101162\n",
            "65     \t [ 2.48055822 -2.07768997]. \t  -77.91168201801715 \t -0.021761456759101162\n",
            "66     \t [2.96980569 1.03041568]. \t  -5.312557232212405 \t -0.021761456759101162\n",
            "67     \t [0.40128712 2.84430967]. \t  -498.30632354137344 \t -0.021761456759101162\n",
            "68     \t [ 4.48980304 -1.20368817]. \t  -17.248115546286638 \t -0.021761456759101162\n",
            "69     \t [2.26004594 0.99505905]. \t  -1.7442481176453555 \t -0.021761456759101162\n",
            "70     \t [-2.32250706  0.05577675]. \t  -21.885012055803116 \t -0.021761456759101162\n",
            "71     \t [ 5.24493667 -1.41543128]. \t  -21.08499936214264 \t -0.021761456759101162\n",
            "72     \t [9.87482425 3.93322583]. \t  -966.2904962785457 \t -0.021761456759101162\n",
            "73     \t [8.15486285 1.08691017]. \t  -118.28926402061984 \t -0.021761456759101162\n",
            "74     \t [7.42857409 3.56574496]. \t  -689.3625783898757 \t -0.021761456759101162\n",
            "75     \t [6.3369491  0.92569189]. \t  -71.22983838981094 \t -0.021761456759101162\n",
            "76     \t [-9.61642662 -2.64136866]. \t  -1223.806175069267 \t -0.021761456759101162\n",
            "77     \t [-9.6263604  -0.07715197]. \t  -298.7118496611652 \t -0.021761456759101162\n",
            "78     \t [ 3.16739154 -1.47344775]. \t  -7.457449705698808 \t -0.021761456759101162\n",
            "79     \t [ 0.70760839 -0.37314409]. \t  -0.4538071853858664 \t -0.021761456759101162\n",
            "80     \t [ 1.75936005 -0.04383179]. \t  -6.740311750250135 \t -0.021761456759101162\n",
            "81     \t [1.04852511 0.67514145]. \t  -0.03983415712446472 \t -0.021761456759101162\n",
            "82     \t [-4.05024547  7.19737999]. \t  -23204.618124812292 \t -0.021761456759101162\n",
            "83     \t [ 0.50532017 -6.05037639]. \t  -10573.380617831723 \t -0.021761456759101162\n",
            "84     \t [8.71106375 2.33137669]. \t  -68.78799614598407 \t -0.021761456759101162\n",
            "85     \t [-5.14931039  8.85200777]. \t  -52438.625272453966 \t -0.021761456759101162\n",
            "86     \t [-4.17190662 -7.01645764]. \t  -21093.9217020648 \t -0.021761456759101162\n",
            "87     \t [9.79185003 1.30121326]. \t  -159.3584646484842 \t -0.021761456759101162\n",
            "88     \t [-6.98147209  5.98337434]. \t  -12414.282514773455 \t -0.021761456759101162\n",
            "89     \t [5.75050819 1.8107523 ]. \t  -23.870276729847898 \t -0.021761456759101162\n",
            "90     \t [-7.15846338  8.10771801]. \t  -38502.36355553295 \t -0.021761456759101162\n",
            "91     \t [3.9288047  0.96962721]. \t  -16.97019880376162 \t -0.021761456759101162\n",
            "92     \t [7.8119471  4.05705686]. \t  -1307.1730922817023 \t -0.021761456759101162\n",
            "93     \t [-0.9094418   3.29480777]. \t  -1027.0614777163194 \t -0.021761456759101162\n",
            "94     \t [5.82498226 4.34207538]. \t  -2056.236826655591 \t -0.021761456759101162\n",
            "95     \t [0.57946981 0.50442577]. \t  -0.18680845491087136 \t -0.021761456759101162\n",
            "96     \t [7.29773569 2.43456983]. \t  -81.18531119895279 \t -0.021761456759101162\n",
            "97     \t [4.42086303 1.4401702 ]. \t  -11.851015480024909 \t -0.021761456759101162\n",
            "98     \t [-9.7307755  -8.74354945]. \t  -53012.24150789965 \t -0.021761456759101162\n",
            "99     \t [ 0.61756185 -0.10703041]. \t  -0.8534781633502295 \t -0.021761456759101162\n",
            "100    \t [-3.51159788  9.08679019]. \t  -56906.74807362155 \t -0.021761456759101162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kM4bcwSteJO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc26038f-5175-4158-fc48-9bc45f0cc88f"
      },
      "source": [
        "end_lose = time.time()\r\n",
        "end_lose\r\n",
        "\r\n",
        "time_lose = end_lose - start_lose\r\n",
        "time_lose\r\n",
        "\r\n",
        "start_win = time.time()\r\n",
        "start_win"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1616069502.955487"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NMzLM7nteQH",
        "outputId": "0e7302ee-c588-40a4-e4fd-198df0bcf921"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_winner_1 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_1 = dGPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 2.24340351 -6.61860491]. \t  -14577.093458365041 \t -540.0726783817402\n",
            "init   \t [-1.27881961  5.38524945]. \t  -7033.582264098692 \t -540.0726783817402\n",
            "init   \t [-4.09349391 -7.01674086]. \t  -21064.19892235138 \t -540.0726783817402\n",
            "init   \t [-9.55043351 -1.59551015]. \t  -540.0726783817402 \t -540.0726783817402\n",
            "init   \t [-5.22635718 -3.24687616]. \t  -1423.2804229542423 \t -540.0726783817402\n",
            "1      \t [8.37416725 6.27688756]. \t  -9973.589615047737 \t -540.0726783817402\n",
            "2      \t [-9.76713775  9.96269922]. \t  -86875.28945668103 \t -540.0726783817402\n",
            "3      \t [ 9.98926049 -9.3463478 ]. \t  -54345.61921377557 \t -540.0726783817402\n",
            "4      \t [ 9.44490266 -0.94685252]. \t  \u001b[92m-188.41779181630653\u001b[0m \t -188.41779181630653\n",
            "5      \t [3.33951676 0.29151535]. \t  \u001b[92m-25.565488361329496\u001b[0m \t -25.565488361329496\n",
            "6      \t [-10. -10.]. \t  -88321.0 \t -25.565488361329496\n",
            "7      \t [3.1213907  9.77966003]. \t  -70814.45944220333 \t -25.565488361329496\n",
            "8      \t [-8.39356713  3.79485161]. \t  -2855.229393107849 \t -25.565488361329496\n",
            "9      \t [-1.06492013 -0.23704842]. \t  \u001b[92m-7.035984782774505\u001b[0m \t -7.035984782774505\n",
            "10     \t [3.73729516 3.87282622]. \t  -1386.6964129825967 \t -7.035984782774505\n",
            "11     \t [ 6.55461332 -3.95181908]. \t  -1248.9731092210711 \t -7.035984782774505\n",
            "12     \t [-3.45829526  9.5441169 ]. \t  -68943.27824726085 \t -7.035984782774505\n",
            "13     \t [-4.9256546   1.42566147]. \t  -196.77788657540822 \t -7.035984782774505\n",
            "14     \t [-0.70944433 -2.87780253]. \t  -599.6310532405525 \t -7.035984782774505\n",
            "15     \t [-1.86354998  0.42257721]. \t  -18.062873189048837 \t -7.035984782774505\n",
            "16     \t [0.43130961 1.18920016]. \t  -11.815436319310754 \t -7.035984782774505\n",
            "17     \t [6.29095807 0.1301104 ]. \t  -106.2968542772631 \t -7.035984782774505\n",
            "18     \t [9.81087286 1.95202726]. \t  -87.22413591234051 \t -7.035984782774505\n",
            "19     \t [ 5.00168139 -9.59215212]. \t  -64110.24551483478 \t -7.035984782774505\n",
            "20     \t [-8.47290176 -5.4294155 ]. \t  -9183.34760048564 \t -7.035984782774505\n",
            "21     \t [ 9.63529606 -4.70636043]. \t  -2477.8041716782013 \t -7.035984782774505\n",
            "22     \t [ 2.10667478 -1.40697019]. \t  -8.087911610910195 \t -7.035984782774505\n",
            "23     \t [ 3.12657415 -2.28769879]. \t  -112.28988188826028 \t -7.035984782774505\n",
            "24     \t [-0.42543688  0.07870148]. \t  \u001b[92m-2.415251261688739\u001b[0m \t -2.415251261688739\n",
            "25     \t [ 2.68125121 -0.05700443]. \t  -17.135204222732877 \t -2.415251261688739\n",
            "26     \t [ 1.71930958 -1.45904151]. \t  -13.403286277244282 \t -2.415251261688739\n",
            "27     \t [-5.18427868  5.17207526]. \t  -6926.104605471746 \t -2.415251261688739\n",
            "28     \t [-1.34483863  0.86349999]. \t  -21.585229676053643 \t -2.415251261688739\n",
            "29     \t [ 2.07131228 -1.22298534]. \t  -2.8407823837670105 \t -2.415251261688739\n",
            "30     \t [6.96535621 2.62145806]. \t  -127.4877947494711 \t -2.415251261688739\n",
            "31     \t [0.53704974 2.33728369]. \t  -216.0661741304401 \t -2.415251261688739\n",
            "32     \t [0.2202159 0.4435797]. \t  \u001b[92m-0.6681359533645824\u001b[0m \t -0.6681359533645824\n",
            "33     \t [7.43336651 9.91035573]. \t  -71481.07096276061 \t -0.6681359533645824\n",
            "34     \t [-1.77820937 -9.92177327]. \t  -78940.4055435768 \t -0.6681359533645824\n",
            "35     \t [-0.67284472  0.07520596]. \t  -3.7345499250685545 \t -0.6681359533645824\n",
            "36     \t [1.47370503 0.48830577]. \t  -2.2116965852242347 \t -0.6681359533645824\n",
            "37     \t [-3.36962251 -0.18173171]. \t  -42.70133096956171 \t -0.6681359533645824\n",
            "38     \t [-7.17646816  0.17192206]. \t  -171.56194147368984 \t -0.6681359533645824\n",
            "39     \t [ 4.31794567 -0.79687577]. \t  -29.588440926738635 \t -0.6681359533645824\n",
            "40     \t [1.14697865 0.64308355]. \t  \u001b[92m-0.22623091172615428\u001b[0m \t -0.22623091172615428\n",
            "41     \t [1.91296055 0.55388432]. \t  -4.210299085552981 \t -0.22623091172615428\n",
            "42     \t [0.39956306 6.00813304]. \t  -10309.62344575111 \t -0.22623091172615428\n",
            "43     \t [-2.35116103  1.16846887]. \t  -62.87966349738941 \t -0.22623091172615428\n",
            "44     \t [0.04977028 0.76301413]. \t  -3.387651208810879 \t -0.22623091172615428\n",
            "45     \t [3.62928088 9.60519888]. \t  -65449.59401056733 \t -0.22623091172615428\n",
            "46     \t [-9.81678463 -3.62781302]. \t  -2729.0342417360357 \t -0.22623091172615428\n",
            "47     \t [-5.91223484  8.2221967 ]. \t  -39878.20582274619 \t -0.22623091172615428\n",
            "48     \t [-6.51551697  3.36011556]. \t  -1749.6688622490576 \t -0.22623091172615428\n",
            "49     \t [ 1.8721179  -0.65523484]. \t  -2.8147616294887534 \t -0.22623091172615428\n",
            "50     \t [-9.67701872  0.80035905]. \t  -354.1616178156454 \t -0.22623091172615428\n",
            "51     \t [-2.11069268  6.54093577]. \t  -15384.670965355263 \t -0.22623091172615428\n",
            "52     \t [5.96478258 5.29373115]. \t  -5041.14190295565 \t -0.22623091172615428\n",
            "53     \t [1.04414074 0.77201658]. \t  \u001b[92m-0.04568447350163922\u001b[0m \t -0.04568447350163922\n",
            "54     \t [-2.48030052 -0.53597222]. \t  -30.776500629086115 \t -0.04568447350163922\n",
            "55     \t [-2.53389748 -8.241353  ]. \t  -38307.04787877346 \t -0.04568447350163922\n",
            "56     \t [ 0.90438136 -0.6183934 ]. \t  -0.04809723355965922 \t -0.04568447350163922\n",
            "57     \t [1.04065441 0.54129265]. \t  -0.415082284966141 \t -0.04568447350163922\n",
            "58     \t [3.93131005 2.43882584]. \t  -135.4569623453088 \t -0.04568447350163922\n",
            "59     \t [1.87578473 0.8398646 ]. \t  -1.1995226063309452 \t -0.04568447350163922\n",
            "60     \t [4.12134635 1.64621431]. \t  -13.116029555278471 \t -0.04568447350163922\n",
            "61     \t [3.76080112 1.67064888]. \t  -14.256539681943636 \t -0.04568447350163922\n",
            "62     \t [1.26768323 6.17745561]. \t  -11266.3472363487 \t -0.04568447350163922\n",
            "63     \t [-5.51046832  6.32629507]. \t  -14681.526646814731 \t -0.04568447350163922\n",
            "64     \t [-4.91725821  5.69084557]. \t  -9748.045106821997 \t -0.04568447350163922\n",
            "65     \t [-1.05056018  9.72447146]. \t  -72342.02090492527 \t -0.04568447350163922\n",
            "66     \t [-0.28079526 -1.18962417]. \t  -20.999649416284967 \t -0.04568447350163922\n",
            "67     \t [4.17942517 1.92441113]. \t  -30.939561935603656 \t -0.04568447350163922\n",
            "68     \t [4.63232884 0.15132843]. \t  -55.26629568426086 \t -0.04568447350163922\n",
            "69     \t [ 0.66852413 -0.85686487]. \t  -1.38959045539405 \t -0.04568447350163922\n",
            "70     \t [ 0.54438518 -0.71259045]. \t  -0.651615722105739 \t -0.04568447350163922\n",
            "71     \t [6.22407215 1.35304227]. \t  -40.42502717286698 \t -0.04568447350163922\n",
            "72     \t [0.27204965 0.68655086]. \t  -1.4294666570960466 \t -0.04568447350163922\n",
            "73     \t [-0.34990292 -0.58397473]. \t  -3.9521037563266956 \t -0.04568447350163922\n",
            "74     \t [-1.34219568 -9.43450538]. \t  -64346.925071400525 \t -0.04568447350163922\n",
            "75     \t [-1.21685943  1.94656391]. \t  -159.62138470584105 \t -0.04568447350163922\n",
            "76     \t [ 0.61423547 -0.20783688]. \t  -0.7060510196523301 \t -0.04568447350163922\n",
            "77     \t [ 1.05409182 -2.81709776]. \t  -439.1484486340524 \t -0.04568447350163922\n",
            "78     \t [5.02926629 9.79752204]. \t  -69919.53944352253 \t -0.04568447350163922\n",
            "79     \t [-2.11542918 -0.94077489]. \t  -39.90080283256739 \t -0.04568447350163922\n",
            "80     \t [-2.53249873 -8.90909798]. \t  -52032.73112440396 \t -0.04568447350163922\n",
            "81     \t [ 3.2481393  -1.14003583]. \t  -5.895950675392666 \t -0.04568447350163922\n",
            "82     \t [4.40914038 2.89442519]. \t  -316.4822145066299 \t -0.04568447350163922\n",
            "83     \t [ 3.6334737  -9.70929932]. \t  -68388.5118900972 \t -0.04568447350163922\n",
            "84     \t [0.86796068 0.72127877]. \t  -0.0769644466380927 \t -0.04568447350163922\n",
            "85     \t [-8.89096135  3.26803045]. \t  -1928.0780127427365 \t -0.04568447350163922\n",
            "86     \t [ 3.62652117 -8.97584202]. \t  -49622.52268386854 \t -0.04568447350163922\n",
            "87     \t [-1.94956536  6.35408321]. \t  -13686.721634452053 \t -0.04568447350163922\n",
            "88     \t [3.86958984 0.8940705 ]. \t  -18.548208159848564 \t -0.04568447350163922\n",
            "89     \t [4.32847395 3.52225331]. \t  -850.2723996448592 \t -0.04568447350163922\n",
            "90     \t [7.7636231  1.53836845]. \t  -64.11407165013854 \t -0.04568447350163922\n",
            "91     \t [1.27144605 1.61000405]. \t  -30.693378009561993 \t -0.04568447350163922\n",
            "92     \t [ 7.16829238 -3.33687695]. \t  -494.14050371366375 \t -0.04568447350163922\n",
            "93     \t [ 5.26403367 -6.36853727]. \t  -11525.385601354039 \t -0.04568447350163922\n",
            "94     \t [ 3.81949078 -1.11536762]. \t  -11.494785050745495 \t -0.04568447350163922\n",
            "95     \t [ 8.07429825 -8.26933835]. \t  -33172.07978023832 \t -0.04568447350163922\n",
            "96     \t [-8.07480962 -7.10576937]. \t  -23869.963676107633 \t -0.04568447350163922\n",
            "97     \t [-1.0387751   5.84805472]. \t  -9647.50139297433 \t -0.04568447350163922\n",
            "98     \t [ 7.87594017 -2.63372238]. \t  -119.2076989926021 \t -0.04568447350163922\n",
            "99     \t [1.67328896 1.86655206]. \t  -56.52195071756115 \t -0.04568447350163922\n",
            "100    \t [-5.34906297  1.65464129]. \t  -274.660530146399 \t -0.04568447350163922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBLcW6tlteS2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619a0c0f-c7c1-48b7-e579-899c0ca4d037"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_winner_2 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_2 = dGPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.03970985 -8.52192802]. \t  -38238.719775621794 \t -57.443300302345605\n",
            "init   \t [ 7.89863525 -1.27012905]. \t  -91.24969381804144 \t -57.443300302345605\n",
            "init   \t [-7.44644532  1.51715749]. \t  -361.74642651372136 \t -57.443300302345605\n",
            "init   \t [ 6.80941839 -1.29758898]. \t  -57.443300302345605 \t -57.443300302345605\n",
            "init   \t [3.91821121 3.69276194]. \t  -1099.406544633519 \t -57.443300302345605\n",
            "1      \t [-10. -10.]. \t  -88321.0 \t -57.443300302345605\n",
            "2      \t [-2.87227721  9.90072912]. \t  -79154.24352053565 \t -57.443300302345605\n",
            "3      \t [-1.3885243  -5.74216696]. \t  -9073.302042724144 \t -57.443300302345605\n",
            "4      \t [9.87440114 9.90717168]. \t  -69590.82131163792 \t -57.443300302345605\n",
            "5      \t [-9.80908004  8.2473246 ]. \t  -42658.83446871175 \t -57.443300302345605\n",
            "6      \t [-1.55889663  0.86514167]. \t  \u001b[92m-25.224229376658556\u001b[0m \t -25.224229376658556\n",
            "7      \t [-10.          -3.46054654]. \t  -2426.308851254873 \t -25.224229376658556\n",
            "8      \t [9.55883762 3.47985245]. \t  -503.0795369750957 \t -25.224229376658556\n",
            "9      \t [ 2.64204429 -1.94000997]. \t  -50.42731404525917 \t -25.224229376658556\n",
            "10     \t [3.57379202 9.11909317]. \t  -52976.494874433454 \t -25.224229376658556\n",
            "11     \t [ -4.33842781 -10.        ]. \t  -83536.88497196331 \t -25.224229376658556\n",
            "12     \t [ 1.56872435 -9.95745516]. \t  -77408.15056357955 \t -25.224229376658556\n",
            "13     \t [-1.18817533  4.63982674]. \t  -3919.8794279617728 \t -25.224229376658556\n",
            "14     \t [-5.55293388 -2.13420901]. \t  -472.92638173915384 \t -25.224229376658556\n",
            "15     \t [ 5.85332745 -4.1460442 ]. \t  -1651.0244287625735 \t -25.224229376658556\n",
            "16     \t [-5.70845475  4.88694372]. \t  -5763.703838600713 \t -25.224229376658556\n",
            "17     \t [ 5.35583151 -0.31563862]. \t  -72.15382084748747 \t -25.224229376658556\n",
            "18     \t [ 9.99041516 -4.36197466]. \t  -1655.9174046015619 \t -25.224229376658556\n",
            "19     \t [1.07177734 0.64291695]. \t  \u001b[92m-0.12529308537169406\u001b[0m \t -0.12529308537169406\n",
            "20     \t [1.16766381 0.82734279]. \t  \u001b[92m-0.1091773897524129\u001b[0m \t -0.1091773897524129\n",
            "21     \t [6.85598088 6.33790454]. \t  -10833.526750495294 \t -0.1091773897524129\n",
            "22     \t [-0.43909596 -1.4060613 ]. \t  -40.6698763129101 \t -0.1091773897524129\n",
            "23     \t [-5.86919609 -5.11240596]. \t  -6808.306331484121 \t -0.1091773897524129\n",
            "24     \t [9.72250327 0.50940577]. \t  -245.49143252935806 \t -0.1091773897524129\n",
            "25     \t [6.87793692 1.39642928]. \t  -52.28600779234054 \t -0.1091773897524129\n",
            "26     \t [-4.06893245  0.85512085]. \t  -86.88675444822141 \t -0.1091773897524129\n",
            "27     \t [2.07262921 0.01085187]. \t  -9.7401645670528 \t -0.1091773897524129\n",
            "28     \t [-2.51662921 -2.09177896]. \t  -266.2891512710654 \t -0.1091773897524129\n",
            "29     \t [0.82380382 0.75361347]. \t  -0.22581134127282743 \t -0.1091773897524129\n",
            "30     \t [-9.72123156  3.55352652]. \t  -2561.632552010419 \t -0.1091773897524129\n",
            "31     \t [1.7497237  0.56984027]. \t  -2.983352257081203 \t -0.1091773897524129\n",
            "32     \t [ 2.4849051  -5.23908941]. \t  -5496.074919485441 \t -0.1091773897524129\n",
            "33     \t [0.58975652 1.12552854]. \t  -7.7255800472495375 \t -0.1091773897524129\n",
            "34     \t [0.23272385 0.11518293]. \t  -0.6737410248369122 \t -0.1091773897524129\n",
            "35     \t [ 4.04330231 -0.61293341]. \t  -30.935263350021415 \t -0.1091773897524129\n",
            "36     \t [-0.60854477  0.27124232]. \t  -3.7295496879471584 \t -0.1091773897524129\n",
            "37     \t [-9.30549929 -0.21950445]. \t  -282.9933972414611 \t -0.1091773897524129\n",
            "38     \t [ 2.91326097 -0.04129658]. \t  -20.59502335940865 \t -0.1091773897524129\n",
            "39     \t [ 0.75508995 -1.46818765]. \t  -25.351106056591572 \t -0.1091773897524129\n",
            "40     \t [-3.95348592 -3.1273378 ]. \t  -1130.3505347278744 \t -0.1091773897524129\n",
            "41     \t [ 1.2854394  -0.53693253]. \t  -1.0864018795330384 \t -0.1091773897524129\n",
            "42     \t [1.30859837 0.91852788]. \t  -0.3821945218466912 \t -0.1091773897524129\n",
            "43     \t [-3.69818796  1.71305488]. \t  -205.1395047257916 \t -0.1091773897524129\n",
            "44     \t [0.59838142 0.41204679]. \t  -0.29526924665504295 \t -0.1091773897524129\n",
            "45     \t [-2.70827301 -0.16005325]. \t  -28.981047564501424 \t -0.1091773897524129\n",
            "46     \t [-0.08012039  0.47081473]. \t  -1.7146670029266076 \t -0.1091773897524129\n",
            "47     \t [-9.39681078  9.86068833]. \t  -83228.4691757223 \t -0.1091773897524129\n",
            "48     \t [ 0.90766099 -0.37734296]. \t  -0.784499367355668 \t -0.1091773897524129\n",
            "49     \t [-2.7849979 -6.5175139]. \t  -15411.28286027935 \t -0.1091773897524129\n",
            "50     \t [-8.28999214 -2.10020643]. \t  -671.9263016290016 \t -0.1091773897524129\n",
            "51     \t [2.86550724 0.72834129]. \t  -9.992883851980888 \t -0.1091773897524129\n",
            "52     \t [-7.41448581  7.87189908]. \t  -34575.4376890135 \t -0.1091773897524129\n",
            "53     \t [9.76705851 8.21340072]. \t  -31403.330122896183 \t -0.1091773897524129\n",
            "54     \t [-2.80091004  1.81608432]. \t  -191.06295292016443 \t -0.1091773897524129\n",
            "55     \t [-4.1069211   2.83806047]. \t  -843.4616834917406 \t -0.1091773897524129\n",
            "56     \t [-1.59186141 -0.12771658]. \t  -11.995645064568627 \t -0.1091773897524129\n",
            "57     \t [9.60190153 1.30557436]. \t  -150.69555963189242 \t -0.1091773897524129\n",
            "58     \t [-9.42952584 -4.1515827 ]. \t  -3963.336127203735 \t -0.1091773897524129\n",
            "59     \t [-6.46618709  1.68563166]. \t  -350.9352624725696 \t -0.1091773897524129\n",
            "60     \t [1.390785   7.89766594]. \t  -30433.296516509945 \t -0.1091773897524129\n",
            "61     \t [7.67868447 0.02907516]. \t  -162.47729186586284 \t -0.1091773897524129\n",
            "62     \t [9.06782253 7.72878671]. \t  -24441.615352562665 \t -0.1091773897524129\n",
            "63     \t [-2.83471316 -2.43994193]. \t  -449.31961196714644 \t -0.1091773897524129\n",
            "64     \t [8.33220336 7.60650159]. \t  -23117.065066203402 \t -0.1091773897524129\n",
            "65     \t [-0.54379441  6.45592031]. \t  -14081.343119217587 \t -0.1091773897524129\n",
            "66     \t [ 0.41045765 -9.86409628]. \t  -75420.11863943869 \t -0.1091773897524129\n",
            "67     \t [ 2.97072111 -4.8998992 ]. \t  -4062.4027889644326 \t -0.1091773897524129\n",
            "68     \t [1.75681562 0.88938156]. \t  -0.6338915115745557 \t -0.1091773897524129\n",
            "69     \t [-5.594978  -5.0845089]. \t  -6609.945274306271 \t -0.1091773897524129\n",
            "70     \t [0.72006032 0.58500891]. \t  \u001b[92m-0.08089944663980621\u001b[0m \t -0.08089944663980621\n",
            "71     \t [2.1130402 1.1241049]. \t  -1.581954353975373 \t -0.08089944663980621\n",
            "72     \t [ 7.70530973 -4.0913636 ]. \t  -1373.4771047270685 \t -0.08089944663980621\n",
            "73     \t [4.65405149 3.57390659]. \t  -886.2670756563372 \t -0.08089944663980621\n",
            "74     \t [-0.31603546 -0.55825071]. \t  -3.4966053616034607 \t -0.08089944663980621\n",
            "75     \t [ 0.60394876 -0.53768646]. \t  -0.15818119664587996 \t -0.08089944663980621\n",
            "76     \t [8.63905631 5.4974074 ]. \t  -5425.6474319473755 \t -0.08089944663980621\n",
            "77     \t [-5.7912885   4.92505611]. \t  -5943.893075288378 \t -0.08089944663980621\n",
            "78     \t [ 1.43691297 -0.75656224]. \t  -0.36158465696657627 \t -0.08089944663980621\n",
            "79     \t [-5.04643942  6.02138366]. \t  -12067.840893946795 \t -0.08089944663980621\n",
            "80     \t [ 1.38126724 -0.85602978]. \t  -0.15957995887680337 \t -0.08089944663980621\n",
            "81     \t [2.6698593  8.31714593]. \t  -36820.89887926603 \t -0.08089944663980621\n",
            "82     \t [-5.20499438  9.18626805]. \t  -60576.60751066257 \t -0.08089944663980621\n",
            "83     \t [9.81821777 7.46230331]. \t  -20704.07848571376 \t -0.08089944663980621\n",
            "84     \t [ 7.96364668 -3.79670713]. \t  -919.2992796888569 \t -0.08089944663980621\n",
            "85     \t [-0.42287358 -3.42135285]. \t  -1138.1612419423348 \t -0.08089944663980621\n",
            "86     \t [ 7.89604122 -2.16448552]. \t  -51.90046479188232 \t -0.08089944663980621\n",
            "87     \t [9.22419433 1.02904472]. \t  -168.63717468394677 \t -0.08089944663980621\n",
            "88     \t [-1.6844257  -7.01140427]. \t  -20008.808295517654 \t -0.08089944663980621\n",
            "89     \t [ 6.95948576 -4.63130315]. \t  -2618.660143559438 \t -0.08089944663980621\n",
            "90     \t [8.17555757 7.6307228 ]. \t  -23500.736860312965 \t -0.08089944663980621\n",
            "91     \t [-4.78552334  8.83038623]. \t  -51706.21634292386 \t -0.08089944663980621\n",
            "92     \t [-1.39136264 -7.11112906]. \t  -21029.567136461766 \t -0.08089944663980621\n",
            "93     \t [-1.75019318  1.96533379]. \t  -187.12493048878747 \t -0.08089944663980621\n",
            "94     \t [ 1.60902345 -0.96752657]. \t  -0.5094494834204859 \t -0.08089944663980621\n",
            "95     \t [ 6.33238924 -5.55543991]. \t  -6165.2998444628665 \t -0.08089944663980621\n",
            "96     \t [ 0.34924663 -0.39242671]. \t  -0.42688294269006555 \t -0.08089944663980621\n",
            "97     \t [7.18033546 2.18284709]. \t  -49.23503654820095 \t -0.08089944663980621\n",
            "98     \t [-5.90291294  3.79305508]. \t  -2452.7008239852353 \t -0.08089944663980621\n",
            "99     \t [ 5.087094   -4.05750602]. \t  -1566.7927954166332 \t -0.08089944663980621\n",
            "100    \t [-3.29500331  4.82568198]. \t  -4992.364448817138 \t -0.08089944663980621\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emgjXwfeuvRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d46450e4-db76-4e02-ce53-d4b5c89f86e7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_winner_3 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_3 = dGPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.03151214 -7.81497449]. \t  -27448.6338140421 \t -1.7863168283775226\n",
            "init   \t [-0.27757442 -0.00337645]. \t  -1.7863168283775226 \t -1.7863168283775226\n",
            "init   \t [ 3.14219998 -5.28230578]. \t  -5551.437171353593 \t -1.7863168283775226\n",
            "init   \t [ 2.25589781 -7.60695193]. \t  -25754.9725373955 \t -1.7863168283775226\n",
            "init   \t [ 4.24404593 -2.54878916]. \t  -163.60006264419064 \t -1.7863168283775226\n",
            "1      \t [-5.61872775  6.96371276]. \t  -21099.51033227109 \t -1.7863168283775226\n",
            "2      \t [-9.6736302  -6.79831075]. \t  -20965.88636571433 \t -1.7863168283775226\n",
            "3      \t [5.95256263 7.26277879]. \t  -19842.284445028028 \t -1.7863168283775226\n",
            "4      \t [-7.005107   -0.20856045]. \t  -164.67755732409705 \t -1.7863168283775226\n",
            "5      \t [ 9.92174856 -1.24037905]. \t  -173.29656257358286 \t -1.7863168283775226\n",
            "6      \t [-3.49504184 -4.56372338]. \t  -4097.283649472463 \t -1.7863168283775226\n",
            "7      \t [-0.48474644  3.89995824]. \t  -1912.3306371647445 \t -1.7863168283775226\n",
            "8      \t [ -4.65018414 -10.        ]. \t  -83795.32031470495 \t -1.7863168283775226\n",
            "9      \t [-10.           3.40113104]. \t  -2316.9074394652316 \t -1.7863168283775226\n",
            "10     \t [0.18037714 9.89102847]. \t  -76429.06005443927 \t -1.7863168283775226\n",
            "11     \t [6.01735261 1.64710048]. \t  -25.873507018285853 \t -1.7863168283775226\n",
            "12     \t [2.14917723 0.24623328]. \t  -9.545491484296612 \t -1.7863168283775226\n",
            "13     \t [ 9.97923216 -7.68882125]. \t  -23519.65817380143 \t -1.7863168283775226\n",
            "14     \t [9.78265058 3.98023095]. \t  -1036.5149354350756 \t -1.7863168283775226\n",
            "15     \t [-1.81396024 -0.42130086]. \t  -17.32705236975884 \t -1.7863168283775226\n",
            "16     \t [ 0.44249921 -1.24783729]. \t  -14.586732700486797 \t -1.7863168283775226\n",
            "17     \t [-9.60471767  9.5167891 ]. \t  -72878.44810758944 \t -1.7863168283775226\n",
            "18     \t [9.8019244  9.90399252]. \t  -69549.65804729251 \t -1.7863168283775226\n",
            "19     \t [3.51940524 2.1159052 ]. \t  -65.4194272420774 \t -1.7863168283775226\n",
            "20     \t [-10.          -1.94125367]. \t  -736.0879391029914 \t -1.7863168283775226\n",
            "21     \t [-4.24758989  1.86146007]. \t  -277.4172362138414 \t -1.7863168283775226\n",
            "22     \t [ 6.52378563 -1.63599301]. \t  -33.25393751607121 \t -1.7863168283775226\n",
            "23     \t [7.03828403 0.05606828]. \t  -135.35883001977373 \t -1.7863168283775226\n",
            "24     \t [-0.16824293 -2.51324239]. \t  -329.0968948675995 \t -1.7863168283775226\n",
            "25     \t [-0.16112697 -0.43879   ]. \t  -1.944885381909589 \t -1.7863168283775226\n",
            "26     \t [ 1.5296912  -0.98904783]. \t  \u001b[92m-0.6447868425974976\u001b[0m \t -0.6447868425974976\n",
            "27     \t [ 0.56926618 -0.50273568]. \t  \u001b[92m-0.1936673644173037\u001b[0m \t -0.1936673644173037\n",
            "28     \t [-0.73479683  0.16673361]. \t  -4.258974926798736 \t -0.1936673644173037\n",
            "29     \t [2.93727018 4.87777889]. \t  -3990.673606317482 \t -0.1936673644173037\n",
            "30     \t [3.38241271 0.95408165]. \t  -10.554760625414772 \t -0.1936673644173037\n",
            "31     \t [4.25025825 0.67153897]. \t  -32.986793741507334 \t -0.1936673644173037\n",
            "32     \t [ 1.05030748 -0.71487019]. \t  \u001b[92m-0.004124564057247064\u001b[0m \t -0.004124564057247064\n",
            "33     \t [ 6.57388159 -3.60798096]. \t  -788.5425604706728 \t -0.004124564057247064\n",
            "34     \t [6.21132536 2.77450206]. \t  -195.8642463605286 \t -0.004124564057247064\n",
            "35     \t [2.23416298 1.56274849]. \t  -15.570306831928129 \t -0.004124564057247064\n",
            "36     \t [-7.52343698 -3.48433805]. \t  -2095.7218051061277 \t -0.004124564057247064\n",
            "37     \t [-4.47311721 -0.43194985]. \t  -76.92784356054268 \t -0.004124564057247064\n",
            "38     \t [ 1.60802288 -1.48655345]. \t  -16.18054914287969 \t -0.004124564057247064\n",
            "39     \t [-1.56437886  0.64539805]. \t  -18.071630711091196 \t -0.004124564057247064\n",
            "40     \t [8.00016058 6.64169452]. \t  -12920.799139913539 \t -0.004124564057247064\n",
            "41     \t [ 0.24952535 -0.64172713]. \t  -1.2223985716171415 \t -0.004124564057247064\n",
            "42     \t [4.89923918 1.56102435]. \t  -15.205381498510047 \t -0.004124564057247064\n",
            "43     \t [ 6.21859053 -1.30896534]. \t  -42.82209361240088 \t -0.004124564057247064\n",
            "44     \t [0.79556865 0.48022734]. \t  -0.26534802129040536 \t -0.004124564057247064\n",
            "45     \t [ 1.85954583 -0.92218021]. \t  -0.789198762757316 \t -0.004124564057247064\n",
            "46     \t [ 2.70586642 -0.7936713 ]. \t  -7.092032960515968 \t -0.004124564057247064\n",
            "47     \t [ 2.78862933 -0.32931037]. \t  -16.426874596858813 \t -0.004124564057247064\n",
            "48     \t [ 1.24163532 -1.0450335 ]. \t  -1.835206414336658 \t -0.004124564057247064\n",
            "49     \t [ 1.36763018 -0.23226965]. \t  -3.309000331813914 \t -0.004124564057247064\n",
            "50     \t [1.03450793 0.42992548]. \t  -0.8852048554706723 \t -0.004124564057247064\n",
            "51     \t [ 0.86748188 -0.16286835]. \t  -1.3441524823017361 \t -0.004124564057247064\n",
            "52     \t [-6.3773704   2.11472078]. \t  -523.919765389744 \t -0.004124564057247064\n",
            "53     \t [ 9.93514852 -3.88876974]. \t  -904.8219058005776 \t -0.004124564057247064\n",
            "54     \t [5.61128105 1.20831125]. \t  -35.74955415345949 \t -0.004124564057247064\n",
            "55     \t [1.70116163 0.89641195]. \t  -0.5093195026013009 \t -0.004124564057247064\n",
            "56     \t [-4.54761256 -1.40127031]. \t  -174.4180854688124 \t -0.004124564057247064\n",
            "57     \t [1.88221773 1.02891965]. \t  -0.8888836986224902 \t -0.004124564057247064\n",
            "58     \t [-0.87813909 -0.2146557 ]. \t  -5.4103443495098364 \t -0.004124564057247064\n",
            "59     \t [8.59274687 0.97096605]. \t  -147.62278128884395 \t -0.004124564057247064\n",
            "60     \t [ 7.84258349 -1.06408862]. \t  -109.04943598589045 \t -0.004124564057247064\n",
            "61     \t [-2.59300614 -9.39339568]. \t  -64141.29417985735 \t -0.004124564057247064\n",
            "62     \t [-1.97006732 -0.32444149]. \t  -18.331261772302526 \t -0.004124564057247064\n",
            "63     \t [1.15948716 0.77169061]. \t  -0.027423882772555513 \t -0.004124564057247064\n",
            "64     \t [ 3.03252856 -4.64622883]. \t  -3226.9486716463393 \t -0.004124564057247064\n",
            "65     \t [-5.92098836 -9.43033498]. \t  -67600.59403596063 \t -0.004124564057247064\n",
            "66     \t [ 0.56816815 -0.02748304]. \t  -0.8286802273916385 \t -0.004124564057247064\n",
            "67     \t [-6.30677823  1.02322762]. \t  -194.53480580590767 \t -0.004124564057247064\n",
            "68     \t [ 0.97628404 -6.80955698]. \t  -16841.20832557118 \t -0.004124564057247064\n",
            "69     \t [4.73898245 3.74417699]. \t  -1099.64257608463 \t -0.004124564057247064\n",
            "70     \t [4.35490102 1.71732416]. \t  -16.020167119629722 \t -0.004124564057247064\n",
            "71     \t [-1.62358223 -1.94224028]. \t  -174.9941159860788 \t -0.004124564057247064\n",
            "72     \t [3.87107329 1.69555418]. \t  -15.302349597613741 \t -0.004124564057247064\n",
            "73     \t [ 3.76716717 -0.49930803]. \t  -29.024054457405057 \t -0.004124564057247064\n",
            "74     \t [ 0.25318592 -3.02381372]. \t  -650.9873297211229 \t -0.004124564057247064\n",
            "75     \t [0.3848995  0.38581899]. \t  -0.39355174081710126 \t -0.004124564057247064\n",
            "76     \t [ 2.85568635 -1.52019938]. \t  -9.683386625166147 \t -0.004124564057247064\n",
            "77     \t [2.29302253 1.04555394]. \t  -1.694658477587745 \t -0.004124564057247064\n",
            "78     \t [-0.7973397 -0.612444 ]. \t  -8.020035398703488 \t -0.004124564057247064\n",
            "79     \t [5.50025847 2.631525  ]. \t  -159.68360540471554 \t -0.004124564057247064\n",
            "80     \t [ 7.7878485  -0.83363241]. \t  -127.94273632673325 \t -0.004124564057247064\n",
            "81     \t [0.33716028 0.69442895]. \t  -1.226374231584093 \t -0.004124564057247064\n",
            "82     \t [3.33980119 3.6619608 ]. \t  -1108.1060419131188 \t -0.004124564057247064\n",
            "83     \t [0.82387246 0.76164079]. \t  -0.25724444734025764 \t -0.004124564057247064\n",
            "84     \t [-3.50885816  4.46413975]. \t  -3781.5402603850184 \t -0.004124564057247064\n",
            "85     \t [-8.65102928  9.910253  ]. \t  -84206.5061584383 \t -0.004124564057247064\n",
            "86     \t [1.82850804 4.73345635]. \t  -3695.7131544669023 \t -0.004124564057247064\n",
            "87     \t [ 0.65459451 -1.51311626]. \t  -30.92187508731874 \t -0.004124564057247064\n",
            "88     \t [-9.65611648  3.01445109]. \t  -1662.5647470123702 \t -0.004124564057247064\n",
            "89     \t [-0.21351966 -3.97757932]. \t  -2031.0558726958338 \t -0.004124564057247064\n",
            "90     \t [-9.26743751  1.50557426]. \t  -486.3524509050705 \t -0.004124564057247064\n",
            "91     \t [2.49362315 1.53912676]. \t  -12.30377062266507 \t -0.004124564057247064\n",
            "92     \t [5.50969674 1.87017177]. \t  -24.75012071444285 \t -0.004124564057247064\n",
            "93     \t [ 3.47587984 -4.77822747]. \t  -3565.6219455686582 \t -0.004124564057247064\n",
            "94     \t [-3.33938631 -3.56105429]. \t  -1666.394147176159 \t -0.004124564057247064\n",
            "95     \t [-3.27077896 -1.59613603]. \t  -158.22212195947031 \t -0.004124564057247064\n",
            "96     \t [ 8.41909521 -0.51026664]. \t  -179.81087431797224 \t -0.004124564057247064\n",
            "97     \t [-6.11691153  5.8954905 ]. \t  -11490.603962595726 \t -0.004124564057247064\n",
            "98     \t [-8.88127832 -9.83888317]. \t  -82100.84304076748 \t -0.004124564057247064\n",
            "99     \t [3.4531206  1.35373789]. \t  -6.107766665553132 \t -0.004124564057247064\n",
            "100    \t [ 9.497473   -0.19024079]. \t  -249.87168618536793 \t -0.004124564057247064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8riJpBBKuvT4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd1c6395-a235-473e-a22d-980b4730efff"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_winner_4 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_4 = dGPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [6.78722095 6.4258993 ]. \t  -11523.905614569041 \t -765.9755148718126\n",
            "init   \t [2.8441433  3.34445237]. \t  -765.9755148718126 \t -765.9755148718126\n",
            "init   \t [-9.21720189 -8.68655223]. \t  -51387.35679532384 \t -765.9755148718126\n",
            "init   \t [-4.4699872  -3.67278993]. \t  -2007.9696367442386 \t -765.9755148718126\n",
            "init   \t [-6.12815176 -6.05814236]. \t  -12700.957148627067 \t -765.9755148718126\n",
            "1      \t [ 5.15020416 -5.71066603]. \t  -7234.8050437839365 \t -765.9755148718126\n",
            "2      \t [-3.97305893  9.21389704]. \t  -60413.181901672484 \t -765.9755148718126\n",
            "3      \t [-8.51899843  2.33131307]. \t  -842.4810529068852 \t -765.9755148718126\n",
            "4      \t [ -0.08816825 -10.        ]. \t  -80071.73425919778 \t -765.9755148718126\n",
            "5      \t [8.27826207 0.32767616]. \t  \u001b[92m-183.01376768554263\u001b[0m \t -183.01376768554263\n",
            "6      \t [ 9.67802381 -9.83797028]. \t  -67708.7958293111 \t -183.01376768554263\n",
            "7      \t [-2.83803863  2.67646344]. \t  -604.0016776781533 \t -183.01376768554263\n",
            "8      \t [-10.           7.83183069]. \t  -35326.38761526054 \t -183.01376768554263\n",
            "9      \t [ 0.75202853 -1.7859465 ]. \t  \u001b[92m-63.391827927845476\u001b[0m \t -63.391827927845476\n",
            "10     \t [1.85893393 9.46393719]. \t  -62852.36972535914 \t -63.391827927845476\n",
            "11     \t [-10.          -3.00093023]. \t  -1690.2506727519876 \t -63.391827927845476\n",
            "12     \t [-0.43476566 -4.81699878]. \t  -4390.352040313924 \t -63.391827927845476\n",
            "13     \t [ 4.12450282 -0.68838776]. \t  \u001b[92m-29.945966020624326\u001b[0m \t -29.945966020624326\n",
            "14     \t [ 7.38444106 -2.37879273]. \t  -71.69599958393343 \t -29.945966020624326\n",
            "15     \t [9.86518382 3.28980966]. \t  -356.1523837990128 \t -29.945966020624326\n",
            "16     \t [-6.0952986   4.98949329]. \t  -6296.695796448582 \t -29.945966020624326\n",
            "17     \t [9.19083537 9.88939013]. \t  -69563.89849609062 \t -29.945966020624326\n",
            "18     \t [0.01700021 0.89149128]. \t  \u001b[92m-5.911884012481753\u001b[0m \t -5.911884012481753\n",
            "19     \t [-5.46209268  0.06431123]. \t  -101.60841824500434 \t -5.911884012481753\n",
            "20     \t [-2.14453348 -0.60144883]. \t  -26.339109365664953 \t -5.911884012481753\n",
            "21     \t [ 8.8265108  -4.11307361]. \t  -1312.078224120276 \t -5.911884012481753\n",
            "22     \t [ 6.27895625 -1.68115611]. \t  -28.65209429465904 \t -5.911884012481753\n",
            "23     \t [1.324979   0.29714366]. \t  \u001b[92m-2.743211870019812\u001b[0m \t -2.743211870019812\n",
            "24     \t [0.78392147 0.55856548]. \t  \u001b[92m-0.09784557877956032\u001b[0m \t -0.09784557877956032\n",
            "25     \t [5.2730853 1.4697123]. \t  -20.075587573179618 \t -0.09784557877956032\n",
            "26     \t [ 3.44247018 -2.01645131]. \t  -49.95188680167296 \t -0.09784557877956032\n",
            "27     \t [-0.33857912  4.46069147]. \t  -3223.286325311457 \t -0.09784557877956032\n",
            "28     \t [0.10783759 0.16183711]. \t  -0.8021042933517469 \t -0.09784557877956032\n",
            "29     \t [0.28729995 0.16287836]. \t  -0.6176792684089881 \t -0.09784557877956032\n",
            "30     \t [ 4.51316977 -0.00578921]. \t  -53.078554567915205 \t -0.09784557877956032\n",
            "31     \t [6.56738654 2.53878052]. \t  -110.96723860821487 \t -0.09784557877956032\n",
            "32     \t [ 1.67581283 -0.14772669]. \t  -5.784657796609966 \t -0.09784557877956032\n",
            "33     \t [ 3.02908829 -0.90178523]. \t  -8.052081803742315 \t -0.09784557877956032\n",
            "34     \t [0.61593827 0.40960477]. \t  -0.30473618225390753 \t -0.09784557877956032\n",
            "35     \t [-0.43199815 -0.42367312]. \t  -3.301967937980036 \t -0.09784557877956032\n",
            "36     \t [ 4.93353927 -9.42015033]. \t  -59558.9750999321 \t -0.09784557877956032\n",
            "37     \t [-4.66553059 -9.24547315]. \t  -61719.02942258552 \t -0.09784557877956032\n",
            "38     \t [1.56415682 0.63942284]. \t  -1.4325993920996853 \t -0.09784557877956032\n",
            "39     \t [-0.76538432 -0.97728436]. \t  -17.433757641452484 \t -0.09784557877956032\n",
            "40     \t [-7.52040435 -0.40475527]. \t  -195.78131945034403 \t -0.09784557877956032\n",
            "41     \t [-1.24675416 -4.65154341]. \t  -3969.191768420114 \t -0.09784557877956032\n",
            "42     \t [0.87078742 0.73772548]. \t  -0.11147406073222062 \t -0.09784557877956032\n",
            "43     \t [ 5.19604065 -4.67146654]. \t  -2974.2823669943177 \t -0.09784557877956032\n",
            "44     \t [ 5.49572533 -3.44560664]. \t  -686.2405513026092 \t -0.09784557877956032\n",
            "45     \t [ 6.00694352 -0.78773091]. \t  -70.49715672008995 \t -0.09784557877956032\n",
            "46     \t [ 8.24489025 -1.50244286]. \t  -80.31753445412974 \t -0.09784557877956032\n",
            "47     \t [1.88471733 0.46564511]. \t  -4.993913294706808 \t -0.09784557877956032\n",
            "48     \t [ 6.90434474 -1.53073588]. \t  -44.70069012510097 \t -0.09784557877956032\n",
            "49     \t [ 2.64092556 -2.61533876]. \t  -246.41468407480542 \t -0.09784557877956032\n",
            "50     \t [ 9.76733397 -1.30497578]. \t  -157.8012295434315 \t -0.09784557877956032\n",
            "51     \t [ 3.69429924 -1.43118265]. \t  -7.582888023723159 \t -0.09784557877956032\n",
            "52     \t [ 0.44699959 -0.27742485]. \t  -0.47759008467274966 \t -0.09784557877956032\n",
            "53     \t [ 1.27648713 -0.63292458]. \t  -0.5282654422747914 \t -0.09784557877956032\n",
            "54     \t [ 6.49196718 -7.21609816]. \t  -19102.05751112033 \t -0.09784557877956032\n",
            "55     \t [0.59590251 0.11718733]. \t  -0.8095355024495225 \t -0.09784557877956032\n",
            "56     \t [ 0.75567059 -0.85408622]. \t  -1.048834726082826 \t -0.09784557877956032\n",
            "57     \t [3.20122798 0.02553411]. \t  -25.324431859395908 \t -0.09784557877956032\n",
            "58     \t [-5.54828522 -2.83642507]. \t  -979.3639882971961 \t -0.09784557877956032\n",
            "59     \t [-1.38572057 -0.02231777]. \t  -9.537629265829219 \t -0.09784557877956032\n",
            "60     \t [ 0.78374146 -0.25038619]. \t  -0.9136302895674375 \t -0.09784557877956032\n",
            "61     \t [ 7.7589623  -0.11838766]. \t  -165.21816050757536 \t -0.09784557877956032\n",
            "62     \t [ 5.10272363 -8.37402313]. \t  -36545.6085694846 \t -0.09784557877956032\n",
            "63     \t [-3.36428627 -4.54178341]. \t  -4000.9142702923155 \t -0.09784557877956032\n",
            "64     \t [-9.52691162  7.81135867]. \t  -34727.70400980378 \t -0.09784557877956032\n",
            "65     \t [0.70007031 2.81598637]. \t  -459.71012085825157 \t -0.09784557877956032\n",
            "66     \t [-3.66699095 -1.76817563]. \t  -218.58888550221934 \t -0.09784557877956032\n",
            "67     \t [ 3.74941644 -7.90499041]. \t  -29400.175506072163 \t -0.09784557877956032\n",
            "68     \t [0.56436426 4.51406788]. \t  -3230.541957069111 \t -0.09784557877956032\n",
            "69     \t [0.64978378 1.78762377]. \t  -66.05031469522473 \t -0.09784557877956032\n",
            "70     \t [0.29754546 0.39250371]. \t  -0.4936659581205672 \t -0.09784557877956032\n",
            "71     \t [-0.35825835  0.57811025]. \t  -3.9530146075919443 \t -0.09784557877956032\n",
            "72     \t [0.66141394 4.89445901]. \t  -4465.2479659832015 \t -0.09784557877956032\n",
            "73     \t [-3.30036674 -0.10506223]. \t  -40.570407569071875 \t -0.09784557877956032\n",
            "74     \t [-4.11355789 -8.05623696]. \t  -35894.995196675634 \t -0.09784557877956032\n",
            "75     \t [ 7.85982694 -4.90624431]. \t  -3292.4393334292486 \t -0.09784557877956032\n",
            "76     \t [0.44655222 0.85244886]. \t  -2.333540136191661 \t -0.09784557877956032\n",
            "77     \t [7.03050385 8.26918204]. \t  -33695.19910931697 \t -0.09784557877956032\n",
            "78     \t [8.53890693 8.20984019]. \t  -31942.053167629954 \t -0.09784557877956032\n",
            "79     \t [ 2.12312966 -8.41193406]. \t  -38864.9440071836 \t -0.09784557877956032\n",
            "80     \t [ 0.58834663 -0.86429065]. \t  -1.809862380366049 \t -0.09784557877956032\n",
            "81     \t [3.76477698 3.76714555]. \t  -1219.7352679061078 \t -0.09784557877956032\n",
            "82     \t [1.11275264 1.12080428]. \t  -3.93076369019918 \t -0.09784557877956032\n",
            "83     \t [ 2.40932222 -0.51328481]. \t  -9.0730459066795 \t -0.09784557877956032\n",
            "84     \t [7.23246528 9.31484792]. \t  -55350.43524845048 \t -0.09784557877956032\n",
            "85     \t [8.73485241 4.70702164]. \t  -2591.309797284475 \t -0.09784557877956032\n",
            "86     \t [ 3.03157594 -9.76296096]. \t  -70391.07523319387 \t -0.09784557877956032\n",
            "87     \t [-9.74162944  5.69879592]. \t  -11273.823476747046 \t -0.09784557877956032\n",
            "88     \t [ 9.77528004 -4.79792497]. \t  -2707.289737859233 \t -0.09784557877956032\n",
            "89     \t [-3.70262294 -8.69285128]. \t  -47969.21868999177 \t -0.09784557877956032\n",
            "90     \t [ 8.63075039 -2.72922408]. \t  -136.76834407521463 \t -0.09784557877956032\n",
            "91     \t [9.59066697 1.62142869]. \t  -111.34249031964976 \t -0.09784557877956032\n",
            "92     \t [ 5.37857082 -2.58028248]. \t  -145.16840990953483 \t -0.09784557877956032\n",
            "93     \t [ 5.18160093 -2.11211452]. \t  -45.46778762752092 \t -0.09784557877956032\n",
            "94     \t [0.08161118 1.76828527]. \t  -77.03194911698674 \t -0.09784557877956032\n",
            "95     \t [ 0.70846827 -0.76932106]. \t  -0.5366997564626311 \t -0.09784557877956032\n",
            "96     \t [ 6.62625188 -0.43495995]. \t  -109.72650818812865 \t -0.09784557877956032\n",
            "97     \t [5.684713   1.29474753]. \t  -32.82271019417604 \t -0.09784557877956032\n",
            "98     \t [-5.98245003  7.30174925]. \t  -25412.370855398018 \t -0.09784557877956032\n",
            "99     \t [-0.24093743 -0.70878098]. \t  -4.643355030458981 \t -0.09784557877956032\n",
            "100    \t [ 6.29792024 -4.77555238]. \t  -3119.230411100237 \t -0.09784557877956032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrvkDXP0uvWU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24f12353-82ff-41e3-d613-513d71e7d5ab"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_winner_5 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_5 = dGPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.14706549 0.83521255]. \t  -0.14473002518929054 \t -0.14473002518929054\n",
            "init   \t [ 1.20847567 -2.35903813]. \t  -196.92158721117866 \t -0.14473002518929054\n",
            "init   \t [-7.61811642  6.91998231]. \t  -21453.432632023236 \t -0.14473002518929054\n",
            "init   \t [4.79483225 8.64715129]. \t  -41920.46290057033 \t -0.14473002518929054\n",
            "init   \t [7.18656013 4.16209252]. \t  -1546.3183076435794 \t -0.14473002518929054\n",
            "1      \t [-0.89169287  2.93285963]. \t  -658.4382985845798 \t -0.14473002518929054\n",
            "2      \t [2.20317475 0.72550179]. \t  -4.094787565175626 \t -0.14473002518929054\n",
            "3      \t [ 9.14614046 -9.26496158]. \t  -52900.23738128742 \t -0.14473002518929054\n",
            "4      \t [-8.79367401 -8.68508685]. \t  -51075.444313405846 \t -0.14473002518929054\n",
            "5      \t [-6.79044491 -1.098666  ]. \t  -230.13957570574365 \t -0.14473002518929054\n",
            "6      \t [-1.09714177 -9.06979247]. \t  -54863.977603543106 \t -0.14473002518929054\n",
            "7      \t [ 7.29938804 -2.28804907]. \t  -59.792125224958696 \t -0.14473002518929054\n",
            "8      \t [-2.37867812  7.92793379]. \t  -32821.89775160643 \t -0.14473002518929054\n",
            "9      \t [1.09282252 0.52985745]. \t  -0.573227858099874 \t -0.14473002518929054\n",
            "10     \t [ 4.07880143 -6.31559581]. \t  -11468.85406495024 \t -0.14473002518929054\n",
            "11     \t [-10.           1.98726565]. \t  -761.7089906898417 \t -0.14473002518929054\n",
            "12     \t [1.51362333 0.91625389]. \t  -0.3185358417804601 \t -0.14473002518929054\n",
            "13     \t [9.7646637  7.23264849]. \t  -18072.802721559758 \t -0.14473002518929054\n",
            "14     \t [-2.98647549 -3.9038653 ]. \t  -2255.945941331935 \t -0.14473002518929054\n",
            "15     \t [4.20259317 0.41886835]. \t  -39.92766224939684 \t -0.14473002518929054\n",
            "16     \t [ 9.77356591 -4.70226344]. \t  -2450.442238167864 \t -0.14473002518929054\n",
            "17     \t [9.98706881 0.45188222]. \t  -264.2693865262832 \t -0.14473002518929054\n",
            "18     \t [-5.15446112  2.50810544]. \t  -666.9837380248581 \t -0.14473002518929054\n",
            "19     \t [-10.          -3.50718752]. \t  -2515.4208428668903 \t -0.14473002518929054\n",
            "20     \t [-1.13966357 -0.0320076 ]. \t  -7.1851752484479725 \t -0.14473002518929054\n",
            "21     \t [-3.06569084 -0.21874878]. \t  -36.51865216350279 \t -0.14473002518929054\n",
            "22     \t [ 4.48310826 -1.68217338]. \t  -14.899436149451605 \t -0.14473002518929054\n",
            "23     \t [1.67157763 4.7041732 ]. \t  -3627.7413927186735 \t -0.14473002518929054\n",
            "24     \t [1.77158044 0.70495829]. \t  -1.8048094120767724 \t -0.14473002518929054\n",
            "25     \t [0.28203088 0.0848536 ]. \t  -0.6587319495480773 \t -0.14473002518929054\n",
            "26     \t [ 3.83478808 -9.9376873 ]. \t  -75032.28455584684 \t -0.14473002518929054\n",
            "27     \t [6.98127063 0.46375328]. \t  -121.6103752555632 \t -0.14473002518929054\n",
            "28     \t [-0.51342463 -1.06629413]. \t  -17.829551174013496 \t -0.14473002518929054\n",
            "29     \t [-6.1206579  -5.28545139]. \t  -7736.881267671692 \t -0.14473002518929054\n",
            "30     \t [1.41274688 0.73645163]. \t  -0.38556061627576815 \t -0.14473002518929054\n",
            "31     \t [-1.1019199  -0.76599313]. \t  -14.773056314225064 \t -0.14473002518929054\n",
            "32     \t [ 5.16551346 -1.9124842 ]. \t  -26.593735036871458 \t -0.14473002518929054\n",
            "33     \t [-0.27780683 -0.24833815]. \t  -1.9546337286343718 \t -0.14473002518929054\n",
            "34     \t [ 0.45730514 -5.35960526]. \t  -6496.797064097895 \t -0.14473002518929054\n",
            "35     \t [0.44948625 0.34367089]. \t  -0.39403092200703094 \t -0.14473002518929054\n",
            "36     \t [3.36350434 1.42187031]. \t  -6.51075159132709 \t -0.14473002518929054\n",
            "37     \t [ 3.81291412 -1.70491774]. \t  -15.917085658895143 \t -0.14473002518929054\n",
            "38     \t [4.66748031 1.66322905]. \t  -14.94748955801512 \t -0.14473002518929054\n",
            "39     \t [-4.04270974 -8.11068269]. \t  -36805.06171120061 \t -0.14473002518929054\n",
            "40     \t [-0.28103395  0.03961992]. \t  -1.802557068230817 \t -0.14473002518929054\n",
            "41     \t [-9.27996744 -9.35645456]. \t  -68087.65443597634 \t -0.14473002518929054\n",
            "42     \t [1.32335163 0.79300969]. \t  \u001b[92m-0.11316900627292288\u001b[0m \t -0.11316900627292288\n",
            "43     \t [2.35814469 1.49510568]. \t  -10.770184550160003 \t -0.11316900627292288\n",
            "44     \t [-7.13427636  8.61588551]. \t  -48489.65974397891 \t -0.11316900627292288\n",
            "45     \t [4.45538132 2.10986772]. \t  -51.50377054399574 \t -0.11316900627292288\n",
            "46     \t [ 0.77714801 -3.85269067]. \t  -1671.5452318093176 \t -0.11316900627292288\n",
            "47     \t [1.27716604 0.95311502]. \t  -0.6593525627251564 \t -0.11316900627292288\n",
            "48     \t [2.33876796 1.16434649]. \t  -2.0700170911971227 \t -0.11316900627292288\n",
            "49     \t [-0.5993517  5.5202509]. \t  -7578.302277915097 \t -0.11316900627292288\n",
            "50     \t [ 3.79147322 -2.34836021]. \t  -112.57303144427304 \t -0.11316900627292288\n",
            "51     \t [ 4.07137886 -1.32069074]. \t  -10.112984762030093 \t -0.11316900627292288\n",
            "52     \t [-6.19530758 -1.30673651]. \t  -236.49311107238395 \t -0.11316900627292288\n",
            "53     \t [3.84153089 1.08964236]. \t  -12.377830141620116 \t -0.11316900627292288\n",
            "54     \t [4.67260813 1.14513609]. \t  -21.89251567738929 \t -0.11316900627292288\n",
            "55     \t [5.09988088 1.63586808]. \t  -16.93628122109199 \t -0.11316900627292288\n",
            "56     \t [-3.15868785  7.96922798]. \t  -33908.812544066655 \t -0.11316900627292288\n",
            "57     \t [-9.12550271 -8.71878829]. \t  -52047.64348572759 \t -0.11316900627292288\n",
            "58     \t [-7.09555014 -1.87065862]. \t  -462.83524980013703 \t -0.11316900627292288\n",
            "59     \t [ 3.06732641 -1.0919566 ]. \t  -5.205691187420133 \t -0.11316900627292288\n",
            "60     \t [-7.6378306   1.13910327]. \t  -284.03836725705537 \t -0.11316900627292288\n",
            "61     \t [-3.98124685  2.83415066]. \t  -828.5024076874499 \t -0.11316900627292288\n",
            "62     \t [-4.75469008 -7.1150874 ]. \t  -22506.654565598223 \t -0.11316900627292288\n",
            "63     \t [-9.77742119 -6.60586329]. \t  -18954.443229372755 \t -0.11316900627292288\n",
            "64     \t [ 2.59912835 -9.37357639]. \t  -59949.67873668477 \t -0.11316900627292288\n",
            "65     \t [6.80905509 0.11733726]. \t  -125.72312142648643 \t -0.11316900627292288\n",
            "66     \t [4.83577875 1.65862836]. \t  -15.601156239351546 \t -0.11316900627292288\n",
            "67     \t [0.6540322  0.45814347]. \t  -0.22943170710098815 \t -0.11316900627292288\n",
            "68     \t [5.7893277  1.18182345]. \t  -40.88866554112498 \t -0.11316900627292288\n",
            "69     \t [-6.24072439  0.690922  ]. \t  -155.97769002463605 \t -0.11316900627292288\n",
            "70     \t [-7.98074325 -9.01144947]. \t  -58148.32966529846 \t -0.11316900627292288\n",
            "71     \t [1.01211383 9.9703023 ]. \t  -78251.05859560592 \t -0.11316900627292288\n",
            "72     \t [ 2.49053743 -3.98211312]. \t  -1710.2958083542276 \t -0.11316900627292288\n",
            "73     \t [-8.33248565  0.93367549]. \t  -290.14625559474524 \t -0.11316900627292288\n",
            "74     \t [ 7.03740317 -5.13546999]. \t  -4215.020903821437 \t -0.11316900627292288\n",
            "75     \t [-7.43781506 -0.14536251]. \t  -183.0997841788702 \t -0.11316900627292288\n",
            "76     \t [ 3.09580336 -0.75057457]. \t  -12.146935801868079 \t -0.11316900627292288\n",
            "77     \t [9.54647128 9.218395  ]. \t  -51536.510783866055 \t -0.11316900627292288\n",
            "78     \t [-3.92414716  1.66843292]. \t  -204.42376146817975 \t -0.11316900627292288\n",
            "79     \t [-2.42001209  0.58590387]. \t  -30.99814611887559 \t -0.11316900627292288\n",
            "80     \t [ 8.09948126 -1.83169523]. \t  -54.26275683971769 \t -0.11316900627292288\n",
            "81     \t [-4.95930111  0.07786774]. \t  -84.9434600027748 \t -0.11316900627292288\n",
            "82     \t [-3.82134416  0.8305768 ]. \t  -77.34740531434511 \t -0.11316900627292288\n",
            "83     \t [1.9516029  9.10809664]. \t  -53768.79758512499 \t -0.11316900627292288\n",
            "84     \t [-2.28619733 -0.55364355]. \t  -27.6102743076288 \t -0.11316900627292288\n",
            "85     \t [-4.56399038  2.07938376]. \t  -380.05412095102264 \t -0.11316900627292288\n",
            "86     \t [2.56789062 4.34321879]. \t  -2474.798140262368 \t -0.11316900627292288\n",
            "87     \t [-7.69365142  1.92166151]. \t  -530.3451666391649 \t -0.11316900627292288\n",
            "88     \t [ 1.57698761 -7.3710349 ]. \t  -22935.722571382918 \t -0.11316900627292288\n",
            "89     \t [ 7.25300217 -4.22881872]. \t  -1665.061079179567 \t -0.11316900627292288\n",
            "90     \t [1.2900814  0.79116915]. \t  \u001b[92m-0.08706327757258457\u001b[0m \t -0.08706327757258457\n",
            "91     \t [ 5.69178918 -2.90534432]. \t  -272.45681326353787 \t -0.08706327757258457\n",
            "92     \t [ 7.66154238 -3.16993381]. \t  -353.6554037215183 \t -0.08706327757258457\n",
            "93     \t [ 6.47628172 -1.68202071]. \t  -31.32756390552134 \t -0.08706327757258457\n",
            "94     \t [ 4.94612449 -0.65496353]. \t  -48.998167162584295 \t -0.08706327757258457\n",
            "95     \t [ 8.44278079 -7.19997955]. \t  -18195.42650662692 \t -0.08706327757258457\n",
            "96     \t [7.94956547 1.96494096]. \t  -48.40004505770033 \t -0.08706327757258457\n",
            "97     \t [ 9.17998145 -4.28924253]. \t  -1592.1130148597142 \t -0.08706327757258457\n",
            "98     \t [ 7.78048757 -0.88454727]. \t  -123.2433687812482 \t -0.08706327757258457\n",
            "99     \t [8.84354262 2.12877206]. \t  -61.61778337996536 \t -0.08706327757258457\n",
            "100    \t [ 7.20684485 -1.92494545]. \t  -38.60814276548771 \t -0.08706327757258457\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO3I_9cbuvY2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3248528f-e716-4068-97c6-f9f0de66b505"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_winner_6 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_6 = dGPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.85720303 -3.36040389]. \t  -480.81536490368427 \t -43.91981950896418\n",
            "init   \t [ 6.42458246 -9.16606749]. \t  -52264.36633669738 \t -43.91981950896418\n",
            "init   \t [-7.8468664   1.90104128]. \t  -532.7651696660866 \t -43.91981950896418\n",
            "init   \t [ 0.59634724 -1.62385143]. \t  -43.91981950896418 \t -43.91981950896418\n",
            "init   \t [-3.29184301  2.45038864]. \t  -486.6398213674611 \t -43.91981950896418\n",
            "1      \t [-10.          -9.08379396]. \t  -61392.439547084185 \t -43.91981950896418\n",
            "2      \t [7.02990054 8.4753693 ]. \t  -37373.988071206615 \t -43.91981950896418\n",
            "3      \t [-7.69566798  9.86414506]. \t  -81924.88656432132 \t -43.91981950896418\n",
            "4      \t [-1.71570199 -9.52643263]. \t  -67147.64008994015 \t -43.91981950896418\n",
            "5      \t [0.12987728 7.74382654]. \t  -28706.668528284437 \t -43.91981950896418\n",
            "6      \t [-5.8138096  -3.70808954]. \t  -2266.0286467422725 \t -43.91981950896418\n",
            "7      \t [5.18193691 1.8785775 ]. \t  \u001b[92m-24.52862347443047\u001b[0m \t -24.52862347443047\n",
            "8      \t [ 2.16735142 -5.29303535]. \t  -5804.25963330402 \t -24.52862347443047\n",
            "9      \t [1.40352754 2.27618595]. \t  -160.67290344848865 \t -24.52862347443047\n",
            "10     \t [7.84511055 0.73655756]. \t  -138.252805876501 \t -24.52862347443047\n",
            "11     \t [-9.73988323 -3.24085103]. \t  -2005.9921445937919 \t -24.52862347443047\n",
            "12     \t [ 4.14363928 -1.05584601]. \t  \u001b[92m-17.20939540834366\u001b[0m \t -17.20939540834366\n",
            "13     \t [-2.49108873 -1.44499952]. \t  -101.08910455593394 \t -17.20939540834366\n",
            "14     \t [-9.60354959  5.10967356]. \t  -7756.12646878678 \t -17.20939540834366\n",
            "15     \t [3.56051751 4.57228876]. \t  -2932.8543802022127 \t -17.20939540834366\n",
            "16     \t [-4.26590437  5.84803192]. \t  -10588.093214096301 \t -17.20939540834366\n",
            "17     \t [9.24917075 4.02536331]. \t  -1140.6281518901199 \t -17.20939540834366\n",
            "18     \t [ 2.6740843  -0.30263186]. \t  \u001b[92m-15.211845637910779\u001b[0m \t -15.211845637910779\n",
            "19     \t [-1.92501872 -4.84180281]. \t  -4773.608098973131 \t -15.211845637910779\n",
            "20     \t [ 9.94339114 -5.83893736]. \t  -6864.475500501584 \t -15.211845637910779\n",
            "21     \t [4.59832225 0.71684608]. \t  -38.446086554663424 \t -15.211845637910779\n",
            "22     \t [-6.0078005  -7.27449166]. \t  -25067.38555033605 \t -15.211845637910779\n",
            "23     \t [ 9.96499222 -0.71356254]. \t  -240.45615016727385 \t -15.211845637910779\n",
            "24     \t [ 3.89066217 -1.67046061]. \t  \u001b[92m-14.069582211209266\u001b[0m \t -14.069582211209266\n",
            "25     \t [5.88964709 2.25663061]. \t  -60.804696684463735 \t -14.069582211209266\n",
            "26     \t [ 4.87559878 -2.29210452]. \t  -78.45657901504396 \t -14.069582211209266\n",
            "27     \t [-0.52729332 -0.33376025]. \t  \u001b[92m-3.45788031331116\u001b[0m \t -3.45788031331116\n",
            "28     \t [-0.87031007  0.4752706 ]. \t  -6.993820956482738 \t -3.45788031331116\n",
            "29     \t [-5.26160141 -0.66418989]. \t  -114.70261340481355 \t -3.45788031331116\n",
            "30     \t [ 1.95165392 -0.92110338]. \t  \u001b[92m-1.0354821205920615\u001b[0m \t -1.0354821205920615\n",
            "31     \t [-0.09522157  0.76541955]. \t  -4.409863937399255 \t -1.0354821205920615\n",
            "32     \t [-0.42019684  3.3457463 ]. \t  -1042.447981117573 \t -1.0354821205920615\n",
            "33     \t [ 0.17159355 -0.09583341]. \t  \u001b[92m-0.7332133365035686\u001b[0m \t -0.7332133365035686\n",
            "34     \t [ 2.38209451 -9.36475898]. \t  -59870.51695466323 \t -0.7332133365035686\n",
            "35     \t [2.50656106 0.80738462]. \t  -5.1632839508269495 \t -0.7332133365035686\n",
            "36     \t [-0.76794136  0.21917599]. \t  -4.6186694091425124 \t -0.7332133365035686\n",
            "37     \t [-9.54461185 -6.92456773]. \t  -22348.016015363617 \t -0.7332133365035686\n",
            "38     \t [3.41810998 1.35589004]. \t  -5.98117516174452 \t -0.7332133365035686\n",
            "39     \t [ 5.94554724 -8.19734584]. \t  -33021.93297494271 \t -0.7332133365035686\n",
            "40     \t [ 1.42375998 -0.0946624 ]. \t  -4.13233369619334 \t -0.7332133365035686\n",
            "41     \t [1.18761478 3.40904926]. \t  -972.9358498398087 \t -0.7332133365035686\n",
            "42     \t [0.96602887 1.41715216]. \t  -18.613616469358348 \t -0.7332133365035686\n",
            "43     \t [0.01530393 0.0557642 ]. \t  -0.9697914120824674 \t -0.7332133365035686\n",
            "44     \t [1.96318373 1.5661275 ]. \t  -18.24229893131104 \t -0.7332133365035686\n",
            "45     \t [-0.3398488   1.92037519]. \t  -120.85401780007317 \t -0.7332133365035686\n",
            "46     \t [ 2.08814131 -1.30608399]. \t  -4.687723806207211 \t -0.7332133365035686\n",
            "47     \t [2.91044103 0.17532819]. \t  -19.88294340642394 \t -0.7332133365035686\n",
            "48     \t [ 1.89215726 -0.52247624]. \t  -4.420423351604363 \t -0.7332133365035686\n",
            "49     \t [-0.16456003 -0.13873414]. \t  -1.4386621904660821 \t -0.7332133365035686\n",
            "50     \t [ 5.8856001  -0.65100397]. \t  -74.63172985332676 \t -0.7332133365035686\n",
            "51     \t [ 0.1936546  -0.03357554]. \t  \u001b[92m-0.7234607995698785\u001b[0m \t -0.7234607995698785\n",
            "52     \t [-0.4451848  1.4532218]. \t  -45.685663811473155 \t -0.7234607995698785\n",
            "53     \t [ 0.76943227 -0.57532787]. \t  \u001b[92m-0.07624300964060415\u001b[0m \t -0.07624300964060415\n",
            "54     \t [-1.93619994 -0.062346  ]. \t  -16.179339840733512 \t -0.07624300964060415\n",
            "55     \t [ 2.76017179 -1.16922918]. \t  -3.09955444048452 \t -0.07624300964060415\n",
            "56     \t [-0.21144436 -1.20357676]. \t  -20.794862832311225 \t -0.07624300964060415\n",
            "57     \t [ 3.2188219  -1.39082617]. \t  -5.768100386170028 \t -0.07624300964060415\n",
            "58     \t [3.18938356 6.55768788]. \t  -13722.156434697847 \t -0.07624300964060415\n",
            "59     \t [2.85077011 1.26671318]. \t  -3.682185817366334 \t -0.07624300964060415\n",
            "60     \t [-1.63418844  0.72984082]. \t  -21.513805052537112 \t -0.07624300964060415\n",
            "61     \t [6.93219279 7.74582913]. \t  -25601.921290181013 \t -0.07624300964060415\n",
            "62     \t [6.7686097  4.92442389]. \t  -3516.278259609777 \t -0.07624300964060415\n",
            "63     \t [ 4.38161345 -0.57842972]. \t  -38.99990287655739 \t -0.07624300964060415\n",
            "64     \t [-7.67011304 -0.86221049]. \t  -242.86948040975417 \t -0.07624300964060415\n",
            "65     \t [-9.37022401 -5.04390326]. \t  -7368.183318605593 \t -0.07624300964060415\n",
            "66     \t [7.18587785 1.24681412]. \t  -71.5054688868222 \t -0.07624300964060415\n",
            "67     \t [2.37092546 2.83696544]. \t  -378.6757268110483 \t -0.07624300964060415\n",
            "68     \t [ 1.71770918 -0.76035962]. \t  -1.1454815744750615 \t -0.07624300964060415\n",
            "69     \t [-6.25673327 -5.84750285]. \t  -11195.908384770653 \t -0.07624300964060415\n",
            "70     \t [5.73857878 1.29789408]. \t  -33.68338548335779 \t -0.07624300964060415\n",
            "71     \t [-7.45020871 -5.3634945 ]. \t  -8517.338851062503 \t -0.07624300964060415\n",
            "72     \t [-9.77025633 -0.07356597]. \t  -307.3374821557221 \t -0.07624300964060415\n",
            "73     \t [ 1.31820967 -0.04303052]. \t  -3.5571116527695414 \t -0.07624300964060415\n",
            "74     \t [ 1.9347878  -7.36349954]. \t  -22688.55318229337 \t -0.07624300964060415\n",
            "75     \t [-5.54358842  5.51304305]. \t  -8842.3887107496 \t -0.07624300964060415\n",
            "76     \t [7.78650154 3.89916615]. \t  -1069.4298990570135 \t -0.07624300964060415\n",
            "77     \t [4.66219434 9.80236653]. \t  -70333.85057301067 \t -0.07624300964060415\n",
            "78     \t [7.41697042 2.43771962]. \t  -81.10326257400561 \t -0.07624300964060415\n",
            "79     \t [-2.66403716 -9.86890604]. \t  -77990.1026981319 \t -0.07624300964060415\n",
            "80     \t [-0.70520444  6.38911094]. \t  -13564.859667105267 \t -0.07624300964060415\n",
            "81     \t [ 0.76413099 -0.43252571]. \t  -0.3597936617961455 \t -0.07624300964060415\n",
            "82     \t [-8.11969226  2.93973397]. \t  -1373.8712307481312 \t -0.07624300964060415\n",
            "83     \t [-3.2346585   9.96897607]. \t  -81622.39842562245 \t -0.07624300964060415\n",
            "84     \t [-8.6698848  -1.14240882]. \t  -347.9871088865486 \t -0.07624300964060415\n",
            "85     \t [-1.97214442 -6.10811556]. \t  -11740.98045602341 \t -0.07624300964060415\n",
            "86     \t [-5.90404442  1.22824693]. \t  -206.8423570253295 \t -0.07624300964060415\n",
            "87     \t [-0.11919758 -0.37954313]. \t  -1.5843956053981343 \t -0.07624300964060415\n",
            "88     \t [ 1.11231044 -0.80776528]. \t  -0.08684866822332754 \t -0.07624300964060415\n",
            "89     \t [3.13460263 1.09742008]. \t  -5.610509042982613 \t -0.07624300964060415\n",
            "90     \t [-3.93194223 -1.05429257]. \t  -100.09229765168764 \t -0.07624300964060415\n",
            "91     \t [-1.16078165 -2.97335998]. \t  -714.7501598059326 \t -0.07624300964060415\n",
            "92     \t [ 7.61381756 -5.9672286 ]. \t  -8134.124752290816 \t -0.07624300964060415\n",
            "93     \t [4.20304447 1.58540247]. \t  -11.61730593979691 \t -0.07624300964060415\n",
            "94     \t [-0.05444533  5.13783057]. \t  -5587.156302415063 \t -0.07624300964060415\n",
            "95     \t [6.57010489 8.77670648]. \t  -43538.24471272127 \t -0.07624300964060415\n",
            "96     \t [-8.81909582 -6.38636263]. \t  -16437.245746249155 \t -0.07624300964060415\n",
            "97     \t [-7.38079581 -5.17206868]. \t  -7483.323761270523 \t -0.07624300964060415\n",
            "98     \t [ 1.30913725 -0.76022489]. \t  -0.1425391002847861 \t -0.07624300964060415\n",
            "99     \t [-3.06810703  6.60137068]. \t  -16297.396964435744 \t -0.07624300964060415\n",
            "100    \t [-7.12382562 -8.06620731]. \t  -37741.79479062148 \t -0.07624300964060415\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rQbLZD8uvbI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9ce332a-be66-4cfb-a2db-ac53a7156cba"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_winner_7 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_7 = dGPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-6.9467253  -3.95286781]. \t  -2981.1801714787616 \t -297.17705346549894\n",
            "init   \t [-8.75927171 -0.80279317]. \t  -297.17705346549894 \t -297.17705346549894\n",
            "init   \t [6.70506769 8.53994095]. \t  -38761.39667089972 \t -297.17705346549894\n",
            "init   \t [4.53977969 5.36992444]. \t  -5658.63331787327 \t -297.17705346549894\n",
            "init   \t [-4.61589867  2.88058584]. \t  -931.3893916140455 \t -297.17705346549894\n",
            "1      \t [ 2.44677649 -4.93762112]. \t  -4291.960403637314 \t -297.17705346549894\n",
            "2      \t [-8.63046085  9.55793493]. \t  -73313.74290460398 \t -297.17705346549894\n",
            "3      \t [ 9.83790456 -9.57130198]. \t  -60200.563023412855 \t -297.17705346549894\n",
            "4      \t [-2.47559954 -9.95929554]. \t  -80694.11504672318 \t -297.17705346549894\n",
            "5      \t [ 9.0936247  -0.78669084]. \t  \u001b[92m-188.93582542124247\u001b[0m \t -188.93582542124247\n",
            "6      \t [-10. -10.]. \t  -88321.0 \t -188.93582542124247\n",
            "7      \t [0.48374205 0.70472029]. \t  \u001b[92m-0.7857421467280545\u001b[0m \t -0.7857421467280545\n",
            "8      \t [-1.02443955  6.99962386]. \t  -19611.606266568295 \t -0.7857421467280545\n",
            "9      \t [-2.11068264 -1.72354906]. \t  -139.34335110938585 \t -0.7857421467280545\n",
            "10     \t [3.05069563 0.30473622]. \t  -20.6214279194194 \t -0.7857421467280545\n",
            "11     \t [-10.           3.12855034]. \t  -1870.4386767115468 \t -0.7857421467280545\n",
            "12     \t [ 7.07786567 -4.73559299]. \t  -2890.662850586515 \t -0.7857421467280545\n",
            "13     \t [  3.18463283 -10.        ]. \t  -77477.35012635695 \t -0.7857421467280545\n",
            "14     \t [9.31931623 4.02601811]. \t  -1136.2764887802944 \t -0.7857421467280545\n",
            "15     \t [5.83734397 0.68838043]. \t  -71.21644383628458 \t -0.7857421467280545\n",
            "16     \t [0.26914014 1.47098476]. \t  -33.47622447967923 \t -0.7857421467280545\n",
            "17     \t [0.30336456 0.07214782]. \t  \u001b[92m-0.6569449626509897\u001b[0m \t -0.6569449626509897\n",
            "18     \t [0.9623809  0.45839217]. \t  \u001b[92m-0.589234035635376\u001b[0m \t -0.589234035635376\n",
            "19     \t [2.14192521 1.9868882 ]. \t  -67.51007466572261 \t -0.589234035635376\n",
            "20     \t [-2.95022289 -5.17322093]. \t  -6394.377021036696 \t -0.589234035635376\n",
            "21     \t [-6.22633622  5.75282287]. \t  -10540.454617234245 \t -0.589234035635376\n",
            "22     \t [-5.60280791 -0.25058201]. \t  -109.22598995052391 \t -0.589234035635376\n",
            "23     \t [0.50977244 0.19575997]. \t  -0.6155236750725124 \t -0.589234035635376\n",
            "24     \t [2.31336931 9.94396457]. \t  -76404.29743799097 \t -0.589234035635376\n",
            "25     \t [-0.64507889  0.90525694]. \t  -13.140137075623606 \t -0.589234035635376\n",
            "26     \t [ 5.05441017 -0.86454058]. \t  -41.779024573082665 \t -0.589234035635376\n",
            "27     \t [-6.25904711 -7.34013925]. \t  -26051.231151450007 \t -0.589234035635376\n",
            "28     \t [ 0.61620894 -1.77288234]. \t  -64.44542820258975 \t -0.589234035635376\n",
            "29     \t [-2.6356218   0.73133399]. \t  -40.676547330074584 \t -0.589234035635376\n",
            "30     \t [ 3.35635613 -0.94804479]. \t  -10.411993704469197 \t -0.589234035635376\n",
            "31     \t [ 4.06518763 -1.79072378]. \t  -20.42342127852703 \t -0.589234035635376\n",
            "32     \t [ 0.0161191  -0.11549953]. \t  -0.968244709594219 \t -0.589234035635376\n",
            "33     \t [-0.90115178 -0.08042966]. \t  -5.285497793560269 \t -0.589234035635376\n",
            "34     \t [-1.57820718  3.01153701]. \t  -784.160820291581 \t -0.589234035635376\n",
            "35     \t [ 1.33632763 -0.07436351]. \t  -3.625785579881615 \t -0.589234035635376\n",
            "36     \t [6.92891817 1.93938604]. \t  -35.856598610869725 \t -0.589234035635376\n",
            "37     \t [-4.43300249  9.32219148]. \t  -63568.16363615236 \t -0.589234035635376\n",
            "38     \t [ 2.03404561 -0.73545391]. \t  -2.882851291403675 \t -0.589234035635376\n",
            "39     \t [-9.96437286 -3.94228768]. \t  -3490.034516593319 \t -0.589234035635376\n",
            "40     \t [ 6.91830618 -0.97951523]. \t  -85.01446901917672 \t -0.589234035635376\n",
            "41     \t [-8.89914072  6.61653604]. \t  -18705.6050443137 \t -0.589234035635376\n",
            "42     \t [0.08634534 0.77924378]. \t  -3.3799677853776378 \t -0.589234035635376\n",
            "43     \t [ 2.52275686 -1.14237522]. \t  -2.334025939058194 \t -0.589234035635376\n",
            "44     \t [-0.93790762  0.05836855]. \t  -5.540482992036896 \t -0.589234035635376\n",
            "45     \t [6.58221165 0.19048903]. \t  -115.91190012352226 \t -0.589234035635376\n",
            "46     \t [4.33353588 1.53290015]. \t  -11.380417121805811 \t -0.589234035635376\n",
            "47     \t [5.74217289 2.00005449]. \t  -32.687707498101396 \t -0.589234035635376\n",
            "48     \t [ 0.54012536 -0.22009734]. \t  -0.6044075147402664 \t -0.589234035635376\n",
            "49     \t [ 0.4417793  -0.17961327]. \t  -0.5962567107735699 \t -0.589234035635376\n",
            "50     \t [-1.68077183  6.07669278]. \t  -11417.703688746778 \t -0.589234035635376\n",
            "51     \t [ 1.39187057 -0.59535695]. \t  -1.08646071422604 \t -0.589234035635376\n",
            "52     \t [ 1.85564636 -1.17112257]. \t  -2.307122944736434 \t -0.589234035635376\n",
            "53     \t [6.23459498 1.65460092]. \t  -28.553713076247945 \t -0.589234035635376\n",
            "54     \t [-3.31372564  0.19537071]. \t  -41.59331128735792 \t -0.589234035635376\n",
            "55     \t [ 0.44199886 -0.20268829]. \t  \u001b[92m-0.5703259654853345\u001b[0m \t -0.5703259654853345\n",
            "56     \t [3.74090555 0.23893028]. \t  -33.81890697352523 \t -0.5703259654853345\n",
            "57     \t [-2.28693964  0.27262581]. \t  -22.668162728585386 \t -0.5703259654853345\n",
            "58     \t [0.79335407 0.96336706]. \t  -2.30178224996131 \t -0.5703259654853345\n",
            "59     \t [0.11041528 0.37747678]. \t  -0.85230486962366 \t -0.5703259654853345\n",
            "60     \t [ 9.97061403 -4.03368571]. \t  -1099.3383010647756 \t -0.5703259654853345\n",
            "61     \t [ 8.3389707  -1.14735903]. \t  -118.9797646176284 \t -0.5703259654853345\n",
            "62     \t [9.45672564 4.78440042]. \t  -2710.4183504109697 \t -0.5703259654853345\n",
            "63     \t [ 9.65168305 -3.44245725]. \t  -469.61957102685653 \t -0.5703259654853345\n",
            "64     \t [8.96842509 2.83333379]. \t  -163.9507809711411 \t -0.5703259654853345\n",
            "65     \t [ 9.37853996 -2.74557173]. \t  -135.12951350193214 \t -0.5703259654853345\n",
            "66     \t [-7.35504328 -2.27957323]. \t  -699.7863099239688 \t -0.5703259654853345\n",
            "67     \t [ 6.34112375 -3.83553244]. \t  -1094.038378129007 \t -0.5703259654853345\n",
            "68     \t [-1.73689293 -2.46917106]. \t  -395.6084880472298 \t -0.5703259654853345\n",
            "69     \t [ 1.4703293  -0.89485366]. \t  \u001b[92m-0.2556348756579331\u001b[0m \t -0.2556348756579331\n",
            "70     \t [-2.92670956 -0.51002889]. \t  -39.18223157807998 \t -0.2556348756579331\n",
            "71     \t [ 5.40567626 -2.71578015]. \t  -194.07727988262448 \t -0.2556348756579331\n",
            "72     \t [-0.60794976 -6.81148879]. \t  -17449.970281161743 \t -0.2556348756579331\n",
            "73     \t [ 2.70158251 -1.39741152]. \t  -5.794304072941612 \t -0.2556348756579331\n",
            "74     \t [ 3.82993686 -1.20880945]. \t  -9.655641684822255 \t -0.2556348756579331\n",
            "75     \t [-1.67836209 -0.38892624]. \t  -15.021468529265658 \t -0.2556348756579331\n",
            "76     \t [0.5117976 0.4381675]. \t  -0.2710154883400123 \t -0.2556348756579331\n",
            "77     \t [-5.43939435 -1.20356317]. \t  -180.46102889329177 \t -0.2556348756579331\n",
            "78     \t [1.07329673 0.74406266]. \t  \u001b[92m-0.007679210655349976\u001b[0m \t -0.007679210655349976\n",
            "79     \t [ 3.50269434 -1.48339176]. \t  -7.877033812407978 \t -0.007679210655349976\n",
            "80     \t [ 5.13145893 -6.98450545]. \t  -17105.591663517112 \t -0.007679210655349976\n",
            "81     \t [-7.23078149  1.78999388]. \t  -439.7870074793266 \t -0.007679210655349976\n",
            "82     \t [-6.86179769 -6.00034947]. \t  -12502.820114758297 \t -0.007679210655349976\n",
            "83     \t [9.01949008 0.36589373]. \t  -217.49790444167454 \t -0.007679210655349976\n",
            "84     \t [1.3880762 0.5397669]. \t  -1.4478757054788032 \t -0.007679210655349976\n",
            "85     \t [4.32363851 5.40727383]. \t  -5876.267471595092 \t -0.007679210655349976\n",
            "86     \t [-5.64153068 -1.58224571]. \t  -270.8924671331364 \t -0.007679210655349976\n",
            "87     \t [-2.10379631 -5.07196877]. \t  -5745.594840224066 \t -0.007679210655349976\n",
            "88     \t [-1.65719838  1.17430137]. \t  -46.04808128627561 \t -0.007679210655349976\n",
            "89     \t [6.63753775 3.99835237]. \t  -1315.6183299249508 \t -0.007679210655349976\n",
            "90     \t [-9.46301857  0.66459733]. \t  -323.5706524987455 \t -0.007679210655349976\n",
            "91     \t [ 4.74676856 -2.01197137]. \t  -36.473748104095634 \t -0.007679210655349976\n",
            "92     \t [ 0.71210475 -0.51965567]. \t  -0.14206593561364175 \t -0.007679210655349976\n",
            "93     \t [5.14855436 1.57944188]. \t  -17.26124414408385 \t -0.007679210655349976\n",
            "94     \t [ 0.75676517 -4.31883246]. \t  -2671.551655887892 \t -0.007679210655349976\n",
            "95     \t [-2.55040769 -1.69504259]. \t  -150.27739847390453 \t -0.007679210655349976\n",
            "96     \t [-5.90528167 -5.65835373]. \t  -9830.670664961111 \t -0.007679210655349976\n",
            "97     \t [ 1.37175544 -1.3771187 ]. \t  -11.862198666920797 \t -0.007679210655349976\n",
            "98     \t [ 5.61987587 -7.25406049]. \t  -19870.792210242922 \t -0.007679210655349976\n",
            "99     \t [-6.33179118  5.3289402 ]. \t  -8023.791023469346 \t -0.007679210655349976\n",
            "100    \t [-7.27090799  3.58797785]. \t  -2248.7948831675008 \t -0.007679210655349976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YRio_skuvd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a1d62cb-182c-4c3b-a063-bca5c0e2fc05"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_winner_8 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_8 = dGPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-7.8100025  -9.15569971]. \t  -61652.61989589963 \t -123.0699213709516\n",
            "init   \t [ 1.9938517 -4.0025859]. \t  -1806.696586806874 \t -123.0699213709516\n",
            "init   \t [ 5.93223564 -2.6760775 ]. \t  -165.12946982275756 \t -123.0699213709516\n",
            "init   \t [ 7.61216688 -4.04316733]. \t  -1301.9580077172761 \t -123.0699213709516\n",
            "init   \t [-3.41793472  1.37433832]. \t  -123.0699213709516 \t -123.0699213709516\n",
            "1      \t [6.56161104 9.13365295]. \t  -51413.89230031877 \t -123.0699213709516\n",
            "2      \t [-9.10684612  8.65627415]. \t  -50644.440793557515 \t -123.0699213709516\n",
            "3      \t [-10.         -1.0665418]. \t  -422.3523707348583 \t -123.0699213709516\n",
            "4      \t [-0.33239204  7.74857709]. \t  -29000.49408219279 \t -123.0699213709516\n",
            "5      \t [  4.82194421 -10.        ]. \t  -76203.55418218843 \t -123.0699213709516\n",
            "6      \t [4.00600666 2.63786598]. \t  -205.47872409303594 \t -123.0699213709516\n",
            "7      \t [9.9050887  3.11370436]. \t  -259.2394377182353 \t -123.0699213709516\n",
            "8      \t [-3.84313377 -4.115115  ]. \t  -2867.766906413108 \t -123.0699213709516\n",
            "9      \t [-1.55087801 -9.50196081]. \t  -66345.82717309236 \t -123.0699213709516\n",
            "10     \t [-7.64821324  3.1348157 ]. \t  -1565.6284618700665 \t -123.0699213709516\n",
            "11     \t [-0.09115133  0.42203595]. \t  \u001b[92m-1.5909089950281161\u001b[0m \t -1.5909089950281161\n",
            "12     \t [-0.27471865  2.92275473]. \t  -604.3439032763521 \t -1.5909089950281161\n",
            "13     \t [-0.16616717 -1.14807649]. \t  -17.066012420053486 \t -1.5909089950281161\n",
            "14     \t [ 2.33873718 -0.2520255 ]. \t  -11.575481765529673 \t -1.5909089950281161\n",
            "15     \t [ 9.84333107 -8.2419934 ]. \t  -31839.06504341117 \t -1.5909089950281161\n",
            "16     \t [ 0.47114081 -0.40063569]. \t  \u001b[92m-0.32476581065786236\u001b[0m \t -0.32476581065786236\n",
            "17     \t [7.71936456 0.13888305]. \t  -163.1388549509941 \t -0.32476581065786236\n",
            "18     \t [-0.25793126 -0.22061096]. \t  -1.8348241205470743 \t -0.32476581065786236\n",
            "19     \t [-3.96485884  4.77327806]. \t  -4931.730709740688 \t -0.32476581065786236\n",
            "20     \t [-10.          -5.09747177]. \t  -7801.174337940944 \t -0.32476581065786236\n",
            "21     \t [9.96547899 6.16124452]. \t  -8780.870351741778 \t -0.32476581065786236\n",
            "22     \t [-5.93643354 -0.84115455]. \t  -156.20367068731662 \t -0.32476581065786236\n",
            "23     \t [6.42256851 3.78006133]. \t  -1011.1063774843094 \t -0.32476581065786236\n",
            "24     \t [4.60808273 0.40159656]. \t  -49.749678074431955 \t -0.32476581065786236\n",
            "25     \t [-4.20138848  9.31820197]. \t  -63294.81913283325 \t -0.32476581065786236\n",
            "26     \t [-0.4692321   0.01873558]. \t  -2.600319184299668 \t -0.32476581065786236\n",
            "27     \t [ 0.24332737 -0.26341393]. \t  -0.5944163631245485 \t -0.32476581065786236\n",
            "28     \t [0.63610329 0.37474442]. \t  -0.38480680472320267 \t -0.32476581065786236\n",
            "29     \t [ 1.07210502 -0.40706939]. \t  -1.1024544780860785 \t -0.32476581065786236\n",
            "30     \t [3.80192223 5.30372313]. \t  -5511.332008917288 \t -0.32476581065786236\n",
            "31     \t [1.28727522 0.13986241]. \t  -3.1982950231029417 \t -0.32476581065786236\n",
            "32     \t [2.0374998  1.40578502]. \t  -8.410574465691145 \t -0.32476581065786236\n",
            "33     \t [ 4.99079925 -5.77435322]. \t  -7628.598605526773 \t -0.32476581065786236\n",
            "34     \t [1.15585495 0.80508808]. \t  \u001b[92m-0.06375929072157648\u001b[0m \t -0.06375929072157648\n",
            "35     \t [ 9.51741271 -0.14869969]. \t  -252.02895826208731 \t -0.06375929072157648\n",
            "36     \t [-0.11224753  0.1229503 ]. \t  -1.277696284017129 \t -0.06375929072157648\n",
            "37     \t [-1.75997782  0.11864031]. \t  -14.012287256721972 \t -0.06375929072157648\n",
            "38     \t [1.09892793 0.68534179]. \t  \u001b[92m-0.06069351335825453\u001b[0m \t -0.06069351335825453\n",
            "39     \t [-1.82314968 -0.58008768]. \t  -20.431734797124786 \t -0.06069351335825453\n",
            "40     \t [ 5.74898858 -1.2505808 ]. \t  -36.29305403216088 \t -0.06069351335825453\n",
            "41     \t [ 4.72147872 -1.72425438]. \t  -16.84882935049343 \t -0.06069351335825453\n",
            "42     \t [ 3.13745005 -0.87479751]. \t  -9.733003784786302 \t -0.06069351335825453\n",
            "43     \t [ 0.93529965 -0.25051774]. \t  -1.3156778895830032 \t -0.06069351335825453\n",
            "44     \t [0.26388089 0.44404726]. \t  -0.5759188141122675 \t -0.06069351335825453\n",
            "45     \t [ 3.90502091 -1.15095709]. \t  -11.592292007673878 \t -0.06069351335825453\n",
            "46     \t [-5.2530543   9.29050083]. \t  -63321.594022155434 \t -0.06069351335825453\n",
            "47     \t [ 0.33371736 -0.56558523]. \t  -0.631273038861805 \t -0.06069351335825453\n",
            "48     \t [-6.78208322 -2.61962127]. \t  -901.6278942682517 \t -0.06069351335825453\n",
            "49     \t [4.19396467 1.66803015]. \t  -13.958962388530542 \t -0.06069351335825453\n",
            "50     \t [-3.23428689 -0.57425663]. \t  -48.25298214641066 \t -0.06069351335825453\n",
            "51     \t [5.06834609 5.96825386]. \t  -8773.955635900966 \t -0.06069351335825453\n",
            "52     \t [-5.70811187  0.43499748]. \t  -119.09112874880427 \t -0.06069351335825453\n",
            "53     \t [1.00927738 0.78270075]. \t  -0.09336657669638086 \t -0.06069351335825453\n",
            "54     \t [-5.76574288  8.570498  ]. \t  -46663.62987881991 \t -0.06069351335825453\n",
            "55     \t [ 6.13223941 -8.32988771]. \t  -35214.04422492559 \t -0.06069351335825453\n",
            "56     \t [-6.13893734 -1.59182807]. \t  -302.14783823643666 \t -0.06069351335825453\n",
            "57     \t [3.13283848 1.26747228]. \t  -4.5618427417618035 \t -0.06069351335825453\n",
            "58     \t [4.30514143 8.85815479]. \t  -46601.94656579291 \t -0.06069351335825453\n",
            "59     \t [-6.16955622  0.44482737]. \t  -137.60883826786048 \t -0.06069351335825453\n",
            "60     \t [-5.26021872  0.57031597]. \t  -109.06401744077854 \t -0.06069351335825453\n",
            "61     \t [-1.31521669 -1.20963095]. \t  -41.343091088664025 \t -0.06069351335825453\n",
            "62     \t [-7.01656824  4.39875358]. \t  -4243.920950143667 \t -0.06069351335825453\n",
            "63     \t [4.34856894 1.2782591 ]. \t  -13.548636417950672 \t -0.06069351335825453\n",
            "64     \t [ 7.72355912 -9.4819064 ]. \t  -59274.82494528694 \t -0.06069351335825453\n",
            "65     \t [-9.88556797  1.86668964]. \t  -686.6525866765733 \t -0.06069351335825453\n",
            "66     \t [-9.87418281 -2.53173674]. \t  -1148.243606507633 \t -0.06069351335825453\n",
            "67     \t [3.31352881 1.53087528]. \t  -9.126131078748006 \t -0.06069351335825453\n",
            "68     \t [-5.39998576 -6.84540931]. \t  -19690.208465023108 \t -0.06069351335825453\n",
            "69     \t [4.31523028 4.55266855]. \t  -2769.505070887002 \t -0.06069351335825453\n",
            "70     \t [ 5.14449148 -0.25238526]. \t  -67.51929105981897 \t -0.06069351335825453\n",
            "71     \t [ 9.42253238 -0.3995322 ]. \t  -236.67848034395837 \t -0.06069351335825453\n",
            "72     \t [-2.21950213  4.78261049]. \t  -4611.883322732063 \t -0.06069351335825453\n",
            "73     \t [ 3.84551887 -0.07237014]. \t  -37.5121025003141 \t -0.06069351335825453\n",
            "74     \t [-8.84729879 -5.24014396]. \t  -8229.051424410012 \t -0.06069351335825453\n",
            "75     \t [3.885948   1.31961911]. \t  -8.653769855501162 \t -0.06069351335825453\n",
            "76     \t [ 8.86426913 -4.30594698]. \t  -1654.3678980790066 \t -0.06069351335825453\n",
            "77     \t [-5.91462627e+00 -2.10471570e-03]. \t  -117.77787374168585 \t -0.06069351335825453\n",
            "78     \t [ 9.87354977 -8.45660868]. \t  -35539.20050149259 \t -0.06069351335825453\n",
            "79     \t [-0.61430943 -9.6560787 ]. \t  -70010.95138703822 \t -0.06069351335825453\n",
            "80     \t [ 2.1454274  -0.78001985]. \t  -3.0364716378073604 \t -0.06069351335825453\n",
            "81     \t [1.13377697 0.69766944]. \t  -0.06928311330863457 \t -0.06069351335825453\n",
            "82     \t [-5.28630706  7.33387952]. \t  -25513.321947840155 \t -0.06069351335825453\n",
            "83     \t [ 7.09723595 -3.04258965]. \t  -297.89342045477264 \t -0.06069351335825453\n",
            "84     \t [ 7.13666325 -1.84147435]. \t  -37.9101290234754 \t -0.06069351335825453\n",
            "85     \t [-1.53502454 -5.06789235]. \t  -5603.688678125964 \t -0.06069351335825453\n",
            "86     \t [ 0.93232909 -0.2382274 ]. \t  -1.3455264914714893 \t -0.06069351335825453\n",
            "87     \t [1.27632162 2.44985546]. \t  -230.22465187294836 \t -0.06069351335825453\n",
            "88     \t [1.96352415 0.98137928]. \t  -0.9311633886251407 \t -0.06069351335825453\n",
            "89     \t [ 1.52017907 -1.78452586]. \t  -47.29397676328524 \t -0.06069351335825453\n",
            "90     \t [ 0.66501211 -3.19880746]. \t  -784.1705345699172 \t -0.06069351335825453\n",
            "91     \t [-6.54058379  8.44174   ]. \t  -44498.522389758466 \t -0.06069351335825453\n",
            "92     \t [-0.39591866  9.01209183]. \t  -53030.154493987946 \t -0.06069351335825453\n",
            "93     \t [ 2.97050752 -1.4050746 ]. \t  -5.795718183824131 \t -0.06069351335825453\n",
            "94     \t [ 4.0504259  -1.84565091]. \t  -24.567122228735258 \t -0.06069351335825453\n",
            "95     \t [-4.18805224  5.0527217 ]. \t  -6131.608945360351 \t -0.06069351335825453\n",
            "96     \t [-9.842156   -3.27093426]. \t  -2069.4497773451208 \t -0.06069351335825453\n",
            "97     \t [ 9.86423471 -7.70892022]. \t  -23836.50570445349 \t -0.06069351335825453\n",
            "98     \t [ 2.08086811 -1.21197818]. \t  -2.636879467086902 \t -0.06069351335825453\n",
            "99     \t [-0.2734736  -5.50592384]. \t  -7420.184072100605 \t -0.06069351335825453\n",
            "100    \t [ 0.84601829 -1.36380733]. \t  -16.542572189622856 \t -0.06069351335825453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejw6v-Ihuvf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "509ab57d-da2a-4fb4-ecc7-0bb78af32d00"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_winner_9 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_9 = dGPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.44557117 -0.23843202]. \t  -28.18355402366269 \t -28.18355402366269\n",
            "init   \t [ 6.50990348 -9.37107225]. \t  -57236.27240949214 \t -28.18355402366269\n",
            "init   \t [6.16099927 1.31234839]. \t  -41.39446954533859 \t -28.18355402366269\n",
            "init   \t [-4.04755003 -9.06608559]. \t  -56766.42271365435 \t -28.18355402366269\n",
            "init   \t [ 9.81254799 -9.86348534]. \t  -68353.23444364112 \t -28.18355402366269\n",
            "1      \t [-8.12773813  6.26616826]. \t  -15102.348898507695 \t -28.18355402366269\n",
            "2      \t [3.58702664 9.74831565]. \t  -69550.52203005194 \t -28.18355402366269\n",
            "3      \t [-9.43477705 -1.75785015]. \t  -596.5317353689182 \t -28.18355402366269\n",
            "4      \t [-2.2390099  -1.73641554]. \t  -147.25342571928326 \t -28.18355402366269\n",
            "5      \t [-1.00367067  4.27897404]. \t  -2834.981159859746 \t -28.18355402366269\n",
            "6      \t [ 9.27370946 -3.20211659]. \t  -320.83245363494626 \t -28.18355402366269\n",
            "7      \t [8.89638698 5.527643  ]. \t  -5514.807675944536 \t -28.18355402366269\n",
            "8      \t [ 1.36987667 -5.71796242]. \t  -8197.337465414073 \t -28.18355402366269\n",
            "9      \t [-10.          -7.38196391]. \t  -28436.708713333905 \t -28.18355402366269\n",
            "10     \t [-2.75093026  9.62298821]. \t  -70668.0339402211 \t -28.18355402366269\n",
            "11     \t [-5.8147762   1.45564221]. \t  -248.5492870528541 \t -28.18355402366269\n",
            "12     \t [9.87513368 9.90474429]. \t  -69518.58177407873 \t -28.18355402366269\n",
            "13     \t [-6.02203414 -3.8627435 ]. \t  -2621.705848833965 \t -28.18355402366269\n",
            "14     \t [ 5.3123475  -3.04115197]. \t  -366.27756959445645 \t -28.18355402366269\n",
            "15     \t [3.83695092 3.30455624]. \t  -656.281086102878 \t -28.18355402366269\n",
            "16     \t [ 0.52414869 -0.1072242 ]. \t  \u001b[92m-0.7287464016565732\u001b[0m \t -0.7287464016565732\n",
            "17     \t [ 0.83297163 -0.42477681]. \t  \u001b[92m-0.4736571010974906\u001b[0m \t -0.4736571010974906\n",
            "18     \t [9.31253201 0.86788036]. \t  -190.96856354894862 \t -0.4736571010974906\n",
            "19     \t [ 0.57664004 -0.89594019]. \t  -2.2960003794156894 \t -0.4736571010974906\n",
            "20     \t [-0.40773134  0.39363215]. \t  -3.0116755985543158 \t -0.4736571010974906\n",
            "21     \t [ 0.88558488 -0.05796094]. \t  -1.557901487776943 \t -0.4736571010974906\n",
            "22     \t [ 0.44528415 -0.28429176]. \t  \u001b[92m-0.46861358509810086\u001b[0m \t -0.46861358509810086\n",
            "23     \t [ 2.01277923 -9.55754188]. \t  -65291.86440518609 \t -0.46861358509810086\n",
            "24     \t [-10.           1.88319662]. \t  -705.3316599066318 \t -0.46861358509810086\n",
            "25     \t [ 1.1465174  -1.52825582]. \t  -24.867278945912584 \t -0.46861358509810086\n",
            "26     \t [ 5.86890583 -0.40519695]. \t  -85.10133798247173 \t -0.46861358509810086\n",
            "27     \t [-2.64357647  0.85147194]. \t  -46.79053217013646 \t -0.46861358509810086\n",
            "28     \t [0.78674205 0.45759508]. \t  \u001b[92m-0.31626149785962837\u001b[0m \t -0.31626149785962837\n",
            "29     \t [-0.14794848  0.42494163]. \t  -1.8361498491509356 \t -0.31626149785962837\n",
            "30     \t [-2.40402673 -5.44219706]. \t  -7610.331000610567 \t -0.31626149785962837\n",
            "31     \t [1.91406395 0.53132709]. \t  -4.4775273112542715 \t -0.31626149785962837\n",
            "32     \t [0.38439037 0.83173806]. \t  -2.3757206652801393 \t -0.31626149785962837\n",
            "33     \t [-4.50809061  4.88518664]. \t  -5487.995544544278 \t -0.31626149785962837\n",
            "34     \t [ 7.57892659 -5.18912668]. \t  -4326.060330460954 \t -0.31626149785962837\n",
            "35     \t [1.97194873 5.95761444]. \t  -9526.916957846863 \t -0.31626149785962837\n",
            "36     \t [1.02523531 1.14361819]. \t  -5.0599526580928 \t -0.31626149785962837\n",
            "37     \t [7.28286031 1.91169004]. \t  -39.475712508551204 \t -0.31626149785962837\n",
            "38     \t [ 2.77461248 -1.02101885]. \t  -4.10049332447318 \t -0.31626149785962837\n",
            "39     \t [ 2.1479141  -0.49235396]. \t  -6.849438590481957 \t -0.31626149785962837\n",
            "40     \t [-4.3517255  -0.64133564]. \t  -82.18872652515466 \t -0.31626149785962837\n",
            "41     \t [-2.92168769 -5.05486212]. \t  -5852.770046987735 \t -0.31626149785962837\n",
            "42     \t [0.92075275 0.52342196]. \t  \u001b[92m-0.28425718885340884\u001b[0m \t -0.28425718885340884\n",
            "43     \t [-9.2651553   9.70867601]. \t  -78340.761065285 \t -0.28425718885340884\n",
            "44     \t [4.84022995 4.91379184]. \t  -3790.6341503370104 \t -0.28425718885340884\n",
            "45     \t [ 7.39616664 -0.73667976]. \t  -120.5626468185589 \t -0.28425718885340884\n",
            "46     \t [3.73054145 1.70938678]. \t  -16.38932404844521 \t -0.28425718885340884\n",
            "47     \t [4.28737905 1.12026767]. \t  -17.12501863145796 \t -0.28425718885340884\n",
            "48     \t [3.12787377 9.51196001]. \t  -63249.335349295434 \t -0.28425718885340884\n",
            "49     \t [0.98498249 0.79425871]. \t  \u001b[92m-0.15336383505731177\u001b[0m \t -0.15336383505731177\n",
            "50     \t [-0.61879016  0.70754953]. \t  -7.869559056713758 \t -0.15336383505731177\n",
            "51     \t [ 3.15553514 -1.82589276]. \t  -29.317901276308568 \t -0.15336383505731177\n",
            "52     \t [2.84967697 1.79962463]. \t  -29.740567598378856 \t -0.15336383505731177\n",
            "53     \t [-7.54571784  4.77818483]. \t  -5735.1731847284855 \t -0.15336383505731177\n",
            "54     \t [-5.06554453  3.76103289]. \t  -2262.073843733766 \t -0.15336383505731177\n",
            "55     \t [-8.55978671 -9.9309001 ]. \t  -84803.06583085285 \t -0.15336383505731177\n",
            "56     \t [ 7.98605608 -0.48736107]. \t  -161.63565597696189 \t -0.15336383505731177\n",
            "57     \t [-2.64801284  0.01406951]. \t  -27.33613532821893 \t -0.15336383505731177\n",
            "58     \t [-7.30446077 -4.45298308]. \t  -4479.931410554043 \t -0.15336383505731177\n",
            "59     \t [3.19764279 4.65963946]. \t  -3241.2264667529867 \t -0.15336383505731177\n",
            "60     \t [1.49521071 3.85263359]. \t  -1589.6380746331806 \t -0.15336383505731177\n",
            "61     \t [ 9.01563964 -8.25265235]. \t  -32422.354289695904 \t -0.15336383505731177\n",
            "62     \t [ 7.01098383 -5.27149603]. \t  -4753.513997951199 \t -0.15336383505731177\n",
            "63     \t [2.28876159 9.60659356]. \t  -66456.95186784181 \t -0.15336383505731177\n",
            "64     \t [ 1.35127257 -6.48666679]. \t  -13712.604773039777 \t -0.15336383505731177\n",
            "65     \t [7.82567942 8.46717967]. \t  -36799.93362102439 \t -0.15336383505731177\n",
            "66     \t [ 0.89919885 -2.90745419]. \t  -512.4826454303164 \t -0.15336383505731177\n",
            "67     \t [ 1.32890142 -1.14151099]. \t  -3.3706212636673056 \t -0.15336383505731177\n",
            "68     \t [-8.75020178 -5.68207233]. \t  -10847.325259422847 \t -0.15336383505731177\n",
            "69     \t [-2.72271128 -6.53641866]. \t  -15562.549628196019 \t -0.15336383505731177\n",
            "70     \t [ 2.75394985 -3.95416581]. \t  -1629.5040380870096 \t -0.15336383505731177\n",
            "71     \t [ 3.75929949 -1.62624911]. \t  -12.29597963833867 \t -0.15336383505731177\n",
            "72     \t [1.72739661 2.71616151]. \t  -339.96948038768585 \t -0.15336383505731177\n",
            "73     \t [1.79604589 0.32932879]. \t  -5.620998353343156 \t -0.15336383505731177\n",
            "74     \t [2.34136886 0.56182981]. \t  -7.647903985958054 \t -0.15336383505731177\n",
            "75     \t [ 0.67615036 -0.89514612]. \t  -1.8213969141604853 \t -0.15336383505731177\n",
            "76     \t [-4.31190326  4.05092865]. \t  -2785.779603738259 \t -0.15336383505731177\n",
            "77     \t [-3.4499885   2.43934428]. \t  -491.0958758465844 \t -0.15336383505731177\n",
            "78     \t [-3.40110708 -5.96485396]. \t  -11137.778595501333 \t -0.15336383505731177\n",
            "79     \t [-2.28071149 -6.41422133]. \t  -14313.303102479305 \t -0.15336383505731177\n",
            "80     \t [-0.61382331 -4.51937018]. \t  -3441.004449642181 \t -0.15336383505731177\n",
            "81     \t [ 1.02525228 -0.6934796 ]. \t  \u001b[92m-0.008682979046621776\u001b[0m \t -0.008682979046621776\n",
            "82     \t [-9.04863361  1.2583201 ]. \t  -399.40569125181196 \t -0.008682979046621776\n",
            "83     \t [ 2.08147282 -1.08714472]. \t  -1.3289637893227715 \t -0.008682979046621776\n",
            "84     \t [-2.23416867  0.48900376]. \t  -25.174270082102176 \t -0.008682979046621776\n",
            "85     \t [-6.91943809 -0.22521392]. \t  -161.3030317594138 \t -0.008682979046621776\n",
            "86     \t [8.73188655 3.77820009]. \t  -845.2649612317308 \t -0.008682979046621776\n",
            "87     \t [8.49147411 2.59623271]. \t  -105.90989941353337 \t -0.008682979046621776\n",
            "88     \t [-6.99691523  3.70409887]. \t  -2435.848917227575 \t -0.008682979046621776\n",
            "89     \t [-4.54889154 -3.53520937]. \t  -1776.5217755378185 \t -0.008682979046621776\n",
            "90     \t [5.57518348 7.26651834]. \t  -20032.691716494577 \t -0.008682979046621776\n",
            "91     \t [ 3.02522329 -1.55940012]. \t  -10.859739127352245 \t -0.008682979046621776\n",
            "92     \t [-5.34625143 -7.38849223]. \t  -26272.63213176849 \t -0.008682979046621776\n",
            "93     \t [ 4.34379926 -2.21067563]. \t  -70.158921569656 \t -0.008682979046621776\n",
            "94     \t [-6.10772408 -4.44348195]. \t  -4208.653841123318 \t -0.008682979046621776\n",
            "95     \t [-6.31343574 -2.1723769 ]. \t  -549.7296951375912 \t -0.008682979046621776\n",
            "96     \t [ 9.69587589 -7.31644498]. \t  -19035.44955913957 \t -0.008682979046621776\n",
            "97     \t [-5.07031387 -4.65553508]. \t  -4725.5175973484165 \t -0.008682979046621776\n",
            "98     \t [-3.20025038  3.94954699]. \t  -2384.099734581828 \t -0.008682979046621776\n",
            "99     \t [-2.21002255  4.40918558]. \t  -3387.3857635547797 \t -0.008682979046621776\n",
            "100    \t [ 5.85237815 -0.64502844]. \t  -73.95148943738262 \t -0.008682979046621776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4xG5dbuuvig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ed09b4-b753-47c1-da70-1459cf2a429b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_winner_10 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_10 = dGPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.07179171 -7.69986114]. \t  -26686.60864226137 \t -217.28346804397978\n",
            "init   \t [ 9.00565729 -0.35617197]. \t  -217.28346804397978 \t -217.28346804397978\n",
            "init   \t [ 7.4494907  -5.75334638]. \t  -6945.3080829060855 \t -217.28346804397978\n",
            "init   \t [-9.1858075  -2.05611077]. \t  -726.1597680698058 \t -217.28346804397978\n",
            "init   \t [-5.33735605  6.83481449]. \t  -19549.897628464034 \t -217.28346804397978\n",
            "1      \t [7.70674409 9.05288798]. \t  -48843.620887164754 \t -217.28346804397978\n",
            "2      \t [-6.08246046 -9.42114812]. \t  -67466.99755823516 \t -217.28346804397978\n",
            "3      \t [1.36988239 1.21701407]. \t  \u001b[92m-5.2080597150791546\u001b[0m \t -5.2080597150791546\n",
            "4      \t [-2.27926953 -2.9633827 ]. \t  -798.2066835481544 \t -5.2080597150791546\n",
            "5      \t [1.36021708 6.563982  ]. \t  -14386.111644694158 \t -5.2080597150791546\n",
            "6      \t [-5.31740159  1.14603964]. \t  -166.13067670922092 \t -5.2080597150791546\n",
            "7      \t [ 3.90219447 -1.60391022]. \t  -11.512142184559368 \t -5.2080597150791546\n",
            "8      \t [-10.           3.27382027]. \t  -2097.418824392088 \t -5.2080597150791546\n",
            "9      \t [7.29815912 3.47480791]. \t  -607.5401820018474 \t -5.2080597150791546\n",
            "10     \t [-10.          -6.61893829]. \t  -19180.597504393187 \t -5.2080597150791546\n",
            "11     \t [-1.17703149  0.92616442]. \t  -21.473649409337206 \t -5.2080597150791546\n",
            "12     \t [3.7134726  1.09690717]. \t  -10.779755153205578 \t -5.2080597150791546\n",
            "13     \t [ 1.37632378 -1.3892829 ]. \t  -12.481040315154427 \t -5.2080597150791546\n",
            "14     \t [  7.66990468 -10.        ]. \t  -74026.21876210123 \t -5.2080597150791546\n",
            "15     \t [-1.33825741 -9.59555515]. \t  -68816.77771125651 \t -5.2080597150791546\n",
            "16     \t [-1.85547474  3.24043297]. \t  -1052.9733964521038 \t -5.2080597150791546\n",
            "17     \t [-0.8500533  0.0424886]. \t  \u001b[92m-4.880181192490879\u001b[0m \t -4.880181192490879\n",
            "18     \t [-1.17020302  0.35341036]. \t  -8.742585637538326 \t -4.880181192490879\n",
            "19     \t [0.14647991 0.43536399]. \t  \u001b[92m-0.8367055127577001\u001b[0m \t -0.8367055127577001\n",
            "20     \t [ 2.48838478 -3.27171142]. \t  -718.1334364502949 \t -0.8367055127577001\n",
            "21     \t [ 1.35065969 -0.92209768]. \t  \u001b[92m-0.3677782752382126\u001b[0m \t -0.3677782752382126\n",
            "22     \t [-0.5425077   0.64180521]. \t  -6.113075686632996 \t -0.3677782752382126\n",
            "23     \t [-0.17227374 -0.56665625]. \t  -2.7009561907116635 \t -0.3677782752382126\n",
            "24     \t [-1.60930849  9.62557553]. \t  -69879.53604277525 \t -0.3677782752382126\n",
            "25     \t [ 4.95423793 -0.94134273]. \t  -35.8860629365449 \t -0.3677782752382126\n",
            "26     \t [ 3.61273833 -0.90139331]. \t  -14.72845153600678 \t -0.3677782752382126\n",
            "27     \t [-5.6243385  -4.64195691]. \t  -4791.132633182388 \t -0.3677782752382126\n",
            "28     \t [ 0.8119368  -1.53568669]. \t  -30.529207382282067 \t -0.3677782752382126\n",
            "29     \t [3.34530631 2.48992099]. \t  -169.45416007565 \t -0.3677782752382126\n",
            "30     \t [-10.           8.54146072]. \t  -48738.7909369126 \t -0.3677782752382126\n",
            "31     \t [ 4.81933682 -2.14239938]. \t  -52.613742888219356 \t -0.3677782752382126\n",
            "32     \t [4.43805264 0.79229875]. \t  -32.07781182897834 \t -0.3677782752382126\n",
            "33     \t [3.50401141 0.45794937]. \t  -25.29929312783836 \t -0.3677782752382126\n",
            "34     \t [0.53370241 0.59582478]. \t  \u001b[92m-0.2796052337667702\u001b[0m \t -0.2796052337667702\n",
            "35     \t [-0.50780535 -0.26290934]. \t  -3.1082329756069855 \t -0.2796052337667702\n",
            "36     \t [9.50914864 5.25826633]. \t  -4265.777054551323 \t -0.2796052337667702\n",
            "37     \t [ 2.09194532 -0.8152044 ]. \t  -2.356160376872505 \t -0.2796052337667702\n",
            "38     \t [-0.21428061  0.08121729]. \t  -1.5779654394253926 \t -0.2796052337667702\n",
            "39     \t [ 2.01219802 -1.12481688]. \t  -1.5616654028165504 \t -0.2796052337667702\n",
            "40     \t [2.81487717 9.95317318]. \t  -76300.31377148995 \t -0.2796052337667702\n",
            "41     \t [ 9.8356841  -3.07346653]. \t  -242.11726431745427 \t -0.2796052337667702\n",
            "42     \t [0.19672427 0.13828216]. \t  -0.6954839503444933 \t -0.2796052337667702\n",
            "43     \t [-6.26411032 -0.80890763]. \t  -167.46109329337384 \t -0.2796052337667702\n",
            "44     \t [-0.72865278 -5.2713542 ]. \t  -6343.044516227641 \t -0.2796052337667702\n",
            "45     \t [5.25807014 5.25386701]. \t  -5007.769911145954 \t -0.2796052337667702\n",
            "46     \t [-6.52327647  2.87128272]. \t  -1115.686065094399 \t -0.2796052337667702\n",
            "47     \t [-3.7964225   0.83678657]. \t  -77.02008596605735 \t -0.2796052337667702\n",
            "48     \t [ 1.66515713 -1.01749726]. \t  -0.7712040669231897 \t -0.2796052337667702\n",
            "49     \t [ 7.16280029 -2.2032717 ]. \t  -50.94446266057528 \t -0.2796052337667702\n",
            "50     \t [ 0.36116911 -0.15596855]. \t  -0.6034383259726894 \t -0.2796052337667702\n",
            "51     \t [ 2.97120906 -1.07977989]. \t  -4.703227179544539 \t -0.2796052337667702\n",
            "52     \t [2.49167412 0.58480545]. \t  -8.760500512957396 \t -0.2796052337667702\n",
            "53     \t [4.12422151 9.30667802]. \t  -57202.28252018437 \t -0.2796052337667702\n",
            "54     \t [ 6.56496223 -0.51701441]. \t  -103.6991537004162 \t -0.2796052337667702\n",
            "55     \t [ 2.18668403 -0.94848687]. \t  -1.7084220196504911 \t -0.2796052337667702\n",
            "56     \t [1.32258178 0.51841283]. \t  -1.3367540929219164 \t -0.2796052337667702\n",
            "57     \t [ 6.76050866 -2.05206792]. \t  -38.70433769107767 \t -0.2796052337667702\n",
            "58     \t [-4.24352113  0.81128115]. \t  -89.31894166189932 \t -0.2796052337667702\n",
            "59     \t [9.85185394 1.85521542]. \t  -95.97580540935868 \t -0.2796052337667702\n",
            "60     \t [ 9.55393667 -6.86338487]. \t  -14407.180303689185 \t -0.2796052337667702\n",
            "61     \t [ 1.15650232 -0.94251044]. \t  -0.7936639004760861 \t -0.2796052337667702\n",
            "62     \t [0.93082393 2.05055275]. \t  -111.86696956390618 \t -0.2796052337667702\n",
            "63     \t [ 1.51171163 -0.78200951]. \t  -0.4284678478953032 \t -0.2796052337667702\n",
            "64     \t [2.30669218 5.53075925]. \t  -6933.509486673679 \t -0.2796052337667702\n",
            "65     \t [ 6.22064745 -9.68865117]. \t  -65925.74995138083 \t -0.2796052337667702\n",
            "66     \t [1.36411546 0.85671381]. \t  \u001b[92m-0.15412963316900286\u001b[0m \t -0.15412963316900286\n",
            "67     \t [ 1.21699006 -0.76321018]. \t  \u001b[92m-0.05249486936997505\u001b[0m \t -0.05249486936997505\n",
            "68     \t [-1.70729211 -9.56820662]. \t  -68315.6542930769 \t -0.05249486936997505\n",
            "69     \t [ 7.38047154 -9.25721189]. \t  -53840.23648129998 \t -0.05249486936997505\n",
            "70     \t [ 0.23797604 -0.43970884]. \t  -0.6249108354130698 \t -0.05249486936997505\n",
            "71     \t [1.27292744 0.82576303]. \t  -0.09099382248244976 \t -0.05249486936997505\n",
            "72     \t [ 1.7330996  -0.09082251]. \t  -6.430880850962315 \t -0.05249486936997505\n",
            "73     \t [ 7.97926101 -7.98257397]. \t  -28591.867752221548 \t -0.05249486936997505\n",
            "74     \t [9.94231219 3.27114541]. \t  -342.55812679072415 \t -0.05249486936997505\n",
            "75     \t [-8.46680573  8.44266127]. \t  -45706.03566664887 \t -0.05249486936997505\n",
            "76     \t [ 3.25293074 -5.22260964]. \t  -5268.121662610138 \t -0.05249486936997505\n",
            "77     \t [0.74889545 0.19969893]. \t  -0.9585397973824428 \t -0.05249486936997505\n",
            "78     \t [ 1.43472543 -8.73045771]. \t  -45606.45220107105 \t -0.05249486936997505\n",
            "79     \t [1.71818045 9.78017345]. \t  -71885.7673970747 \t -0.05249486936997505\n",
            "80     \t [2.10296936 1.01421474]. \t  -1.2207195311318293 \t -0.05249486936997505\n",
            "81     \t [ 9.86970464 -8.11441696]. \t  -29757.834494110662 \t -0.05249486936997505\n",
            "82     \t [1.97726697 1.16196716]. \t  -2.000706542374218 \t -0.05249486936997505\n",
            "83     \t [-5.71077353  6.87092161]. \t  -20097.02860416015 \t -0.05249486936997505\n",
            "84     \t [-9.84186936 -0.80085221]. \t  -365.05948714640203 \t -0.05249486936997505\n",
            "85     \t [ 8.13288047 -6.31727247]. \t  -10327.7779569444 \t -0.05249486936997505\n",
            "86     \t [2.67343951 1.13394865]. \t  -2.8211101562583947 \t -0.05249486936997505\n",
            "87     \t [-5.05129396  0.33177034]. \t  -92.1942562736133 \t -0.05249486936997505\n",
            "88     \t [ 7.11878012 -1.20910938]. \t  -72.63365938492225 \t -0.05249486936997505\n",
            "89     \t [-8.17943794 -0.93216136]. \t  -280.967265407088 \t -0.05249486936997505\n",
            "90     \t [-0.06526875  2.98658386]. \t  -642.2867100258273 \t -0.05249486936997505\n",
            "91     \t [ 2.25504466 -3.58192906]. \t  -1097.1994125105232 \t -0.05249486936997505\n",
            "92     \t [3.84302816 0.29679479]. \t  -34.97444363718047 \t -0.05249486936997505\n",
            "93     \t [5.81368138 7.99447649]. \t  -29795.869878511236 \t -0.05249486936997505\n",
            "94     \t [ 2.1995583  -9.43301258]. \t  -61787.33215146634 \t -0.05249486936997505\n",
            "95     \t [-1.41958965 -0.15751026]. \t  -10.171562020910802 \t -0.05249486936997505\n",
            "96     \t [-3.95788396  5.24743984]. \t  -6993.457889775188 \t -0.05249486936997505\n",
            "97     \t [-3.87570309  8.8357695 ]. \t  -51234.876069122845 \t -0.05249486936997505\n",
            "98     \t [-3.18917536 -1.71868298]. \t  -183.05701438345855 \t -0.05249486936997505\n",
            "99     \t [-2.92348034 -0.27284857]. \t  -34.2726500274852 \t -0.05249486936997505\n",
            "100    \t [ 3.08047435 -1.39963581]. \t  -5.731140554348829 \t -0.05249486936997505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSj_CQIAuvk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6fef185-a141-49af-9985-fcfe8f226399"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_winner_11 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_11 = dGPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [9.62071325 6.42495699]. \t  -10714.638013792866 \t -9.740000060755142\n",
            "init   \t [ 2.91787228 -1.52623973]. \t  -9.740000060755142 \t -9.740000060755142\n",
            "init   \t [-5.95378042 -0.16186459]. \t  -120.5034756901465 \t -9.740000060755142\n",
            "init   \t [-7.22883337 -0.94957929]. \t  -230.87623811313406 \t -9.740000060755142\n",
            "init   \t [-7.72530745 -9.94151023]. \t  -84448.33746856163 \t -9.740000060755142\n",
            "1      \t [ 9.71879818 -6.86089495]. \t  -14331.165445255861 \t -9.740000060755142\n",
            "2      \t [-0.41525252  9.69422331]. \t  -70969.40865155176 \t -9.740000060755142\n",
            "3      \t [-9.5194484  9.1420568]. \t  -62537.95985021731 \t -9.740000060755142\n",
            "4      \t [ 0.94955298 -9.31626642]. \t  -59606.43954696483 \t -9.740000060755142\n",
            "5      \t [-0.73492853  2.66899284]. \t  -451.9290728979969 \t -9.740000060755142\n",
            "6      \t [ 8.16818502 -0.05450252]. \t  -184.62732969317273 \t -9.740000060755142\n",
            "7      \t [-1.84830906 -3.03508237]. \t  -830.0012916819509 \t -9.740000060755142\n",
            "8      \t [4.0982604  4.61962131]. \t  -2986.978599578018 \t -9.740000060755142\n",
            "9      \t [-10.          -4.60249827]. \t  -5605.391861037526 \t -9.740000060755142\n",
            "10     \t [-10.           3.02695087]. \t  -1725.5957399679423 \t -9.740000060755142\n",
            "11     \t [ 4.64514967 -4.90395584]. \t  -3789.512037334314 \t -9.740000060755142\n",
            "12     \t [-5.17604036  3.47843611]. \t  -1763.933649059507 \t -9.740000060755142\n",
            "13     \t [6.87610821 9.95490249]. \t  -73244.31967606739 \t -9.740000060755142\n",
            "14     \t [-5.5172727  -4.84045387]. \t  -5529.229571420188 \t -9.740000060755142\n",
            "15     \t [3.81024431 0.42372301]. \t  -31.71851035694783 \t -9.740000060755142\n",
            "16     \t [ 1.21552147 -1.06258316]. \t  \u001b[92m-2.2206645418739717\u001b[0m \t -2.2206645418739717\n",
            "17     \t [ 0.93547022 -0.63215467]. \t  \u001b[92m-0.04128196038444784\u001b[0m \t -0.04128196038444784\n",
            "18     \t [ 0.86977675 -0.51949665]. \t  -0.23478873047938975 \t -0.04128196038444784\n",
            "19     \t [  6.07180076 -10.        ]. \t  -75242.01608635244 \t -0.04128196038444784\n",
            "20     \t [-4.57749415  7.29273429]. \t  -24648.890183253094 \t -0.04128196038444784\n",
            "21     \t [ 1.00614797 -0.5066877 ]. \t  -0.485511136812936 \t -0.04128196038444784\n",
            "22     \t [-3.5787386   0.06074801]. \t  -46.685349674750675 \t -0.04128196038444784\n",
            "23     \t [ 9.22226494 -2.27702436]. \t  -70.23876294356214 \t -0.04128196038444784\n",
            "24     \t [7.5178201 2.2895434]. \t  -60.078638751211855 \t -0.04128196038444784\n",
            "25     \t [1.80824395 0.86910591]. \t  -0.830334799634687 \t -0.04128196038444784\n",
            "26     \t [1.87079499 1.19411038]. \t  -2.683022385928261 \t -0.04128196038444784\n",
            "27     \t [ 4.42012724 -1.45599882]. \t  -11.762259154948016 \t -0.04128196038444784\n",
            "28     \t [ 0.51753171 -4.65136484]. \t  -3655.846522957253 \t -0.04128196038444784\n",
            "29     \t [ 5.14182964 -1.39003463]. \t  -20.418443676855386 \t -0.04128196038444784\n",
            "30     \t [-3.63016019 -8.43277636]. \t  -42567.98036989122 \t -0.04128196038444784\n",
            "31     \t [-9.94522097 -0.65158699]. \t  -352.83394027924084 \t -0.04128196038444784\n",
            "32     \t [0.69236161 5.26030958]. \t  -5973.1980296971515 \t -0.04128196038444784\n",
            "33     \t [ 0.09208998 -0.15594518]. \t  -0.8280767839047932 \t -0.04128196038444784\n",
            "34     \t [ 0.20107465 -0.59627987]. \t  -1.1585321581818193 \t -0.04128196038444784\n",
            "35     \t [ 2.47617007 -0.74497698]. \t  -5.91202108079125 \t -0.04128196038444784\n",
            "36     \t [ 2.15944885 -0.0955775 ]. \t  -10.513614384293689 \t -0.04128196038444784\n",
            "37     \t [-0.34840984 -0.39820313]. \t  -2.7040995724777024 \t -0.04128196038444784\n",
            "38     \t [ 4.04756338 -1.0163608 ]. \t  -17.140999524830974 \t -0.04128196038444784\n",
            "39     \t [5.35554432 1.41779926]. \t  -22.536470405569574 \t -0.04128196038444784\n",
            "40     \t [-9.67059838 -7.32174896]. \t  -27438.822738488598 \t -0.04128196038444784\n",
            "41     \t [5.40004105 0.33813119]. \t  -72.84661323377577 \t -0.04128196038444784\n",
            "42     \t [ 7.43668814 -2.52668697]. \t  -98.282997945517 \t -0.04128196038444784\n",
            "43     \t [1.54713351 0.75732063]. \t  -0.6194582100546269 \t -0.04128196038444784\n",
            "44     \t [9.70138281 2.21105108]. \t  -75.72564851673485 \t -0.04128196038444784\n",
            "45     \t [ 2.03874845 -1.00216889]. \t  -1.0808059614557575 \t -0.04128196038444784\n",
            "46     \t [-4.24128232 -1.30296066]. \t  -144.10927054462957 \t -0.04128196038444784\n",
            "47     \t [ 0.67758561 -0.50797742]. \t  -0.15611780076293735 \t -0.04128196038444784\n",
            "48     \t [-2.82695935 -5.75623563]. \t  -9563.009858686777 \t -0.04128196038444784\n",
            "49     \t [2.29444523 0.77259326]. \t  -4.098425280046522 \t -0.04128196038444784\n",
            "50     \t [-0.38202811 -0.08194416]. \t  -2.222775432715206 \t -0.04128196038444784\n",
            "51     \t [2.47707147 1.03401369]. \t  -2.41117936187056 \t -0.04128196038444784\n",
            "52     \t [-2.10673737  0.37331894]. \t  -21.03276499898878 \t -0.04128196038444784\n",
            "53     \t [ 8.67533972 -0.06846705]. \t  -209.10871247679876 \t -0.04128196038444784\n",
            "54     \t [ 4.7535342  -6.15824446]. \t  -10122.920851352947 \t -0.04128196038444784\n",
            "55     \t [ 5.53284339 -8.2539833 ]. \t  -34197.885286768586 \t -0.04128196038444784\n",
            "56     \t [6.30151111 1.53062367]. \t  -33.32824327185425 \t -0.04128196038444784\n",
            "57     \t [-0.88657591  2.10122396]. \t  -192.39391201053718 \t -0.04128196038444784\n",
            "58     \t [ 2.34349292 -6.85063238]. \t  -16753.199375838183 \t -0.04128196038444784\n",
            "59     \t [0.29081205 0.31102919]. \t  -0.521895261082693 \t -0.04128196038444784\n",
            "60     \t [-2.09260966  1.07402624]. \t  -48.278504002050845 \t -0.04128196038444784\n",
            "61     \t [-2.40289239 -0.02655246]. \t  -23.141017254048244 \t -0.04128196038444784\n",
            "62     \t [-0.1267749   0.71140914]. \t  -3.8641763781631817 \t -0.04128196038444784\n",
            "63     \t [0.82529107 0.61207243]. \t  -0.04208304017686198 \t -0.04128196038444784\n",
            "64     \t [-1.20867554  0.9997912 ]. \t  -25.45872794461483 \t -0.04128196038444784\n",
            "65     \t [-0.17762626  6.75297554]. \t  -16703.08617415138 \t -0.04128196038444784\n",
            "66     \t [1.97432265 2.53632665]. \t  -238.20247058643878 \t -0.04128196038444784\n",
            "67     \t [-2.91957718 -1.81146054]. \t  -195.19322620271217 \t -0.04128196038444784\n",
            "68     \t [0.69214307 0.89675167]. \t  -1.773562344423428 \t -0.04128196038444784\n",
            "69     \t [ 5.36038683 -2.60184878]. \t  -152.80005864493495 \t -0.04128196038444784\n",
            "70     \t [1.05324439 0.95561125]. \t  -1.1983300228542726 \t -0.04128196038444784\n",
            "71     \t [-8.16673039  1.17899306]. \t  -323.69291488217124 \t -0.04128196038444784\n",
            "72     \t [1.26688474 0.82203192]. \t  -0.08553779603923299 \t -0.04128196038444784\n",
            "73     \t [ 2.32782963 -5.9831748 ]. \t  -9598.132944240877 \t -0.04128196038444784\n",
            "74     \t [4.63518173 0.35127548]. \t  -51.730529059530966 \t -0.04128196038444784\n",
            "75     \t [-3.94498833 -7.05855256]. \t  -21486.774807150385 \t -0.04128196038444784\n",
            "76     \t [-6.37212638  0.81928152]. \t  -173.37745216130904 \t -0.04128196038444784\n",
            "77     \t [ 9.67852138 -2.76893952]. \t  -139.28678989564332 \t -0.04128196038444784\n",
            "78     \t [ 1.05483998 -0.73713101]. \t  \u001b[92m-0.005040638158741799\u001b[0m \t -0.005040638158741799\n",
            "79     \t [-0.69810267 -7.36598808]. \t  -23858.13215841738 \t -0.005040638158741799\n",
            "80     \t [ 0.86122385 -0.87110861]. \t  -0.8810767827626406 \t -0.005040638158741799\n",
            "81     \t [ 1.39442669 -4.26455993]. \t  -2447.1487452458628 \t -0.005040638158741799\n",
            "82     \t [2.74498008 6.90529164]. \t  -17160.390579779105 \t -0.005040638158741799\n",
            "83     \t [8.75404465 3.15419183]. \t  -308.4941042225309 \t -0.005040638158741799\n",
            "84     \t [ 8.32326456 -5.33167961]. \t  -4764.020014189273 \t -0.005040638158741799\n",
            "85     \t [-6.34646301  6.46072723]. \t  -16192.27346302961 \t -0.005040638158741799\n",
            "86     \t [6.66733201 2.22350281]. \t  -52.863147833650004 \t -0.005040638158741799\n",
            "87     \t [-9.81419467  4.16506142]. \t  -4079.167887251405 \t -0.005040638158741799\n",
            "88     \t [1.6064581  0.92048308]. \t  -0.38332172654251145 \t -0.005040638158741799\n",
            "89     \t [-1.6326541   0.02254101]. \t  -12.268624837834865 \t -0.005040638158741799\n",
            "90     \t [-2.07678822  8.62162833]. \t  -45455.60988660681 \t -0.005040638158741799\n",
            "91     \t [-1.13533016 -4.2962465 ]. \t  -2900.286107739129 \t -0.005040638158741799\n",
            "92     \t [-1.36834628  4.22089697]. \t  -2743.651788682231 \t -0.005040638158741799\n",
            "93     \t [0.63553986 0.34860046]. \t  -0.4409363110340654 \t -0.005040638158741799\n",
            "94     \t [9.93973523 1.38317954]. \t  -154.6653030668174 \t -0.005040638158741799\n",
            "95     \t [5.77193761 2.4238855 ]. \t  -94.25641518721703 \t -0.005040638158741799\n",
            "96     \t [0.54794103 0.86379893]. \t  -1.9879743804767294 \t -0.005040638158741799\n",
            "97     \t [ 7.47631853 -0.76898239]. \t  -121.16277987722734 \t -0.005040638158741799\n",
            "98     \t [-0.21372332  5.24303817]. \t  -6093.924112011358 \t -0.005040638158741799\n",
            "99     \t [-5.23804805  6.39593121]. \t  -15195.683301944866 \t -0.005040638158741799\n",
            "100    \t [ 9.75240598 -3.36757363]. \t  -410.9070903804599 \t -0.005040638158741799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l98Nt7Tguvna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25f5eb92-8556-41c3-cf1e-218533c70092"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_winner_12 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_12 = dGPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-6.16961099  2.44217542]. \t  -706.482335023616 \t -706.482335023616\n",
            "init   \t [-1.24544522  5.70717167]. \t  -8820.05719953671 \t -706.482335023616\n",
            "init   \t [ 5.59951616 -4.54814789]. \t  -2580.3963775443385 \t -706.482335023616\n",
            "init   \t [-4.4707149   6.03744355]. \t  -12002.832292900235 \t -706.482335023616\n",
            "init   \t [9.16278707 7.51865269]. \t  -21656.00779050222 \t -706.482335023616\n",
            "1      \t [-10. -10.]. \t  -88321.0 \t -706.482335023616\n",
            "2      \t [-2.84346081 -5.74360125]. \t  -9487.534509082592 \t -706.482335023616\n",
            "3      \t [-10.          -2.67137482]. \t  -1299.3072333954537 \t -706.482335023616\n",
            "4      \t [4.40623432 2.31889614]. \t  \u001b[92m-92.204874598729\u001b[0m \t -92.204874598729\n",
            "5      \t [ 2.10496664 -9.86915982]. \t  -74264.45923358954 \t -92.204874598729\n",
            "6      \t [9.13459748 0.31787954]. \t  -225.75088397017362 \t -92.204874598729\n",
            "7      \t [-10.           8.29867251]. \t  -43772.81043974757 \t -92.204874598729\n",
            "8      \t [ 9.78203716 -8.58451552]. \t  -37947.82343329771 \t -92.204874598729\n",
            "9      \t [-0.24101414 -0.28772299]. \t  \u001b[92m-1.8707358721308247\u001b[0m \t -1.8707358721308247\n",
            "10     \t [3.55786815 8.72800574]. \t  -44288.41108286048 \t -1.8707358721308247\n",
            "11     \t [-3.27632032 -0.8654776 ]. \t  -63.877150886411414 \t -1.8707358721308247\n",
            "12     \t [ 1.36046346 -3.35681923]. \t  -896.9764967890118 \t -1.8707358721308247\n",
            "13     \t [-0.39760975  0.92022402]. \t  -10.699834288931541 \t -1.8707358721308247\n",
            "14     \t [-7.23323739 -5.86377421]. \t  -11620.069925546995 \t -1.8707358721308247\n",
            "15     \t [-9.79604695  1.53789784]. \t  -538.5817881502703 \t -1.8707358721308247\n",
            "16     \t [ 9.60496963 -3.43585864]. \t  -466.3412103610556 \t -1.8707358721308247\n",
            "17     \t [1.85442979 0.37194223]. \t  -5.708626146653418 \t -1.8707358721308247\n",
            "18     \t [ 5.38103143 -0.49495839]. \t  -67.03844681976565 \t -1.8707358721308247\n",
            "19     \t [-6.87823681 -0.86352514]. \t  -202.16662728979242 \t -1.8707358721308247\n",
            "20     \t [-1.79784127  9.57157263]. \t  -68478.43588762768 \t -1.8707358721308247\n",
            "21     \t [ -4.61089872 -10.        ]. \t  -83762.72193157354 \t -1.8707358721308247\n",
            "22     \t [1.37426553 2.37027232]. \t  -194.66274810865633 \t -1.8707358721308247\n",
            "23     \t [7.729956   2.90476783]. \t  -212.56885482821832 \t -1.8707358721308247\n",
            "24     \t [-2.69734757  1.80229371]. \t  -182.72497336723282 \t -1.8707358721308247\n",
            "25     \t [ 3.23046965 -1.13503509]. \t  -5.830061609159467 \t -1.8707358721308247\n",
            "26     \t [ 1.59073033 -0.48953534]. \t  -2.8195628667886927 \t -1.8707358721308247\n",
            "27     \t [-0.28128431  0.131719  ]. \t  \u001b[92m-1.8413813861208632\u001b[0m \t -1.8413813861208632\n",
            "28     \t [-0.62399236 -0.8233358 ]. \t  -10.476219127828683 \t -1.8413813861208632\n",
            "29     \t [5.36712493 4.64100569]. \t  -2863.2720202879955 \t -1.8413813861208632\n",
            "30     \t [ 2.47873378 -0.11215935]. \t  -14.2267076251672 \t -1.8413813861208632\n",
            "31     \t [ 0.85062771 -0.37427065]. \t  \u001b[92m-0.6731856539483387\u001b[0m \t -0.6731856539483387\n",
            "32     \t [0.47081769 0.28791741]. \t  \u001b[92m-0.4661142043144742\u001b[0m \t -0.4661142043144742\n",
            "33     \t [4.22547576 1.86197023]. \t  -25.074452447533126 \t -0.4661142043144742\n",
            "34     \t [-8.70420626  3.95213572]. \t  -3285.051678100838 \t -0.4661142043144742\n",
            "35     \t [-1.26468288 -1.66789007]. \t  -98.38281184101038 \t -0.4661142043144742\n",
            "36     \t [ 1.21849214 -0.19010732]. \t  -2.6753361176468773 \t -0.4661142043144742\n",
            "37     \t [9.9794062 3.1204156]. \t  -260.9238683374435 \t -0.4661142043144742\n",
            "38     \t [-0.88814345  0.22858237]. \t  -5.535766883494606 \t -0.4661142043144742\n",
            "39     \t [ 3.46573351 -1.73971576]. \t  -19.47003344424344 \t -0.4661142043144742\n",
            "40     \t [-8.18910393 -0.74667754]. \t  -257.5743670756286 \t -0.4661142043144742\n",
            "41     \t [ 0.51146573 -0.86219677]. \t  -2.1410890847320045 \t -0.4661142043144742\n",
            "42     \t [-4.72672964 -2.68320391]. \t  -764.3952363598074 \t -0.4661142043144742\n",
            "43     \t [ 7.5371475  -1.34334555]. \t  -73.59255520099123 \t -0.4661142043144742\n",
            "44     \t [5.62194607 0.89026162]. \t  -53.95412916553596 \t -0.4661142043144742\n",
            "45     \t [2.50068712 0.85822615]. \t  -4.363914987022905 \t -0.4661142043144742\n",
            "46     \t [ 0.16106039 -0.32524712]. \t  -0.7089223925176974 \t -0.4661142043144742\n",
            "47     \t [ 0.19399261 -0.70713056]. \t  -1.9491605499994042 \t -0.4661142043144742\n",
            "48     \t [ 1.23966921 -0.6307238 ]. \t  \u001b[92m-0.45179180458815715\u001b[0m \t -0.45179180458815715\n",
            "49     \t [-1.29103693 -0.49484572]. \t  -11.591215740150982 \t -0.45179180458815715\n",
            "50     \t [ 0.42410574 -0.16121444]. \t  -0.608609047374506 \t -0.45179180458815715\n",
            "51     \t [ 1.00972224 -9.34210833]. \t  -60232.43939583875 \t -0.45179180458815715\n",
            "52     \t [3.73691354 0.93872109]. \t  -15.28814616853369 \t -0.45179180458815715\n",
            "53     \t [3.03088671 1.13638606]. \t  -4.5261599691359695 \t -0.45179180458815715\n",
            "54     \t [-5.95860536  9.73469198]. \t  -76478.80440664064 \t -0.45179180458815715\n",
            "55     \t [-4.24107999  0.33662509]. \t  -67.38983657548968 \t -0.45179180458815715\n",
            "56     \t [ 8.87043805 -6.82608291]. \t  -14281.795986853876 \t -0.45179180458815715\n",
            "57     \t [ 3.62863644 -1.19040256]. \t  -8.172253430646183 \t -0.45179180458815715\n",
            "58     \t [-3.26689353  3.06923042]. \t  -995.6668768159794 \t -0.45179180458815715\n",
            "59     \t [-2.68213897  0.45994372]. \t  -32.843121402390814 \t -0.45179180458815715\n",
            "60     \t [-2.37598484  0.96203763]. \t  -47.13263010098504 \t -0.45179180458815715\n",
            "61     \t [-3.1389655   2.30059096]. \t  -393.84951075171017 \t -0.45179180458815715\n",
            "62     \t [-3.8133098   1.07529751]. \t  -98.21976460867705 \t -0.45179180458815715\n",
            "63     \t [ 6.77588793 -4.7398027 ]. \t  -2945.056143172623 \t -0.45179180458815715\n",
            "64     \t [ 6.85791138 -6.5587892 ]. \t  -12572.4745327583 \t -0.45179180458815715\n",
            "65     \t [ 6.8431608  -0.20728075]. \t  -125.46284750646188 \t -0.45179180458815715\n",
            "66     \t [ 0.65681684 -0.64620393]. \t  \u001b[92m-0.1813865615295248\u001b[0m \t -0.1813865615295248\n",
            "67     \t [-2.41680753 -0.1507834 ]. \t  -23.80020743061185 \t -0.1813865615295248\n",
            "68     \t [-3.81265908 -4.99503952]. \t  -5793.441585001492 \t -0.1813865615295248\n",
            "69     \t [-5.86891714 -9.7963886 ]. \t  -78302.69821855053 \t -0.1813865615295248\n",
            "70     \t [3.28012838 3.6359738 ]. \t  -1078.0148941175532 \t -0.1813865615295248\n",
            "71     \t [2.8332751  0.84188193]. \t  -7.369563952369247 \t -0.1813865615295248\n",
            "72     \t [-1.85989975  0.1682356 ]. \t  -15.525018434221353 \t -0.1813865615295248\n",
            "73     \t [5.48138469 1.6134936 ]. \t  -20.233686594525135 \t -0.1813865615295248\n",
            "74     \t [1.78790199 0.71938063]. \t  -1.7544612336680387 \t -0.1813865615295248\n",
            "75     \t [ 3.73467487 -0.45974599]. \t  -29.416367796750425 \t -0.1813865615295248\n",
            "76     \t [6.63283614 3.72989125]. \t  -929.8747398348743 \t -0.1813865615295248\n",
            "77     \t [ 2.01271661 -6.01062813]. \t  -9869.066925274641 \t -0.1813865615295248\n",
            "78     \t [ 4.62301327 -5.15861912]. \t  -4736.985788552286 \t -0.1813865615295248\n",
            "79     \t [ 1.05585763 -0.65251617]. \t  \u001b[92m-0.0865994411139051\u001b[0m \t -0.0865994411139051\n",
            "80     \t [6.26398562 3.76637446]. \t  -1005.1632549001689 \t -0.0865994411139051\n",
            "81     \t [-0.28452549 -7.4087819 ]. \t  -24230.092896333244 \t -0.0865994411139051\n",
            "82     \t [8.0865315  1.66853601]. \t  -62.904680315937874 \t -0.0865994411139051\n",
            "83     \t [7.43594702 1.60305864]. \t  -51.967888692075356 \t -0.0865994411139051\n",
            "84     \t [5.28926316 2.49920395]. \t  -122.15779008534562 \t -0.0865994411139051\n",
            "85     \t [ 5.22229897 -1.8265433 ]. \t  -22.034095554277616 \t -0.0865994411139051\n",
            "86     \t [8.29695751 2.34424219]. \t  -67.76070386816677 \t -0.0865994411139051\n",
            "87     \t [3.9638764  6.95957822]. \t  -17272.422004480584 \t -0.0865994411139051\n",
            "88     \t [-9.54284769 -3.55890536]. \t  -2543.6078971587644 \t -0.0865994411139051\n",
            "89     \t [ 6.00106142 -6.84294708]. \t  -15390.331879948177 \t -0.0865994411139051\n",
            "90     \t [-7.64447184  4.58158647]. \t  -5000.274323967172 \t -0.0865994411139051\n",
            "91     \t [-3.17673625 -2.51350093]. \t  -517.4907760011075 \t -0.0865994411139051\n",
            "92     \t [0.81705881 0.31402923]. \t  -0.8018461592972518 \t -0.0865994411139051\n",
            "93     \t [-2.84232082 -7.60664852]. \t  -28129.85622966966 \t -0.0865994411139051\n",
            "94     \t [-9.46905503  2.44046659]. \t  -1023.8791326775374 \t -0.0865994411139051\n",
            "95     \t [-6.84857657  9.85863994]. \t  -81051.95808320583 \t -0.0865994411139051\n",
            "96     \t [ 1.04779365 -7.94495041]. \t  -31348.41990580627 \t -0.0865994411139051\n",
            "97     \t [4.90338108 9.29475615]. \t  -56383.70145988153 \t -0.0865994411139051\n",
            "98     \t [-6.1461776   5.99112053]. \t  -12198.24388708779 \t -0.0865994411139051\n",
            "99     \t [-4.9205641  -9.43264269]. \t  -66917.97104986853 \t -0.0865994411139051\n",
            "100    \t [-2.10756868 -2.72354809]. \t  -583.7872836272707 \t -0.0865994411139051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bpn-kmNuvqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab9df048-205d-4fc8-ebba-57723b8e6259"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_winner_13 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_13 = dGPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 3.21288618 -1.72798696]. \t  -20.120935450808645 \t -20.120935450808645\n",
            "init   \t [ 3.36205116 -5.40113156]. \t  -6051.706374022101 \t -20.120935450808645\n",
            "init   \t [6.15356672 2.73636929]. \t  -182.2099268583363 \t -20.120935450808645\n",
            "init   \t [-6.55612296 -4.79228264]. \t  -5567.089701718418 \t -20.120935450808645\n",
            "init   \t [ 8.30639981 -0.74368981]. \t  -157.07069920616067 \t -20.120935450808645\n",
            "1      \t [-7.5961809  7.6177101]. \t  -30655.10899619148 \t -20.120935450808645\n",
            "2      \t [9.21200838 9.90573549]. \t  -70031.75643065354 \t -20.120935450808645\n",
            "3      \t [-0.35482891  6.84457911]. \t  -17693.150381619864 \t -20.120935450808645\n",
            "4      \t [-10.           0.97621088]. \t  -404.50449942230665 \t -20.120935450808645\n",
            "5      \t [ 8.38757304 -9.87720684]. \t  -69791.40203706689 \t -20.120935450808645\n",
            "6      \t [-1.81138042 -9.77546462]. \t  -74452.48916498461 \t -20.120935450808645\n",
            "7      \t [-2.1943543   0.23088913]. \t  -20.79286098486515 \t -20.120935450808645\n",
            "8      \t [-10. -10.]. \t  -88321.0 \t -20.120935450808645\n",
            "9      \t [-5.44567888  2.43911643]. \t  -643.193261116388 \t -20.120935450808645\n",
            "10     \t [-1.5030724 -3.6746114]. \t  -1631.7470885572116 \t -20.120935450808645\n",
            "11     \t [9.81264863 4.60115038]. \t  -2193.872126615322 \t -20.120935450808645\n",
            "12     \t [3.62436881 9.40766209]. \t  -60130.80141957688 \t -20.120935450808645\n",
            "13     \t [1.50006223 1.79084293]. \t  -48.548285025938384 \t -20.120935450808645\n",
            "14     \t [ 3.20518159 -9.61981179]. \t  -66162.89528524783 \t -20.120935450808645\n",
            "15     \t [ 7.27524995 -4.17007839]. \t  -1552.3031263200014 \t -20.120935450808645\n",
            "16     \t [3.42094237 4.11825422]. \t  -1866.2503355938993 \t -20.120935450808645\n",
            "17     \t [ 4.1820351  -0.02427814]. \t  -45.08446521226741 \t -20.120935450808645\n",
            "18     \t [-4.56922825 -1.07899735]. \t  -126.17280168986707 \t -20.120935450808645\n",
            "19     \t [-0.59448683  0.19487018]. \t  \u001b[92m-3.4413560532254315\u001b[0m \t -3.4413560532254315\n",
            "20     \t [ 0.73878052 -0.54082016]. \t  \u001b[92m-0.11554919045977216\u001b[0m \t -0.11554919045977216\n",
            "21     \t [ 1.30442433 -0.2602743 ]. \t  -2.8255105024825 \t -0.11554919045977216\n",
            "22     \t [-1.00823259  2.16125436]. \t  -218.28931902221197 \t -0.11554919045977216\n",
            "23     \t [ 1.31057711 -1.73330398]. \t  -44.240900478619544 \t -0.11554919045977216\n",
            "24     \t [ 0.37506243 -0.3977598 ]. \t  -0.39742349400352756 \t -0.11554919045977216\n",
            "25     \t [ 0.66850655 -0.35454642]. \t  -0.4578330901526263 \t -0.11554919045977216\n",
            "26     \t [-7.97046565 -1.24878144]. \t  -326.41776516646826 \t -0.11554919045977216\n",
            "27     \t [-9.8842948   4.47078389]. \t  -5090.529408185037 \t -0.11554919045977216\n",
            "28     \t [-3.86327434  9.189004  ]. \t  -59701.08116625261 \t -0.11554919045977216\n",
            "29     \t [ 0.40458748 -0.46305945]. \t  -0.35569323032869476 \t -0.11554919045977216\n",
            "30     \t [-2.82296718  3.65511888]. \t  -1760.163848412105 \t -0.11554919045977216\n",
            "31     \t [ 5.02449524 -2.03527978]. \t  -37.454791889002266 \t -0.11554919045977216\n",
            "32     \t [ 5.69244536 -0.1629766 ]. \t  -85.62296261428516 \t -0.11554919045977216\n",
            "33     \t [-9.78148192 -3.11630421]. \t  -1822.0092858813484 \t -0.11554919045977216\n",
            "34     \t [6.47945747 6.3528011 ]. \t  -11052.201520222883 \t -0.11554919045977216\n",
            "35     \t [-1.39576485 -0.00544225]. \t  -9.636339017556331 \t -0.11554919045977216\n",
            "36     \t [-6.02429845 -9.17071284]. \t  -60760.304410798686 \t -0.11554919045977216\n",
            "37     \t [-7.54127851 -2.72185118]. \t  -1072.7339937604422 \t -0.11554919045977216\n",
            "38     \t [ 2.68264592 -0.98901779]. \t  -3.886418079514091 \t -0.11554919045977216\n",
            "39     \t [3.03388896 0.23834055]. \t  -21.192734300871763 \t -0.11554919045977216\n",
            "40     \t [ 5.82415001 -9.03224803]. \t  -49534.30542867251 \t -0.11554919045977216\n",
            "41     \t [ 1.76252312 -0.47910843]. \t  -3.9793185044899944 \t -0.11554919045977216\n",
            "42     \t [ 2.33466359 -8.52038101]. \t  -40819.23845881398 \t -0.11554919045977216\n",
            "43     \t [-0.43466967 -0.17774057]. \t  -2.553992448108702 \t -0.11554919045977216\n",
            "44     \t [9.4631004  1.62675135]. \t  -106.40955030065507 \t -0.11554919045977216\n",
            "45     \t [0.26088445 3.78095731]. \t  -1605.7685070364544 \t -0.11554919045977216\n",
            "46     \t [0.13255652 1.0102979 ]. \t  -8.039853190905582 \t -0.11554919045977216\n",
            "47     \t [1.59606588 0.80179035]. \t  -0.547904392505433 \t -0.11554919045977216\n",
            "48     \t [-0.90797514  0.89588663]. \t  -16.272726213340462 \t -0.11554919045977216\n",
            "49     \t [1.64692545 1.09693155]. \t  -1.572473199451157 \t -0.11554919045977216\n",
            "50     \t [1.36237147 0.68901181]. \t  -0.47228081410991873 \t -0.11554919045977216\n",
            "51     \t [ 1.98610175 -0.25119819]. \t  -7.890857885451686 \t -0.11554919045977216\n",
            "52     \t [-2.21527563 -0.51625863]. \t  -25.444540249624076 \t -0.11554919045977216\n",
            "53     \t [-0.19256959  0.6314155 ]. \t  -3.3821872186628372 \t -0.11554919045977216\n",
            "54     \t [-5.64174875 -3.92160871]. \t  -2694.0005780827005 \t -0.11554919045977216\n",
            "55     \t [1.43987715 0.84271774]. \t  -0.19425480704825995 \t -0.11554919045977216\n",
            "56     \t [4.06194435 6.61264068]. \t  -13917.85656268981 \t -0.11554919045977216\n",
            "57     \t [-2.41865885 -4.25492326]. \t  -2995.8397045771317 \t -0.11554919045977216\n",
            "58     \t [3.64003973 6.97317961]. \t  -17532.794323167178 \t -0.11554919045977216\n",
            "59     \t [ 0.62636285 -0.52987832]. \t  -0.14800819098751414 \t -0.11554919045977216\n",
            "60     \t [7.40763866 5.51959415]. \t  -5770.737914270966 \t -0.11554919045977216\n",
            "61     \t [ 2.3350368  -1.11960563]. \t  -1.8414890070958787 \t -0.11554919045977216\n",
            "62     \t [0.63733264 0.65791808]. \t  -0.23584225442787377 \t -0.11554919045977216\n",
            "63     \t [ 9.80067197 -5.81223533]. \t  -6750.69273000951 \t -0.11554919045977216\n",
            "64     \t [-4.09931234  1.97253809]. \t  -308.3252665364161 \t -0.11554919045977216\n",
            "65     \t [-2.18532263 -9.6048243 ]. \t  -69716.92313625086 \t -0.11554919045977216\n",
            "66     \t [0.40466002 0.28197741]. \t  -0.4751052495294833 \t -0.11554919045977216\n",
            "67     \t [4.08981309 0.7279362 ]. \t  -27.909119138463204 \t -0.11554919045977216\n",
            "68     \t [4.80742649 1.22440502]. \t  -21.042118328385396 \t -0.11554919045977216\n",
            "69     \t [ 3.57649453 -8.66059476]. \t  -42893.23753008888 \t -0.11554919045977216\n",
            "70     \t [-7.74233475  5.74188437]. \t  -10934.15107277863 \t -0.11554919045977216\n",
            "71     \t [1.03697539 0.82026617]. \t  -0.19195585381044677 \t -0.11554919045977216\n",
            "72     \t [-3.98859714  0.66553368]. \t  -72.40696445989835 \t -0.11554919045977216\n",
            "73     \t [7.28700357 7.52424658]. \t  -22486.75882202814 \t -0.11554919045977216\n",
            "74     \t [0.94257086 0.73041907]. \t  \u001b[92m-0.03427529202884841\u001b[0m \t -0.03427529202884841\n",
            "75     \t [ 9.91286349 -2.22542941]. \t  -79.43925695773675 \t -0.03427529202884841\n",
            "76     \t [ 5.40287929 -3.75820845]. \t  -1063.208321874586 \t -0.03427529202884841\n",
            "77     \t [ 5.39885301 -1.01843705]. \t  -41.453510262228875 \t -0.03427529202884841\n",
            "78     \t [ 4.38080036 -1.14466931]. \t  -17.62687475297985 \t -0.03427529202884841\n",
            "79     \t [ 1.46228444 -6.62887934]. \t  -14937.668100017143 \t -0.03427529202884841\n",
            "80     \t [2.01928411 9.20045794]. \t  -55964.60885655887 \t -0.03427529202884841\n",
            "81     \t [ 5.67102364 -3.82147844]. \t  -1129.740002070439 \t -0.03427529202884841\n",
            "82     \t [2.139813  4.7225701]. \t  -3607.940565359596 \t -0.03427529202884841\n",
            "83     \t [ 0.83746726 -0.59958682]. \t  -0.0544817487459128 \t -0.03427529202884841\n",
            "84     \t [-3.2540645  -8.91447672]. \t  -52629.19515382502 \t -0.03427529202884841\n",
            "85     \t [ 4.86723784 -1.70504543]. \t  -16.749608540448087 \t -0.03427529202884841\n",
            "86     \t [-8.69033094  3.93211189]. \t  -3232.3357421883297 \t -0.03427529202884841\n",
            "87     \t [ 6.07419361 -1.5161135 ]. \t  -30.110459256989284 \t -0.03427529202884841\n",
            "88     \t [-1.84270439  9.92074067]. \t  -78959.45602882205 \t -0.03427529202884841\n",
            "89     \t [ 5.35086292 -4.73006518]. \t  -3123.046485571771 \t -0.03427529202884841\n",
            "90     \t [-0.56609623 -1.23158242]. \t  -28.368146674778032 \t -0.03427529202884841\n",
            "91     \t [0.82574087 0.64960229]. \t  \u001b[92m-0.03103057252125801\u001b[0m \t -0.03103057252125801\n",
            "92     \t [ 8.76963296 -9.60728067]. \t  -61892.78078871723 \t -0.03103057252125801\n",
            "93     \t [-2.97216071  7.09633929]. \t  -21518.27341684187 \t -0.03103057252125801\n",
            "94     \t [-1.85301802 -9.63884415]. \t  -70446.43417333013 \t -0.03103057252125801\n",
            "95     \t [-0.17249246  1.00137998]. \t  -10.862247659516049 \t -0.03103057252125801\n",
            "96     \t [-7.06018259  3.18491234]. \t  -1560.7397815242407 \t -0.03103057252125801\n",
            "97     \t [-4.1757566   7.11125473]. \t  -22209.55836836178 \t -0.03103057252125801\n",
            "98     \t [-5.39054826  7.86566842]. \t  -33388.93420504962 \t -0.03103057252125801\n",
            "99     \t [ 5.99666688 -0.69597377]. \t  -75.526395702937 \t -0.03103057252125801\n",
            "100    \t [9.84428029 6.27909718]. \t  -9602.938005260863 \t -0.03103057252125801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NdFRXtPuvsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "500df50f-bb40-45cd-bfcd-69bed06a2435"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_winner_14 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_14 = dGPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [2.95102099 0.14299376]. \t  -20.744156010505872 \t -20.744156010505872\n",
            "init   \t [0.5668276  7.92570408]. \t  -31283.56791285067 \t -20.744156010505872\n",
            "init   \t [3.99982381 4.28594204]. \t  -2152.65365220131 \t -20.744156010505872\n",
            "init   \t [ 4.34676764 -5.54361076]. \t  -6535.783782469756 \t -20.744156010505872\n",
            "init   \t [-6.49690953 -0.86317023]. \t  -183.78911629570374 \t -20.744156010505872\n",
            "1      \t [-10. -10.]. \t  -88321.0 \t -20.744156010505872\n",
            "2      \t [-9.41010567  7.02228954]. \t  -23451.5819466236 \t -20.744156010505872\n",
            "3      \t [8.38403624 9.89223058]. \t  -70238.39460245872 \t -20.744156010505872\n",
            "4      \t [-2.12485184 -9.78521923]. \t  -74991.72959757554 \t -20.744156010505872\n",
            "5      \t [ 8.58953583 -1.38728645]. \t  -102.54399844056245 \t -20.744156010505872\n",
            "6      \t [ 9.69005186 -9.66164402]. \t  -62736.82929961719 \t -20.744156010505872\n",
            "7      \t [-1.39159844 -2.85854778]. \t  -634.72268353115 \t -20.744156010505872\n",
            "8      \t [-2.75635572  2.76322822]. \t  -664.0712531351925 \t -20.744156010505872\n",
            "9      \t [9.62647615 3.68327915]. \t  -687.3791864613818 \t -20.744156010505872\n",
            "10     \t [-9.57779667 -4.25469862]. \t  -4304.0041921551 \t -20.744156010505872\n",
            "11     \t [-4.56391037  9.5532816 ]. \t  -70039.50318832915 \t -20.744156010505872\n",
            "12     \t [-10.           0.93752682]. \t  -397.49703922072456 \t -20.744156010505872\n",
            "13     \t [-6.58718285  2.83002229]. \t  -1079.5586621620394 \t -20.744156010505872\n",
            "14     \t [-5.40845885 -5.80999193]. \t  -10675.858643859383 \t -20.744156010505872\n",
            "15     \t [  3.26653646 -10.        ]. \t  -77413.24854387224 \t -20.744156010505872\n",
            "16     \t [ 0.8829593  -0.32247159]. \t  \u001b[92m-0.9249038322399636\u001b[0m \t -0.9249038322399636\n",
            "17     \t [0.01558116 0.34011125]. \t  -1.0621939903267643 \t -0.9249038322399636\n",
            "18     \t [ 8.83895874 -5.1022578 ]. \t  -3798.615463378062 \t -0.9249038322399636\n",
            "19     \t [ 4.93462122 -0.58925448]. \t  -51.43948921692635 \t -0.9249038322399636\n",
            "20     \t [ 0.50753071 -0.02882959]. \t  \u001b[92m-0.7543317202356656\u001b[0m \t -0.7543317202356656\n",
            "21     \t [ 2.10769036 -1.01173501]. \t  -1.2342923708095817 \t -0.7543317202356656\n",
            "22     \t [6.71931316 2.01849781]. \t  -36.79664669915398 \t -0.7543317202356656\n",
            "23     \t [ 2.1965068 -1.8525049]. \t  -44.994190696060855 \t -0.7543317202356656\n",
            "24     \t [0.61296093 1.5588887 ]. \t  -36.229033368851034 \t -0.7543317202356656\n",
            "25     \t [ 0.11274661 -5.51946044]. \t  -7397.993126440041 \t -0.7543317202356656\n",
            "26     \t [ 1.2472389  -0.60553303]. \t  \u001b[92m-0.5893102104051812\u001b[0m \t -0.5893102104051812\n",
            "27     \t [-3.92766661 -0.84593231]. \t  -81.71686435562106 \t -0.5893102104051812\n",
            "28     \t [-0.93556894  0.27805709]. \t  -6.123501063469769 \t -0.5893102104051812\n",
            "29     \t [0.5283986 0.0404275]. \t  -0.7739305609058844 \t -0.5893102104051812\n",
            "30     \t [7.62347837 0.07621067]. \t  -159.75135928568363 \t -0.5893102104051812\n",
            "31     \t [-4.20361918  4.96659293]. \t  -5759.650410624194 \t -0.5893102104051812\n",
            "32     \t [2.39081742 0.00810567]. \t  -13.365132340896892 \t -0.5893102104051812\n",
            "33     \t [ 3.40480207 -1.0241268 ]. \t  -9.200254218016186 \t -0.5893102104051812\n",
            "34     \t [0.76601658 3.35811344]. \t  -949.4742757050986 \t -0.5893102104051812\n",
            "35     \t [0.01435025 1.06983849]. \t  -11.320558296373072 \t -0.5893102104051812\n",
            "36     \t [ 0.39187111 -0.58076119]. \t  \u001b[92m-0.5296548163272305\u001b[0m \t -0.5296548163272305\n",
            "37     \t [0.50278976 0.98528529]. \t  -4.387419478417842 \t -0.5296548163272305\n",
            "38     \t [1.37136117 1.02204989]. \t  -1.1684138042290408 \t -0.5296548163272305\n",
            "39     \t [1.19362664 3.27434479]. \t  -820.0848116939306 \t -0.5296548163272305\n",
            "40     \t [ 1.69668058 -0.46466337]. \t  -3.685087754591003 \t -0.5296548163272305\n",
            "41     \t [6.71033581 6.17579679]. \t  -9712.743624806299 \t -0.5296548163272305\n",
            "42     \t [ 9.83033304 -0.11402884]. \t  -270.22447244245876 \t -0.5296548163272305\n",
            "43     \t [ 6.30058571 -2.60237028]. \t  -133.04949636637338 \t -0.5296548163272305\n",
            "44     \t [ 1.45304025 -1.47538326]. \t  -17.030712921107778 \t -0.5296548163272305\n",
            "45     \t [-0.45021791 -0.41321916]. \t  -3.356766956889102 \t -0.5296548163272305\n",
            "46     \t [-9.62072898  7.76694621]. \t  -34054.19813525778 \t -0.5296548163272305\n",
            "47     \t [ 2.13604967 -0.88491648]. \t  -1.9401701784238612 \t -0.5296548163272305\n",
            "48     \t [ 3.77285617 -1.8905101 ]. \t  -30.472691179992328 \t -0.5296548163272305\n",
            "49     \t [4.25359511 9.52227214]. \t  -62734.97005593802 \t -0.5296548163272305\n",
            "50     \t [-0.40469897  0.02911919]. \t  -2.303492698167297 \t -0.5296548163272305\n",
            "51     \t [-9.78985189  3.04528552]. \t  -1722.435073549983 \t -0.5296548163272305\n",
            "52     \t [ 0.59657896 -0.87463874]. \t  -1.905245381443566 \t -0.5296548163272305\n",
            "53     \t [1.57681058 0.91742324]. \t  \u001b[92m-0.3554035696668708\u001b[0m \t -0.3554035696668708\n",
            "54     \t [-0.07747108 -0.68013174]. \t  -3.1714755200876468 \t -0.3554035696668708\n",
            "55     \t [3.15341595 1.11949666]. \t  -5.474082933663663 \t -0.3554035696668708\n",
            "56     \t [2.30961752 0.803673  ]. \t  -3.7870821057205486 \t -0.3554035696668708\n",
            "57     \t [ 1.14127837 -0.86980207]. \t  \u001b[92m-0.29647898937252903\u001b[0m \t -0.29647898937252903\n",
            "58     \t [-4.2717033   5.34641832]. \t  -7577.557966132889 \t -0.29647898937252903\n",
            "59     \t [-4.06539427  3.19432283]. \t  -1223.4932107840998 \t -0.29647898937252903\n",
            "60     \t [-6.44961741  0.89599703]. \t  -185.27054250434546 \t -0.29647898937252903\n",
            "61     \t [-4.80405407 -9.43966596]. \t  -67025.33281313878 \t -0.29647898937252903\n",
            "62     \t [4.8658571  0.74186757]. \t  -43.29714011536524 \t -0.29647898937252903\n",
            "63     \t [-8.14075251  2.79073766]. \t  -1208.563170401845 \t -0.29647898937252903\n",
            "64     \t [ 0.74047536 -0.6718284 ]. \t  \u001b[92m-0.11999111469854173\u001b[0m \t -0.11999111469854173\n",
            "65     \t [1.98598711 1.37926054]. \t  -7.58774386904533 \t -0.11999111469854173\n",
            "66     \t [-6.75387576 -3.05541974]. \t  -1352.9894685659553 \t -0.11999111469854173\n",
            "67     \t [-2.72107543  0.1331451 ]. \t  -29.043324982355198 \t -0.11999111469854173\n",
            "68     \t [ 9.95090493 -5.4944436 ]. \t  -5165.866424489724 \t -0.11999111469854173\n",
            "69     \t [-8.34277372 -3.1991881 ]. \t  -1747.5942162311192 \t -0.11999111469854173\n",
            "70     \t [ 1.38161605 -0.71176654]. \t  -0.41705736122133086 \t -0.11999111469854173\n",
            "71     \t [ 0.66987384 -0.62645711]. \t  -0.13544394405799282 \t -0.11999111469854173\n",
            "72     \t [-1.68661541 -0.1437374 ]. \t  -13.189430023387171 \t -0.11999111469854173\n",
            "73     \t [-9.8685891   8.29621224]. \t  -43644.11708412386 \t -0.11999111469854173\n",
            "74     \t [4.12031716 0.18517141]. \t  -42.56957734983289 \t -0.11999111469854173\n",
            "75     \t [-3.28487492 -7.62804324]. \t  -28654.8945934056 \t -0.11999111469854173\n",
            "76     \t [ 8.24492868 -8.27259918]. \t  -33142.230176738776 \t -0.11999111469854173\n",
            "77     \t [ 3.1867796 -9.2083516]. \t  -55383.167089094495 \t -0.11999111469854173\n",
            "78     \t [-8.28717958 -9.40295244]. \t  -68623.7541473987 \t -0.11999111469854173\n",
            "79     \t [ 6.74504647 -8.46425389]. \t  -37320.518206723325 \t -0.11999111469854173\n",
            "80     \t [-5.78481188  3.27956315]. \t  -1536.1638856835025 \t -0.11999111469854173\n",
            "81     \t [-7.29670082  2.55242434]. \t  -895.163642157233 \t -0.11999111469854173\n",
            "82     \t [-5.18300448 -5.94565128]. \t  -11555.15717335925 \t -0.11999111469854173\n",
            "83     \t [-2.35585868 -5.29245568]. \t  -6826.784218325494 \t -0.11999111469854173\n",
            "84     \t [ 6.79908003 -6.61180915]. \t  -13036.977935772078 \t -0.11999111469854173\n",
            "85     \t [ 3.3611301  -7.53643968]. \t  -24308.960123070992 \t -0.11999111469854173\n",
            "86     \t [4.96304012 2.96040326]. \t  -331.4608594387051 \t -0.11999111469854173\n",
            "87     \t [5.31173553 2.19912676]. \t  -56.6204054299652 \t -0.11999111469854173\n",
            "88     \t [8.16694511 2.96724987]. \t  -229.67532676975765 \t -0.11999111469854173\n",
            "89     \t [-0.52126876  7.08258155]. \t  -20342.626382327842 \t -0.11999111469854173\n",
            "90     \t [ 8.15236323 -4.41999009]. \t  -1963.2814553179464 \t -0.11999111469854173\n",
            "91     \t [-6.10918752  7.5731273 ]. \t  -29242.441528680607 \t -0.11999111469854173\n",
            "92     \t [-1.61079141  2.53916057]. \t  -427.6332137066193 \t -0.11999111469854173\n",
            "93     \t [ 4.88716679 -3.65327652]. \t  -966.0864564035858 \t -0.11999111469854173\n",
            "94     \t [ 3.07547527 -1.3070775 ]. \t  -4.5407436475625635 \t -0.11999111469854173\n",
            "95     \t [2.50158433 4.22111784]. \t  -2197.9902345163655 \t -0.11999111469854173\n",
            "96     \t [ 5.84761504 -3.69315877]. \t  -942.0944713662909 \t -0.11999111469854173\n",
            "97     \t [ 6.23828634 -1.61805778]. \t  -29.447909731891453 \t -0.11999111469854173\n",
            "98     \t [ 9.7014008  -1.18928938]. \t  -170.17915159284428 \t -0.11999111469854173\n",
            "99     \t [ 8.99303271 -1.29363549]. \t  -127.64426897721775 \t -0.11999111469854173\n",
            "100    \t [3.42904445 1.08562664]. \t  -8.1980849878459 \t -0.11999111469854173\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d86panpOuvum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1494ba7e-3e38-4464-b690-a519bcba7162"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_winner_15 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_15 = dGPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.55763717 -4.88997339]. \t  -4281.404311682752 \t -3267.0472724202245\n",
            "init   \t [-4.94946261  4.19808709]. \t  -3267.0472724202245 \t -3267.0472724202245\n",
            "init   \t [-1.04895274 -5.46114084]. \t  -7372.467127638806 \t -3267.0472724202245\n",
            "init   \t [-1.96580869  7.6451547 ]. \t  -28265.42637457811 \t -3267.0472724202245\n",
            "init   \t [-1.25573066  7.56850362]. \t  -26833.737231776184 \t -3267.0472724202245\n",
            "1      \t [-9.76533982 -4.68487047]. \t  -5874.976174155925 \t -3267.0472724202245\n",
            "2      \t [8.28335803 6.88744078]. \t  -15048.8154067911 \t -3267.0472724202245\n",
            "3      \t [ 9.8362843  -6.65190316]. \t  -12452.665647642647 \t -3267.0472724202245\n",
            "4      \t [-8.7683881   9.94621396]. \t  -85481.32833161899 \t -3267.0472724202245\n",
            "5      \t [3.6439568  1.63346221]. \t  \u001b[92m-12.719219233371792\u001b[0m \t -12.719219233371792\n",
            "6      \t [ -5.90931827 -10.        ]. \t  -84845.0333801437 \t -12.719219233371792\n",
            "7      \t [8.93552199 0.20048667]. \t  -219.79924073753483 \t -12.719219233371792\n",
            "8      \t [  2.81431851 -10.        ]. \t  -77767.67772264044 \t -12.719219233371792\n",
            "9      \t [-9.95019527  1.16479823]. \t  -440.64563062249374 \t -12.719219233371792\n",
            "10     \t [-5.38183542 -1.19089368]. \t  -175.80843542955807 \t -12.719219233371792\n",
            "11     \t [-0.70799511  0.54701321]. \t  \u001b[92m-6.330828705476208\u001b[0m \t -6.330828705476208\n",
            "12     \t [ 5.90128271 -3.02530477]. \t  -331.72389537591124 \t -6.330828705476208\n",
            "13     \t [3.72040235 9.59245682]. \t  -65030.831284512555 \t -6.330828705476208\n",
            "14     \t [0.25704043 2.82956974]. \t  -497.0480868879133 \t -6.330828705476208\n",
            "15     \t [-1.64474107  0.24852498]. \t  -13.248215739731327 \t -6.330828705476208\n",
            "16     \t [-0.29624273 -0.13664048]. \t  \u001b[92m-1.9028017476955412\u001b[0m \t -1.9028017476955412\n",
            "17     \t [5.70675062 2.91171691]. \t  -275.25330864976627 \t -1.9028017476955412\n",
            "18     \t [-0.62852283  0.17243145]. \t  -3.598741674038115 \t -1.9028017476955412\n",
            "19     \t [ 2.34486745 -0.39358539]. \t  -10.091513538313905 \t -1.9028017476955412\n",
            "20     \t [ 4.5418368  -0.13641344]. \t  -53.12780245507546 \t -1.9028017476955412\n",
            "21     \t [ 0.70044559 -0.85632892]. \t  \u001b[92m-1.2637132327559573\u001b[0m \t -1.2637132327559573\n",
            "22     \t [-8.99212837  4.07774151]. \t  -3669.643163979375 \t -1.2637132327559573\n",
            "23     \t [ 1.18511157 -0.64530194]. \t  \u001b[92m-0.28247203087051836\u001b[0m \t -0.28247203087051836\n",
            "24     \t [3.24250352 4.49238034]. \t  -2760.885721187458 \t -0.28247203087051836\n",
            "25     \t [-5.48073587 -4.59305668]. \t  -4587.444137736713 \t -0.28247203087051836\n",
            "26     \t [2.63865087 1.24998131]. \t  -3.1580657986018545 \t -0.28247203087051836\n",
            "27     \t [ 9.34139514 -2.88519841]. \t  -176.37344287785447 \t -0.28247203087051836\n",
            "28     \t [3.09906289 1.17150039]. \t  -4.6570321287409415 \t -0.28247203087051836\n",
            "29     \t [ 0.31754563 -0.33152858]. \t  -0.48484363007750664 \t -0.28247203087051836\n",
            "30     \t [ 5.62709174 -6.36322755]. \t  -11377.932328955048 \t -0.28247203087051836\n",
            "31     \t [ 1.77231288 -0.66430961]. \t  -2.1795935255634977 \t -0.28247203087051836\n",
            "32     \t [9.94034579 3.01432936]. \t  -215.46199536446963 \t -0.28247203087051836\n",
            "33     \t [-2.46627227  1.2703317 ]. \t  -76.85279283651194 \t -0.28247203087051836\n",
            "34     \t [-0.61675267 -1.21932817]. \t  -28.394038899741517 \t -0.28247203087051836\n",
            "35     \t [-9.9948344  -8.27133975]. \t  -43236.022323985 \t -0.28247203087051836\n",
            "36     \t [ 0.15773305 -0.2597102 ]. \t  -0.7104564240007516 \t -0.28247203087051836\n",
            "37     \t [ 0.07548293 -0.56950331]. \t  -1.5118141254390878 \t -0.28247203087051836\n",
            "38     \t [-2.38932745 -1.54437988]. \t  -114.00573559907444 \t -0.28247203087051836\n",
            "39     \t [2.09879845 0.44825969]. \t  -6.9664666537539865 \t -0.28247203087051836\n",
            "40     \t [ 2.38815579 -1.26345171]. \t  -3.22130319108295 \t -0.28247203087051836\n",
            "41     \t [-8.01213182 -1.42232433]. \t  -372.01623404127486 \t -0.28247203087051836\n",
            "42     \t [ 8.52218378 -9.84249941]. \t  -68674.9668152529 \t -0.28247203087051836\n",
            "43     \t [ 0.91848763 -0.65606032]. \t  \u001b[92m-0.013293005193297873\u001b[0m \t -0.013293005193297873\n",
            "44     \t [ 5.95424587 -0.3960496 ]. \t  -88.17582882496595 \t -0.013293005193297873\n",
            "45     \t [-4.59687493  0.73365113]. \t  -95.69910200973725 \t -0.013293005193297873\n",
            "46     \t [ 2.97032613 -1.44687801]. \t  -6.842347041782533 \t -0.013293005193297873\n",
            "47     \t [ 0.3794039  -0.42480457]. \t  -0.3858229843734759 \t -0.013293005193297873\n",
            "48     \t [6.62649932 9.19341493]. \t  -52786.490281699094 \t -0.013293005193297873\n",
            "49     \t [ 1.09969936 -0.9512198 ]. \t  -1.0179663780971386 \t -0.013293005193297873\n",
            "50     \t [-6.5107799  0.8593455]. \t  -184.01945269076842 \t -0.013293005193297873\n",
            "51     \t [ 0.91774604 -0.55424122]. \t  -0.1908438035179484 \t -0.013293005193297873\n",
            "52     \t [-4.73836531 -2.7767814 ]. \t  -845.7312696634915 \t -0.013293005193297873\n",
            "53     \t [1.26374272 0.38186474]. \t  -1.959522322902396 \t -0.013293005193297873\n",
            "54     \t [-1.5166449  -9.88752666]. \t  -77658.23039318589 \t -0.013293005193297873\n",
            "55     \t [3.97051309 1.06646228]. \t  -14.575623335779952 \t -0.013293005193297873\n",
            "56     \t [2.43116934 5.18720372]. \t  -5282.4727117377015 \t -0.013293005193297873\n",
            "57     \t [-8.41074348 -8.70603717]. \t  -51289.14438183774 \t -0.013293005193297873\n",
            "58     \t [-2.07398088  5.0103825 ]. \t  -5476.232284473293 \t -0.013293005193297873\n",
            "59     \t [-4.38452908 -0.14070639]. \t  -68.13892913132634 \t -0.013293005193297873\n",
            "60     \t [ 9.8845375  -3.76667601]. \t  -762.7809199151517 \t -0.013293005193297873\n",
            "61     \t [ 9.7594397  -1.34951802]. \t  -151.56418774222593 \t -0.013293005193297873\n",
            "62     \t [ 3.64445271 -4.63530097]. \t  -3100.30982698243 \t -0.013293005193297873\n",
            "63     \t [-9.07829232  3.54027274]. \t  -2433.382440195059 \t -0.013293005193297873\n",
            "64     \t [ 2.89586453 -3.25500391]. \t  -672.952472255595 \t -0.013293005193297873\n",
            "65     \t [-5.25129345  2.12510715]. \t  -447.11280209366765 \t -0.013293005193297873\n",
            "66     \t [ 8.35331433 -2.73762228]. \t  -142.13990155243454 \t -0.013293005193297873\n",
            "67     \t [0.29177252 3.76797388]. \t  -1580.112963263459 \t -0.013293005193297873\n",
            "68     \t [0.41907194 1.55406398]. \t  -39.254102583257065 \t -0.013293005193297873\n",
            "69     \t [-1.00101073  1.07403191]. \t  -25.89109270084497 \t -0.013293005193297873\n",
            "70     \t [-3.18739938 -0.01749346]. \t  -37.861147313249504 \t -0.013293005193297873\n",
            "71     \t [-0.09713035 -4.53205644]. \t  -3392.1628502213453 \t -0.013293005193297873\n",
            "72     \t [-8.52083373  2.0108288 ]. \t  -642.2775854060376 \t -0.013293005193297873\n",
            "73     \t [ 2.26559052 -1.72662489]. \t  -28.935511292074477 \t -0.013293005193297873\n",
            "74     \t [-9.32629639  3.54705748]. \t  -2485.687738481979 \t -0.013293005193297873\n",
            "75     \t [ 2.81695214 -2.03831764]. \t  -63.63698645942854 \t -0.013293005193297873\n",
            "76     \t [ 4.01917162 -7.50134124]. \t  -23562.76031559984 \t -0.013293005193297873\n",
            "77     \t [-2.44688676  3.52765592]. \t  -1506.3510075462314 \t -0.013293005193297873\n",
            "78     \t [1.7701565  0.99272264]. \t  -0.6738144249807004 \t -0.013293005193297873\n",
            "79     \t [1.67760986 0.55580812]. \t  -2.7053568584678565 \t -0.013293005193297873\n",
            "80     \t [ 2.74928098 -0.8012842 ]. \t  -7.353419800148547 \t -0.013293005193297873\n",
            "81     \t [ 3.39898761 -1.40720989]. \t  -6.3856875357102965 \t -0.013293005193297873\n",
            "82     \t [2.37740544 1.45046469]. \t  -8.597170244719994 \t -0.013293005193297873\n",
            "83     \t [-0.32753979 -7.9482235 ]. \t  -32095.407427567097 \t -0.013293005193297873\n",
            "84     \t [3.34558955 0.86833311]. \t  -12.255225877529249 \t -0.013293005193297873\n",
            "85     \t [1.92851199 9.24974436]. \t  -57249.36396355148 \t -0.013293005193297873\n",
            "86     \t [-4.91063715  3.2103253 ]. \t  -1337.7841449282078 \t -0.013293005193297873\n",
            "87     \t [-8.26180241 -0.01564831]. \t  -222.31192709882077 \t -0.013293005193297873\n",
            "88     \t [-0.072469   -3.07692734]. \t  -723.7151885814683 \t -0.013293005193297873\n",
            "89     \t [-1.97101032 -4.06030242]. \t  -2450.870425270952 \t -0.013293005193297873\n",
            "90     \t [-2.89804816 -1.01231984]. \t  -64.15288597645016 \t -0.013293005193297873\n",
            "91     \t [6.47232451 0.25390567]. \t  -110.42348666866121 \t -0.013293005193297873\n",
            "92     \t [-4.06431762  0.37047618]. \t  -63.298079846082146 \t -0.013293005193297873\n",
            "93     \t [-9.03410237  6.33762444]. \t  -16072.914496692341 \t -0.013293005193297873\n",
            "94     \t [7.47848886 4.32620845]. \t  -1836.4155354062034 \t -0.013293005193297873\n",
            "95     \t [-2.5427784   0.00949146]. \t  -25.484555399307027 \t -0.013293005193297873\n",
            "96     \t [ 7.43827626 -8.5017821 ]. \t  -37646.51348997539 \t -0.013293005193297873\n",
            "97     \t [5.4040928  1.08001512]. \t  -38.26090964952843 \t -0.013293005193297873\n",
            "98     \t [ 0.9744419  -0.64215307]. \t  -0.045485839241017643 \t -0.013293005193297873\n",
            "99     \t [-4.26426686 -2.52050317]. \t  -603.6837600478647 \t -0.013293005193297873\n",
            "100    \t [-4.95102259 -9.69287804]. \t  -74421.35628980433 \t -0.013293005193297873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "any0xrgYuvxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ad67ee7-7196-468b-eada-d618abbed19b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_winner_16 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_16 = dGPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.27896664 -3.67267997]. \t  -815.4627708918207 \t -28.992408538517264\n",
            "init   \t [3.44301557 0.15823335]. \t  -28.992408538517264 \t -28.992408538517264\n",
            "init   \t [ 5.63320853 -4.32636643]. \t  -2044.16089755496 \t -28.992408538517264\n",
            "init   \t [-5.32582432  1.27599373]. \t  -187.32245577722023 \t -28.992408538517264\n",
            "init   \t [7.50048729 4.37885841]. \t  -1945.4933626881252 \t -28.992408538517264\n",
            "1      \t [-10. -10.]. \t  -88321.0 \t -28.992408538517264\n",
            "2      \t [-0.94408028  9.32615359]. \t  -61182.65342174163 \t -28.992408538517264\n",
            "3      \t [-9.58291173  9.55394704]. \t  -73946.57346618871 \t -28.992408538517264\n",
            "4      \t [-0.59203298 -8.9817774 ]. \t  -52449.51345826705 \t -28.992408538517264\n",
            "5      \t [-5.79185865 -4.56009413]. \t  -4536.00515211046 \t -28.992408538517264\n",
            "6      \t [  7.96507791 -10.        ]. \t  -73803.33491760446 \t -28.992408538517264\n",
            "7      \t [-0.24521386 -2.8607653 ]. \t  -553.5453995096109 \t -28.992408538517264\n",
            "8      \t [-10.          -0.63562548]. \t  -354.62743596678575 \t -28.992408538517264\n",
            "9      \t [1.54287264 4.00172595]. \t  -1858.934501939139 \t -28.992408538517264\n",
            "10     \t [9.93199294 8.89866837]. \t  -44149.00661955901 \t -28.992408538517264\n",
            "11     \t [-9.07651582  3.89167607]. \t  -3201.0262374563513 \t -28.992408538517264\n",
            "12     \t [4.41793994 8.49712815]. \t  -39202.96555783549 \t -28.992408538517264\n",
            "13     \t [-3.13014938  4.48362184]. \t  -3773.05494913333 \t -28.992408538517264\n",
            "14     \t [ 6.970631   -0.05954088]. \t  -132.63023454599448 \t -28.992408538517264\n",
            "15     \t [-1.3637802   0.53897056]. \t  \u001b[92m-13.15162985248233\u001b[0m \t -13.15162985248233\n",
            "16     \t [-9.84340217 -4.82425604]. \t  -6477.310057977559 \t -13.15162985248233\n",
            "17     \t [-5.28048151 -9.39587845]. \t  -66175.04896243762 \t -13.15162985248233\n",
            "18     \t [-3.2387826 -1.1994672]. \t  -92.78371278024855 \t -13.15162985248233\n",
            "19     \t [ 9.80009726 -0.4530983 ]. \t  -253.767174459005 \t -13.15162985248233\n",
            "20     \t [ 3.82037368 -8.10677306]. \t  -32581.257158434615 \t -13.15162985248233\n",
            "21     \t [-5.58595344  7.51017478]. \t  -28076.423841961063 \t -13.15162985248233\n",
            "22     \t [ 1.63660318 -0.31345176]. \t  \u001b[92m-4.553034831889238\u001b[0m \t -4.553034831889238\n",
            "23     \t [ 2.63606259 -1.17735944]. \t  \u001b[92m-2.713849590250892\u001b[0m \t -2.713849590250892\n",
            "24     \t [0.20017665 0.56547066]. \t  \u001b[92m-1.0257522600307503\u001b[0m \t -1.0257522600307503\n",
            "25     \t [ 9.59677179 -5.40648713]. \t  -4849.174932793484 \t -1.0257522600307503\n",
            "26     \t [ 0.03095551 -0.10316497]. \t  \u001b[92m-0.9392342300294892\u001b[0m \t -0.9392342300294892\n",
            "27     \t [-7.4518512  -1.44068434]. \t  -340.6927253870056 \t -0.9392342300294892\n",
            "28     \t [ 2.20764331 -2.7287567 ]. \t  -323.25569280354216 \t -0.9392342300294892\n",
            "29     \t [ 2.28246724 -0.43434055]. \t  -8.90402047610433 \t -0.9392342300294892\n",
            "30     \t [ 3.91836736 -1.31825664]. \t  -8.908951894009828 \t -0.9392342300294892\n",
            "31     \t [ 3.39262659 -1.17470379]. \t  -6.525454196200927 \t -0.9392342300294892\n",
            "32     \t [4.92712691 2.03708852]. \t  -38.16757663631995 \t -0.9392342300294892\n",
            "33     \t [ 0.45009633 -0.10943813]. \t  \u001b[92m-0.6655896279405071\u001b[0m \t -0.6655896279405071\n",
            "34     \t [4.66692272 0.30688142]. \t  -53.56150622486405 \t -0.6655896279405071\n",
            "35     \t [ 2.12311785 -0.83002014]. \t  -2.372191778972521 \t -0.6655896279405071\n",
            "36     \t [0.82024317 0.39381456]. \t  \u001b[92m-0.5526417820785301\u001b[0m \t -0.5526417820785301\n",
            "37     \t [9.98331123 3.0518692 ]. \t  -230.15464001583865 \t -0.5526417820785301\n",
            "38     \t [-0.80034209  1.4197909 ]. \t  -49.936800268480376 \t -0.5526417820785301\n",
            "39     \t [0.68323943 0.44123089]. \t  \u001b[92m-0.27305644178590766\u001b[0m \t -0.27305644178590766\n",
            "40     \t [-2.01832129 -4.82538742]. \t  -4730.512899196404 \t -0.27305644178590766\n",
            "41     \t [-1.13474372 -0.43160206]. \t  -9.10106384862732 \t -0.27305644178590766\n",
            "42     \t [-1.40079456 -0.10387474]. \t  -9.810112813031612 \t -0.27305644178590766\n",
            "43     \t [0.01472024 0.10744131]. \t  -0.9709162214753672 \t -0.27305644178590766\n",
            "44     \t [3.40918956 1.18415265]. \t  -6.535650500633373 \t -0.27305644178590766\n",
            "45     \t [-2.38542659  1.20209129]. \t  -67.12235528828012 \t -0.27305644178590766\n",
            "46     \t [0.82950982 0.01096577]. \t  -1.4044421387685726 \t -0.27305644178590766\n",
            "47     \t [0.75265546 0.67385716]. \t  \u001b[92m-0.10954696660003323\u001b[0m \t -0.10954696660003323\n",
            "48     \t [1.52984226 0.66694884]. \t  -1.1004467975004684 \t -0.10954696660003323\n",
            "49     \t [4.67432013 1.10826181]. \t  -23.338183015419787 \t -0.10954696660003323\n",
            "50     \t [0.54062145 0.69794402]. \t  -0.5870990673779629 \t -0.10954696660003323\n",
            "51     \t [-8.03251586  4.12578886]. \t  -3622.4977333168395 \t -0.10954696660003323\n",
            "52     \t [3.79072506 1.0628854 ]. \t  -12.477748356831995 \t -0.10954696660003323\n",
            "53     \t [-6.21234888 -9.10408776]. \t  -59207.06821301126 \t -0.10954696660003323\n",
            "54     \t [ 4.02919038 -6.52720778]. \t  -13189.460951353916 \t -0.10954696660003323\n",
            "55     \t [ 0.71733069 -0.46317606]. \t  -0.24609716901289924 \t -0.10954696660003323\n",
            "56     \t [ 7.04365201 -4.0924781 ]. \t  -1436.0589353566659 \t -0.10954696660003323\n",
            "57     \t [-5.87599396  3.20777726]. \t  -1447.0835971801562 \t -0.10954696660003323\n",
            "58     \t [ 8.52377976 -9.26055523]. \t  -53189.38260389485 \t -0.10954696660003323\n",
            "59     \t [ 6.69722723 -1.72533586]. \t  -33.56445716067143 \t -0.10954696660003323\n",
            "60     \t [ 8.15388496 -2.02679179]. \t  -51.18572945723824 \t -0.10954696660003323\n",
            "61     \t [ 7.88889833 -2.66404985]. \t  -126.97368496358243 \t -0.10954696660003323\n",
            "62     \t [ 4.49545361 -1.45649364]. \t  -12.345916787527846 \t -0.10954696660003323\n",
            "63     \t [-0.94734154 -2.76240333]. \t  -529.2610645527533 \t -0.10954696660003323\n",
            "64     \t [2.81283766 0.79729967]. \t  -8.038603677751825 \t -0.10954696660003323\n",
            "65     \t [ 8.47139825 -1.67110709]. \t  -72.48209795439618 \t -0.10954696660003323\n",
            "66     \t [ 7.59911051 -1.92789779]. \t  -43.60306036955924 \t -0.10954696660003323\n",
            "67     \t [-2.53598717 -7.62877066]. \t  -28302.273204681238 \t -0.10954696660003323\n",
            "68     \t [ 7.17931782 -0.57421152]. \t  -123.20164106679526 \t -0.10954696660003323\n",
            "69     \t [6.33797836 5.1826123 ]. \t  -4518.405148097174 \t -0.10954696660003323\n",
            "70     \t [ 2.41619727 -0.83707023]. \t  -4.065350701685278 \t -0.10954696660003323\n",
            "71     \t [-0.0710583   7.88508369]. \t  -30961.893643346364 \t -0.10954696660003323\n",
            "72     \t [-1.00200229 -1.00765412]. \t  -22.402987782058112 \t -0.10954696660003323\n",
            "73     \t [ 4.10949713 -1.83279979]. \t  -23.28078275232637 \t -0.10954696660003323\n",
            "74     \t [3.73819659 1.41966354]. \t  -7.66905843336126 \t -0.10954696660003323\n",
            "75     \t [-6.52094594  0.82226348]. \t  -180.5385669132019 \t -0.10954696660003323\n",
            "76     \t [-5.53701779  5.0656976 ]. \t  -6508.76130733897 \t -0.10954696660003323\n",
            "77     \t [5.7634928  1.40118201]. \t  -29.43905215242559 \t -0.10954696660003323\n",
            "78     \t [-0.10241599  0.11671667]. \t  -1.2489452582909175 \t -0.10954696660003323\n",
            "79     \t [ 5.17783276 -1.11985997]. \t  -31.708456144429796 \t -0.10954696660003323\n",
            "80     \t [9.33855769 4.80413045]. \t  -2781.0713822845873 \t -0.10954696660003323\n",
            "81     \t [9.89781807 2.02557847]. \t  -84.89609446702457 \t -0.10954696660003323\n",
            "82     \t [ 6.88349109 -5.5330375 ]. \t  -5941.485893656064 \t -0.10954696660003323\n",
            "83     \t [ 8.16371685 -9.0944609 ]. \t  -49509.41347498307 \t -0.10954696660003323\n",
            "84     \t [ 2.52627823 -3.2849107 ]. \t  -728.5154544669196 \t -0.10954696660003323\n",
            "85     \t [-3.52935148 -6.33099623]. \t  -14029.343310965036 \t -0.10954696660003323\n",
            "86     \t [ 9.41192064 -2.15264295]. \t  -70.8019830075267 \t -0.10954696660003323\n",
            "87     \t [ 1.52087332 -1.74486749]. \t  -42.00915806357894 \t -0.10954696660003323\n",
            "88     \t [-8.10679451 -3.97384878]. \t  -3233.485078639014 \t -0.10954696660003323\n",
            "89     \t [0.61124987 4.40812793]. \t  -2926.5720044905806 \t -0.10954696660003323\n",
            "90     \t [ 1.33338474 -6.7834835 ]. \t  -16452.33364704236 \t -0.10954696660003323\n",
            "91     \t [5.10551579 7.71280412]. \t  -25949.235124425155 \t -0.10954696660003323\n",
            "92     \t [-0.34321087 -7.74449847]. \t  -28944.888935127223 \t -0.10954696660003323\n",
            "93     \t [-7.69801172 -6.19002446]. \t  -14299.028906402948 \t -0.10954696660003323\n",
            "94     \t [-4.58912948  6.6451055 ]. \t  -17293.53875567509 \t -0.10954696660003323\n",
            "95     \t [2.81436951 1.12710952]. \t  -3.4416700738560446 \t -0.10954696660003323\n",
            "96     \t [ 3.00188445 -1.29398595]. \t  -4.248241124859602 \t -0.10954696660003323\n",
            "97     \t [1.96532329 0.94996542]. \t  -0.9833404761362226 \t -0.10954696660003323\n",
            "98     \t [5.05108621 5.33718012]. \t  -5407.76273916 \t -0.10954696660003323\n",
            "99     \t [-7.08194145  8.05431634]. \t  -37508.00702857726 \t -0.10954696660003323\n",
            "100    \t [-6.40214562  9.5152672 ]. \t  -70354.36649607628 \t -0.10954696660003323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reLyKt6Quvzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e614e588-ab05-4d04-e59c-c4c1ee1632c0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_winner_17 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_17 = dGPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [3.03648056 4.61039249]. \t  -3120.691750998894 \t -133.8839693070206\n",
            "init   \t [-6.13265275  0.39354476]. \t  -133.8839693070206 \t -133.8839693070206\n",
            "init   \t [3.52766533 6.08889746]. \t  -9981.232217710998 \t -133.8839693070206\n",
            "init   \t [0.67025729 8.16068359]. \t  -35124.935571397844 \t -133.8839693070206\n",
            "init   \t [-1.13037742 -4.8805558 ]. \t  -4761.569630397088 \t -133.8839693070206\n",
            "1      \t [-10. -10.]. \t  -88321.0 \t -133.8839693070206\n",
            "2      \t [ 8.81602768 -5.83496092]. \t  -7088.736716131159 \t -133.8839693070206\n",
            "3      \t [-9.20794452  7.56905046]. \t  -30751.632331903027 \t -133.8839693070206\n",
            "4      \t [9.38422178 1.56611414]. \t  \u001b[92m-110.41438012079544\u001b[0m \t -110.41438012079544\n",
            "5      \t [9.65812984 9.94541442]. \t  -70886.65075415972 \t -110.41438012079544\n",
            "6      \t [  3.47186906 -10.        ]. \t  -77252.72263625673 \t -110.41438012079544\n",
            "7      \t [-10.          -3.40609209]. \t  -2325.8686709968674 \t -110.41438012079544\n",
            "8      \t [ 4.40164635 -1.09432882]. \t  \u001b[92m-19.623565232231844\u001b[0m \t -19.623565232231844\n",
            "9      \t [ -3.66266664 -10.        ]. \t  -82978.70403007531 \t -19.623565232231844\n",
            "10     \t [-0.48952655  0.82299161]. \t  \u001b[92m-9.02051887654278\u001b[0m \t -9.02051887654278\n",
            "11     \t [ 4.11034631 -4.77421311]. \t  -3450.170712605144 \t -9.02051887654278\n",
            "12     \t [-3.60023019  4.03284354]. \t  -2631.6101602627455 \t -9.02051887654278\n",
            "13     \t [-10.           2.02536711]. \t  -783.7875332521941 \t -9.02051887654278\n",
            "14     \t [-5.84426957 -4.76713793]. \t  -5309.296080010843 \t -9.02051887654278\n",
            "15     \t [5.87325742 1.33501319]. \t  -34.409171014982014 \t -9.02051887654278\n",
            "16     \t [2.10964749 0.26706144]. \t  \u001b[92m-8.969525986215302\u001b[0m \t -8.969525986215302\n",
            "17     \t [8.94935273 4.74793873]. \t  -2674.8842545175467 \t -8.969525986215302\n",
            "18     \t [-2.78033159 -0.61035296]. \t  -39.14769935954958 \t -8.969525986215302\n",
            "19     \t [ 6.97143126 -1.50528316]. \t  -47.562034070111636 \t -8.969525986215302\n",
            "20     \t [-4.31603945  9.03343694]. \t  -56155.504913194534 \t -8.969525986215302\n",
            "21     \t [0.07184013 2.71879997]. \t  -433.74186423537907 \t -8.969525986215302\n",
            "22     \t [-1.08936959  0.30659643]. \t  \u001b[92m-7.628825424209454\u001b[0m \t -7.628825424209454\n",
            "23     \t [-0.66912713  0.49402902]. \t  \u001b[92m-5.464470517086421\u001b[0m \t -5.464470517086421\n",
            "24     \t [-0.99388893  0.67332362]. \t  -11.200293006089183 \t -5.464470517086421\n",
            "25     \t [ 1.21904659 -0.62562383]. \t  \u001b[92m-0.4285855229412167\u001b[0m \t -0.4285855229412167\n",
            "26     \t [0.23241781 0.49599077]. \t  -0.7239624577660823 \t -0.4285855229412167\n",
            "27     \t [-1.22174322 -0.91675362]. \t  -21.786521081145576 \t -0.4285855229412167\n",
            "28     \t [ 1.60553295 -1.29162804]. \t  -6.35989790974607 \t -0.4285855229412167\n",
            "29     \t [  8.47261654 -10.        ]. \t  -73421.31722952986 \t -0.4285855229412167\n",
            "30     \t [ 0.78085351 -1.11597829]. \t  -5.89596239273167 \t -0.4285855229412167\n",
            "31     \t [-7.2405433  3.6542144]. \t  -2372.7189854361277 \t -0.4285855229412167\n",
            "32     \t [ 9.14025457 -1.48686197]. \t  -110.79671194174874 \t -0.4285855229412167\n",
            "33     \t [4.4425982  0.21561349]. \t  -49.68986783908259 \t -0.4285855229412167\n",
            "34     \t [4.53817702 9.84643969]. \t  -71731.91582202556 \t -0.4285855229412167\n",
            "35     \t [3.37773423 1.82837463]. \t  -27.5416416202328 \t -0.4285855229412167\n",
            "36     \t [ 1.92822454 -0.13695529]. \t  -8.011177273607737 \t -0.4285855229412167\n",
            "37     \t [ 3.19269616 -1.17287301]. \t  -5.1976443590780255 \t -0.4285855229412167\n",
            "38     \t [0.87398899 0.43288011]. \t  -0.5143172272325615 \t -0.4285855229412167\n",
            "39     \t [-4.32521197 -1.4076729 ]. \t  -165.74964857457095 \t -0.4285855229412167\n",
            "40     \t [ 1.13834316 -0.01145227]. \t  -2.6095948718678277 \t -0.4285855229412167\n",
            "41     \t [-2.13032677  0.0429478 ]. \t  -18.906992501480126 \t -0.4285855229412167\n",
            "42     \t [7.52748488 0.64187579]. \t  -132.4812346999585 \t -0.4285855229412167\n",
            "43     \t [ 2.82522772 -1.0140684 ]. \t  -4.512819869158035 \t -0.4285855229412167\n",
            "44     \t [2.37867083 1.27196326]. \t  -3.3700091519790423 \t -0.4285855229412167\n",
            "45     \t [ 1.76595693 -3.04971269]. \t  -567.4573711313951 \t -0.4285855229412167\n",
            "46     \t [1.76785012 0.67465846]. \t  -2.0602819143244298 \t -0.4285855229412167\n",
            "47     \t [5.48982849 2.01291751]. \t  -33.82293489953708 \t -0.4285855229412167\n",
            "48     \t [0.97245948 0.39402661]. \t  -0.8771022960713682 \t -0.4285855229412167\n",
            "49     \t [ 1.84277394 -0.99182988]. \t  -0.7413576560665873 \t -0.4285855229412167\n",
            "50     \t [ 1.6603376  -1.14892722]. \t  -2.3557872097884056 \t -0.4285855229412167\n",
            "51     \t [8.46779831 7.12961414]. \t  -17426.383009816836 \t -0.4285855229412167\n",
            "52     \t [-8.87122825 -6.11949031]. \t  -14131.446577500716 \t -0.4285855229412167\n",
            "53     \t [-7.01007846 -1.14467003]. \t  -249.65894016951592 \t -0.4285855229412167\n",
            "54     \t [1.14493633 0.64073547]. \t  \u001b[92m-0.23076734911195318\u001b[0m \t -0.23076734911195318\n",
            "55     \t [6.33522202 2.62135662]. \t  -138.2155666086074 \t -0.23076734911195318\n",
            "56     \t [ 1.92185207 -6.41993558]. \t  -12964.342570814926 \t -0.23076734911195318\n",
            "57     \t [ 2.06816624 -1.19286077]. \t  -2.3505122608645435 \t -0.23076734911195318\n",
            "58     \t [5.21559652 1.3983291 ]. \t  -21.177032512686978 \t -0.23076734911195318\n",
            "59     \t [ 0.03828497 -4.77979984]. \t  -4169.625990517897 \t -0.23076734911195318\n",
            "60     \t [-5.50634066  4.88851737]. \t  -5724.441837423282 \t -0.23076734911195318\n",
            "61     \t [8.76679784 4.29391193]. \t  -1640.5056011511847 \t -0.23076734911195318\n",
            "62     \t [-8.98890867  0.60653298]. \t  -288.9168339602197 \t -0.23076734911195318\n",
            "63     \t [1.92514088 1.03845944]. \t  -0.9632138562469709 \t -0.23076734911195318\n",
            "64     \t [ 2.45485905 -8.43743202]. \t  -39160.49674376253 \t -0.23076734911195318\n",
            "65     \t [-3.55615229  0.65539769]. \t  -59.747292713954124 \t -0.23076734911195318\n",
            "66     \t [-1.85054602 -0.57104   ]. \t  -20.65282131045403 \t -0.23076734911195318\n",
            "67     \t [0.31687549 0.60476593]. \t  -0.8104589458355942 \t -0.23076734911195318\n",
            "68     \t [-9.0776245  -0.18304074]. \t  -268.8071160142377 \t -0.23076734911195318\n",
            "69     \t [ 0.86268324 -0.20039651]. \t  -1.2430481436937413 \t -0.23076734911195318\n",
            "70     \t [-0.182838    5.53966783]. \t  -7580.3405946686125 \t -0.23076734911195318\n",
            "71     \t [1.25853039 7.04421599]. \t  -19201.570925244465 \t -0.23076734911195318\n",
            "72     \t [8.06193148 8.60590626]. \t  -39284.21404271839 \t -0.23076734911195318\n",
            "73     \t [8.18071397 2.5580906 ]. \t  -99.71879345198715 \t -0.23076734911195318\n",
            "74     \t [-3.02927854 -1.38291775]. \t  -110.19524358375693 \t -0.23076734911195318\n",
            "75     \t [7.71939719 1.3140332 ]. \t  -81.54833457870807 \t -0.23076734911195318\n",
            "76     \t [-9.21247784  5.10503346]. \t  -7628.3109577788855 \t -0.23076734911195318\n",
            "77     \t [-5.41577237  1.00350708]. \t  -151.56674301338865 \t -0.23076734911195318\n",
            "78     \t [-4.97692702  8.28742049]. \t  -40556.84917917206 \t -0.23076734911195318\n",
            "79     \t [9.37697898 0.9479332 ]. \t  -185.08124840416218 \t -0.23076734911195318\n",
            "80     \t [4.92905531 1.29978043]. \t  -20.243697047984963 \t -0.23076734911195318\n",
            "81     \t [1.55605664 5.36534893]. \t  -6276.315307365103 \t -0.23076734911195318\n",
            "82     \t [ 2.15702298 -1.01548418]. \t  -1.3566030334491161 \t -0.23076734911195318\n",
            "83     \t [9.63573364 2.38494157]. \t  -80.63220154824702 \t -0.23076734911195318\n",
            "84     \t [4.83509331 2.71011485]. \t  -208.92443335302386 \t -0.23076734911195318\n",
            "85     \t [ 0.41241383 -0.75033658]. \t  -1.3636963983763384 \t -0.23076734911195318\n",
            "86     \t [3.65612439 0.84169355]. \t  -17.08328371258108 \t -0.23076734911195318\n",
            "87     \t [2.37416828 2.43785355]. \t  -182.84811143666028 \t -0.23076734911195318\n",
            "88     \t [ 0.07728894 -0.57940525]. \t  -1.557381264160874 \t -0.23076734911195318\n",
            "89     \t [2.94132903 0.99654855]. \t  -5.593232474913366 \t -0.23076734911195318\n",
            "90     \t [2.2903605  1.00965938]. \t  -1.7915713285785562 \t -0.23076734911195318\n",
            "91     \t [ 6.96562817 -2.40903003]. \t  -78.6706257254646 \t -0.23076734911195318\n",
            "92     \t [-4.91703696 -6.17035295]. \t  -13177.608925696788 \t -0.23076734911195318\n",
            "93     \t [0.78199885 1.18467404]. \t  -8.24801580874012 \t -0.23076734911195318\n",
            "94     \t [3.32960703 1.51822677]. \t  -8.706009480528564 \t -0.23076734911195318\n",
            "95     \t [ 3.71376938 -3.98894139]. \t  -1587.6567265140147 \t -0.23076734911195318\n",
            "96     \t [0.92084033 0.80901701]. \t  -0.3076285578771469 \t -0.23076734911195318\n",
            "97     \t [ 3.50179272 -1.61930937]. \t  -12.331809060118754 \t -0.23076734911195318\n",
            "98     \t [ 3.63808948 -2.08005183]. \t  -57.26280967866541 \t -0.23076734911195318\n",
            "99     \t [0.57308471 0.50838026]. \t  \u001b[92m-0.18856988701464567\u001b[0m \t -0.18856988701464567\n",
            "100    \t [ 5.6383431  -1.17743875]. \t  -37.93777214261284 \t -0.18856988701464567\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI3FtfYSuv2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7ae00b-3220-407b-a111-546c05da4fb8"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_winner_18 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_18 = dGPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [7.46858806 9.37081326]. \t  -56594.51755971677 \t -0.3114572247523004\n",
            "init   \t [7.3838908  0.61711383]. \t  -128.46197097057788 \t -0.3114572247523004\n",
            "init   \t [-5.34543344 -9.77202391]. \t  -77131.46635795479 \t -0.3114572247523004\n",
            "init   \t [-1.39062363 -1.9529728 ]. \t  -168.39363959555394 \t -0.3114572247523004\n",
            "init   \t [ 0.45349343 -0.43216408]. \t  -0.3114572247523004 \t -0.3114572247523004\n",
            "1      \t [1.13550865 0.60170966]. \t  -0.3568618976266401 \t -0.3114572247523004\n",
            "2      \t [ 1.67165671 -0.12390767]. \t  -5.836559747718036 \t -0.3114572247523004\n",
            "3      \t [0.12526212 2.11923098]. \t  -157.65869365560948 \t -0.3114572247523004\n",
            "4      \t [-9.29644966  8.17908998]. \t  -41056.3576701472 \t -0.3114572247523004\n",
            "5      \t [0.92511104 0.01836694]. \t  -1.7147734873951832 \t -0.3114572247523004\n",
            "6      \t [1.66604697 1.20804969]. \t  -3.5822391262536306 \t -0.3114572247523004\n",
            "7      \t [1.36201303 0.34927444]. \t  -2.6310255789882553 \t -0.3114572247523004\n",
            "8      \t [ 2.16404808 -1.13866395]. \t  -1.7231982027180828 \t -0.3114572247523004\n",
            "9      \t [ 2.7889811  -0.20032134]. \t  -17.87482291105457 \t -0.3114572247523004\n",
            "10     \t [ 7.41163264 -8.66839759]. \t  -40865.12634336282 \t -0.3114572247523004\n",
            "11     \t [-8.8857121  -1.85868522]. \t  -596.6997945649014 \t -0.3114572247523004\n",
            "12     \t [-2.03424237  8.23443927]. \t  -37902.17174791396 \t -0.3114572247523004\n",
            "13     \t [ 0.98053047 -7.6614379 ]. \t  -27104.77956195355 \t -0.3114572247523004\n",
            "14     \t [-5.72603444  2.90213917]. \t  -1064.1267962750474 \t -0.3114572247523004\n",
            "15     \t [3.06988674 5.66017051]. \t  -7447.546854428891 \t -0.3114572247523004\n",
            "16     \t [ 1.04826073 -1.07623433]. \t  -3.2194985240956266 \t -0.3114572247523004\n",
            "17     \t [ 6.08054609 -3.52851595]. \t  -734.2196003122394 \t -0.3114572247523004\n",
            "18     \t [-10.          -7.26630587]. \t  -26846.98112047119 \t -0.3114572247523004\n",
            "19     \t [-5.09221281 -4.08445861]. \t  -2995.1224585671707 \t -0.3114572247523004\n",
            "20     \t [ 0.01659496 -0.14973594]. \t  -0.9686812212706989 \t -0.3114572247523004\n",
            "21     \t [0.58763415 0.08320862]. \t  -0.8285081806874215 \t -0.3114572247523004\n",
            "22     \t [8.57511704 3.54654007]. \t  -607.2266536294765 \t -0.3114572247523004\n",
            "23     \t [-10.           2.60145234]. \t  -1228.802665880718 \t -0.3114572247523004\n",
            "24     \t [ 9.59028489 -4.45334997]. \t  -1882.728088645827 \t -0.3114572247523004\n",
            "25     \t [-3.36138532  0.19614902]. \t  -42.665966122694144 \t -0.3114572247523004\n",
            "26     \t [ 1.96426186 -3.62713918]. \t  -1186.5815140569784 \t -0.3114572247523004\n",
            "27     \t [5.06662515 1.11222961]. \t  -29.979715855126713 \t -0.3114572247523004\n",
            "28     \t [2.97195274 9.26045351]. \t  -56815.38267029214 \t -0.3114572247523004\n",
            "29     \t [ 1.83926137 -0.8817943 ]. \t  -0.8658295747543601 \t -0.3114572247523004\n",
            "30     \t [-5.37682421e-01 -3.09277148e-04]. \t  -2.9426724109848945 \t -0.3114572247523004\n",
            "31     \t [ 1.87681674 -0.627249  ]. \t  -3.144720360987095 \t -0.3114572247523004\n",
            "32     \t [-5.62596508 -0.55946769]. \t  -122.07775260294139 \t -0.3114572247523004\n",
            "33     \t [ 0.83978172 -0.30290454]. \t  -0.8870751760697538 \t -0.3114572247523004\n",
            "34     \t [ 9.93123557 -1.03679448]. \t  -200.86578315975765 \t -0.3114572247523004\n",
            "35     \t [3.99220091 1.47233143]. \t  -9.189001855955105 \t -0.3114572247523004\n",
            "36     \t [2.23443811 1.1396289 ]. \t  -1.7874770199350236 \t -0.3114572247523004\n",
            "37     \t [2.80554862 1.31722499]. \t  -4.143431307457025 \t -0.3114572247523004\n",
            "38     \t [ 3.52379904 -1.39673085]. \t  -6.655201258341222 \t -0.3114572247523004\n",
            "39     \t [-1.49813453  4.29546865]. \t  -2955.3969766540035 \t -0.3114572247523004\n",
            "40     \t [5.62425562 2.15732446]. \t  -48.52512370969774 \t -0.3114572247523004\n",
            "41     \t [0.54083633 0.53164756]. \t  \u001b[92m-0.21202804910567885\u001b[0m \t -0.21202804910567885\n",
            "42     \t [2.40945614 2.03170028]. \t  -70.34164411051712 \t -0.21202804910567885\n",
            "43     \t [-5.58391301  5.62330512]. \t  -9517.669097437969 \t -0.21202804910567885\n",
            "44     \t [3.09613265 0.45041759]. \t  -18.87006804082171 \t -0.21202804910567885\n",
            "45     \t [-1.39776658  0.42057798]. \t  -11.885057027403253 \t -0.21202804910567885\n",
            "46     \t [ 4.56340341 -3.89517175]. \t  -1342.0510182032006 \t -0.21202804910567885\n",
            "47     \t [4.62723449 5.899278  ]. \t  -8456.847142327448 \t -0.21202804910567885\n",
            "48     \t [-2.27532918 -5.53989921]. \t  -8114.975737161444 \t -0.21202804910567885\n",
            "49     \t [5.40625162 2.25062429]. \t  -64.05435489191615 \t -0.21202804910567885\n",
            "50     \t [ 3.92571954 -1.22337911]. \t  -10.2985990501231 \t -0.21202804910567885\n",
            "51     \t [ 3.26917884 -1.15849686]. \t  -5.833502986754204 \t -0.21202804910567885\n",
            "52     \t [ 0.82287258 -0.61404287]. \t  \u001b[92m-0.04083420247943442\u001b[0m \t -0.04083420247943442\n",
            "53     \t [2.04372944 0.86848932]. \t  -1.6622107728372302 \t -0.04083420247943442\n",
            "54     \t [0.24495725 0.31633115]. \t  -0.5741083784610287 \t -0.04083420247943442\n",
            "55     \t [2.50544651 0.48761377]. \t  -10.507455765258028 \t -0.04083420247943442\n",
            "56     \t [-4.74828467 -2.63371785]. \t  -726.542753551181 \t -0.04083420247943442\n",
            "57     \t [ 5.42294888 -0.66929407]. \t  -60.55065485266968 \t -0.04083420247943442\n",
            "58     \t [1.09198943 9.34927193]. \t  -60361.30298145525 \t -0.04083420247943442\n",
            "59     \t [-3.0981333  -0.89090892]. \t  -60.703842337642286 \t -0.04083420247943442\n",
            "60     \t [ 3.2337545  -3.46410672]. \t  -867.4694419582441 \t -0.04083420247943442\n",
            "61     \t [ 9.37102078 -4.5953657 ]. \t  -2230.126257753908 \t -0.04083420247943442\n",
            "62     \t [-4.2259071 -1.20745  ]. \t  -129.32009368261657 \t -0.04083420247943442\n",
            "63     \t [-1.70201168  0.03086872]. \t  -13.1075363768011 \t -0.04083420247943442\n",
            "64     \t [ 2.91741373 -6.11095378]. \t  -10305.570540281722 \t -0.04083420247943442\n",
            "65     \t [-6.47398976 -8.29725134]. \t  -41621.6570495819 \t -0.04083420247943442\n",
            "66     \t [ 5.80697974 -3.99152321]. \t  -1381.0973566289983 \t -0.04083420247943442\n",
            "67     \t [ 7.06007118 -3.00333298]. \t  -277.842927159919 \t -0.04083420247943442\n",
            "68     \t [ 5.47593125 -1.90463899]. \t  -26.366262053400884 \t -0.04083420247943442\n",
            "69     \t [-6.18178689  6.26894534]. \t  -14427.260880981674 \t -0.04083420247943442\n",
            "70     \t [-9.40595051 -9.81454609]. \t  -81762.00729734289 \t -0.04083420247943442\n",
            "71     \t [-8.1245724  -0.05300373]. \t  -215.4578390440497 \t -0.04083420247943442\n",
            "72     \t [-4.11978008  9.49501985]. \t  -68055.49271965375 \t -0.04083420247943442\n",
            "73     \t [ 2.10945507 -0.96475045]. \t  -1.3538670190258737 \t -0.04083420247943442\n",
            "74     \t [-3.84115924 -5.4095626 ]. \t  -7802.945286291192 \t -0.04083420247943442\n",
            "75     \t [1.61856633 4.40196963]. \t  -2758.5635071437023 \t -0.04083420247943442\n",
            "76     \t [8.15566945 8.31233999]. \t  -33869.05380235113 \t -0.04083420247943442\n",
            "77     \t [ 0.56975078 -9.60594027]. \t  -67696.30772250493 \t -0.04083420247943442\n",
            "78     \t [-4.30895206  9.99717508]. \t  -83420.17417554371 \t -0.04083420247943442\n",
            "79     \t [ 5.86573475 -2.14580764]. \t  -46.029963320916195 \t -0.04083420247943442\n",
            "80     \t [-0.05327668  1.84279366]. \t  -94.81891040193774 \t -0.04083420247943442\n",
            "81     \t [-0.58991652  7.26217391]. \t  -22503.477832359345 \t -0.04083420247943442\n",
            "82     \t [ 6.20097963 -8.15946825]. \t  -32261.109856807794 \t -0.04083420247943442\n",
            "83     \t [0.18833227 9.67605813]. \t  -69986.44152893897 \t -0.04083420247943442\n",
            "84     \t [5.89672057 3.89720334]. \t  -1222.4860890315838 \t -0.04083420247943442\n",
            "85     \t [4.42364716 2.04501333]. \t  -42.77662797788825 \t -0.04083420247943442\n",
            "86     \t [ 2.98452481 -2.57364942]. \t  -214.5891996912539 \t -0.04083420247943442\n",
            "87     \t [ 2.80501    -1.38171919]. \t  -5.311557554916068 \t -0.04083420247943442\n",
            "88     \t [0.83684724 0.83076622]. \t  -0.6173984752961477 \t -0.04083420247943442\n",
            "89     \t [-2.56543486 -7.77172208]. \t  -30450.44183915338 \t -0.04083420247943442\n",
            "90     \t [-9.23238755 -6.67352048]. \t  -19432.10910463239 \t -0.04083420247943442\n",
            "91     \t [-1.41444092 -3.81254297]. \t  -1864.549814117909 \t -0.04083420247943442\n",
            "92     \t [5.19677773 1.79513438]. \t  -20.729135131722362 \t -0.04083420247943442\n",
            "93     \t [-4.25079556 -3.48029923]. \t  -1649.3087842062514 \t -0.04083420247943442\n",
            "94     \t [7.72771931 8.68302332]. \t  -40978.77317930908 \t -0.04083420247943442\n",
            "95     \t [-1.46501658 -2.1326753 ]. \t  -229.1721286101595 \t -0.04083420247943442\n",
            "96     \t [ 7.96616065 -5.40487512]. \t  -5140.787007532487 \t -0.04083420247943442\n",
            "97     \t [-2.85345816 -9.83264323]. \t  -77015.66153958983 \t -0.04083420247943442\n",
            "98     \t [-4.75559923  8.75778337]. \t  -50057.95735549469 \t -0.04083420247943442\n",
            "99     \t [ 9.77428042 -6.17916373]. \t  -8945.403788429308 \t -0.04083420247943442\n",
            "100    \t [-0.98210894  5.46957316]. \t  -7400.7526049408825 \t -0.04083420247943442\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YT-CgKvuv4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ed0d1d-1fee-4fa7-da2d-dc13f2c6b1e9"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_winner_19 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_19 = dGPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.18891153 5.22983981]. \t  -5727.433065247983 \t -79.36780179787098\n",
            "init   \t [-5.10936891  5.43378939]. \t  -8270.721382692287 \t -79.36780179787098\n",
            "init   \t [-2.91055837  5.93394799]. \t  -10771.054116716396 \t -79.36780179787098\n",
            "init   \t [-3.07999955 -1.12251129]. \t  -79.36780179787098 \t -79.36780179787098\n",
            "init   \t [-3.93392821 -1.1577216 ]. \t  -111.8486364354089 \t -79.36780179787098\n",
            "1      \t [ 1.46418724 -8.68671967]. \t  -44673.21769976243 \t -79.36780179787098\n",
            "2      \t [ 9.77192101 -1.83559751]. \t  -95.34580222090166 \t -79.36780179787098\n",
            "3      \t [-10.          -9.90826103]. \t  -85279.39447828391 \t -79.36780179787098\n",
            "4      \t [9.60900479 6.31645299]. \t  -9926.306792942905 \t -79.36780179787098\n",
            "5      \t [ 3.64268514 -1.21478072]. \t  \u001b[92m-7.939578135638334\u001b[0m \t -7.939578135638334\n",
            "6      \t [-9.8876954  -2.46578762]. \t  -1090.762809634548 \t -7.939578135638334\n",
            "7      \t [ 8.31994416 -7.65544864]. \t  -23768.448806121734 \t -7.939578135638334\n",
            "8      \t [-4.83750344 -6.21023539]. \t  -13472.746706872147 \t -7.939578135638334\n",
            "9      \t [6.43163373 1.66859041]. \t  -30.993031708523233 \t -7.939578135638334\n",
            "10     \t [-10.           2.79292422]. \t  -1431.807180589236 \t -7.939578135638334\n",
            "11     \t [-9.75067889  9.53644709]. \t  -73566.07228821774 \t -7.939578135638334\n",
            "12     \t [ 0.30972536 -3.12049945]. \t  -735.0945571588179 \t -7.939578135638334\n",
            "13     \t [4.78533373 8.98851618]. \t  -49187.75733270886 \t -7.939578135638334\n",
            "14     \t [ 5.75202169 -3.49672209]. \t  -722.1194772732323 \t -7.939578135638334\n",
            "15     \t [1.74129892 0.73635577]. \t  \u001b[92m-1.4124523371347346\u001b[0m \t -1.4124523371347346\n",
            "16     \t [2.5723173  0.92539981]. \t  -3.9499637311384097 \t -1.4124523371347346\n",
            "17     \t [9.70250737 1.09497334]. \t  -182.44724067822602 \t -1.4124523371347346\n",
            "18     \t [3.28533039e-03 9.54204293e+00]. \t  -66320.270226604 \t -1.4124523371347346\n",
            "19     \t [-0.3606339   0.92124268]. \t  -10.322135303728457 \t -1.4124523371347346\n",
            "20     \t [ 2.17763156 -0.01097217]. \t  -10.868877377261253 \t -1.4124523371347346\n",
            "21     \t [-6.67891389 -0.34266605]. \t  -154.56570535547192 \t -1.4124523371347346\n",
            "22     \t [-2.63707432  1.56102017]. \t  -126.04780451529274 \t -1.4124523371347346\n",
            "23     \t [5.15188428 4.33168839]. \t  -2113.540674448819 \t -1.4124523371347346\n",
            "24     \t [-2.86495331 -9.6185128 ]. \t  -70625.14993050326 \t -1.4124523371347346\n",
            "25     \t [ 6.75011944 -0.63882227]. \t  -103.4869635145468 \t -1.4124523371347346\n",
            "26     \t [-5.15925848  9.75131885]. \t  -76350.00555886983 \t -1.4124523371347346\n",
            "27     \t [ 4.62212744 -0.25358159]. \t  -53.503254721084026 \t -1.4124523371347346\n",
            "28     \t [1.79906814 1.07747066]. \t  \u001b[92m-1.1851870512626839\u001b[0m \t -1.1851870512626839\n",
            "29     \t [ 0.11916842 -0.0354528 ]. \t  \u001b[92m-0.8030808728079766\u001b[0m \t -0.8030808728079766\n",
            "30     \t [-9.57586417 -5.60069561]. \t  -10569.739604816432 \t -0.8030808728079766\n",
            "31     \t [0.36810082 0.22589496]. \t  \u001b[92m-0.5408551345169831\u001b[0m \t -0.5408551345169831\n",
            "32     \t [ 2.71055971 -0.77001229]. \t  -7.57556791402914 \t -0.5408551345169831\n",
            "33     \t [0.88918939 0.93632397]. \t  -1.5060167632498347 \t -0.5408551345169831\n",
            "34     \t [ 2.79740563 -1.63587076]. \t  -16.28406693367883 \t -0.5408551345169831\n",
            "35     \t [ 2.99903421 -0.31207157]. \t  -19.723850948687268 \t -0.5408551345169831\n",
            "36     \t [2.69181575 1.3665404 ]. \t  -5.038145354950235 \t -0.5408551345169831\n",
            "37     \t [-5.14321571  1.61957868]. \t  -253.6136216415022 \t -0.5408551345169831\n",
            "38     \t [ 1.68596507 -0.44724688]. \t  -3.7776541833733646 \t -0.5408551345169831\n",
            "39     \t [0.87369925 0.24053285]. \t  -1.1650409147737517 \t -0.5408551345169831\n",
            "40     \t [-0.18777685 -0.14018029]. \t  -1.5139425615306104 \t -0.5408551345169831\n",
            "41     \t [-6.1345957  -2.52099597]. \t  -761.2034500023925 \t -0.5408551345169831\n",
            "42     \t [ 9.16444022 -3.57509708]. \t  -604.4598798975373 \t -0.5408551345169831\n",
            "43     \t [-0.55451056 -0.09011807]. \t  -3.0680212516129677 \t -0.5408551345169831\n",
            "44     \t [-0.10499882  1.73161804]. \t  -75.68985159432924 \t -0.5408551345169831\n",
            "45     \t [3.68304584 0.8257315 ]. \t  -17.957789697658253 \t -0.5408551345169831\n",
            "46     \t [ 3.0475687 -4.9449983]. \t  -4210.186987292322 \t -0.5408551345169831\n",
            "47     \t [7.6036917  1.76752141]. \t  -47.28311319734221 \t -0.5408551345169831\n",
            "48     \t [8.79560734 9.70913571]. \t  -64673.00577030062 \t -0.5408551345169831\n",
            "49     \t [0.12559158 0.68074226]. \t  -2.048524233365259 \t -0.5408551345169831\n",
            "50     \t [0.04101544 0.51743679]. \t  -1.4086450926190945 \t -0.5408551345169831\n",
            "51     \t [0.35826006 0.58456507]. \t  -0.6233045647577613 \t -0.5408551345169831\n",
            "52     \t [-4.84351285 -1.7428499 ]. \t  -272.57673817873376 \t -0.5408551345169831\n",
            "53     \t [-0.34831895 -0.60888976]. \t  -4.193346260550166 \t -0.5408551345169831\n",
            "54     \t [-0.54855345 -0.56155017]. \t  -5.179187579680356 \t -0.5408551345169831\n",
            "55     \t [ 3.26844467 -1.37435164]. \t  -5.66449237608785 \t -0.5408551345169831\n",
            "56     \t [-2.60848898 -2.46389377]. \t  -448.1481996483268 \t -0.5408551345169831\n",
            "57     \t [ 0.15649361 -0.14735122]. \t  -0.7370721618211916 \t -0.5408551345169831\n",
            "58     \t [-1.978036    0.37015039]. \t  -19.012234426135624 \t -0.5408551345169831\n",
            "59     \t [-9.39811631 -8.00122495]. \t  -37886.15345066343 \t -0.5408551345169831\n",
            "60     \t [-2.39137125 -0.37636837]. \t  -25.80919894726097 \t -0.5408551345169831\n",
            "61     \t [1.08911614 0.79448986]. \t  \u001b[92m-0.0680158721041496\u001b[0m \t -0.0680158721041496\n",
            "62     \t [-5.27526564  0.92650615]. \t  -137.15768614026456 \t -0.0680158721041496\n",
            "63     \t [-5.26775709  4.094441  ]. \t  -3049.6450899565607 \t -0.0680158721041496\n",
            "64     \t [1.17571297 0.88564968]. \t  -0.3398323941865823 \t -0.0680158721041496\n",
            "65     \t [-8.51581064 -6.35642805]. \t  -16048.163840611456 \t -0.0680158721041496\n",
            "66     \t [-0.12631791 -8.28057783]. \t  -37683.12253195882 \t -0.0680158721041496\n",
            "67     \t [ 8.99129795 -2.75625089]. \t  -140.80384847932044 \t -0.0680158721041496\n",
            "68     \t [1.419442   4.07963932]. \t  -2031.2474747312667 \t -0.0680158721041496\n",
            "69     \t [ 7.94287408 -1.37985042]. \t  -82.39829188515836 \t -0.0680158721041496\n",
            "70     \t [1.12848428 2.7885834 ]. \t  -416.1148931666915 \t -0.0680158721041496\n",
            "71     \t [3.68954082 1.72482088]. \t  -17.45310921132101 \t -0.0680158721041496\n",
            "72     \t [2.32381192 9.31181879]. \t  -58549.521470673964 \t -0.0680158721041496\n",
            "73     \t [5.5072197  0.97956616]. \t  -46.06423921695451 \t -0.0680158721041496\n",
            "74     \t [ 8.4179015  -2.46283236]. \t  -82.60074836574749 \t -0.0680158721041496\n",
            "75     \t [ 7.55133276 -4.59334745]. \t  -2443.658936169019 \t -0.0680158721041496\n",
            "76     \t [5.95833852 5.32796835]. \t  -5189.148320446915 \t -0.0680158721041496\n",
            "77     \t [ 0.78732041 -0.15335217]. \t  -1.141281211052061 \t -0.0680158721041496\n",
            "78     \t [-1.24901293 -9.33659734]. \t  -61670.94055074521 \t -0.0680158721041496\n",
            "79     \t [-4.9410944  6.2398932]. \t  -13751.490771490682 \t -0.0680158721041496\n",
            "80     \t [-0.23602593  2.10356679]. \t  -166.63900281264824 \t -0.0680158721041496\n",
            "81     \t [0.71643498 4.29508963]. \t  -2617.942914090971 \t -0.0680158721041496\n",
            "82     \t [3.54646185 3.37512976]. \t  -746.5734107299555 \t -0.0680158721041496\n",
            "83     \t [-4.80892289  3.33160919]. \t  -1492.625271016946 \t -0.0680158721041496\n",
            "84     \t [-3.58395999  2.64618885]. \t  -639.7297481857347 \t -0.0680158721041496\n",
            "85     \t [2.81089834 1.15366833]. \t  -3.323753070715753 \t -0.0680158721041496\n",
            "86     \t [4.36764224 0.90137324]. \t  -26.385764062026414 \t -0.0680158721041496\n",
            "87     \t [-2.14809702  0.70543514]. \t  -29.672121282019546 \t -0.0680158721041496\n",
            "88     \t [-0.68370568 -6.59381483]. \t  -15364.547152679263 \t -0.0680158721041496\n",
            "89     \t [-2.79604495  4.4167307 ]. \t  -3510.74028683285 \t -0.0680158721041496\n",
            "90     \t [-2.5565048   1.26777645]. \t  -79.2580482941634 \t -0.0680158721041496\n",
            "91     \t [3.46580254 1.48891162]. \t  -7.953893596541249 \t -0.0680158721041496\n",
            "92     \t [8.88254343 7.4035404 ]. \t  -20360.147479058807 \t -0.0680158721041496\n",
            "93     \t [ 4.31696136 -1.52430922]. \t  -11.22013278578763 \t -0.0680158721041496\n",
            "94     \t [4.33586629 0.94515392]. \t  -24.125196097896414 \t -0.0680158721041496\n",
            "95     \t [-4.56624156 -3.7311091 ]. \t  -2131.6166846930196 \t -0.0680158721041496\n",
            "96     \t [5.49561145 2.25206586]. \t  -63.41814188043291 \t -0.0680158721041496\n",
            "97     \t [5.96686349 3.20976704]. \t  -453.2320428473832 \t -0.0680158721041496\n",
            "98     \t [-7.3883821   2.35828361]. \t  -755.707722767639 \t -0.0680158721041496\n",
            "99     \t [-9.88634809  7.60304415]. \t  -31618.46472055067 \t -0.0680158721041496\n",
            "100    \t [1.05089398 0.78511245]. \t  -0.06877206463151544 \t -0.0680158721041496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHz_Jg2_uv7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c306c2c6-4b6b-480d-efca-9b1bc3fa9118"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_winner_20 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_20 = dGPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.41034571 1.2905751 ]. \t  -7.547501684041513 \t -7.547501684041513\n",
            "init   \t [-0.23116335 -3.27044493]. \t  -936.6052904493877 \t -7.547501684041513\n",
            "init   \t [-2.48263644  0.64071738]. \t  -33.95727983513621 \t -7.547501684041513\n",
            "init   \t [-8.63787423  1.6905812 ]. \t  -504.96347244129606 \t -7.547501684041513\n",
            "init   \t [-5.24204473 -6.78486837]. \t  -18977.794322697024 \t -7.547501684041513\n",
            "1      \t [ 9.84240059 -0.83299926]. \t  -221.1494177066687 \t -7.547501684041513\n",
            "2      \t [6.73867561 8.36273465]. \t  -35481.183803304404 \t -7.547501684041513\n",
            "3      \t [ 8.38641873 -9.74808405]. \t  -66058.08004040587 \t -7.547501684041513\n",
            "4      \t [-3.83701174  9.3377774 ]. \t  -63551.82998472117 \t -7.547501684041513\n",
            "5      \t [ 1.28249957 -9.77166709]. \t  -71963.50047565464 \t -7.547501684041513\n",
            "6      \t [1.04222689 5.72992205]. \t  -8351.952392862642 \t -7.547501684041513\n",
            "7      \t [-10.           7.25151661]. \t  -26648.79086636612 \t -7.547501684041513\n",
            "8      \t [ 5.43692712 -4.06202851]. \t  -1539.1489700206573 \t -7.547501684041513\n",
            "9      \t [-10.          -3.42356568]. \t  -2357.6817425412482 \t -7.547501684041513\n",
            "10     \t [-10. -10.]. \t  -88321.0 \t -7.547501684041513\n",
            "11     \t [5.81364024 2.25027198]. \t  -60.38900626320926 \t -7.547501684041513\n",
            "12     \t [-4.97493202 -1.90964603]. \t  -336.7284612470188 \t -7.547501684041513\n",
            "13     \t [-3.79684754  3.53072432]. \t  -1673.7063819819016 \t -7.547501684041513\n",
            "14     \t [9.25668229 3.88152593]. \t  -939.7712605560133 \t -7.547501684041513\n",
            "15     \t [ 9.57479367 -4.59830407]. \t  -2213.9396015619786 \t -7.547501684041513\n",
            "16     \t [ 3.00925451 -0.04743686]. \t  -22.094196906477737 \t -7.547501684041513\n",
            "17     \t [-0.71846719  0.09535146]. \t  \u001b[92m-4.038438853432253\u001b[0m \t -4.038438853432253\n",
            "18     \t [ 5.51440894 -0.4957413 ]. \t  -70.83873744507414 \t -4.038438853432253\n",
            "19     \t [-0.40159791  0.8959089 ]. \t  -10.019799451347584 \t -4.038438853432253\n",
            "20     \t [2.24537612 1.87345321]. \t  -47.13841828913171 \t -4.038438853432253\n",
            "21     \t [0.30153338 1.65904641]. \t  -54.6372794747351 \t -4.038438853432253\n",
            "22     \t [-0.95610003  0.39799096]. \t  -7.066843761047282 \t -4.038438853432253\n",
            "23     \t [0.95610563 0.4414375 ]. \t  \u001b[92m-0.6434800845239662\u001b[0m \t -0.6434800845239662\n",
            "24     \t [2.04126339 0.2563074 ]. \t  -8.379485400996032 \t -0.6434800845239662\n",
            "25     \t [2.89502442 9.87660257]. \t  -73884.91156089552 \t -0.6434800845239662\n",
            "26     \t [1.52179163 0.60755282]. \t  -1.5001701443017033 \t -0.6434800845239662\n",
            "27     \t [ 2.35187261 -0.88310616]. \t  -3.08246657976607 \t -0.6434800845239662\n",
            "28     \t [ 2.8953706  -1.06566316]. \t  -4.3714179900303245 \t -0.6434800845239662\n",
            "29     \t [7.17614776 0.80553273]. \t  -107.25554680980332 \t -0.6434800845239662\n",
            "30     \t [ 3.08937579 -1.91176107]. \t  -39.987102000861455 \t -0.6434800845239662\n",
            "31     \t [ 0.93865871 -0.22913929]. \t  -1.3937043458940253 \t -0.6434800845239662\n",
            "32     \t [-7.06736362 -1.18718653]. \t  -260.5557558799544 \t -0.6434800845239662\n",
            "33     \t [4.84821643 4.00314247]. \t  -1494.715398322395 \t -0.6434800845239662\n",
            "34     \t [-0.46424221  0.38160145]. \t  -3.2855099476439045 \t -0.6434800845239662\n",
            "35     \t [-1.68516898 -0.50785562]. \t  -16.89896653218432 \t -0.6434800845239662\n",
            "36     \t [5.47730952 1.6687507 ]. \t  -20.06328314370341 \t -0.6434800845239662\n",
            "37     \t [-0.31083107  0.17096781]. \t  -1.9910298573398262 \t -0.6434800845239662\n",
            "38     \t [1.2794607  0.49242837]. \t  -1.3405248153936646 \t -0.6434800845239662\n",
            "39     \t [-5.65446154  0.59766657]. \t  -125.40692418036254 \t -0.6434800845239662\n",
            "40     \t [6.38750293 1.59090715]. \t  -32.539256927608974 \t -0.6434800845239662\n",
            "41     \t [ 2.61151067 -5.43927486]. \t  -6400.644086143425 \t -0.6434800845239662\n",
            "42     \t [-2.64568232 -9.90080355]. \t  -78974.68745282889 \t -0.6434800845239662\n",
            "43     \t [ 0.26024204 -0.36613138]. \t  \u001b[92m-0.54736546440461\u001b[0m \t -0.54736546440461\n",
            "44     \t [-7.36547092  3.99795355]. \t  -3164.1093525856995 \t -0.54736546440461\n",
            "45     \t [2.34202676 6.18017505]. \t  -10967.747204003006 \t -0.54736546440461\n",
            "46     \t [-7.37019544 -5.39694407]. \t  -8683.13303435588 \t -0.54736546440461\n",
            "47     \t [0.29635712 0.12307452]. \t  -0.6366917552553779 \t -0.54736546440461\n",
            "48     \t [7.41942349 4.26504765]. \t  -1718.785273908734 \t -0.54736546440461\n",
            "49     \t [0.94526687 0.5560444 ]. \t  \u001b[92m-0.21671786321625783\u001b[0m \t -0.21671786321625783\n",
            "50     \t [-4.42871958 -2.82461813]. \t  -860.6208323518431 \t -0.21671786321625783\n",
            "51     \t [ 7.60864111 -1.21331383]. \t  -87.1870222845973 \t -0.21671786321625783\n",
            "52     \t [9.55322663 6.75971883]. \t  -13466.88613990843 \t -0.21671786321625783\n",
            "53     \t [-3.75530813 -0.08039253]. \t  -51.01213105957952 \t -0.21671786321625783\n",
            "54     \t [2.28949089 7.88077999]. \t  -29732.535394841874 \t -0.21671786321625783\n",
            "55     \t [ 5.62561432 -5.31808828]. \t  -5210.860236817998 \t -0.21671786321625783\n",
            "56     \t [ 5.02624    -1.52125245]. \t  -16.527133174138513 \t -0.21671786321625783\n",
            "57     \t [-1.70044178  7.3703767 ]. \t  -24359.483718936048 \t -0.21671786321625783\n",
            "58     \t [-8.13851557  3.27121202]. \t  -1828.7559536916694 \t -0.21671786321625783\n",
            "59     \t [1.13001729 0.86621904]. \t  -0.2916726196116057 \t -0.21671786321625783\n",
            "60     \t [ 5.77250497 -3.05899245]. \t  -357.7864087892431 \t -0.21671786321625783\n",
            "61     \t [ 9.30911183 -3.67573839]. \t  -696.5416206577552 \t -0.21671786321625783\n",
            "62     \t [-1.27487039 -5.93674144]. \t  -10305.51024121098 \t -0.21671786321625783\n",
            "63     \t [ 3.99422947 -1.33483875]. \t  -9.336312598995555 \t -0.21671786321625783\n",
            "64     \t [ 8.23985224 -1.70229546]. \t  -64.36400634172232 \t -0.21671786321625783\n",
            "65     \t [ 8.67495404 -2.69019428]. \t  -126.16952713016198 \t -0.21671786321625783\n",
            "66     \t [ 9.92636565 -2.32880448]. \t  -81.37388917743273 \t -0.21671786321625783\n",
            "67     \t [0.40128712 2.84430967]. \t  -498.30632354137344 \t -0.21671786321625783\n",
            "68     \t [ 4.48980304 -1.20368817]. \t  -17.248115546286638 \t -0.21671786321625783\n",
            "69     \t [4.45118899 5.23504676]. \t  -5084.218305375657 \t -0.21671786321625783\n",
            "70     \t [0.02052282 1.1664541 ]. \t  -15.547017694471727 \t -0.21671786321625783\n",
            "71     \t [ 0.78850177 -0.69759944]. \t  \u001b[92m-0.11302485229660898\u001b[0m \t -0.11302485229660898\n",
            "72     \t [5.31084142 2.50385232]. \t  -123.06298078482557 \t -0.11302485229660898\n",
            "73     \t [-3.07061282 -5.73791022]. \t  -9515.907317149677 \t -0.11302485229660898\n",
            "74     \t [7.42857409 3.56574496]. \t  -689.3625783898757 \t -0.11302485229660898\n",
            "75     \t [8.63996262 1.93831345]. \t  -60.90408055432281 \t -0.11302485229660898\n",
            "76     \t [7.59659082 2.15145464]. \t  -49.03234310503855 \t -0.11302485229660898\n",
            "77     \t [ 5.63854004 -4.65830064]. \t  -2873.3007456023993 \t -0.11302485229660898\n",
            "78     \t [5.59743814 0.32563304]. \t  -79.14073655233214 \t -0.11302485229660898\n",
            "79     \t [ 7.04624565 -3.80494059]. \t  -996.5569013431959 \t -0.11302485229660898\n",
            "80     \t [4.88899487 1.09802047]. \t  -27.402245424328697 \t -0.11302485229660898\n",
            "81     \t [1.04852511 0.67514145]. \t  \u001b[92m-0.039834157124496394\u001b[0m \t -0.039834157124496394\n",
            "82     \t [ 2.03856719 -1.30084846]. \t  -4.701226034498959 \t -0.039834157124496394\n",
            "83     \t [7.27888051 2.54173362]. \t  -103.08729352513252 \t -0.039834157124496394\n",
            "84     \t [ 5.49141774 -1.62215617]. \t  -20.277382545596446 \t -0.039834157124496394\n",
            "85     \t [-5.14931039  8.85200777]. \t  -52438.625272453966 \t -0.039834157124496394\n",
            "86     \t [ 5.72211655 -2.07797589]. \t  -39.279440753768824 \t -0.039834157124496394\n",
            "87     \t [-6.03804648 -2.8417385 ]. \t  -1034.2377038127527 \t -0.039834157124496394\n",
            "88     \t [-6.98147209  5.98337434]. \t  -12414.282514773455 \t -0.039834157124496394\n",
            "89     \t [-9.19495539  8.93306858]. \t  -57087.02526509337 \t -0.039834157124496394\n",
            "90     \t [-7.15846338  8.10771801]. \t  -38502.36355553295 \t -0.039834157124496394\n",
            "91     \t [-0.32014405  3.15326937]. \t  -818.3367131916744 \t -0.039834157124496394\n",
            "92     \t [-4.69658286 -0.95666768]. \t  -117.65474799875122 \t -0.039834157124496394\n",
            "93     \t [-0.9094418   3.29480777]. \t  -1027.0614777163194 \t -0.039834157124496394\n",
            "94     \t [ 0.40407557 -0.33970468]. \t  -0.41517578720733866 \t -0.039834157124496394\n",
            "95     \t [ 1.47552646 -1.18912417]. \t  -3.8846711523373267 \t -0.039834157124496394\n",
            "96     \t [7.6995205  9.17775011]. \t  -51734.15813973444 \t -0.039834157124496394\n",
            "97     \t [2.94291848 8.91224474]. \t  -48621.6854745948 \t -0.039834157124496394\n",
            "98     \t [5.50375777 1.20815362]. \t  -33.642984457311385 \t -0.039834157124496394\n",
            "99     \t [3.75327601 0.61841263]. \t  -25.44168944463722 \t -0.039834157124496394\n",
            "100    \t [-3.51159788  9.08679019]. \t  -56906.74807362155 \t -0.039834157124496394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUnhsKpCuv9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58e9420d-2628-4a4a-c1c9-a1b8b8c702fa"
      },
      "source": [
        "end_win = time.time()\r\n",
        "end_win\r\n",
        "\r\n",
        "time_win = end_win - start_win\r\n",
        "time_win"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2280.01766705513"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJG0SLpwuwAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a437ace9-0dc5-4def-df94-5e72953cd88a"
      },
      "source": [
        "### Training regret minimization: run number = 1\r\n",
        "\r\n",
        "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_1 = np.log(y_global_orig - loser_output_1)\r\n",
        "regret_winner_1 = np.log(y_global_orig - winner_output_1)\r\n",
        "\r\n",
        "train_regret_loser_1 = min_max_array(regret_loser_1)\r\n",
        "train_regret_winner_1 = min_max_array(regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1 = min(train_regret_loser_1)\r\n",
        "min_train_regret_winner_1 = min(train_regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1, min_train_regret_winner_1"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.0859967871325082, -3.0859967871325082)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lA9eZf0uwCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afdb5482-bb61-4b47-aa17-b2b5e5fe470b"
      },
      "source": [
        "### Training regret minimization: run number = 2\r\n",
        "\r\n",
        "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_2 = np.log(y_global_orig - loser_output_2)\r\n",
        "regret_winner_2 = np.log(y_global_orig - winner_output_2)\r\n",
        "\r\n",
        "train_regret_loser_2 = min_max_array(regret_loser_2)\r\n",
        "train_regret_winner_2 = min_max_array(regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2 = min(train_regret_loser_2)\r\n",
        "min_train_regret_winner_2 = min(train_regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2, min_train_regret_winner_2"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.6533479564675457, -2.5145482949929234)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glrTGcpAuwFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6294a069-aabd-48f8-a12c-2d648c239e7c"
      },
      "source": [
        "### Training regret minimization: run number = 3\r\n",
        "\r\n",
        "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_3 = np.log(y_global_orig - loser_output_3)\r\n",
        "regret_winner_3 = np.log(y_global_orig - winner_output_3)\r\n",
        "\r\n",
        "train_regret_loser_3 = min_max_array(regret_loser_3)\r\n",
        "train_regret_winner_3 = min_max_array(regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3 = min(train_regret_loser_3)\r\n",
        "min_train_regret_winner_3 = min(train_regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3, min_train_regret_winner_3"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.490794947871965, -5.490794947871965)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaRwfbxeuwIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "128e5063-9d47-4448-b2ff-96c40b5bd3f3"
      },
      "source": [
        "### Training regret minimization: run number = 4\r\n",
        "\r\n",
        "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_4 = np.log(y_global_orig - loser_output_4)\r\n",
        "regret_winner_4 = np.log(y_global_orig - winner_output_4)\r\n",
        "\r\n",
        "train_regret_loser_4 = min_max_array(regret_loser_4)\r\n",
        "train_regret_winner_4 = min_max_array(regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4 = min(train_regret_loser_4)\r\n",
        "min_train_regret_winner_4 = min(train_regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4, min_train_regret_winner_4"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.9055797794302745, -2.3243647698137986)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nb5NkfyuwKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab2c6e86-55c1-4100-9307-0fa685456c25"
      },
      "source": [
        "### Training regret minimization: run number = 5\r\n",
        "\r\n",
        "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_5 = np.log(y_global_orig - loser_output_5)\r\n",
        "regret_winner_5 = np.log(y_global_orig - winner_output_5)\r\n",
        "\r\n",
        "train_regret_loser_5 = min_max_array(regret_loser_5)\r\n",
        "train_regret_winner_5 = min_max_array(regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5 = min(train_regret_loser_5)\r\n",
        "min_train_regret_winner_5 = min(train_regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5, min_train_regret_winner_5"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.2241822881875337, -2.4411200962821726)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0Q-WfXbuwNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1ed58a-be81-4cdd-f85c-3dbb940357a6"
      },
      "source": [
        "### Training regret minimization: run number = 6\r\n",
        "\r\n",
        "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_6 = np.log(y_global_orig - loser_output_6)\r\n",
        "regret_winner_6 = np.log(y_global_orig - winner_output_6)\r\n",
        "\r\n",
        "train_regret_loser_6 = min_max_array(regret_loser_6)\r\n",
        "train_regret_winner_6 = min_max_array(regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6 = min(train_regret_loser_6)\r\n",
        "min_train_regret_winner_6 = min(train_regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6, min_train_regret_winner_6"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.950268075483134, -2.5738295445417303)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqqS7VLcuwPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b0ac69-ca74-43d2-80d8-ada431211563"
      },
      "source": [
        "### Training regret minimization: run number = 7\r\n",
        "\r\n",
        "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_7 = np.log(y_global_orig - loser_output_7)\r\n",
        "regret_winner_7 = np.log(y_global_orig - winner_output_7)\r\n",
        "\r\n",
        "train_regret_loser_7 = min_max_array(regret_loser_7)\r\n",
        "train_regret_winner_7 = min_max_array(regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7 = min(train_regret_loser_7)\r\n",
        "min_train_regret_winner_7 = min(train_regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7, min_train_regret_winner_7"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.869238516356011, -4.869238516356011)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOi5iX8guwSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e66f522-9b7f-4091-b8ae-8feaeff3cc94"
      },
      "source": [
        "### Training regret minimization: run number = 8\r\n",
        "\r\n",
        "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_8 = np.log(y_global_orig - loser_output_8)\r\n",
        "regret_winner_8 = np.log(y_global_orig - winner_output_8)\r\n",
        "\r\n",
        "train_regret_loser_8 = min_max_array(regret_loser_8)\r\n",
        "train_regret_winner_8 = min_max_array(regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8 = min(train_regret_loser_8)\r\n",
        "min_train_regret_winner_8 = min(train_regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8, min_train_regret_winner_8"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.901063041781975, -2.8019184505767214)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFVApeazuwU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e48f2ca3-3386-48fb-e41b-592a7e59f552"
      },
      "source": [
        "### Training regret minimization: run number = 9\r\n",
        "\r\n",
        "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_9 = np.log(y_global_orig - loser_output_9)\r\n",
        "regret_winner_9 = np.log(y_global_orig - winner_output_9)\r\n",
        "\r\n",
        "train_regret_loser_9 = min_max_array(regret_loser_9)\r\n",
        "train_regret_winner_9 = min_max_array(regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9 = min(train_regret_loser_9)\r\n",
        "min_train_regret_winner_9 = min(train_regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9, min_train_regret_winner_9"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.8375231943646626, -4.746390601056169)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g92jk9WJuwXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "874f0005-cd99-4efc-be66-94501538c696"
      },
      "source": [
        "### Training regret minimization: run number = 10\r\n",
        "\r\n",
        "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_10 = np.log(y_global_orig - loser_output_10)\r\n",
        "regret_winner_10 = np.log(y_global_orig - winner_output_10)\r\n",
        "\r\n",
        "train_regret_loser_10 = min_max_array(regret_loser_10)\r\n",
        "train_regret_winner_10 = min_max_array(regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10 = min(train_regret_loser_10)\r\n",
        "min_train_regret_winner_10 = min(train_regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10, min_train_regret_winner_10"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-7.724081906157097, -2.947039840446273)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmcF1x-NuwZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c66bca9-9881-433b-84e2-fb33aa7c4205"
      },
      "source": [
        "### Training regret minimization: run number = 11\r\n",
        "\r\n",
        "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_11 = np.log(y_global_orig - loser_output_11)\r\n",
        "regret_winner_11 = np.log(y_global_orig - winner_output_11)\r\n",
        "\r\n",
        "train_regret_loser_11 = min_max_array(regret_loser_11)\r\n",
        "train_regret_winner_11 = min_max_array(regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11 = min(train_regret_loser_11)\r\n",
        "min_train_regret_winner_11 = min(train_regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11, min_train_regret_winner_11"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.290222586116367, -5.290222586116367)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8axVhb6uwcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "320e985e-aed9-40fd-caa0-20f5219788e8"
      },
      "source": [
        "### Training regret minimization: run number = 12\r\n",
        "\r\n",
        "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_12 = np.log(y_global_orig - loser_output_12)\r\n",
        "regret_winner_12 = np.log(y_global_orig - winner_output_12)\r\n",
        "\r\n",
        "train_regret_loser_12 = min_max_array(regret_loser_12)\r\n",
        "train_regret_winner_12 = min_max_array(regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12 = min(train_regret_loser_12)\r\n",
        "min_train_regret_winner_12 = min(train_regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12, min_train_regret_winner_12"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.8996216443716853, -2.446461917084629)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzlrL8XFuwfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3959cd03-7204-4779-e257-7ceaa45b18ba"
      },
      "source": [
        "### Training regret minimization: run number = 13\r\n",
        "\r\n",
        "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_13 = np.log(y_global_orig - loser_output_13)\r\n",
        "regret_winner_13 = np.log(y_global_orig - winner_output_13)\r\n",
        "\r\n",
        "train_regret_loser_13 = min_max_array(regret_loser_13)\r\n",
        "train_regret_winner_13 = min_max_array(regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13 = min(train_regret_loser_13)\r\n",
        "min_train_regret_winner_13 = min(train_regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13, min_train_regret_winner_13"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.671419910926664, -3.4727823501197728)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uZlLJ1quwh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea67c6aa-767a-4ac5-c73d-0e111ae23ce6"
      },
      "source": [
        "### Training regret minimization: run number = 14\r\n",
        "\r\n",
        "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_14 = np.log(y_global_orig - loser_output_14)\r\n",
        "regret_winner_14 = np.log(y_global_orig - winner_output_14)\r\n",
        "\r\n",
        "train_regret_loser_14 = min_max_array(regret_loser_14)\r\n",
        "train_regret_winner_14 = min_max_array(regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14 = min(train_regret_loser_14)\r\n",
        "min_train_regret_winner_14 = min(train_regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14, min_train_regret_winner_14"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.090293360199478, -2.1203375831203153)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVBcHRiMuwjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10101c30-a0e8-4e84-b1fb-ec69e30eebc9"
      },
      "source": [
        "### Training regret minimization: run number = 15\r\n",
        "\r\n",
        "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_15 = np.log(y_global_orig - loser_output_15)\r\n",
        "regret_winner_15 = np.log(y_global_orig - winner_output_15)\r\n",
        "\r\n",
        "train_regret_loser_15 = min_max_array(regret_loser_15)\r\n",
        "train_regret_winner_15 = min_max_array(regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15 = min(train_regret_loser_15)\r\n",
        "min_train_regret_winner_15 = min(train_regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15, min_train_regret_winner_15"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.320517307417607, -4.320517307417607)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8nZ_DrKuwnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b2297f-c397-4077-f3ee-494cbef1b2cc"
      },
      "source": [
        "### Training regret minimization: run number = 16\r\n",
        "\r\n",
        "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_16 = np.log(y_global_orig - loser_output_16)\r\n",
        "regret_winner_16 = np.log(y_global_orig - winner_output_16)\r\n",
        "\r\n",
        "train_regret_loser_16 = min_max_array(regret_loser_16)\r\n",
        "train_regret_winner_16 = min_max_array(regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16 = min(train_regret_loser_16)\r\n",
        "min_train_regret_winner_16 = min(train_regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16, min_train_regret_winner_16"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.81624406813844, -2.2114019029631575)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg6qzzQFuwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2d1ee35-606a-44bd-fc5f-c34e7c23173c"
      },
      "source": [
        "### Training regret minimization: run number = 17\r\n",
        "\r\n",
        "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_17 = np.log(y_global_orig - loser_output_17)\r\n",
        "regret_winner_17 = np.log(y_global_orig - winner_output_17)\r\n",
        "\r\n",
        "train_regret_loser_17 = min_max_array(regret_loser_17)\r\n",
        "train_regret_winner_17 = min_max_array(regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17 = min(train_regret_loser_17)\r\n",
        "min_train_regret_winner_17 = min(train_regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17, min_train_regret_winner_17"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.4558938357040816, -1.6682865874127824)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrRtDLMFuwsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7583dc47-1fda-42fa-b65d-3608875750ab"
      },
      "source": [
        "### Training regret minimization: run number = 18\r\n",
        "\r\n",
        "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_18 = np.log(y_global_orig - loser_output_18)\r\n",
        "regret_winner_18 = np.log(y_global_orig - winner_output_18)\r\n",
        "\r\n",
        "train_regret_loser_18 = min_max_array(regret_loser_18)\r\n",
        "train_regret_winner_18 = min_max_array(regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18 = min(train_regret_loser_18)\r\n",
        "min_train_regret_winner_18 = min(train_regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18, min_train_regret_winner_18"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.9685717878941154, -3.198235252681353)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkmSu4CUuwuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9cb07c1-bb6e-4d61-d7eb-3bbb24ea875d"
      },
      "source": [
        "### Training regret minimization: run number = 19\r\n",
        "\r\n",
        "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_19 = np.log(y_global_orig - loser_output_19)\r\n",
        "regret_winner_19 = np.log(y_global_orig - winner_output_19)\r\n",
        "\r\n",
        "train_regret_loser_19 = min_max_array(regret_loser_19)\r\n",
        "train_regret_winner_19 = min_max_array(regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19 = min(train_regret_loser_19)\r\n",
        "min_train_regret_winner_19 = min(train_regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19, min_train_regret_winner_19"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.812156669041909, -2.688014187746358)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eymecjwkuwxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2855915c-8ddb-4c68-c2c1-639411bd7325"
      },
      "source": [
        "### Training regret minimization: run number = 20\r\n",
        "\r\n",
        "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_20 = np.log(y_global_orig - loser_output_20)\r\n",
        "regret_winner_20 = np.log(y_global_orig - winner_output_20)\r\n",
        "\r\n",
        "train_regret_loser_20 = min_max_array(regret_loser_20)\r\n",
        "train_regret_winner_20 = min_max_array(regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20 = min(train_regret_loser_20)\r\n",
        "min_train_regret_winner_20 = min(train_regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20, min_train_regret_winner_20"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.8276149125853527, -3.223030515542829)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5P6kq2Kuw0d"
      },
      "source": [
        "# Iteration1 :\r\n",
        "\r\n",
        "slice1 = 0\r\n",
        "\r\n",
        "loser1 = [train_regret_loser_1[slice1],\r\n",
        "       train_regret_loser_2[slice1],\r\n",
        "       train_regret_loser_3[slice1],\r\n",
        "       train_regret_loser_4[slice1],\r\n",
        "       train_regret_loser_5[slice1],\r\n",
        "       train_regret_loser_6[slice1],\r\n",
        "       train_regret_loser_7[slice1],\r\n",
        "       train_regret_loser_8[slice1],\r\n",
        "       train_regret_loser_9[slice1],\r\n",
        "       train_regret_loser_10[slice1],\r\n",
        "       train_regret_loser_11[slice1],\r\n",
        "       train_regret_loser_12[slice1],\r\n",
        "       train_regret_loser_13[slice1],\r\n",
        "       train_regret_loser_14[slice1],\r\n",
        "       train_regret_loser_15[slice1],\r\n",
        "       train_regret_loser_16[slice1],\r\n",
        "       train_regret_loser_17[slice1],\r\n",
        "       train_regret_loser_18[slice1],\r\n",
        "       train_regret_loser_19[slice1],\r\n",
        "       train_regret_loser_20[slice1]]\r\n",
        "\r\n",
        "winner1 = [train_regret_winner_1[slice1],\r\n",
        "       train_regret_winner_2[slice1],\r\n",
        "       train_regret_winner_3[slice1],\r\n",
        "       train_regret_winner_4[slice1],\r\n",
        "       train_regret_winner_5[slice1],\r\n",
        "       train_regret_winner_6[slice1],\r\n",
        "       train_regret_winner_7[slice1],\r\n",
        "       train_regret_winner_8[slice1],\r\n",
        "       train_regret_winner_9[slice1],\r\n",
        "       train_regret_winner_10[slice1],\r\n",
        "       train_regret_winner_11[slice1],\r\n",
        "       train_regret_winner_12[slice1],\r\n",
        "       train_regret_winner_13[slice1],\r\n",
        "       train_regret_winner_14[slice1],\r\n",
        "       train_regret_winner_15[slice1],\r\n",
        "       train_regret_winner_16[slice1],\r\n",
        "       train_regret_winner_17[slice1],\r\n",
        "       train_regret_winner_18[slice1],\r\n",
        "       train_regret_winner_19[slice1],\r\n",
        "       train_regret_winner_20[slice1]]\r\n",
        "\r\n",
        "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\r\n",
        "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\r\n",
        "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\r\n",
        "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\r\n",
        "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\r\n",
        "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXcnj2pNuw3X"
      },
      "source": [
        "# Iteration11 :\r\n",
        "\r\n",
        "slice11 = 10\r\n",
        "\r\n",
        "loser11 = [train_regret_loser_1[slice11],\r\n",
        "       train_regret_loser_2[slice11],\r\n",
        "       train_regret_loser_3[slice11],\r\n",
        "       train_regret_loser_4[slice11],\r\n",
        "       train_regret_loser_5[slice11],\r\n",
        "       train_regret_loser_6[slice11],\r\n",
        "       train_regret_loser_7[slice11],\r\n",
        "       train_regret_loser_8[slice11],\r\n",
        "       train_regret_loser_9[slice11],\r\n",
        "       train_regret_loser_10[slice11],\r\n",
        "       train_regret_loser_11[slice11],\r\n",
        "       train_regret_loser_12[slice11],\r\n",
        "       train_regret_loser_13[slice11],\r\n",
        "       train_regret_loser_14[slice11],\r\n",
        "       train_regret_loser_15[slice11],\r\n",
        "       train_regret_loser_16[slice11],\r\n",
        "       train_regret_loser_17[slice11],\r\n",
        "       train_regret_loser_18[slice11],\r\n",
        "       train_regret_loser_19[slice11],\r\n",
        "       train_regret_loser_20[slice11]]\r\n",
        "\r\n",
        "winner11 = [train_regret_winner_1[slice11],\r\n",
        "       train_regret_winner_2[slice11],\r\n",
        "       train_regret_winner_3[slice11],\r\n",
        "       train_regret_winner_4[slice11],\r\n",
        "       train_regret_winner_5[slice11],\r\n",
        "       train_regret_winner_6[slice11],\r\n",
        "       train_regret_winner_7[slice11],\r\n",
        "       train_regret_winner_8[slice11],\r\n",
        "       train_regret_winner_9[slice11],\r\n",
        "       train_regret_winner_10[slice11],\r\n",
        "       train_regret_winner_11[slice11],\r\n",
        "       train_regret_winner_12[slice11],\r\n",
        "       train_regret_winner_13[slice11],\r\n",
        "       train_regret_winner_14[slice11],\r\n",
        "       train_regret_winner_15[slice11],\r\n",
        "       train_regret_winner_16[slice11],\r\n",
        "       train_regret_winner_17[slice11],\r\n",
        "       train_regret_winner_18[slice11],\r\n",
        "       train_regret_winner_19[slice11],\r\n",
        "       train_regret_winner_20[slice11]]\r\n",
        "\r\n",
        "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\r\n",
        "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\r\n",
        "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\r\n",
        "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\r\n",
        "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\r\n",
        "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxQa6BAVuw6L"
      },
      "source": [
        "# Iteration21 :\r\n",
        "\r\n",
        "slice21 = 20\r\n",
        "\r\n",
        "loser21 = [train_regret_loser_1[slice21],\r\n",
        "       train_regret_loser_2[slice21],\r\n",
        "       train_regret_loser_3[slice21],\r\n",
        "       train_regret_loser_4[slice21],\r\n",
        "       train_regret_loser_5[slice21],\r\n",
        "       train_regret_loser_6[slice21],\r\n",
        "       train_regret_loser_7[slice21],\r\n",
        "       train_regret_loser_8[slice21],\r\n",
        "       train_regret_loser_9[slice21],\r\n",
        "       train_regret_loser_10[slice21],\r\n",
        "       train_regret_loser_11[slice21],\r\n",
        "       train_regret_loser_12[slice21],\r\n",
        "       train_regret_loser_13[slice21],\r\n",
        "       train_regret_loser_14[slice21],\r\n",
        "       train_regret_loser_15[slice21],\r\n",
        "       train_regret_loser_16[slice21],\r\n",
        "       train_regret_loser_17[slice21],\r\n",
        "       train_regret_loser_18[slice21],\r\n",
        "       train_regret_loser_19[slice21],\r\n",
        "       train_regret_loser_20[slice21]]\r\n",
        "\r\n",
        "winner21 = [train_regret_winner_1[slice21],\r\n",
        "       train_regret_winner_2[slice21],\r\n",
        "       train_regret_winner_3[slice21],\r\n",
        "       train_regret_winner_4[slice21],\r\n",
        "       train_regret_winner_5[slice21],\r\n",
        "       train_regret_winner_6[slice21],\r\n",
        "       train_regret_winner_7[slice21],\r\n",
        "       train_regret_winner_8[slice21],\r\n",
        "       train_regret_winner_9[slice21],\r\n",
        "       train_regret_winner_10[slice21],\r\n",
        "       train_regret_winner_11[slice21],\r\n",
        "       train_regret_winner_12[slice21],\r\n",
        "       train_regret_winner_13[slice21],\r\n",
        "       train_regret_winner_14[slice21],\r\n",
        "       train_regret_winner_15[slice21],\r\n",
        "       train_regret_winner_16[slice21],\r\n",
        "       train_regret_winner_17[slice21],\r\n",
        "       train_regret_winner_18[slice21],\r\n",
        "       train_regret_winner_19[slice21],\r\n",
        "       train_regret_winner_20[slice21]]\r\n",
        "\r\n",
        "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\r\n",
        "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\r\n",
        "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\r\n",
        "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\r\n",
        "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\r\n",
        "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wdJdIsWuw8r"
      },
      "source": [
        "# Iteration31 :\r\n",
        "\r\n",
        "slice31 = 30\r\n",
        "\r\n",
        "loser31 = [train_regret_loser_1[slice31],\r\n",
        "       train_regret_loser_2[slice31],\r\n",
        "       train_regret_loser_3[slice31],\r\n",
        "       train_regret_loser_4[slice31],\r\n",
        "       train_regret_loser_5[slice31],\r\n",
        "       train_regret_loser_6[slice31],\r\n",
        "       train_regret_loser_7[slice31],\r\n",
        "       train_regret_loser_8[slice31],\r\n",
        "       train_regret_loser_9[slice31],\r\n",
        "       train_regret_loser_10[slice31],\r\n",
        "       train_regret_loser_11[slice31],\r\n",
        "       train_regret_loser_12[slice31],\r\n",
        "       train_regret_loser_13[slice31],\r\n",
        "       train_regret_loser_14[slice31],\r\n",
        "       train_regret_loser_15[slice31],\r\n",
        "       train_regret_loser_16[slice31],\r\n",
        "       train_regret_loser_17[slice31],\r\n",
        "       train_regret_loser_18[slice31],\r\n",
        "       train_regret_loser_19[slice31],\r\n",
        "       train_regret_loser_20[slice31]]\r\n",
        "\r\n",
        "winner31 = [train_regret_winner_1[slice31],\r\n",
        "       train_regret_winner_2[slice31],\r\n",
        "       train_regret_winner_3[slice31],\r\n",
        "       train_regret_winner_4[slice31],\r\n",
        "       train_regret_winner_5[slice31],\r\n",
        "       train_regret_winner_6[slice31],\r\n",
        "       train_regret_winner_7[slice31],\r\n",
        "       train_regret_winner_8[slice31],\r\n",
        "       train_regret_winner_9[slice31],\r\n",
        "       train_regret_winner_10[slice31],\r\n",
        "       train_regret_winner_11[slice31],\r\n",
        "       train_regret_winner_12[slice31],\r\n",
        "       train_regret_winner_13[slice31],\r\n",
        "       train_regret_winner_14[slice31],\r\n",
        "       train_regret_winner_15[slice31],\r\n",
        "       train_regret_winner_16[slice31],\r\n",
        "       train_regret_winner_17[slice31],\r\n",
        "       train_regret_winner_18[slice31],\r\n",
        "       train_regret_winner_19[slice31],\r\n",
        "       train_regret_winner_20[slice31]]\r\n",
        "\r\n",
        "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\r\n",
        "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\r\n",
        "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\r\n",
        "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\r\n",
        "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\r\n",
        "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWZBDufuw_Z"
      },
      "source": [
        "# Iteration41 :\r\n",
        "\r\n",
        "slice41 = 40\r\n",
        "\r\n",
        "loser41 = [train_regret_loser_1[slice41],\r\n",
        "       train_regret_loser_2[slice41],\r\n",
        "       train_regret_loser_3[slice41],\r\n",
        "       train_regret_loser_4[slice41],\r\n",
        "       train_regret_loser_5[slice41],\r\n",
        "       train_regret_loser_6[slice41],\r\n",
        "       train_regret_loser_7[slice41],\r\n",
        "       train_regret_loser_8[slice41],\r\n",
        "       train_regret_loser_9[slice41],\r\n",
        "       train_regret_loser_10[slice41],\r\n",
        "       train_regret_loser_11[slice41],\r\n",
        "       train_regret_loser_12[slice41],\r\n",
        "       train_regret_loser_13[slice41],\r\n",
        "       train_regret_loser_14[slice41],\r\n",
        "       train_regret_loser_15[slice41],\r\n",
        "       train_regret_loser_16[slice41],\r\n",
        "       train_regret_loser_17[slice41],\r\n",
        "       train_regret_loser_18[slice41],\r\n",
        "       train_regret_loser_19[slice41],\r\n",
        "       train_regret_loser_20[slice41]]\r\n",
        "\r\n",
        "winner41 = [train_regret_winner_1[slice41],\r\n",
        "       train_regret_winner_2[slice41],\r\n",
        "       train_regret_winner_3[slice41],\r\n",
        "       train_regret_winner_4[slice41],\r\n",
        "       train_regret_winner_5[slice41],\r\n",
        "       train_regret_winner_6[slice41],\r\n",
        "       train_regret_winner_7[slice41],\r\n",
        "       train_regret_winner_8[slice41],\r\n",
        "       train_regret_winner_9[slice41],\r\n",
        "       train_regret_winner_10[slice41],\r\n",
        "       train_regret_winner_11[slice41],\r\n",
        "       train_regret_winner_12[slice41],\r\n",
        "       train_regret_winner_13[slice41],\r\n",
        "       train_regret_winner_14[slice41],\r\n",
        "       train_regret_winner_15[slice41],\r\n",
        "       train_regret_winner_16[slice41],\r\n",
        "       train_regret_winner_17[slice41],\r\n",
        "       train_regret_winner_18[slice41],\r\n",
        "       train_regret_winner_19[slice41],\r\n",
        "       train_regret_winner_20[slice41]]\r\n",
        "\r\n",
        "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\r\n",
        "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\r\n",
        "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\r\n",
        "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\r\n",
        "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\r\n",
        "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwi_GzjiuxB6"
      },
      "source": [
        "# Iteration51 :\r\n",
        "\r\n",
        "slice51 = 50\r\n",
        "\r\n",
        "loser51 = [train_regret_loser_1[slice51],\r\n",
        "       train_regret_loser_2[slice51],\r\n",
        "       train_regret_loser_3[slice51],\r\n",
        "       train_regret_loser_4[slice51],\r\n",
        "       train_regret_loser_5[slice51],\r\n",
        "       train_regret_loser_6[slice51],\r\n",
        "       train_regret_loser_7[slice51],\r\n",
        "       train_regret_loser_8[slice51],\r\n",
        "       train_regret_loser_9[slice51],\r\n",
        "       train_regret_loser_10[slice51],\r\n",
        "       train_regret_loser_11[slice51],\r\n",
        "       train_regret_loser_12[slice51],\r\n",
        "       train_regret_loser_13[slice51],\r\n",
        "       train_regret_loser_14[slice51],\r\n",
        "       train_regret_loser_15[slice51],\r\n",
        "       train_regret_loser_16[slice51],\r\n",
        "       train_regret_loser_17[slice51],\r\n",
        "       train_regret_loser_18[slice51],\r\n",
        "       train_regret_loser_19[slice51],\r\n",
        "       train_regret_loser_20[slice51]]\r\n",
        "\r\n",
        "winner51 = [train_regret_winner_1[slice51],\r\n",
        "       train_regret_winner_2[slice51],\r\n",
        "       train_regret_winner_3[slice51],\r\n",
        "       train_regret_winner_4[slice51],\r\n",
        "       train_regret_winner_5[slice51],\r\n",
        "       train_regret_winner_6[slice51],\r\n",
        "       train_regret_winner_7[slice51],\r\n",
        "       train_regret_winner_8[slice51],\r\n",
        "       train_regret_winner_9[slice51],\r\n",
        "       train_regret_winner_10[slice51],\r\n",
        "       train_regret_winner_11[slice51],\r\n",
        "       train_regret_winner_12[slice51],\r\n",
        "       train_regret_winner_13[slice51],\r\n",
        "       train_regret_winner_14[slice51],\r\n",
        "       train_regret_winner_15[slice51],\r\n",
        "       train_regret_winner_16[slice51],\r\n",
        "       train_regret_winner_17[slice51],\r\n",
        "       train_regret_winner_18[slice51],\r\n",
        "       train_regret_winner_19[slice51],\r\n",
        "       train_regret_winner_20[slice51]]\r\n",
        "\r\n",
        "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\r\n",
        "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\r\n",
        "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\r\n",
        "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\r\n",
        "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\r\n",
        "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk9nlS6puxFZ"
      },
      "source": [
        "# Iteration61 :\r\n",
        "\r\n",
        "slice61 = 60\r\n",
        "\r\n",
        "loser61 = [train_regret_loser_1[slice61],\r\n",
        "       train_regret_loser_2[slice61],\r\n",
        "       train_regret_loser_3[slice61],\r\n",
        "       train_regret_loser_4[slice61],\r\n",
        "       train_regret_loser_5[slice61],\r\n",
        "       train_regret_loser_6[slice61],\r\n",
        "       train_regret_loser_7[slice61],\r\n",
        "       train_regret_loser_8[slice61],\r\n",
        "       train_regret_loser_9[slice61],\r\n",
        "       train_regret_loser_10[slice61],\r\n",
        "       train_regret_loser_11[slice61],\r\n",
        "       train_regret_loser_12[slice61],\r\n",
        "       train_regret_loser_13[slice61],\r\n",
        "       train_regret_loser_14[slice61],\r\n",
        "       train_regret_loser_15[slice61],\r\n",
        "       train_regret_loser_16[slice61],\r\n",
        "       train_regret_loser_17[slice61],\r\n",
        "       train_regret_loser_18[slice61],\r\n",
        "       train_regret_loser_19[slice61],\r\n",
        "       train_regret_loser_20[slice61]]\r\n",
        "\r\n",
        "winner61 = [train_regret_winner_1[slice61],\r\n",
        "       train_regret_winner_2[slice61],\r\n",
        "       train_regret_winner_3[slice61],\r\n",
        "       train_regret_winner_4[slice61],\r\n",
        "       train_regret_winner_5[slice61],\r\n",
        "       train_regret_winner_6[slice61],\r\n",
        "       train_regret_winner_7[slice61],\r\n",
        "       train_regret_winner_8[slice61],\r\n",
        "       train_regret_winner_9[slice61],\r\n",
        "       train_regret_winner_10[slice61],\r\n",
        "       train_regret_winner_11[slice61],\r\n",
        "       train_regret_winner_12[slice61],\r\n",
        "       train_regret_winner_13[slice61],\r\n",
        "       train_regret_winner_14[slice61],\r\n",
        "       train_regret_winner_15[slice61],\r\n",
        "       train_regret_winner_16[slice61],\r\n",
        "       train_regret_winner_17[slice61],\r\n",
        "       train_regret_winner_18[slice61],\r\n",
        "       train_regret_winner_19[slice61],\r\n",
        "       train_regret_winner_20[slice61]]\r\n",
        "\r\n",
        "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\r\n",
        "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\r\n",
        "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\r\n",
        "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\r\n",
        "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\r\n",
        "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMus9vDpuxHN"
      },
      "source": [
        "# Iteration71 :\r\n",
        "\r\n",
        "slice71 = 70\r\n",
        "\r\n",
        "loser71 = [train_regret_loser_1[slice71],\r\n",
        "       train_regret_loser_2[slice71],\r\n",
        "       train_regret_loser_3[slice71],\r\n",
        "       train_regret_loser_4[slice71],\r\n",
        "       train_regret_loser_5[slice71],\r\n",
        "       train_regret_loser_6[slice71],\r\n",
        "       train_regret_loser_7[slice71],\r\n",
        "       train_regret_loser_8[slice71],\r\n",
        "       train_regret_loser_9[slice71],\r\n",
        "       train_regret_loser_10[slice71],\r\n",
        "       train_regret_loser_11[slice71],\r\n",
        "       train_regret_loser_12[slice71],\r\n",
        "       train_regret_loser_13[slice71],\r\n",
        "       train_regret_loser_14[slice71],\r\n",
        "       train_regret_loser_15[slice71],\r\n",
        "       train_regret_loser_16[slice71],\r\n",
        "       train_regret_loser_17[slice71],\r\n",
        "       train_regret_loser_18[slice71],\r\n",
        "       train_regret_loser_19[slice71],\r\n",
        "       train_regret_loser_20[slice71]]\r\n",
        "\r\n",
        "winner71 = [train_regret_winner_1[slice71],\r\n",
        "       train_regret_winner_2[slice71],\r\n",
        "       train_regret_winner_3[slice71],\r\n",
        "       train_regret_winner_4[slice71],\r\n",
        "       train_regret_winner_5[slice71],\r\n",
        "       train_regret_winner_6[slice71],\r\n",
        "       train_regret_winner_7[slice71],\r\n",
        "       train_regret_winner_8[slice71],\r\n",
        "       train_regret_winner_9[slice71],\r\n",
        "       train_regret_winner_10[slice71],\r\n",
        "       train_regret_winner_11[slice71],\r\n",
        "       train_regret_winner_12[slice71],\r\n",
        "       train_regret_winner_13[slice71],\r\n",
        "       train_regret_winner_14[slice71],\r\n",
        "       train_regret_winner_15[slice71],\r\n",
        "       train_regret_winner_16[slice71],\r\n",
        "       train_regret_winner_17[slice71],\r\n",
        "       train_regret_winner_18[slice71],\r\n",
        "       train_regret_winner_19[slice71],\r\n",
        "       train_regret_winner_20[slice71]]\r\n",
        "\r\n",
        "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\r\n",
        "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\r\n",
        "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\r\n",
        "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\r\n",
        "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\r\n",
        "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Mg0x9vuxL4"
      },
      "source": [
        "# Iteration81 :\r\n",
        "\r\n",
        "slice81 = 80\r\n",
        "\r\n",
        "loser81 = [train_regret_loser_1[slice81],\r\n",
        "       train_regret_loser_2[slice81],\r\n",
        "       train_regret_loser_3[slice81],\r\n",
        "       train_regret_loser_4[slice81],\r\n",
        "       train_regret_loser_5[slice81],\r\n",
        "       train_regret_loser_6[slice81],\r\n",
        "       train_regret_loser_7[slice81],\r\n",
        "       train_regret_loser_8[slice81],\r\n",
        "       train_regret_loser_9[slice81],\r\n",
        "       train_regret_loser_10[slice81],\r\n",
        "       train_regret_loser_11[slice81],\r\n",
        "       train_regret_loser_12[slice81],\r\n",
        "       train_regret_loser_13[slice81],\r\n",
        "       train_regret_loser_14[slice81],\r\n",
        "       train_regret_loser_15[slice81],\r\n",
        "       train_regret_loser_16[slice81],\r\n",
        "       train_regret_loser_17[slice81],\r\n",
        "       train_regret_loser_18[slice81],\r\n",
        "       train_regret_loser_19[slice81],\r\n",
        "       train_regret_loser_20[slice81]]\r\n",
        "\r\n",
        "winner81 = [train_regret_winner_1[slice81],\r\n",
        "       train_regret_winner_2[slice81],\r\n",
        "       train_regret_winner_3[slice81],\r\n",
        "       train_regret_winner_4[slice81],\r\n",
        "       train_regret_winner_5[slice81],\r\n",
        "       train_regret_winner_6[slice81],\r\n",
        "       train_regret_winner_7[slice81],\r\n",
        "       train_regret_winner_8[slice81],\r\n",
        "       train_regret_winner_9[slice81],\r\n",
        "       train_regret_winner_10[slice81],\r\n",
        "       train_regret_winner_11[slice81],\r\n",
        "       train_regret_winner_12[slice81],\r\n",
        "       train_regret_winner_13[slice81],\r\n",
        "       train_regret_winner_14[slice81],\r\n",
        "       train_regret_winner_15[slice81],\r\n",
        "       train_regret_winner_16[slice81],\r\n",
        "       train_regret_winner_17[slice81],\r\n",
        "       train_regret_winner_18[slice81],\r\n",
        "       train_regret_winner_19[slice81],\r\n",
        "       train_regret_winner_20[slice81]]\r\n",
        "\r\n",
        "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\r\n",
        "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\r\n",
        "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\r\n",
        "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\r\n",
        "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\r\n",
        "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gu9K8XlteV4"
      },
      "source": [
        "# Iteration91 :\r\n",
        "\r\n",
        "slice91 = 90\r\n",
        "\r\n",
        "loser91 = [train_regret_loser_1[slice91],\r\n",
        "       train_regret_loser_2[slice91],\r\n",
        "       train_regret_loser_3[slice91],\r\n",
        "       train_regret_loser_4[slice91],\r\n",
        "       train_regret_loser_5[slice91],\r\n",
        "       train_regret_loser_6[slice91],\r\n",
        "       train_regret_loser_7[slice91],\r\n",
        "       train_regret_loser_8[slice91],\r\n",
        "       train_regret_loser_9[slice91],\r\n",
        "       train_regret_loser_10[slice91],\r\n",
        "       train_regret_loser_11[slice91],\r\n",
        "       train_regret_loser_12[slice91],\r\n",
        "       train_regret_loser_13[slice91],\r\n",
        "       train_regret_loser_14[slice91],\r\n",
        "       train_regret_loser_15[slice91],\r\n",
        "       train_regret_loser_16[slice91],\r\n",
        "       train_regret_loser_17[slice91],\r\n",
        "       train_regret_loser_18[slice91],\r\n",
        "       train_regret_loser_19[slice91],\r\n",
        "       train_regret_loser_20[slice91]]\r\n",
        "\r\n",
        "winner91 = [train_regret_winner_1[slice91],\r\n",
        "       train_regret_winner_2[slice91],\r\n",
        "       train_regret_winner_3[slice91],\r\n",
        "       train_regret_winner_4[slice91],\r\n",
        "       train_regret_winner_5[slice91],\r\n",
        "       train_regret_winner_6[slice91],\r\n",
        "       train_regret_winner_7[slice91],\r\n",
        "       train_regret_winner_8[slice91],\r\n",
        "       train_regret_winner_9[slice91],\r\n",
        "       train_regret_winner_10[slice91],\r\n",
        "       train_regret_winner_11[slice91],\r\n",
        "       train_regret_winner_12[slice91],\r\n",
        "       train_regret_winner_13[slice91],\r\n",
        "       train_regret_winner_14[slice91],\r\n",
        "       train_regret_winner_15[slice91],\r\n",
        "       train_regret_winner_16[slice91],\r\n",
        "       train_regret_winner_17[slice91],\r\n",
        "       train_regret_winner_18[slice91],\r\n",
        "       train_regret_winner_19[slice91],\r\n",
        "       train_regret_winner_20[slice91]]\r\n",
        "\r\n",
        "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\r\n",
        "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\r\n",
        "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\r\n",
        "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\r\n",
        "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\r\n",
        "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVVeft6steXv"
      },
      "source": [
        "# Iteration101 :\r\n",
        "\r\n",
        "slice101 = 100\r\n",
        "\r\n",
        "loser101 = [train_regret_loser_1[slice101],\r\n",
        "       train_regret_loser_2[slice101],\r\n",
        "       train_regret_loser_3[slice101],\r\n",
        "       train_regret_loser_4[slice101],\r\n",
        "       train_regret_loser_5[slice101],\r\n",
        "       train_regret_loser_6[slice101],\r\n",
        "       train_regret_loser_7[slice101],\r\n",
        "       train_regret_loser_8[slice101],\r\n",
        "       train_regret_loser_9[slice101],\r\n",
        "       train_regret_loser_10[slice101],\r\n",
        "       train_regret_loser_11[slice101],\r\n",
        "       train_regret_loser_12[slice101],\r\n",
        "       train_regret_loser_13[slice101],\r\n",
        "       train_regret_loser_14[slice101],\r\n",
        "       train_regret_loser_15[slice101],\r\n",
        "       train_regret_loser_16[slice101],\r\n",
        "       train_regret_loser_17[slice101],\r\n",
        "       train_regret_loser_18[slice101],\r\n",
        "       train_regret_loser_19[slice101],\r\n",
        "       train_regret_loser_20[slice101]]\r\n",
        "\r\n",
        "winner101 = [train_regret_winner_1[slice101],\r\n",
        "       train_regret_winner_2[slice101],\r\n",
        "       train_regret_winner_3[slice101],\r\n",
        "       train_regret_winner_4[slice101],\r\n",
        "       train_regret_winner_5[slice101],\r\n",
        "       train_regret_winner_6[slice101],\r\n",
        "       train_regret_winner_7[slice101],\r\n",
        "       train_regret_winner_8[slice101],\r\n",
        "       train_regret_winner_9[slice101],\r\n",
        "       train_regret_winner_10[slice101],\r\n",
        "       train_regret_winner_11[slice101],\r\n",
        "       train_regret_winner_12[slice101],\r\n",
        "       train_regret_winner_13[slice101],\r\n",
        "       train_regret_winner_14[slice101],\r\n",
        "       train_regret_winner_15[slice101],\r\n",
        "       train_regret_winner_16[slice101],\r\n",
        "       train_regret_winner_17[slice101],\r\n",
        "       train_regret_winner_18[slice101],\r\n",
        "       train_regret_winner_19[slice101],\r\n",
        "       train_regret_winner_20[slice101]]\r\n",
        "\r\n",
        "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\r\n",
        "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\r\n",
        "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\r\n",
        "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\r\n",
        "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\r\n",
        "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maqbtTXXtebT"
      },
      "source": [
        "# Iteration2 :\r\n",
        "\r\n",
        "slice2 = 1\r\n",
        "\r\n",
        "loser2 = [train_regret_loser_1[slice2],\r\n",
        "       train_regret_loser_2[slice2],\r\n",
        "       train_regret_loser_3[slice2],\r\n",
        "       train_regret_loser_4[slice2],\r\n",
        "       train_regret_loser_5[slice2],\r\n",
        "       train_regret_loser_6[slice2],\r\n",
        "       train_regret_loser_7[slice2],\r\n",
        "       train_regret_loser_8[slice2],\r\n",
        "       train_regret_loser_9[slice2],\r\n",
        "       train_regret_loser_10[slice2],\r\n",
        "       train_regret_loser_11[slice2],\r\n",
        "       train_regret_loser_12[slice2],\r\n",
        "       train_regret_loser_13[slice2],\r\n",
        "       train_regret_loser_14[slice2],\r\n",
        "       train_regret_loser_15[slice2],\r\n",
        "       train_regret_loser_16[slice2],\r\n",
        "       train_regret_loser_17[slice2],\r\n",
        "       train_regret_loser_18[slice2],\r\n",
        "       train_regret_loser_19[slice2],\r\n",
        "       train_regret_loser_20[slice2]]\r\n",
        "\r\n",
        "winner2 = [train_regret_winner_1[slice2],\r\n",
        "       train_regret_winner_2[slice2],\r\n",
        "       train_regret_winner_3[slice2],\r\n",
        "       train_regret_winner_4[slice2],\r\n",
        "       train_regret_winner_5[slice2],\r\n",
        "       train_regret_winner_6[slice2],\r\n",
        "       train_regret_winner_7[slice2],\r\n",
        "       train_regret_winner_8[slice2],\r\n",
        "       train_regret_winner_9[slice2],\r\n",
        "       train_regret_winner_10[slice2],\r\n",
        "       train_regret_winner_11[slice2],\r\n",
        "       train_regret_winner_12[slice2],\r\n",
        "       train_regret_winner_13[slice2],\r\n",
        "       train_regret_winner_14[slice2],\r\n",
        "       train_regret_winner_15[slice2],\r\n",
        "       train_regret_winner_16[slice2],\r\n",
        "       train_regret_winner_17[slice2],\r\n",
        "       train_regret_winner_18[slice2],\r\n",
        "       train_regret_winner_19[slice2],\r\n",
        "       train_regret_winner_20[slice2]]\r\n",
        "\r\n",
        "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\r\n",
        "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\r\n",
        "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\r\n",
        "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\r\n",
        "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\r\n",
        "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIBh0dQw-ZM"
      },
      "source": [
        "# Iteration12 :\r\n",
        "\r\n",
        "slice12 = 11\r\n",
        "\r\n",
        "loser12 = [train_regret_loser_1[slice12],\r\n",
        "       train_regret_loser_2[slice12],\r\n",
        "       train_regret_loser_3[slice12],\r\n",
        "       train_regret_loser_4[slice12],\r\n",
        "       train_regret_loser_5[slice12],\r\n",
        "       train_regret_loser_6[slice12],\r\n",
        "       train_regret_loser_7[slice12],\r\n",
        "       train_regret_loser_8[slice12],\r\n",
        "       train_regret_loser_9[slice12],\r\n",
        "       train_regret_loser_10[slice12],\r\n",
        "       train_regret_loser_11[slice12],\r\n",
        "       train_regret_loser_12[slice12],\r\n",
        "       train_regret_loser_13[slice12],\r\n",
        "       train_regret_loser_14[slice12],\r\n",
        "       train_regret_loser_15[slice12],\r\n",
        "       train_regret_loser_16[slice12],\r\n",
        "       train_regret_loser_17[slice12],\r\n",
        "       train_regret_loser_18[slice12],\r\n",
        "       train_regret_loser_19[slice12],\r\n",
        "       train_regret_loser_20[slice12]]\r\n",
        "\r\n",
        "winner12 = [train_regret_winner_1[slice12],\r\n",
        "       train_regret_winner_2[slice12],\r\n",
        "       train_regret_winner_3[slice12],\r\n",
        "       train_regret_winner_4[slice12],\r\n",
        "       train_regret_winner_5[slice12],\r\n",
        "       train_regret_winner_6[slice12],\r\n",
        "       train_regret_winner_7[slice12],\r\n",
        "       train_regret_winner_8[slice12],\r\n",
        "       train_regret_winner_9[slice12],\r\n",
        "       train_regret_winner_10[slice12],\r\n",
        "       train_regret_winner_11[slice12],\r\n",
        "       train_regret_winner_12[slice12],\r\n",
        "       train_regret_winner_13[slice12],\r\n",
        "       train_regret_winner_14[slice12],\r\n",
        "       train_regret_winner_15[slice12],\r\n",
        "       train_regret_winner_16[slice12],\r\n",
        "       train_regret_winner_17[slice12],\r\n",
        "       train_regret_winner_18[slice12],\r\n",
        "       train_regret_winner_19[slice12],\r\n",
        "       train_regret_winner_20[slice12]]\r\n",
        "\r\n",
        "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\r\n",
        "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\r\n",
        "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\r\n",
        "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\r\n",
        "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjIhsei3w-cV"
      },
      "source": [
        "# Iteration22 :\r\n",
        "\r\n",
        "slice22 = 21\r\n",
        "\r\n",
        "loser22 = [train_regret_loser_1[slice22],\r\n",
        "       train_regret_loser_2[slice22],\r\n",
        "       train_regret_loser_3[slice22],\r\n",
        "       train_regret_loser_4[slice22],\r\n",
        "       train_regret_loser_5[slice22],\r\n",
        "       train_regret_loser_6[slice22],\r\n",
        "       train_regret_loser_7[slice22],\r\n",
        "       train_regret_loser_8[slice22],\r\n",
        "       train_regret_loser_9[slice22],\r\n",
        "       train_regret_loser_10[slice22],\r\n",
        "       train_regret_loser_11[slice22],\r\n",
        "       train_regret_loser_12[slice22],\r\n",
        "       train_regret_loser_13[slice22],\r\n",
        "       train_regret_loser_14[slice22],\r\n",
        "       train_regret_loser_15[slice22],\r\n",
        "       train_regret_loser_16[slice22],\r\n",
        "       train_regret_loser_17[slice22],\r\n",
        "       train_regret_loser_18[slice22],\r\n",
        "       train_regret_loser_19[slice22],\r\n",
        "       train_regret_loser_20[slice22]]\r\n",
        "\r\n",
        "winner22 = [train_regret_winner_1[slice22],\r\n",
        "       train_regret_winner_2[slice22],\r\n",
        "       train_regret_winner_3[slice22],\r\n",
        "       train_regret_winner_4[slice22],\r\n",
        "       train_regret_winner_5[slice22],\r\n",
        "       train_regret_winner_6[slice22],\r\n",
        "       train_regret_winner_7[slice22],\r\n",
        "       train_regret_winner_8[slice22],\r\n",
        "       train_regret_winner_9[slice22],\r\n",
        "       train_regret_winner_10[slice22],\r\n",
        "       train_regret_winner_11[slice22],\r\n",
        "       train_regret_winner_12[slice22],\r\n",
        "       train_regret_winner_13[slice22],\r\n",
        "       train_regret_winner_14[slice22],\r\n",
        "       train_regret_winner_15[slice22],\r\n",
        "       train_regret_winner_16[slice22],\r\n",
        "       train_regret_winner_17[slice22],\r\n",
        "       train_regret_winner_18[slice22],\r\n",
        "       train_regret_winner_19[slice22],\r\n",
        "       train_regret_winner_20[slice22]]\r\n",
        "\r\n",
        "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\r\n",
        "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\r\n",
        "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\r\n",
        "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\r\n",
        "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\r\n",
        "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur9p9GkHw-fq"
      },
      "source": [
        "# Iteration32 :\r\n",
        "\r\n",
        "slice32 = 31\r\n",
        "\r\n",
        "loser32 = [train_regret_loser_1[slice32],\r\n",
        "       train_regret_loser_2[slice32],\r\n",
        "       train_regret_loser_3[slice32],\r\n",
        "       train_regret_loser_4[slice32],\r\n",
        "       train_regret_loser_5[slice32],\r\n",
        "       train_regret_loser_6[slice32],\r\n",
        "       train_regret_loser_7[slice32],\r\n",
        "       train_regret_loser_8[slice32],\r\n",
        "       train_regret_loser_9[slice32],\r\n",
        "       train_regret_loser_10[slice32],\r\n",
        "       train_regret_loser_11[slice32],\r\n",
        "       train_regret_loser_12[slice32],\r\n",
        "       train_regret_loser_13[slice32],\r\n",
        "       train_regret_loser_14[slice32],\r\n",
        "       train_regret_loser_15[slice32],\r\n",
        "       train_regret_loser_16[slice32],\r\n",
        "       train_regret_loser_17[slice32],\r\n",
        "       train_regret_loser_18[slice32],\r\n",
        "       train_regret_loser_19[slice32],\r\n",
        "       train_regret_loser_20[slice32]]\r\n",
        "\r\n",
        "winner32 = [train_regret_winner_1[slice32],\r\n",
        "       train_regret_winner_2[slice32],\r\n",
        "       train_regret_winner_3[slice32],\r\n",
        "       train_regret_winner_4[slice32],\r\n",
        "       train_regret_winner_5[slice32],\r\n",
        "       train_regret_winner_6[slice32],\r\n",
        "       train_regret_winner_7[slice32],\r\n",
        "       train_regret_winner_8[slice32],\r\n",
        "       train_regret_winner_9[slice32],\r\n",
        "       train_regret_winner_10[slice32],\r\n",
        "       train_regret_winner_11[slice32],\r\n",
        "       train_regret_winner_12[slice32],\r\n",
        "       train_regret_winner_13[slice32],\r\n",
        "       train_regret_winner_14[slice32],\r\n",
        "       train_regret_winner_15[slice32],\r\n",
        "       train_regret_winner_16[slice32],\r\n",
        "       train_regret_winner_17[slice32],\r\n",
        "       train_regret_winner_18[slice32],\r\n",
        "       train_regret_winner_19[slice32],\r\n",
        "       train_regret_winner_20[slice32]]\r\n",
        "\r\n",
        "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\r\n",
        "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\r\n",
        "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\r\n",
        "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\r\n",
        "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\r\n",
        "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_rzHB8w-hy"
      },
      "source": [
        "# Iteration42 :\r\n",
        "\r\n",
        "slice42 = 41\r\n",
        "\r\n",
        "loser42 = [train_regret_loser_1[slice42],\r\n",
        "       train_regret_loser_2[slice42],\r\n",
        "       train_regret_loser_3[slice42],\r\n",
        "       train_regret_loser_4[slice42],\r\n",
        "       train_regret_loser_5[slice42],\r\n",
        "       train_regret_loser_6[slice42],\r\n",
        "       train_regret_loser_7[slice42],\r\n",
        "       train_regret_loser_8[slice42],\r\n",
        "       train_regret_loser_9[slice42],\r\n",
        "       train_regret_loser_10[slice42],\r\n",
        "       train_regret_loser_11[slice42],\r\n",
        "       train_regret_loser_12[slice42],\r\n",
        "       train_regret_loser_13[slice42],\r\n",
        "       train_regret_loser_14[slice42],\r\n",
        "       train_regret_loser_15[slice42],\r\n",
        "       train_regret_loser_16[slice42],\r\n",
        "       train_regret_loser_17[slice42],\r\n",
        "       train_regret_loser_18[slice42],\r\n",
        "       train_regret_loser_19[slice42],\r\n",
        "       train_regret_loser_20[slice42]]\r\n",
        "\r\n",
        "winner42 = [train_regret_winner_1[slice42],\r\n",
        "       train_regret_winner_2[slice42],\r\n",
        "       train_regret_winner_3[slice42],\r\n",
        "       train_regret_winner_4[slice42],\r\n",
        "       train_regret_winner_5[slice42],\r\n",
        "       train_regret_winner_6[slice42],\r\n",
        "       train_regret_winner_7[slice42],\r\n",
        "       train_regret_winner_8[slice42],\r\n",
        "       train_regret_winner_9[slice42],\r\n",
        "       train_regret_winner_10[slice42],\r\n",
        "       train_regret_winner_11[slice42],\r\n",
        "       train_regret_winner_12[slice42],\r\n",
        "       train_regret_winner_13[slice42],\r\n",
        "       train_regret_winner_14[slice42],\r\n",
        "       train_regret_winner_15[slice42],\r\n",
        "       train_regret_winner_16[slice42],\r\n",
        "       train_regret_winner_17[slice42],\r\n",
        "       train_regret_winner_18[slice42],\r\n",
        "       train_regret_winner_19[slice42],\r\n",
        "       train_regret_winner_20[slice42]]\r\n",
        "\r\n",
        "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\r\n",
        "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\r\n",
        "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\r\n",
        "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\r\n",
        "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\r\n",
        "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKUUG6vAw-kz"
      },
      "source": [
        "# Iteration52 :\r\n",
        "\r\n",
        "slice52 = 51\r\n",
        "\r\n",
        "loser52 = [train_regret_loser_1[slice52],\r\n",
        "       train_regret_loser_2[slice52],\r\n",
        "       train_regret_loser_3[slice52],\r\n",
        "       train_regret_loser_4[slice52],\r\n",
        "       train_regret_loser_5[slice52],\r\n",
        "       train_regret_loser_6[slice52],\r\n",
        "       train_regret_loser_7[slice52],\r\n",
        "       train_regret_loser_8[slice52],\r\n",
        "       train_regret_loser_9[slice52],\r\n",
        "       train_regret_loser_10[slice52],\r\n",
        "       train_regret_loser_11[slice52],\r\n",
        "       train_regret_loser_12[slice52],\r\n",
        "       train_regret_loser_13[slice52],\r\n",
        "       train_regret_loser_14[slice52],\r\n",
        "       train_regret_loser_15[slice52],\r\n",
        "       train_regret_loser_16[slice52],\r\n",
        "       train_regret_loser_17[slice52],\r\n",
        "       train_regret_loser_18[slice52],\r\n",
        "       train_regret_loser_19[slice52],\r\n",
        "       train_regret_loser_20[slice52]]\r\n",
        "\r\n",
        "winner52 = [train_regret_winner_1[slice52],\r\n",
        "       train_regret_winner_2[slice52],\r\n",
        "       train_regret_winner_3[slice52],\r\n",
        "       train_regret_winner_4[slice52],\r\n",
        "       train_regret_winner_5[slice52],\r\n",
        "       train_regret_winner_6[slice52],\r\n",
        "       train_regret_winner_7[slice52],\r\n",
        "       train_regret_winner_8[slice52],\r\n",
        "       train_regret_winner_9[slice52],\r\n",
        "       train_regret_winner_10[slice52],\r\n",
        "       train_regret_winner_11[slice52],\r\n",
        "       train_regret_winner_12[slice52],\r\n",
        "       train_regret_winner_13[slice52],\r\n",
        "       train_regret_winner_14[slice52],\r\n",
        "       train_regret_winner_15[slice52],\r\n",
        "       train_regret_winner_16[slice52],\r\n",
        "       train_regret_winner_17[slice52],\r\n",
        "       train_regret_winner_18[slice52],\r\n",
        "       train_regret_winner_19[slice52],\r\n",
        "       train_regret_winner_20[slice52]]\r\n",
        "\r\n",
        "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\r\n",
        "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\r\n",
        "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\r\n",
        "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\r\n",
        "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\r\n",
        "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9cJNkew-nY"
      },
      "source": [
        "# Iteration62 :\r\n",
        "\r\n",
        "slice62 = 61\r\n",
        "\r\n",
        "loser62 = [train_regret_loser_1[slice62],\r\n",
        "       train_regret_loser_2[slice62],\r\n",
        "       train_regret_loser_3[slice62],\r\n",
        "       train_regret_loser_4[slice62],\r\n",
        "       train_regret_loser_5[slice62],\r\n",
        "       train_regret_loser_6[slice62],\r\n",
        "       train_regret_loser_7[slice62],\r\n",
        "       train_regret_loser_8[slice62],\r\n",
        "       train_regret_loser_9[slice62],\r\n",
        "       train_regret_loser_10[slice62],\r\n",
        "       train_regret_loser_11[slice62],\r\n",
        "       train_regret_loser_12[slice62],\r\n",
        "       train_regret_loser_13[slice62],\r\n",
        "       train_regret_loser_14[slice62],\r\n",
        "       train_regret_loser_15[slice62],\r\n",
        "       train_regret_loser_16[slice62],\r\n",
        "       train_regret_loser_17[slice62],\r\n",
        "       train_regret_loser_18[slice62],\r\n",
        "       train_regret_loser_19[slice62],\r\n",
        "       train_regret_loser_20[slice62]]\r\n",
        "\r\n",
        "winner62 = [train_regret_winner_1[slice62],\r\n",
        "       train_regret_winner_2[slice62],\r\n",
        "       train_regret_winner_3[slice62],\r\n",
        "       train_regret_winner_4[slice62],\r\n",
        "       train_regret_winner_5[slice62],\r\n",
        "       train_regret_winner_6[slice62],\r\n",
        "       train_regret_winner_7[slice62],\r\n",
        "       train_regret_winner_8[slice62],\r\n",
        "       train_regret_winner_9[slice62],\r\n",
        "       train_regret_winner_10[slice62],\r\n",
        "       train_regret_winner_11[slice62],\r\n",
        "       train_regret_winner_12[slice62],\r\n",
        "       train_regret_winner_13[slice62],\r\n",
        "       train_regret_winner_14[slice62],\r\n",
        "       train_regret_winner_15[slice62],\r\n",
        "       train_regret_winner_16[slice62],\r\n",
        "       train_regret_winner_17[slice62],\r\n",
        "       train_regret_winner_18[slice62],\r\n",
        "       train_regret_winner_19[slice62],\r\n",
        "       train_regret_winner_20[slice62]]\r\n",
        "\r\n",
        "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\r\n",
        "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\r\n",
        "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\r\n",
        "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\r\n",
        "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\r\n",
        "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DZ8kHHmw-qK"
      },
      "source": [
        "# Iteration72 :\r\n",
        "\r\n",
        "slice72 = 71\r\n",
        "\r\n",
        "loser72 = [train_regret_loser_1[slice72],\r\n",
        "       train_regret_loser_2[slice72],\r\n",
        "       train_regret_loser_3[slice72],\r\n",
        "       train_regret_loser_4[slice72],\r\n",
        "       train_regret_loser_5[slice72],\r\n",
        "       train_regret_loser_6[slice72],\r\n",
        "       train_regret_loser_7[slice72],\r\n",
        "       train_regret_loser_8[slice72],\r\n",
        "       train_regret_loser_9[slice72],\r\n",
        "       train_regret_loser_10[slice72],\r\n",
        "       train_regret_loser_11[slice72],\r\n",
        "       train_regret_loser_12[slice72],\r\n",
        "       train_regret_loser_13[slice72],\r\n",
        "       train_regret_loser_14[slice72],\r\n",
        "       train_regret_loser_15[slice72],\r\n",
        "       train_regret_loser_16[slice72],\r\n",
        "       train_regret_loser_17[slice72],\r\n",
        "       train_regret_loser_18[slice72],\r\n",
        "       train_regret_loser_19[slice72],\r\n",
        "       train_regret_loser_20[slice72]]\r\n",
        "\r\n",
        "winner72 = [train_regret_winner_1[slice72],\r\n",
        "       train_regret_winner_2[slice72],\r\n",
        "       train_regret_winner_3[slice72],\r\n",
        "       train_regret_winner_4[slice72],\r\n",
        "       train_regret_winner_5[slice72],\r\n",
        "       train_regret_winner_6[slice72],\r\n",
        "       train_regret_winner_7[slice72],\r\n",
        "       train_regret_winner_8[slice72],\r\n",
        "       train_regret_winner_9[slice72],\r\n",
        "       train_regret_winner_10[slice72],\r\n",
        "       train_regret_winner_11[slice72],\r\n",
        "       train_regret_winner_12[slice72],\r\n",
        "       train_regret_winner_13[slice72],\r\n",
        "       train_regret_winner_14[slice72],\r\n",
        "       train_regret_winner_15[slice72],\r\n",
        "       train_regret_winner_16[slice72],\r\n",
        "       train_regret_winner_17[slice72],\r\n",
        "       train_regret_winner_18[slice72],\r\n",
        "       train_regret_winner_19[slice72],\r\n",
        "       train_regret_winner_20[slice72]]\r\n",
        "\r\n",
        "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\r\n",
        "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\r\n",
        "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\r\n",
        "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\r\n",
        "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\r\n",
        "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBzBvnVyw-sz"
      },
      "source": [
        "# Iteration82 :\r\n",
        "\r\n",
        "slice82 = 81\r\n",
        "\r\n",
        "loser82 = [train_regret_loser_1[slice82],\r\n",
        "       train_regret_loser_2[slice82],\r\n",
        "       train_regret_loser_3[slice82],\r\n",
        "       train_regret_loser_4[slice82],\r\n",
        "       train_regret_loser_5[slice82],\r\n",
        "       train_regret_loser_6[slice82],\r\n",
        "       train_regret_loser_7[slice82],\r\n",
        "       train_regret_loser_8[slice82],\r\n",
        "       train_regret_loser_9[slice82],\r\n",
        "       train_regret_loser_10[slice82],\r\n",
        "       train_regret_loser_11[slice82],\r\n",
        "       train_regret_loser_12[slice82],\r\n",
        "       train_regret_loser_13[slice82],\r\n",
        "       train_regret_loser_14[slice82],\r\n",
        "       train_regret_loser_15[slice82],\r\n",
        "       train_regret_loser_16[slice82],\r\n",
        "       train_regret_loser_17[slice82],\r\n",
        "       train_regret_loser_18[slice82],\r\n",
        "       train_regret_loser_19[slice82],\r\n",
        "       train_regret_loser_20[slice82]]\r\n",
        "\r\n",
        "winner82 = [train_regret_winner_1[slice82],\r\n",
        "       train_regret_winner_2[slice82],\r\n",
        "       train_regret_winner_3[slice82],\r\n",
        "       train_regret_winner_4[slice82],\r\n",
        "       train_regret_winner_5[slice82],\r\n",
        "       train_regret_winner_6[slice82],\r\n",
        "       train_regret_winner_7[slice82],\r\n",
        "       train_regret_winner_8[slice82],\r\n",
        "       train_regret_winner_9[slice82],\r\n",
        "       train_regret_winner_10[slice82],\r\n",
        "       train_regret_winner_11[slice82],\r\n",
        "       train_regret_winner_12[slice82],\r\n",
        "       train_regret_winner_13[slice82],\r\n",
        "       train_regret_winner_14[slice82],\r\n",
        "       train_regret_winner_15[slice82],\r\n",
        "       train_regret_winner_16[slice82],\r\n",
        "       train_regret_winner_17[slice82],\r\n",
        "       train_regret_winner_18[slice82],\r\n",
        "       train_regret_winner_19[slice82],\r\n",
        "       train_regret_winner_20[slice82]]\r\n",
        "\r\n",
        "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\r\n",
        "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\r\n",
        "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\r\n",
        "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\r\n",
        "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\r\n",
        "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qj--n6Bw-y2"
      },
      "source": [
        "# Iteration92 :\r\n",
        "\r\n",
        "slice92 = 91\r\n",
        "\r\n",
        "loser92 = [train_regret_loser_1[slice92],\r\n",
        "       train_regret_loser_2[slice92],\r\n",
        "       train_regret_loser_3[slice92],\r\n",
        "       train_regret_loser_4[slice92],\r\n",
        "       train_regret_loser_5[slice92],\r\n",
        "       train_regret_loser_6[slice92],\r\n",
        "       train_regret_loser_7[slice92],\r\n",
        "       train_regret_loser_8[slice92],\r\n",
        "       train_regret_loser_9[slice92],\r\n",
        "       train_regret_loser_10[slice92],\r\n",
        "       train_regret_loser_11[slice92],\r\n",
        "       train_regret_loser_12[slice92],\r\n",
        "       train_regret_loser_13[slice92],\r\n",
        "       train_regret_loser_14[slice92],\r\n",
        "       train_regret_loser_15[slice92],\r\n",
        "       train_regret_loser_16[slice92],\r\n",
        "       train_regret_loser_17[slice92],\r\n",
        "       train_regret_loser_18[slice92],\r\n",
        "       train_regret_loser_19[slice92],\r\n",
        "       train_regret_loser_20[slice92]]\r\n",
        "\r\n",
        "winner92 = [train_regret_winner_1[slice92],\r\n",
        "       train_regret_winner_2[slice92],\r\n",
        "       train_regret_winner_3[slice92],\r\n",
        "       train_regret_winner_4[slice92],\r\n",
        "       train_regret_winner_5[slice92],\r\n",
        "       train_regret_winner_6[slice92],\r\n",
        "       train_regret_winner_7[slice92],\r\n",
        "       train_regret_winner_8[slice92],\r\n",
        "       train_regret_winner_9[slice92],\r\n",
        "       train_regret_winner_10[slice92],\r\n",
        "       train_regret_winner_11[slice92],\r\n",
        "       train_regret_winner_12[slice92],\r\n",
        "       train_regret_winner_13[slice92],\r\n",
        "       train_regret_winner_14[slice92],\r\n",
        "       train_regret_winner_15[slice92],\r\n",
        "       train_regret_winner_16[slice92],\r\n",
        "       train_regret_winner_17[slice92],\r\n",
        "       train_regret_winner_18[slice92],\r\n",
        "       train_regret_winner_19[slice92],\r\n",
        "       train_regret_winner_20[slice92]]\r\n",
        "\r\n",
        "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\r\n",
        "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\r\n",
        "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\r\n",
        "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\r\n",
        "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\r\n",
        "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJ5KqLdw-0L"
      },
      "source": [
        "# Iteration3 :\r\n",
        "\r\n",
        "slice3 = 2\r\n",
        "\r\n",
        "loser3 = [train_regret_loser_1[slice3],\r\n",
        "       train_regret_loser_2[slice3],\r\n",
        "       train_regret_loser_3[slice3],\r\n",
        "       train_regret_loser_4[slice3],\r\n",
        "       train_regret_loser_5[slice3],\r\n",
        "       train_regret_loser_6[slice3],\r\n",
        "       train_regret_loser_7[slice3],\r\n",
        "       train_regret_loser_8[slice3],\r\n",
        "       train_regret_loser_9[slice3],\r\n",
        "       train_regret_loser_10[slice3],\r\n",
        "       train_regret_loser_11[slice3],\r\n",
        "       train_regret_loser_12[slice3],\r\n",
        "       train_regret_loser_13[slice3],\r\n",
        "       train_regret_loser_14[slice3],\r\n",
        "       train_regret_loser_15[slice3],\r\n",
        "       train_regret_loser_16[slice3],\r\n",
        "       train_regret_loser_17[slice3],\r\n",
        "       train_regret_loser_18[slice3],\r\n",
        "       train_regret_loser_19[slice3],\r\n",
        "       train_regret_loser_20[slice3]]\r\n",
        "\r\n",
        "winner3 = [train_regret_winner_1[slice3],\r\n",
        "       train_regret_winner_2[slice3],\r\n",
        "       train_regret_winner_3[slice3],\r\n",
        "       train_regret_winner_4[slice3],\r\n",
        "       train_regret_winner_5[slice3],\r\n",
        "       train_regret_winner_6[slice3],\r\n",
        "       train_regret_winner_7[slice3],\r\n",
        "       train_regret_winner_8[slice3],\r\n",
        "       train_regret_winner_9[slice3],\r\n",
        "       train_regret_winner_10[slice3],\r\n",
        "       train_regret_winner_11[slice3],\r\n",
        "       train_regret_winner_12[slice3],\r\n",
        "       train_regret_winner_13[slice3],\r\n",
        "       train_regret_winner_14[slice3],\r\n",
        "       train_regret_winner_15[slice3],\r\n",
        "       train_regret_winner_16[slice3],\r\n",
        "       train_regret_winner_17[slice3],\r\n",
        "       train_regret_winner_18[slice3],\r\n",
        "       train_regret_winner_19[slice3],\r\n",
        "       train_regret_winner_20[slice3]]\r\n",
        "\r\n",
        "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\r\n",
        "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\r\n",
        "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\r\n",
        "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\r\n",
        "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\r\n",
        "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkaoT1dyw-16"
      },
      "source": [
        "# Iteration13 :\r\n",
        "\r\n",
        "slice13 = 12\r\n",
        "\r\n",
        "loser13 = [train_regret_loser_1[slice13],\r\n",
        "       train_regret_loser_2[slice13],\r\n",
        "       train_regret_loser_3[slice13],\r\n",
        "       train_regret_loser_4[slice13],\r\n",
        "       train_regret_loser_5[slice13],\r\n",
        "       train_regret_loser_6[slice13],\r\n",
        "       train_regret_loser_7[slice13],\r\n",
        "       train_regret_loser_8[slice13],\r\n",
        "       train_regret_loser_9[slice13],\r\n",
        "       train_regret_loser_10[slice13],\r\n",
        "       train_regret_loser_11[slice13],\r\n",
        "       train_regret_loser_12[slice13],\r\n",
        "       train_regret_loser_13[slice13],\r\n",
        "       train_regret_loser_14[slice13],\r\n",
        "       train_regret_loser_15[slice13],\r\n",
        "       train_regret_loser_16[slice13],\r\n",
        "       train_regret_loser_17[slice13],\r\n",
        "       train_regret_loser_18[slice13],\r\n",
        "       train_regret_loser_19[slice13],\r\n",
        "       train_regret_loser_20[slice13]]\r\n",
        "\r\n",
        "winner13 = [train_regret_winner_1[slice13],\r\n",
        "       train_regret_winner_2[slice13],\r\n",
        "       train_regret_winner_3[slice13],\r\n",
        "       train_regret_winner_4[slice13],\r\n",
        "       train_regret_winner_5[slice13],\r\n",
        "       train_regret_winner_6[slice13],\r\n",
        "       train_regret_winner_7[slice13],\r\n",
        "       train_regret_winner_8[slice13],\r\n",
        "       train_regret_winner_9[slice13],\r\n",
        "       train_regret_winner_10[slice13],\r\n",
        "       train_regret_winner_11[slice13],\r\n",
        "       train_regret_winner_12[slice13],\r\n",
        "       train_regret_winner_13[slice13],\r\n",
        "       train_regret_winner_14[slice13],\r\n",
        "       train_regret_winner_15[slice13],\r\n",
        "       train_regret_winner_16[slice13],\r\n",
        "       train_regret_winner_17[slice13],\r\n",
        "       train_regret_winner_18[slice13],\r\n",
        "       train_regret_winner_19[slice13],\r\n",
        "       train_regret_winner_20[slice13]]\r\n",
        "\r\n",
        "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\r\n",
        "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\r\n",
        "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\r\n",
        "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\r\n",
        "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZbVMrBaw-4B"
      },
      "source": [
        "# Iteration23 :\r\n",
        "\r\n",
        "slice23 = 22\r\n",
        "\r\n",
        "loser23 = [train_regret_loser_1[slice23],\r\n",
        "       train_regret_loser_2[slice23],\r\n",
        "       train_regret_loser_3[slice23],\r\n",
        "       train_regret_loser_4[slice23],\r\n",
        "       train_regret_loser_5[slice23],\r\n",
        "       train_regret_loser_6[slice23],\r\n",
        "       train_regret_loser_7[slice23],\r\n",
        "       train_regret_loser_8[slice23],\r\n",
        "       train_regret_loser_9[slice23],\r\n",
        "       train_regret_loser_10[slice23],\r\n",
        "       train_regret_loser_11[slice23],\r\n",
        "       train_regret_loser_12[slice23],\r\n",
        "       train_regret_loser_13[slice23],\r\n",
        "       train_regret_loser_14[slice23],\r\n",
        "       train_regret_loser_15[slice23],\r\n",
        "       train_regret_loser_16[slice23],\r\n",
        "       train_regret_loser_17[slice23],\r\n",
        "       train_regret_loser_18[slice23],\r\n",
        "       train_regret_loser_19[slice23],\r\n",
        "       train_regret_loser_20[slice23]]\r\n",
        "\r\n",
        "winner23 = [train_regret_winner_1[slice23],\r\n",
        "       train_regret_winner_2[slice23],\r\n",
        "       train_regret_winner_3[slice23],\r\n",
        "       train_regret_winner_4[slice23],\r\n",
        "       train_regret_winner_5[slice23],\r\n",
        "       train_regret_winner_6[slice23],\r\n",
        "       train_regret_winner_7[slice23],\r\n",
        "       train_regret_winner_8[slice23],\r\n",
        "       train_regret_winner_9[slice23],\r\n",
        "       train_regret_winner_10[slice23],\r\n",
        "       train_regret_winner_11[slice23],\r\n",
        "       train_regret_winner_12[slice23],\r\n",
        "       train_regret_winner_13[slice23],\r\n",
        "       train_regret_winner_14[slice23],\r\n",
        "       train_regret_winner_15[slice23],\r\n",
        "       train_regret_winner_16[slice23],\r\n",
        "       train_regret_winner_17[slice23],\r\n",
        "       train_regret_winner_18[slice23],\r\n",
        "       train_regret_winner_19[slice23],\r\n",
        "       train_regret_winner_20[slice23]]\r\n",
        "\r\n",
        "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\r\n",
        "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\r\n",
        "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\r\n",
        "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\r\n",
        "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\r\n",
        "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUa6HRCUw-7G"
      },
      "source": [
        "# Iteration33 :\r\n",
        "\r\n",
        "slice33 = 32\r\n",
        "\r\n",
        "loser33 = [train_regret_loser_1[slice33],\r\n",
        "       train_regret_loser_2[slice33],\r\n",
        "       train_regret_loser_3[slice33],\r\n",
        "       train_regret_loser_4[slice33],\r\n",
        "       train_regret_loser_5[slice33],\r\n",
        "       train_regret_loser_6[slice33],\r\n",
        "       train_regret_loser_7[slice33],\r\n",
        "       train_regret_loser_8[slice33],\r\n",
        "       train_regret_loser_9[slice33],\r\n",
        "       train_regret_loser_10[slice33],\r\n",
        "       train_regret_loser_11[slice33],\r\n",
        "       train_regret_loser_12[slice33],\r\n",
        "       train_regret_loser_13[slice33],\r\n",
        "       train_regret_loser_14[slice33],\r\n",
        "       train_regret_loser_15[slice33],\r\n",
        "       train_regret_loser_16[slice33],\r\n",
        "       train_regret_loser_17[slice33],\r\n",
        "       train_regret_loser_18[slice33],\r\n",
        "       train_regret_loser_19[slice33],\r\n",
        "       train_regret_loser_20[slice33]]\r\n",
        "\r\n",
        "winner33 = [train_regret_winner_1[slice33],\r\n",
        "       train_regret_winner_2[slice33],\r\n",
        "       train_regret_winner_3[slice33],\r\n",
        "       train_regret_winner_4[slice33],\r\n",
        "       train_regret_winner_5[slice33],\r\n",
        "       train_regret_winner_6[slice33],\r\n",
        "       train_regret_winner_7[slice33],\r\n",
        "       train_regret_winner_8[slice33],\r\n",
        "       train_regret_winner_9[slice33],\r\n",
        "       train_regret_winner_10[slice33],\r\n",
        "       train_regret_winner_11[slice33],\r\n",
        "       train_regret_winner_12[slice33],\r\n",
        "       train_regret_winner_13[slice33],\r\n",
        "       train_regret_winner_14[slice33],\r\n",
        "       train_regret_winner_15[slice33],\r\n",
        "       train_regret_winner_16[slice33],\r\n",
        "       train_regret_winner_17[slice33],\r\n",
        "       train_regret_winner_18[slice33],\r\n",
        "       train_regret_winner_19[slice33],\r\n",
        "       train_regret_winner_20[slice33]]\r\n",
        "\r\n",
        "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\r\n",
        "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\r\n",
        "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\r\n",
        "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\r\n",
        "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\r\n",
        "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EVxQwsxw-9n"
      },
      "source": [
        "# Iteration43 :\r\n",
        "\r\n",
        "slice43 = 42\r\n",
        "\r\n",
        "loser43 = [train_regret_loser_1[slice43],\r\n",
        "       train_regret_loser_2[slice43],\r\n",
        "       train_regret_loser_3[slice43],\r\n",
        "       train_regret_loser_4[slice43],\r\n",
        "       train_regret_loser_5[slice43],\r\n",
        "       train_regret_loser_6[slice43],\r\n",
        "       train_regret_loser_7[slice43],\r\n",
        "       train_regret_loser_8[slice43],\r\n",
        "       train_regret_loser_9[slice43],\r\n",
        "       train_regret_loser_10[slice43],\r\n",
        "       train_regret_loser_11[slice43],\r\n",
        "       train_regret_loser_12[slice43],\r\n",
        "       train_regret_loser_13[slice43],\r\n",
        "       train_regret_loser_14[slice43],\r\n",
        "       train_regret_loser_15[slice43],\r\n",
        "       train_regret_loser_16[slice43],\r\n",
        "       train_regret_loser_17[slice43],\r\n",
        "       train_regret_loser_18[slice43],\r\n",
        "       train_regret_loser_19[slice43],\r\n",
        "       train_regret_loser_20[slice43]]\r\n",
        "\r\n",
        "winner43 = [train_regret_winner_1[slice43],\r\n",
        "       train_regret_winner_2[slice43],\r\n",
        "       train_regret_winner_3[slice43],\r\n",
        "       train_regret_winner_4[slice43],\r\n",
        "       train_regret_winner_5[slice43],\r\n",
        "       train_regret_winner_6[slice43],\r\n",
        "       train_regret_winner_7[slice43],\r\n",
        "       train_regret_winner_8[slice43],\r\n",
        "       train_regret_winner_9[slice43],\r\n",
        "       train_regret_winner_10[slice43],\r\n",
        "       train_regret_winner_11[slice43],\r\n",
        "       train_regret_winner_12[slice43],\r\n",
        "       train_regret_winner_13[slice43],\r\n",
        "       train_regret_winner_14[slice43],\r\n",
        "       train_regret_winner_15[slice43],\r\n",
        "       train_regret_winner_16[slice43],\r\n",
        "       train_regret_winner_17[slice43],\r\n",
        "       train_regret_winner_18[slice43],\r\n",
        "       train_regret_winner_19[slice43],\r\n",
        "       train_regret_winner_20[slice43]]\r\n",
        "\r\n",
        "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\r\n",
        "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\r\n",
        "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\r\n",
        "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\r\n",
        "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\r\n",
        "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQtRyMv-w_AZ"
      },
      "source": [
        "# Iteration53 :\r\n",
        "\r\n",
        "slice53 = 52\r\n",
        "\r\n",
        "loser53 = [train_regret_loser_1[slice53],\r\n",
        "       train_regret_loser_2[slice53],\r\n",
        "       train_regret_loser_3[slice53],\r\n",
        "       train_regret_loser_4[slice53],\r\n",
        "       train_regret_loser_5[slice53],\r\n",
        "       train_regret_loser_6[slice53],\r\n",
        "       train_regret_loser_7[slice53],\r\n",
        "       train_regret_loser_8[slice53],\r\n",
        "       train_regret_loser_9[slice53],\r\n",
        "       train_regret_loser_10[slice53],\r\n",
        "       train_regret_loser_11[slice53],\r\n",
        "       train_regret_loser_12[slice53],\r\n",
        "       train_regret_loser_13[slice53],\r\n",
        "       train_regret_loser_14[slice53],\r\n",
        "       train_regret_loser_15[slice53],\r\n",
        "       train_regret_loser_16[slice53],\r\n",
        "       train_regret_loser_17[slice53],\r\n",
        "       train_regret_loser_18[slice53],\r\n",
        "       train_regret_loser_19[slice53],\r\n",
        "       train_regret_loser_20[slice53]]\r\n",
        "\r\n",
        "winner53 = [train_regret_winner_1[slice53],\r\n",
        "       train_regret_winner_2[slice53],\r\n",
        "       train_regret_winner_3[slice53],\r\n",
        "       train_regret_winner_4[slice53],\r\n",
        "       train_regret_winner_5[slice53],\r\n",
        "       train_regret_winner_6[slice53],\r\n",
        "       train_regret_winner_7[slice53],\r\n",
        "       train_regret_winner_8[slice53],\r\n",
        "       train_regret_winner_9[slice53],\r\n",
        "       train_regret_winner_10[slice53],\r\n",
        "       train_regret_winner_11[slice53],\r\n",
        "       train_regret_winner_12[slice53],\r\n",
        "       train_regret_winner_13[slice53],\r\n",
        "       train_regret_winner_14[slice53],\r\n",
        "       train_regret_winner_15[slice53],\r\n",
        "       train_regret_winner_16[slice53],\r\n",
        "       train_regret_winner_17[slice53],\r\n",
        "       train_regret_winner_18[slice53],\r\n",
        "       train_regret_winner_19[slice53],\r\n",
        "       train_regret_winner_20[slice53]]\r\n",
        "\r\n",
        "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\r\n",
        "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\r\n",
        "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\r\n",
        "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\r\n",
        "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\r\n",
        "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CIMEXXYw_DR"
      },
      "source": [
        "# Iteration63 :\r\n",
        "\r\n",
        "slice63 = 62\r\n",
        "\r\n",
        "loser63 = [train_regret_loser_1[slice63],\r\n",
        "       train_regret_loser_2[slice63],\r\n",
        "       train_regret_loser_3[slice63],\r\n",
        "       train_regret_loser_4[slice63],\r\n",
        "       train_regret_loser_5[slice63],\r\n",
        "       train_regret_loser_6[slice63],\r\n",
        "       train_regret_loser_7[slice63],\r\n",
        "       train_regret_loser_8[slice63],\r\n",
        "       train_regret_loser_9[slice63],\r\n",
        "       train_regret_loser_10[slice63],\r\n",
        "       train_regret_loser_11[slice63],\r\n",
        "       train_regret_loser_12[slice63],\r\n",
        "       train_regret_loser_13[slice63],\r\n",
        "       train_regret_loser_14[slice63],\r\n",
        "       train_regret_loser_15[slice63],\r\n",
        "       train_regret_loser_16[slice63],\r\n",
        "       train_regret_loser_17[slice63],\r\n",
        "       train_regret_loser_18[slice63],\r\n",
        "       train_regret_loser_19[slice63],\r\n",
        "       train_regret_loser_20[slice63]]\r\n",
        "\r\n",
        "winner63 = [train_regret_winner_1[slice63],\r\n",
        "       train_regret_winner_2[slice63],\r\n",
        "       train_regret_winner_3[slice63],\r\n",
        "       train_regret_winner_4[slice63],\r\n",
        "       train_regret_winner_5[slice63],\r\n",
        "       train_regret_winner_6[slice63],\r\n",
        "       train_regret_winner_7[slice63],\r\n",
        "       train_regret_winner_8[slice63],\r\n",
        "       train_regret_winner_9[slice63],\r\n",
        "       train_regret_winner_10[slice63],\r\n",
        "       train_regret_winner_11[slice63],\r\n",
        "       train_regret_winner_12[slice63],\r\n",
        "       train_regret_winner_13[slice63],\r\n",
        "       train_regret_winner_14[slice63],\r\n",
        "       train_regret_winner_15[slice63],\r\n",
        "       train_regret_winner_16[slice63],\r\n",
        "       train_regret_winner_17[slice63],\r\n",
        "       train_regret_winner_18[slice63],\r\n",
        "       train_regret_winner_19[slice63],\r\n",
        "       train_regret_winner_20[slice63]]\r\n",
        "\r\n",
        "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\r\n",
        "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\r\n",
        "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\r\n",
        "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\r\n",
        "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\r\n",
        "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg4GfwU_w_GF"
      },
      "source": [
        "# Iteration73 :\r\n",
        "\r\n",
        "slice73 = 72\r\n",
        "\r\n",
        "loser73 = [train_regret_loser_1[slice73],\r\n",
        "       train_regret_loser_2[slice73],\r\n",
        "       train_regret_loser_3[slice73],\r\n",
        "       train_regret_loser_4[slice73],\r\n",
        "       train_regret_loser_5[slice73],\r\n",
        "       train_regret_loser_6[slice73],\r\n",
        "       train_regret_loser_7[slice73],\r\n",
        "       train_regret_loser_8[slice73],\r\n",
        "       train_regret_loser_9[slice73],\r\n",
        "       train_regret_loser_10[slice73],\r\n",
        "       train_regret_loser_11[slice73],\r\n",
        "       train_regret_loser_12[slice73],\r\n",
        "       train_regret_loser_13[slice73],\r\n",
        "       train_regret_loser_14[slice73],\r\n",
        "       train_regret_loser_15[slice73],\r\n",
        "       train_regret_loser_16[slice73],\r\n",
        "       train_regret_loser_17[slice73],\r\n",
        "       train_regret_loser_18[slice73],\r\n",
        "       train_regret_loser_19[slice73],\r\n",
        "       train_regret_loser_20[slice73]]\r\n",
        "\r\n",
        "winner73 = [train_regret_winner_1[slice73],\r\n",
        "       train_regret_winner_2[slice73],\r\n",
        "       train_regret_winner_3[slice73],\r\n",
        "       train_regret_winner_4[slice73],\r\n",
        "       train_regret_winner_5[slice73],\r\n",
        "       train_regret_winner_6[slice73],\r\n",
        "       train_regret_winner_7[slice73],\r\n",
        "       train_regret_winner_8[slice73],\r\n",
        "       train_regret_winner_9[slice73],\r\n",
        "       train_regret_winner_10[slice73],\r\n",
        "       train_regret_winner_11[slice73],\r\n",
        "       train_regret_winner_12[slice73],\r\n",
        "       train_regret_winner_13[slice73],\r\n",
        "       train_regret_winner_14[slice73],\r\n",
        "       train_regret_winner_15[slice73],\r\n",
        "       train_regret_winner_16[slice73],\r\n",
        "       train_regret_winner_17[slice73],\r\n",
        "       train_regret_winner_18[slice73],\r\n",
        "       train_regret_winner_19[slice73],\r\n",
        "       train_regret_winner_20[slice73]]\r\n",
        "\r\n",
        "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\r\n",
        "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\r\n",
        "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\r\n",
        "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\r\n",
        "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\r\n",
        "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WmpcEV4w_I9"
      },
      "source": [
        "# Iteration83 :\r\n",
        "\r\n",
        "slice83 = 82\r\n",
        "\r\n",
        "loser83 = [train_regret_loser_1[slice83],\r\n",
        "       train_regret_loser_2[slice83],\r\n",
        "       train_regret_loser_3[slice83],\r\n",
        "       train_regret_loser_4[slice83],\r\n",
        "       train_regret_loser_5[slice83],\r\n",
        "       train_regret_loser_6[slice83],\r\n",
        "       train_regret_loser_7[slice83],\r\n",
        "       train_regret_loser_8[slice83],\r\n",
        "       train_regret_loser_9[slice83],\r\n",
        "       train_regret_loser_10[slice83],\r\n",
        "       train_regret_loser_11[slice83],\r\n",
        "       train_regret_loser_12[slice83],\r\n",
        "       train_regret_loser_13[slice83],\r\n",
        "       train_regret_loser_14[slice83],\r\n",
        "       train_regret_loser_15[slice83],\r\n",
        "       train_regret_loser_16[slice83],\r\n",
        "       train_regret_loser_17[slice83],\r\n",
        "       train_regret_loser_18[slice83],\r\n",
        "       train_regret_loser_19[slice83],\r\n",
        "       train_regret_loser_20[slice83]]\r\n",
        "\r\n",
        "winner83 = [train_regret_winner_1[slice83],\r\n",
        "       train_regret_winner_2[slice83],\r\n",
        "       train_regret_winner_3[slice83],\r\n",
        "       train_regret_winner_4[slice83],\r\n",
        "       train_regret_winner_5[slice83],\r\n",
        "       train_regret_winner_6[slice83],\r\n",
        "       train_regret_winner_7[slice83],\r\n",
        "       train_regret_winner_8[slice83],\r\n",
        "       train_regret_winner_9[slice83],\r\n",
        "       train_regret_winner_10[slice83],\r\n",
        "       train_regret_winner_11[slice83],\r\n",
        "       train_regret_winner_12[slice83],\r\n",
        "       train_regret_winner_13[slice83],\r\n",
        "       train_regret_winner_14[slice83],\r\n",
        "       train_regret_winner_15[slice83],\r\n",
        "       train_regret_winner_16[slice83],\r\n",
        "       train_regret_winner_17[slice83],\r\n",
        "       train_regret_winner_18[slice83],\r\n",
        "       train_regret_winner_19[slice83],\r\n",
        "       train_regret_winner_20[slice83]]\r\n",
        "\r\n",
        "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\r\n",
        "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\r\n",
        "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\r\n",
        "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\r\n",
        "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\r\n",
        "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGPwoBZyw_MF"
      },
      "source": [
        "# Iteration93 :\r\n",
        "\r\n",
        "slice93 = 92\r\n",
        "\r\n",
        "loser93 = [train_regret_loser_1[slice93],\r\n",
        "       train_regret_loser_2[slice93],\r\n",
        "       train_regret_loser_3[slice93],\r\n",
        "       train_regret_loser_4[slice93],\r\n",
        "       train_regret_loser_5[slice93],\r\n",
        "       train_regret_loser_6[slice93],\r\n",
        "       train_regret_loser_7[slice93],\r\n",
        "       train_regret_loser_8[slice93],\r\n",
        "       train_regret_loser_9[slice93],\r\n",
        "       train_regret_loser_10[slice93],\r\n",
        "       train_regret_loser_11[slice93],\r\n",
        "       train_regret_loser_12[slice93],\r\n",
        "       train_regret_loser_13[slice93],\r\n",
        "       train_regret_loser_14[slice93],\r\n",
        "       train_regret_loser_15[slice93],\r\n",
        "       train_regret_loser_16[slice93],\r\n",
        "       train_regret_loser_17[slice93],\r\n",
        "       train_regret_loser_18[slice93],\r\n",
        "       train_regret_loser_19[slice93],\r\n",
        "       train_regret_loser_20[slice93]]\r\n",
        "\r\n",
        "winner93 = [train_regret_winner_1[slice93],\r\n",
        "       train_regret_winner_2[slice93],\r\n",
        "       train_regret_winner_3[slice93],\r\n",
        "       train_regret_winner_4[slice93],\r\n",
        "       train_regret_winner_5[slice93],\r\n",
        "       train_regret_winner_6[slice93],\r\n",
        "       train_regret_winner_7[slice93],\r\n",
        "       train_regret_winner_8[slice93],\r\n",
        "       train_regret_winner_9[slice93],\r\n",
        "       train_regret_winner_10[slice93],\r\n",
        "       train_regret_winner_11[slice93],\r\n",
        "       train_regret_winner_12[slice93],\r\n",
        "       train_regret_winner_13[slice93],\r\n",
        "       train_regret_winner_14[slice93],\r\n",
        "       train_regret_winner_15[slice93],\r\n",
        "       train_regret_winner_16[slice93],\r\n",
        "       train_regret_winner_17[slice93],\r\n",
        "       train_regret_winner_18[slice93],\r\n",
        "       train_regret_winner_19[slice93],\r\n",
        "       train_regret_winner_20[slice93]]\r\n",
        "\r\n",
        "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\r\n",
        "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\r\n",
        "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\r\n",
        "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\r\n",
        "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\r\n",
        "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ghdGzPw_Oq"
      },
      "source": [
        "# Iteration4 :\r\n",
        "\r\n",
        "slice4 = 3\r\n",
        "\r\n",
        "loser4 = [train_regret_loser_1[slice4],\r\n",
        "       train_regret_loser_2[slice4],\r\n",
        "       train_regret_loser_3[slice4],\r\n",
        "       train_regret_loser_4[slice4],\r\n",
        "       train_regret_loser_5[slice4],\r\n",
        "       train_regret_loser_6[slice4],\r\n",
        "       train_regret_loser_7[slice4],\r\n",
        "       train_regret_loser_8[slice4],\r\n",
        "       train_regret_loser_9[slice4],\r\n",
        "       train_regret_loser_10[slice4],\r\n",
        "       train_regret_loser_11[slice4],\r\n",
        "       train_regret_loser_12[slice4],\r\n",
        "       train_regret_loser_13[slice4],\r\n",
        "       train_regret_loser_14[slice4],\r\n",
        "       train_regret_loser_15[slice4],\r\n",
        "       train_regret_loser_16[slice4],\r\n",
        "       train_regret_loser_17[slice4],\r\n",
        "       train_regret_loser_18[slice4],\r\n",
        "       train_regret_loser_19[slice4],\r\n",
        "       train_regret_loser_20[slice4]]\r\n",
        "\r\n",
        "winner4 = [train_regret_winner_1[slice4],\r\n",
        "       train_regret_winner_2[slice4],\r\n",
        "       train_regret_winner_3[slice4],\r\n",
        "       train_regret_winner_4[slice4],\r\n",
        "       train_regret_winner_5[slice4],\r\n",
        "       train_regret_winner_6[slice4],\r\n",
        "       train_regret_winner_7[slice4],\r\n",
        "       train_regret_winner_8[slice4],\r\n",
        "       train_regret_winner_9[slice4],\r\n",
        "       train_regret_winner_10[slice4],\r\n",
        "       train_regret_winner_11[slice4],\r\n",
        "       train_regret_winner_12[slice4],\r\n",
        "       train_regret_winner_13[slice4],\r\n",
        "       train_regret_winner_14[slice4],\r\n",
        "       train_regret_winner_15[slice4],\r\n",
        "       train_regret_winner_16[slice4],\r\n",
        "       train_regret_winner_17[slice4],\r\n",
        "       train_regret_winner_18[slice4],\r\n",
        "       train_regret_winner_19[slice4],\r\n",
        "       train_regret_winner_20[slice4]]\r\n",
        "\r\n",
        "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\r\n",
        "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\r\n",
        "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\r\n",
        "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\r\n",
        "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\r\n",
        "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNvDt4QMw_Rz"
      },
      "source": [
        "# Iteration14 :\r\n",
        "\r\n",
        "slice14 = 13\r\n",
        "\r\n",
        "loser14 = [train_regret_loser_1[slice14],\r\n",
        "       train_regret_loser_2[slice14],\r\n",
        "       train_regret_loser_3[slice14],\r\n",
        "       train_regret_loser_4[slice14],\r\n",
        "       train_regret_loser_5[slice14],\r\n",
        "       train_regret_loser_6[slice14],\r\n",
        "       train_regret_loser_7[slice14],\r\n",
        "       train_regret_loser_8[slice14],\r\n",
        "       train_regret_loser_9[slice14],\r\n",
        "       train_regret_loser_10[slice14],\r\n",
        "       train_regret_loser_11[slice14],\r\n",
        "       train_regret_loser_12[slice14],\r\n",
        "       train_regret_loser_13[slice14],\r\n",
        "       train_regret_loser_14[slice14],\r\n",
        "       train_regret_loser_15[slice14],\r\n",
        "       train_regret_loser_16[slice14],\r\n",
        "       train_regret_loser_17[slice14],\r\n",
        "       train_regret_loser_18[slice14],\r\n",
        "       train_regret_loser_19[slice14],\r\n",
        "       train_regret_loser_20[slice14]]\r\n",
        "\r\n",
        "winner14 = [train_regret_winner_1[slice14],\r\n",
        "       train_regret_winner_2[slice14],\r\n",
        "       train_regret_winner_3[slice14],\r\n",
        "       train_regret_winner_4[slice14],\r\n",
        "       train_regret_winner_5[slice14],\r\n",
        "       train_regret_winner_6[slice14],\r\n",
        "       train_regret_winner_7[slice14],\r\n",
        "       train_regret_winner_8[slice14],\r\n",
        "       train_regret_winner_9[slice14],\r\n",
        "       train_regret_winner_10[slice14],\r\n",
        "       train_regret_winner_11[slice14],\r\n",
        "       train_regret_winner_12[slice14],\r\n",
        "       train_regret_winner_13[slice14],\r\n",
        "       train_regret_winner_14[slice14],\r\n",
        "       train_regret_winner_15[slice14],\r\n",
        "       train_regret_winner_16[slice14],\r\n",
        "       train_regret_winner_17[slice14],\r\n",
        "       train_regret_winner_18[slice14],\r\n",
        "       train_regret_winner_19[slice14],\r\n",
        "       train_regret_winner_20[slice14]]\r\n",
        "\r\n",
        "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\r\n",
        "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\r\n",
        "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\r\n",
        "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\r\n",
        "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\r\n",
        "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3kMQV0w_Ua"
      },
      "source": [
        "# Iteration24 :\r\n",
        "\r\n",
        "slice24 = 23\r\n",
        "\r\n",
        "loser24 = [train_regret_loser_1[slice24],\r\n",
        "       train_regret_loser_2[slice24],\r\n",
        "       train_regret_loser_3[slice24],\r\n",
        "       train_regret_loser_4[slice24],\r\n",
        "       train_regret_loser_5[slice24],\r\n",
        "       train_regret_loser_6[slice24],\r\n",
        "       train_regret_loser_7[slice24],\r\n",
        "       train_regret_loser_8[slice24],\r\n",
        "       train_regret_loser_9[slice24],\r\n",
        "       train_regret_loser_10[slice24],\r\n",
        "       train_regret_loser_11[slice24],\r\n",
        "       train_regret_loser_12[slice24],\r\n",
        "       train_regret_loser_13[slice24],\r\n",
        "       train_regret_loser_14[slice24],\r\n",
        "       train_regret_loser_15[slice24],\r\n",
        "       train_regret_loser_16[slice24],\r\n",
        "       train_regret_loser_17[slice24],\r\n",
        "       train_regret_loser_18[slice24],\r\n",
        "       train_regret_loser_19[slice24],\r\n",
        "       train_regret_loser_20[slice24]]\r\n",
        "\r\n",
        "winner24 = [train_regret_winner_1[slice24],\r\n",
        "       train_regret_winner_2[slice24],\r\n",
        "       train_regret_winner_3[slice24],\r\n",
        "       train_regret_winner_4[slice24],\r\n",
        "       train_regret_winner_5[slice24],\r\n",
        "       train_regret_winner_6[slice24],\r\n",
        "       train_regret_winner_7[slice24],\r\n",
        "       train_regret_winner_8[slice24],\r\n",
        "       train_regret_winner_9[slice24],\r\n",
        "       train_regret_winner_10[slice24],\r\n",
        "       train_regret_winner_11[slice24],\r\n",
        "       train_regret_winner_12[slice24],\r\n",
        "       train_regret_winner_13[slice24],\r\n",
        "       train_regret_winner_14[slice24],\r\n",
        "       train_regret_winner_15[slice24],\r\n",
        "       train_regret_winner_16[slice24],\r\n",
        "       train_regret_winner_17[slice24],\r\n",
        "       train_regret_winner_18[slice24],\r\n",
        "       train_regret_winner_19[slice24],\r\n",
        "       train_regret_winner_20[slice24]]\r\n",
        "\r\n",
        "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\r\n",
        "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\r\n",
        "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\r\n",
        "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\r\n",
        "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\r\n",
        "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ljkyMfw_Xj"
      },
      "source": [
        "# Iteration34 :\r\n",
        "\r\n",
        "slice34 = 33\r\n",
        "\r\n",
        "loser34 = [train_regret_loser_1[slice34],\r\n",
        "       train_regret_loser_2[slice34],\r\n",
        "       train_regret_loser_3[slice34],\r\n",
        "       train_regret_loser_4[slice34],\r\n",
        "       train_regret_loser_5[slice34],\r\n",
        "       train_regret_loser_6[slice34],\r\n",
        "       train_regret_loser_7[slice34],\r\n",
        "       train_regret_loser_8[slice34],\r\n",
        "       train_regret_loser_9[slice34],\r\n",
        "       train_regret_loser_10[slice34],\r\n",
        "       train_regret_loser_11[slice34],\r\n",
        "       train_regret_loser_12[slice34],\r\n",
        "       train_regret_loser_13[slice34],\r\n",
        "       train_regret_loser_14[slice34],\r\n",
        "       train_regret_loser_15[slice34],\r\n",
        "       train_regret_loser_16[slice34],\r\n",
        "       train_regret_loser_17[slice34],\r\n",
        "       train_regret_loser_18[slice34],\r\n",
        "       train_regret_loser_19[slice34],\r\n",
        "       train_regret_loser_20[slice34]]\r\n",
        "\r\n",
        "winner34 = [train_regret_winner_1[slice34],\r\n",
        "       train_regret_winner_2[slice34],\r\n",
        "       train_regret_winner_3[slice34],\r\n",
        "       train_regret_winner_4[slice34],\r\n",
        "       train_regret_winner_5[slice34],\r\n",
        "       train_regret_winner_6[slice34],\r\n",
        "       train_regret_winner_7[slice34],\r\n",
        "       train_regret_winner_8[slice34],\r\n",
        "       train_regret_winner_9[slice34],\r\n",
        "       train_regret_winner_10[slice34],\r\n",
        "       train_regret_winner_11[slice34],\r\n",
        "       train_regret_winner_12[slice34],\r\n",
        "       train_regret_winner_13[slice34],\r\n",
        "       train_regret_winner_14[slice34],\r\n",
        "       train_regret_winner_15[slice34],\r\n",
        "       train_regret_winner_16[slice34],\r\n",
        "       train_regret_winner_17[slice34],\r\n",
        "       train_regret_winner_18[slice34],\r\n",
        "       train_regret_winner_19[slice34],\r\n",
        "       train_regret_winner_20[slice34]]\r\n",
        "\r\n",
        "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\r\n",
        "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\r\n",
        "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\r\n",
        "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\r\n",
        "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\r\n",
        "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGKj-NR8w_ae"
      },
      "source": [
        "# Iteration44 :\r\n",
        "\r\n",
        "slice44 = 43\r\n",
        "\r\n",
        "loser44 = [train_regret_loser_1[slice44],\r\n",
        "       train_regret_loser_2[slice44],\r\n",
        "       train_regret_loser_3[slice44],\r\n",
        "       train_regret_loser_4[slice44],\r\n",
        "       train_regret_loser_5[slice44],\r\n",
        "       train_regret_loser_6[slice44],\r\n",
        "       train_regret_loser_7[slice44],\r\n",
        "       train_regret_loser_8[slice44],\r\n",
        "       train_regret_loser_9[slice44],\r\n",
        "       train_regret_loser_10[slice44],\r\n",
        "       train_regret_loser_11[slice44],\r\n",
        "       train_regret_loser_12[slice44],\r\n",
        "       train_regret_loser_13[slice44],\r\n",
        "       train_regret_loser_14[slice44],\r\n",
        "       train_regret_loser_15[slice44],\r\n",
        "       train_regret_loser_16[slice44],\r\n",
        "       train_regret_loser_17[slice44],\r\n",
        "       train_regret_loser_18[slice44],\r\n",
        "       train_regret_loser_19[slice44],\r\n",
        "       train_regret_loser_20[slice44]]\r\n",
        "\r\n",
        "winner44 = [train_regret_winner_1[slice44],\r\n",
        "       train_regret_winner_2[slice44],\r\n",
        "       train_regret_winner_3[slice44],\r\n",
        "       train_regret_winner_4[slice44],\r\n",
        "       train_regret_winner_5[slice44],\r\n",
        "       train_regret_winner_6[slice44],\r\n",
        "       train_regret_winner_7[slice44],\r\n",
        "       train_regret_winner_8[slice44],\r\n",
        "       train_regret_winner_9[slice44],\r\n",
        "       train_regret_winner_10[slice44],\r\n",
        "       train_regret_winner_11[slice44],\r\n",
        "       train_regret_winner_12[slice44],\r\n",
        "       train_regret_winner_13[slice44],\r\n",
        "       train_regret_winner_14[slice44],\r\n",
        "       train_regret_winner_15[slice44],\r\n",
        "       train_regret_winner_16[slice44],\r\n",
        "       train_regret_winner_17[slice44],\r\n",
        "       train_regret_winner_18[slice44],\r\n",
        "       train_regret_winner_19[slice44],\r\n",
        "       train_regret_winner_20[slice44]]\r\n",
        "\r\n",
        "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\r\n",
        "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\r\n",
        "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\r\n",
        "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\r\n",
        "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\r\n",
        "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJzx094Fw_c_"
      },
      "source": [
        "# Iteration54 :\r\n",
        "\r\n",
        "slice54 = 53\r\n",
        "\r\n",
        "loser54 = [train_regret_loser_1[slice54],\r\n",
        "       train_regret_loser_2[slice54],\r\n",
        "       train_regret_loser_3[slice54],\r\n",
        "       train_regret_loser_4[slice54],\r\n",
        "       train_regret_loser_5[slice54],\r\n",
        "       train_regret_loser_6[slice54],\r\n",
        "       train_regret_loser_7[slice54],\r\n",
        "       train_regret_loser_8[slice54],\r\n",
        "       train_regret_loser_9[slice54],\r\n",
        "       train_regret_loser_10[slice54],\r\n",
        "       train_regret_loser_11[slice54],\r\n",
        "       train_regret_loser_12[slice54],\r\n",
        "       train_regret_loser_13[slice54],\r\n",
        "       train_regret_loser_14[slice54],\r\n",
        "       train_regret_loser_15[slice54],\r\n",
        "       train_regret_loser_16[slice54],\r\n",
        "       train_regret_loser_17[slice54],\r\n",
        "       train_regret_loser_18[slice54],\r\n",
        "       train_regret_loser_19[slice54],\r\n",
        "       train_regret_loser_20[slice54]]\r\n",
        "\r\n",
        "winner54 = [train_regret_winner_1[slice54],\r\n",
        "       train_regret_winner_2[slice54],\r\n",
        "       train_regret_winner_3[slice54],\r\n",
        "       train_regret_winner_4[slice54],\r\n",
        "       train_regret_winner_5[slice54],\r\n",
        "       train_regret_winner_6[slice54],\r\n",
        "       train_regret_winner_7[slice54],\r\n",
        "       train_regret_winner_8[slice54],\r\n",
        "       train_regret_winner_9[slice54],\r\n",
        "       train_regret_winner_10[slice54],\r\n",
        "       train_regret_winner_11[slice54],\r\n",
        "       train_regret_winner_12[slice54],\r\n",
        "       train_regret_winner_13[slice54],\r\n",
        "       train_regret_winner_14[slice54],\r\n",
        "       train_regret_winner_15[slice54],\r\n",
        "       train_regret_winner_16[slice54],\r\n",
        "       train_regret_winner_17[slice54],\r\n",
        "       train_regret_winner_18[slice54],\r\n",
        "       train_regret_winner_19[slice54],\r\n",
        "       train_regret_winner_20[slice54]]\r\n",
        "\r\n",
        "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\r\n",
        "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\r\n",
        "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\r\n",
        "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\r\n",
        "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\r\n",
        "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEVYMqj_w_fr"
      },
      "source": [
        "# Iteration64 :\r\n",
        "\r\n",
        "slice64 = 63\r\n",
        "\r\n",
        "loser64 = [train_regret_loser_1[slice64],\r\n",
        "       train_regret_loser_2[slice64],\r\n",
        "       train_regret_loser_3[slice64],\r\n",
        "       train_regret_loser_4[slice64],\r\n",
        "       train_regret_loser_5[slice64],\r\n",
        "       train_regret_loser_6[slice64],\r\n",
        "       train_regret_loser_7[slice64],\r\n",
        "       train_regret_loser_8[slice64],\r\n",
        "       train_regret_loser_9[slice64],\r\n",
        "       train_regret_loser_10[slice64],\r\n",
        "       train_regret_loser_11[slice64],\r\n",
        "       train_regret_loser_12[slice64],\r\n",
        "       train_regret_loser_13[slice64],\r\n",
        "       train_regret_loser_14[slice64],\r\n",
        "       train_regret_loser_15[slice64],\r\n",
        "       train_regret_loser_16[slice64],\r\n",
        "       train_regret_loser_17[slice64],\r\n",
        "       train_regret_loser_18[slice64],\r\n",
        "       train_regret_loser_19[slice64],\r\n",
        "       train_regret_loser_20[slice64]]\r\n",
        "\r\n",
        "winner64 = [train_regret_winner_1[slice64],\r\n",
        "       train_regret_winner_2[slice64],\r\n",
        "       train_regret_winner_3[slice64],\r\n",
        "       train_regret_winner_4[slice64],\r\n",
        "       train_regret_winner_5[slice64],\r\n",
        "       train_regret_winner_6[slice64],\r\n",
        "       train_regret_winner_7[slice64],\r\n",
        "       train_regret_winner_8[slice64],\r\n",
        "       train_regret_winner_9[slice64],\r\n",
        "       train_regret_winner_10[slice64],\r\n",
        "       train_regret_winner_11[slice64],\r\n",
        "       train_regret_winner_12[slice64],\r\n",
        "       train_regret_winner_13[slice64],\r\n",
        "       train_regret_winner_14[slice64],\r\n",
        "       train_regret_winner_15[slice64],\r\n",
        "       train_regret_winner_16[slice64],\r\n",
        "       train_regret_winner_17[slice64],\r\n",
        "       train_regret_winner_18[slice64],\r\n",
        "       train_regret_winner_19[slice64],\r\n",
        "       train_regret_winner_20[slice64]]\r\n",
        "\r\n",
        "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\r\n",
        "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\r\n",
        "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\r\n",
        "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\r\n",
        "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\r\n",
        "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvr3S_p0w_ii"
      },
      "source": [
        "# Iteration74 :\r\n",
        "\r\n",
        "slice74 = 73\r\n",
        "\r\n",
        "loser74 = [train_regret_loser_1[slice74],\r\n",
        "       train_regret_loser_2[slice74],\r\n",
        "       train_regret_loser_3[slice74],\r\n",
        "       train_regret_loser_4[slice74],\r\n",
        "       train_regret_loser_5[slice74],\r\n",
        "       train_regret_loser_6[slice74],\r\n",
        "       train_regret_loser_7[slice74],\r\n",
        "       train_regret_loser_8[slice74],\r\n",
        "       train_regret_loser_9[slice74],\r\n",
        "       train_regret_loser_10[slice74],\r\n",
        "       train_regret_loser_11[slice74],\r\n",
        "       train_regret_loser_12[slice74],\r\n",
        "       train_regret_loser_13[slice74],\r\n",
        "       train_regret_loser_14[slice74],\r\n",
        "       train_regret_loser_15[slice74],\r\n",
        "       train_regret_loser_16[slice74],\r\n",
        "       train_regret_loser_17[slice74],\r\n",
        "       train_regret_loser_18[slice74],\r\n",
        "       train_regret_loser_19[slice74],\r\n",
        "       train_regret_loser_20[slice74]]\r\n",
        "\r\n",
        "winner74 = [train_regret_winner_1[slice74],\r\n",
        "       train_regret_winner_2[slice74],\r\n",
        "       train_regret_winner_3[slice74],\r\n",
        "       train_regret_winner_4[slice74],\r\n",
        "       train_regret_winner_5[slice74],\r\n",
        "       train_regret_winner_6[slice74],\r\n",
        "       train_regret_winner_7[slice74],\r\n",
        "       train_regret_winner_8[slice74],\r\n",
        "       train_regret_winner_9[slice74],\r\n",
        "       train_regret_winner_10[slice74],\r\n",
        "       train_regret_winner_11[slice74],\r\n",
        "       train_regret_winner_12[slice74],\r\n",
        "       train_regret_winner_13[slice74],\r\n",
        "       train_regret_winner_14[slice74],\r\n",
        "       train_regret_winner_15[slice74],\r\n",
        "       train_regret_winner_16[slice74],\r\n",
        "       train_regret_winner_17[slice74],\r\n",
        "       train_regret_winner_18[slice74],\r\n",
        "       train_regret_winner_19[slice74],\r\n",
        "       train_regret_winner_20[slice74]]\r\n",
        "\r\n",
        "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\r\n",
        "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\r\n",
        "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\r\n",
        "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\r\n",
        "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\r\n",
        "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEAvUVK7w_lO"
      },
      "source": [
        "# Iteration84 :\r\n",
        "\r\n",
        "slice84 = 83\r\n",
        "\r\n",
        "loser84 = [train_regret_loser_1[slice84],\r\n",
        "       train_regret_loser_2[slice84],\r\n",
        "       train_regret_loser_3[slice84],\r\n",
        "       train_regret_loser_4[slice84],\r\n",
        "       train_regret_loser_5[slice84],\r\n",
        "       train_regret_loser_6[slice84],\r\n",
        "       train_regret_loser_7[slice84],\r\n",
        "       train_regret_loser_8[slice84],\r\n",
        "       train_regret_loser_9[slice84],\r\n",
        "       train_regret_loser_10[slice84],\r\n",
        "       train_regret_loser_11[slice84],\r\n",
        "       train_regret_loser_12[slice84],\r\n",
        "       train_regret_loser_13[slice84],\r\n",
        "       train_regret_loser_14[slice84],\r\n",
        "       train_regret_loser_15[slice84],\r\n",
        "       train_regret_loser_16[slice84],\r\n",
        "       train_regret_loser_17[slice84],\r\n",
        "       train_regret_loser_18[slice84],\r\n",
        "       train_regret_loser_19[slice84],\r\n",
        "       train_regret_loser_20[slice84]]\r\n",
        "\r\n",
        "winner84 = [train_regret_winner_1[slice84],\r\n",
        "       train_regret_winner_2[slice84],\r\n",
        "       train_regret_winner_3[slice84],\r\n",
        "       train_regret_winner_4[slice84],\r\n",
        "       train_regret_winner_5[slice84],\r\n",
        "       train_regret_winner_6[slice84],\r\n",
        "       train_regret_winner_7[slice84],\r\n",
        "       train_regret_winner_8[slice84],\r\n",
        "       train_regret_winner_9[slice84],\r\n",
        "       train_regret_winner_10[slice84],\r\n",
        "       train_regret_winner_11[slice84],\r\n",
        "       train_regret_winner_12[slice84],\r\n",
        "       train_regret_winner_13[slice84],\r\n",
        "       train_regret_winner_14[slice84],\r\n",
        "       train_regret_winner_15[slice84],\r\n",
        "       train_regret_winner_16[slice84],\r\n",
        "       train_regret_winner_17[slice84],\r\n",
        "       train_regret_winner_18[slice84],\r\n",
        "       train_regret_winner_19[slice84],\r\n",
        "       train_regret_winner_20[slice84]]\r\n",
        "\r\n",
        "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\r\n",
        "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\r\n",
        "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\r\n",
        "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\r\n",
        "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\r\n",
        "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMZzhdGw_oC"
      },
      "source": [
        "# Iteration94 :\r\n",
        "\r\n",
        "slice94 = 93\r\n",
        "\r\n",
        "loser94 = [train_regret_loser_1[slice94],\r\n",
        "       train_regret_loser_2[slice94],\r\n",
        "       train_regret_loser_3[slice94],\r\n",
        "       train_regret_loser_4[slice94],\r\n",
        "       train_regret_loser_5[slice94],\r\n",
        "       train_regret_loser_6[slice94],\r\n",
        "       train_regret_loser_7[slice94],\r\n",
        "       train_regret_loser_8[slice94],\r\n",
        "       train_regret_loser_9[slice94],\r\n",
        "       train_regret_loser_10[slice94],\r\n",
        "       train_regret_loser_11[slice94],\r\n",
        "       train_regret_loser_12[slice94],\r\n",
        "       train_regret_loser_13[slice94],\r\n",
        "       train_regret_loser_14[slice94],\r\n",
        "       train_regret_loser_15[slice94],\r\n",
        "       train_regret_loser_16[slice94],\r\n",
        "       train_regret_loser_17[slice94],\r\n",
        "       train_regret_loser_18[slice94],\r\n",
        "       train_regret_loser_19[slice94],\r\n",
        "       train_regret_loser_20[slice94]]\r\n",
        "\r\n",
        "winner94 = [train_regret_winner_1[slice94],\r\n",
        "       train_regret_winner_2[slice94],\r\n",
        "       train_regret_winner_3[slice94],\r\n",
        "       train_regret_winner_4[slice94],\r\n",
        "       train_regret_winner_5[slice94],\r\n",
        "       train_regret_winner_6[slice94],\r\n",
        "       train_regret_winner_7[slice94],\r\n",
        "       train_regret_winner_8[slice94],\r\n",
        "       train_regret_winner_9[slice94],\r\n",
        "       train_regret_winner_10[slice94],\r\n",
        "       train_regret_winner_11[slice94],\r\n",
        "       train_regret_winner_12[slice94],\r\n",
        "       train_regret_winner_13[slice94],\r\n",
        "       train_regret_winner_14[slice94],\r\n",
        "       train_regret_winner_15[slice94],\r\n",
        "       train_regret_winner_16[slice94],\r\n",
        "       train_regret_winner_17[slice94],\r\n",
        "       train_regret_winner_18[slice94],\r\n",
        "       train_regret_winner_19[slice94],\r\n",
        "       train_regret_winner_20[slice94]]\r\n",
        "\r\n",
        "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\r\n",
        "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\r\n",
        "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\r\n",
        "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\r\n",
        "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\r\n",
        "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMxHQOgaw_qu"
      },
      "source": [
        "# Iteration5 :\r\n",
        "\r\n",
        "slice5 = 4\r\n",
        "\r\n",
        "loser5 = [train_regret_loser_1[slice5],\r\n",
        "       train_regret_loser_2[slice5],\r\n",
        "       train_regret_loser_3[slice5],\r\n",
        "       train_regret_loser_4[slice5],\r\n",
        "       train_regret_loser_5[slice5],\r\n",
        "       train_regret_loser_6[slice5],\r\n",
        "       train_regret_loser_7[slice5],\r\n",
        "       train_regret_loser_8[slice5],\r\n",
        "       train_regret_loser_9[slice5],\r\n",
        "       train_regret_loser_10[slice5],\r\n",
        "       train_regret_loser_11[slice5],\r\n",
        "       train_regret_loser_12[slice5],\r\n",
        "       train_regret_loser_13[slice5],\r\n",
        "       train_regret_loser_14[slice5],\r\n",
        "       train_regret_loser_15[slice5],\r\n",
        "       train_regret_loser_16[slice5],\r\n",
        "       train_regret_loser_17[slice5],\r\n",
        "       train_regret_loser_18[slice5],\r\n",
        "       train_regret_loser_19[slice5],\r\n",
        "       train_regret_loser_20[slice5]]\r\n",
        "\r\n",
        "winner5 = [train_regret_winner_1[slice5],\r\n",
        "       train_regret_winner_2[slice5],\r\n",
        "       train_regret_winner_3[slice5],\r\n",
        "       train_regret_winner_4[slice5],\r\n",
        "       train_regret_winner_5[slice5],\r\n",
        "       train_regret_winner_6[slice5],\r\n",
        "       train_regret_winner_7[slice5],\r\n",
        "       train_regret_winner_8[slice5],\r\n",
        "       train_regret_winner_9[slice5],\r\n",
        "       train_regret_winner_10[slice5],\r\n",
        "       train_regret_winner_11[slice5],\r\n",
        "       train_regret_winner_12[slice5],\r\n",
        "       train_regret_winner_13[slice5],\r\n",
        "       train_regret_winner_14[slice5],\r\n",
        "       train_regret_winner_15[slice5],\r\n",
        "       train_regret_winner_16[slice5],\r\n",
        "       train_regret_winner_17[slice5],\r\n",
        "       train_regret_winner_18[slice5],\r\n",
        "       train_regret_winner_19[slice5],\r\n",
        "       train_regret_winner_20[slice5]]\r\n",
        "\r\n",
        "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\r\n",
        "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\r\n",
        "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\r\n",
        "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\r\n",
        "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\r\n",
        "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPVFQTfNw_wF"
      },
      "source": [
        "# Iteration15 :\r\n",
        "\r\n",
        "slice15 = 14\r\n",
        "\r\n",
        "loser15 = [train_regret_loser_1[slice15],\r\n",
        "       train_regret_loser_2[slice15],\r\n",
        "       train_regret_loser_3[slice15],\r\n",
        "       train_regret_loser_4[slice15],\r\n",
        "       train_regret_loser_5[slice15],\r\n",
        "       train_regret_loser_6[slice15],\r\n",
        "       train_regret_loser_7[slice15],\r\n",
        "       train_regret_loser_8[slice15],\r\n",
        "       train_regret_loser_9[slice15],\r\n",
        "       train_regret_loser_10[slice15],\r\n",
        "       train_regret_loser_11[slice15],\r\n",
        "       train_regret_loser_12[slice15],\r\n",
        "       train_regret_loser_13[slice15],\r\n",
        "       train_regret_loser_14[slice15],\r\n",
        "       train_regret_loser_15[slice15],\r\n",
        "       train_regret_loser_16[slice15],\r\n",
        "       train_regret_loser_17[slice15],\r\n",
        "       train_regret_loser_18[slice15],\r\n",
        "       train_regret_loser_19[slice15],\r\n",
        "       train_regret_loser_20[slice15]]\r\n",
        "\r\n",
        "winner15 = [train_regret_winner_1[slice15],\r\n",
        "       train_regret_winner_2[slice15],\r\n",
        "       train_regret_winner_3[slice15],\r\n",
        "       train_regret_winner_4[slice15],\r\n",
        "       train_regret_winner_5[slice15],\r\n",
        "       train_regret_winner_6[slice15],\r\n",
        "       train_regret_winner_7[slice15],\r\n",
        "       train_regret_winner_8[slice15],\r\n",
        "       train_regret_winner_9[slice15],\r\n",
        "       train_regret_winner_10[slice15],\r\n",
        "       train_regret_winner_11[slice15],\r\n",
        "       train_regret_winner_12[slice15],\r\n",
        "       train_regret_winner_13[slice15],\r\n",
        "       train_regret_winner_14[slice15],\r\n",
        "       train_regret_winner_15[slice15],\r\n",
        "       train_regret_winner_16[slice15],\r\n",
        "       train_regret_winner_17[slice15],\r\n",
        "       train_regret_winner_18[slice15],\r\n",
        "       train_regret_winner_19[slice15],\r\n",
        "       train_regret_winner_20[slice15]]\r\n",
        "\r\n",
        "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\r\n",
        "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\r\n",
        "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\r\n",
        "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\r\n",
        "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\r\n",
        "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADBmBA3ew_zK"
      },
      "source": [
        "# Iteration25 :\r\n",
        "\r\n",
        "slice25 = 24\r\n",
        "\r\n",
        "loser25 = [train_regret_loser_1[slice25],\r\n",
        "       train_regret_loser_2[slice25],\r\n",
        "       train_regret_loser_3[slice25],\r\n",
        "       train_regret_loser_4[slice25],\r\n",
        "       train_regret_loser_5[slice25],\r\n",
        "       train_regret_loser_6[slice25],\r\n",
        "       train_regret_loser_7[slice25],\r\n",
        "       train_regret_loser_8[slice25],\r\n",
        "       train_regret_loser_9[slice25],\r\n",
        "       train_regret_loser_10[slice25],\r\n",
        "       train_regret_loser_11[slice25],\r\n",
        "       train_regret_loser_12[slice25],\r\n",
        "       train_regret_loser_13[slice25],\r\n",
        "       train_regret_loser_14[slice25],\r\n",
        "       train_regret_loser_15[slice25],\r\n",
        "       train_regret_loser_16[slice25],\r\n",
        "       train_regret_loser_17[slice25],\r\n",
        "       train_regret_loser_18[slice25],\r\n",
        "       train_regret_loser_19[slice25],\r\n",
        "       train_regret_loser_20[slice25]]\r\n",
        "\r\n",
        "winner25 = [train_regret_winner_1[slice25],\r\n",
        "       train_regret_winner_2[slice25],\r\n",
        "       train_regret_winner_3[slice25],\r\n",
        "       train_regret_winner_4[slice25],\r\n",
        "       train_regret_winner_5[slice25],\r\n",
        "       train_regret_winner_6[slice25],\r\n",
        "       train_regret_winner_7[slice25],\r\n",
        "       train_regret_winner_8[slice25],\r\n",
        "       train_regret_winner_9[slice25],\r\n",
        "       train_regret_winner_10[slice25],\r\n",
        "       train_regret_winner_11[slice25],\r\n",
        "       train_regret_winner_12[slice25],\r\n",
        "       train_regret_winner_13[slice25],\r\n",
        "       train_regret_winner_14[slice25],\r\n",
        "       train_regret_winner_15[slice25],\r\n",
        "       train_regret_winner_16[slice25],\r\n",
        "       train_regret_winner_17[slice25],\r\n",
        "       train_regret_winner_18[slice25],\r\n",
        "       train_regret_winner_19[slice25],\r\n",
        "       train_regret_winner_20[slice25]]\r\n",
        "\r\n",
        "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\r\n",
        "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\r\n",
        "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\r\n",
        "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\r\n",
        "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\r\n",
        "upper_winner25 = np.asarray(winner25_results[14:15][0])[0]"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUk9UtSTw_2N"
      },
      "source": [
        "# Iteration35 :\r\n",
        "\r\n",
        "slice35 = 34\r\n",
        "\r\n",
        "loser35 = [train_regret_loser_1[slice35],\r\n",
        "       train_regret_loser_2[slice35],\r\n",
        "       train_regret_loser_3[slice35],\r\n",
        "       train_regret_loser_4[slice35],\r\n",
        "       train_regret_loser_5[slice35],\r\n",
        "       train_regret_loser_6[slice35],\r\n",
        "       train_regret_loser_7[slice35],\r\n",
        "       train_regret_loser_8[slice35],\r\n",
        "       train_regret_loser_9[slice35],\r\n",
        "       train_regret_loser_10[slice35],\r\n",
        "       train_regret_loser_11[slice35],\r\n",
        "       train_regret_loser_12[slice35],\r\n",
        "       train_regret_loser_13[slice35],\r\n",
        "       train_regret_loser_14[slice35],\r\n",
        "       train_regret_loser_15[slice35],\r\n",
        "       train_regret_loser_16[slice35],\r\n",
        "       train_regret_loser_17[slice35],\r\n",
        "       train_regret_loser_18[slice35],\r\n",
        "       train_regret_loser_19[slice35],\r\n",
        "       train_regret_loser_20[slice35]]\r\n",
        "\r\n",
        "winner35 = [train_regret_winner_1[slice35],\r\n",
        "       train_regret_winner_2[slice35],\r\n",
        "       train_regret_winner_3[slice35],\r\n",
        "       train_regret_winner_4[slice35],\r\n",
        "       train_regret_winner_5[slice35],\r\n",
        "       train_regret_winner_6[slice35],\r\n",
        "       train_regret_winner_7[slice35],\r\n",
        "       train_regret_winner_8[slice35],\r\n",
        "       train_regret_winner_9[slice35],\r\n",
        "       train_regret_winner_10[slice35],\r\n",
        "       train_regret_winner_11[slice35],\r\n",
        "       train_regret_winner_12[slice35],\r\n",
        "       train_regret_winner_13[slice35],\r\n",
        "       train_regret_winner_14[slice35],\r\n",
        "       train_regret_winner_15[slice35],\r\n",
        "       train_regret_winner_16[slice35],\r\n",
        "       train_regret_winner_17[slice35],\r\n",
        "       train_regret_winner_18[slice35],\r\n",
        "       train_regret_winner_19[slice35],\r\n",
        "       train_regret_winner_20[slice35]]\r\n",
        "\r\n",
        "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\r\n",
        "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\r\n",
        "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\r\n",
        "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\r\n",
        "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\r\n",
        "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX3VgqD_w_4z"
      },
      "source": [
        "# Iteration45 :\r\n",
        "\r\n",
        "slice45 = 44\r\n",
        "\r\n",
        "loser45 = [train_regret_loser_1[slice45],\r\n",
        "       train_regret_loser_2[slice45],\r\n",
        "       train_regret_loser_3[slice45],\r\n",
        "       train_regret_loser_4[slice45],\r\n",
        "       train_regret_loser_5[slice45],\r\n",
        "       train_regret_loser_6[slice45],\r\n",
        "       train_regret_loser_7[slice45],\r\n",
        "       train_regret_loser_8[slice45],\r\n",
        "       train_regret_loser_9[slice45],\r\n",
        "       train_regret_loser_10[slice45],\r\n",
        "       train_regret_loser_11[slice45],\r\n",
        "       train_regret_loser_12[slice45],\r\n",
        "       train_regret_loser_13[slice45],\r\n",
        "       train_regret_loser_14[slice45],\r\n",
        "       train_regret_loser_15[slice45],\r\n",
        "       train_regret_loser_16[slice45],\r\n",
        "       train_regret_loser_17[slice45],\r\n",
        "       train_regret_loser_18[slice45],\r\n",
        "       train_regret_loser_19[slice45],\r\n",
        "       train_regret_loser_20[slice45]]\r\n",
        "\r\n",
        "winner45 = [train_regret_winner_1[slice45],\r\n",
        "       train_regret_winner_2[slice45],\r\n",
        "       train_regret_winner_3[slice45],\r\n",
        "       train_regret_winner_4[slice45],\r\n",
        "       train_regret_winner_5[slice45],\r\n",
        "       train_regret_winner_6[slice45],\r\n",
        "       train_regret_winner_7[slice45],\r\n",
        "       train_regret_winner_8[slice45],\r\n",
        "       train_regret_winner_9[slice45],\r\n",
        "       train_regret_winner_10[slice45],\r\n",
        "       train_regret_winner_11[slice45],\r\n",
        "       train_regret_winner_12[slice45],\r\n",
        "       train_regret_winner_13[slice45],\r\n",
        "       train_regret_winner_14[slice45],\r\n",
        "       train_regret_winner_15[slice45],\r\n",
        "       train_regret_winner_16[slice45],\r\n",
        "       train_regret_winner_17[slice45],\r\n",
        "       train_regret_winner_18[slice45],\r\n",
        "       train_regret_winner_19[slice45],\r\n",
        "       train_regret_winner_20[slice45]]\r\n",
        "\r\n",
        "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\r\n",
        "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\r\n",
        "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\r\n",
        "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\r\n",
        "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\r\n",
        "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o35W-AOzw_8F"
      },
      "source": [
        "# Iteration55 :\r\n",
        "\r\n",
        "slice55 = 54\r\n",
        "\r\n",
        "loser55 = [train_regret_loser_1[slice55],\r\n",
        "       train_regret_loser_2[slice55],\r\n",
        "       train_regret_loser_3[slice55],\r\n",
        "       train_regret_loser_4[slice55],\r\n",
        "       train_regret_loser_5[slice55],\r\n",
        "       train_regret_loser_6[slice55],\r\n",
        "       train_regret_loser_7[slice55],\r\n",
        "       train_regret_loser_8[slice55],\r\n",
        "       train_regret_loser_9[slice55],\r\n",
        "       train_regret_loser_10[slice55],\r\n",
        "       train_regret_loser_11[slice55],\r\n",
        "       train_regret_loser_12[slice55],\r\n",
        "       train_regret_loser_13[slice55],\r\n",
        "       train_regret_loser_14[slice55],\r\n",
        "       train_regret_loser_15[slice55],\r\n",
        "       train_regret_loser_16[slice55],\r\n",
        "       train_regret_loser_17[slice55],\r\n",
        "       train_regret_loser_18[slice55],\r\n",
        "       train_regret_loser_19[slice55],\r\n",
        "       train_regret_loser_20[slice55]]\r\n",
        "\r\n",
        "winner55 = [train_regret_winner_1[slice55],\r\n",
        "       train_regret_winner_2[slice55],\r\n",
        "       train_regret_winner_3[slice55],\r\n",
        "       train_regret_winner_4[slice55],\r\n",
        "       train_regret_winner_5[slice55],\r\n",
        "       train_regret_winner_6[slice55],\r\n",
        "       train_regret_winner_7[slice55],\r\n",
        "       train_regret_winner_8[slice55],\r\n",
        "       train_regret_winner_9[slice55],\r\n",
        "       train_regret_winner_10[slice55],\r\n",
        "       train_regret_winner_11[slice55],\r\n",
        "       train_regret_winner_12[slice55],\r\n",
        "       train_regret_winner_13[slice55],\r\n",
        "       train_regret_winner_14[slice55],\r\n",
        "       train_regret_winner_15[slice55],\r\n",
        "       train_regret_winner_16[slice55],\r\n",
        "       train_regret_winner_17[slice55],\r\n",
        "       train_regret_winner_18[slice55],\r\n",
        "       train_regret_winner_19[slice55],\r\n",
        "       train_regret_winner_20[slice55]]\r\n",
        "\r\n",
        "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\r\n",
        "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\r\n",
        "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\r\n",
        "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\r\n",
        "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\r\n",
        "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUAjKXWdw__J"
      },
      "source": [
        "# Iteration65 :\r\n",
        "\r\n",
        "slice65 = 64\r\n",
        "\r\n",
        "loser65 = [train_regret_loser_1[slice65],\r\n",
        "       train_regret_loser_2[slice65],\r\n",
        "       train_regret_loser_3[slice65],\r\n",
        "       train_regret_loser_4[slice65],\r\n",
        "       train_regret_loser_5[slice65],\r\n",
        "       train_regret_loser_6[slice65],\r\n",
        "       train_regret_loser_7[slice65],\r\n",
        "       train_regret_loser_8[slice65],\r\n",
        "       train_regret_loser_9[slice65],\r\n",
        "       train_regret_loser_10[slice65],\r\n",
        "       train_regret_loser_11[slice65],\r\n",
        "       train_regret_loser_12[slice65],\r\n",
        "       train_regret_loser_13[slice65],\r\n",
        "       train_regret_loser_14[slice65],\r\n",
        "       train_regret_loser_15[slice65],\r\n",
        "       train_regret_loser_16[slice65],\r\n",
        "       train_regret_loser_17[slice65],\r\n",
        "       train_regret_loser_18[slice65],\r\n",
        "       train_regret_loser_19[slice65],\r\n",
        "       train_regret_loser_20[slice65]]\r\n",
        "\r\n",
        "winner65 = [train_regret_winner_1[slice65],\r\n",
        "       train_regret_winner_2[slice65],\r\n",
        "       train_regret_winner_3[slice65],\r\n",
        "       train_regret_winner_4[slice65],\r\n",
        "       train_regret_winner_5[slice65],\r\n",
        "       train_regret_winner_6[slice65],\r\n",
        "       train_regret_winner_7[slice65],\r\n",
        "       train_regret_winner_8[slice65],\r\n",
        "       train_regret_winner_9[slice65],\r\n",
        "       train_regret_winner_10[slice65],\r\n",
        "       train_regret_winner_11[slice65],\r\n",
        "       train_regret_winner_12[slice65],\r\n",
        "       train_regret_winner_13[slice65],\r\n",
        "       train_regret_winner_14[slice65],\r\n",
        "       train_regret_winner_15[slice65],\r\n",
        "       train_regret_winner_16[slice65],\r\n",
        "       train_regret_winner_17[slice65],\r\n",
        "       train_regret_winner_18[slice65],\r\n",
        "       train_regret_winner_19[slice65],\r\n",
        "       train_regret_winner_20[slice65]]\r\n",
        "\r\n",
        "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\r\n",
        "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\r\n",
        "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\r\n",
        "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\r\n",
        "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\r\n",
        "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D00FcmZwxAB9"
      },
      "source": [
        "# Iteration75 :\r\n",
        "\r\n",
        "slice75 = 74\r\n",
        "\r\n",
        "loser75 = [train_regret_loser_1[slice75],\r\n",
        "       train_regret_loser_2[slice75],\r\n",
        "       train_regret_loser_3[slice75],\r\n",
        "       train_regret_loser_4[slice75],\r\n",
        "       train_regret_loser_5[slice75],\r\n",
        "       train_regret_loser_6[slice75],\r\n",
        "       train_regret_loser_7[slice75],\r\n",
        "       train_regret_loser_8[slice75],\r\n",
        "       train_regret_loser_9[slice75],\r\n",
        "       train_regret_loser_10[slice75],\r\n",
        "       train_regret_loser_11[slice75],\r\n",
        "       train_regret_loser_12[slice75],\r\n",
        "       train_regret_loser_13[slice75],\r\n",
        "       train_regret_loser_14[slice75],\r\n",
        "       train_regret_loser_15[slice75],\r\n",
        "       train_regret_loser_16[slice75],\r\n",
        "       train_regret_loser_17[slice75],\r\n",
        "       train_regret_loser_18[slice75],\r\n",
        "       train_regret_loser_19[slice75],\r\n",
        "       train_regret_loser_20[slice75]]\r\n",
        "\r\n",
        "winner75 = [train_regret_winner_1[slice75],\r\n",
        "       train_regret_winner_2[slice75],\r\n",
        "       train_regret_winner_3[slice75],\r\n",
        "       train_regret_winner_4[slice75],\r\n",
        "       train_regret_winner_5[slice75],\r\n",
        "       train_regret_winner_6[slice75],\r\n",
        "       train_regret_winner_7[slice75],\r\n",
        "       train_regret_winner_8[slice75],\r\n",
        "       train_regret_winner_9[slice75],\r\n",
        "       train_regret_winner_10[slice75],\r\n",
        "       train_regret_winner_11[slice75],\r\n",
        "       train_regret_winner_12[slice75],\r\n",
        "       train_regret_winner_13[slice75],\r\n",
        "       train_regret_winner_14[slice75],\r\n",
        "       train_regret_winner_15[slice75],\r\n",
        "       train_regret_winner_16[slice75],\r\n",
        "       train_regret_winner_17[slice75],\r\n",
        "       train_regret_winner_18[slice75],\r\n",
        "       train_regret_winner_19[slice75],\r\n",
        "       train_regret_winner_20[slice75]]\r\n",
        "\r\n",
        "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\r\n",
        "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\r\n",
        "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\r\n",
        "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\r\n",
        "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\r\n",
        "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnzmprxxAFB"
      },
      "source": [
        "# Iteration85 :\r\n",
        "\r\n",
        "slice85 = 84\r\n",
        "\r\n",
        "loser85 = [train_regret_loser_1[slice85],\r\n",
        "       train_regret_loser_2[slice85],\r\n",
        "       train_regret_loser_3[slice85],\r\n",
        "       train_regret_loser_4[slice85],\r\n",
        "       train_regret_loser_5[slice85],\r\n",
        "       train_regret_loser_6[slice85],\r\n",
        "       train_regret_loser_7[slice85],\r\n",
        "       train_regret_loser_8[slice85],\r\n",
        "       train_regret_loser_9[slice85],\r\n",
        "       train_regret_loser_10[slice85],\r\n",
        "       train_regret_loser_11[slice85],\r\n",
        "       train_regret_loser_12[slice85],\r\n",
        "       train_regret_loser_13[slice85],\r\n",
        "       train_regret_loser_14[slice85],\r\n",
        "       train_regret_loser_15[slice85],\r\n",
        "       train_regret_loser_16[slice85],\r\n",
        "       train_regret_loser_17[slice85],\r\n",
        "       train_regret_loser_18[slice85],\r\n",
        "       train_regret_loser_19[slice85],\r\n",
        "       train_regret_loser_20[slice85]]\r\n",
        "\r\n",
        "winner85 = [train_regret_winner_1[slice85],\r\n",
        "       train_regret_winner_2[slice85],\r\n",
        "       train_regret_winner_3[slice85],\r\n",
        "       train_regret_winner_4[slice85],\r\n",
        "       train_regret_winner_5[slice85],\r\n",
        "       train_regret_winner_6[slice85],\r\n",
        "       train_regret_winner_7[slice85],\r\n",
        "       train_regret_winner_8[slice85],\r\n",
        "       train_regret_winner_9[slice85],\r\n",
        "       train_regret_winner_10[slice85],\r\n",
        "       train_regret_winner_11[slice85],\r\n",
        "       train_regret_winner_12[slice85],\r\n",
        "       train_regret_winner_13[slice85],\r\n",
        "       train_regret_winner_14[slice85],\r\n",
        "       train_regret_winner_15[slice85],\r\n",
        "       train_regret_winner_16[slice85],\r\n",
        "       train_regret_winner_17[slice85],\r\n",
        "       train_regret_winner_18[slice85],\r\n",
        "       train_regret_winner_19[slice85],\r\n",
        "       train_regret_winner_20[slice85]]\r\n",
        "\r\n",
        "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\r\n",
        "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\r\n",
        "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\r\n",
        "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\r\n",
        "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\r\n",
        "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztvJhrN6xAH1"
      },
      "source": [
        "# Iteration95 :\r\n",
        "\r\n",
        "slice95 = 94\r\n",
        "\r\n",
        "loser95 = [train_regret_loser_1[slice95],\r\n",
        "       train_regret_loser_2[slice95],\r\n",
        "       train_regret_loser_3[slice95],\r\n",
        "       train_regret_loser_4[slice95],\r\n",
        "       train_regret_loser_5[slice95],\r\n",
        "       train_regret_loser_6[slice95],\r\n",
        "       train_regret_loser_7[slice95],\r\n",
        "       train_regret_loser_8[slice95],\r\n",
        "       train_regret_loser_9[slice95],\r\n",
        "       train_regret_loser_10[slice95],\r\n",
        "       train_regret_loser_11[slice95],\r\n",
        "       train_regret_loser_12[slice95],\r\n",
        "       train_regret_loser_13[slice95],\r\n",
        "       train_regret_loser_14[slice95],\r\n",
        "       train_regret_loser_15[slice95],\r\n",
        "       train_regret_loser_16[slice95],\r\n",
        "       train_regret_loser_17[slice95],\r\n",
        "       train_regret_loser_18[slice95],\r\n",
        "       train_regret_loser_19[slice95],\r\n",
        "       train_regret_loser_20[slice95]]\r\n",
        "\r\n",
        "winner95 = [train_regret_winner_1[slice95],\r\n",
        "       train_regret_winner_2[slice95],\r\n",
        "       train_regret_winner_3[slice95],\r\n",
        "       train_regret_winner_4[slice95],\r\n",
        "       train_regret_winner_5[slice95],\r\n",
        "       train_regret_winner_6[slice95],\r\n",
        "       train_regret_winner_7[slice95],\r\n",
        "       train_regret_winner_8[slice95],\r\n",
        "       train_regret_winner_9[slice95],\r\n",
        "       train_regret_winner_10[slice95],\r\n",
        "       train_regret_winner_11[slice95],\r\n",
        "       train_regret_winner_12[slice95],\r\n",
        "       train_regret_winner_13[slice95],\r\n",
        "       train_regret_winner_14[slice95],\r\n",
        "       train_regret_winner_15[slice95],\r\n",
        "       train_regret_winner_16[slice95],\r\n",
        "       train_regret_winner_17[slice95],\r\n",
        "       train_regret_winner_18[slice95],\r\n",
        "       train_regret_winner_19[slice95],\r\n",
        "       train_regret_winner_20[slice95]]\r\n",
        "\r\n",
        "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\r\n",
        "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\r\n",
        "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\r\n",
        "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\r\n",
        "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\r\n",
        "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK-4nr-OxALA"
      },
      "source": [
        "# Iteration6 :\r\n",
        "\r\n",
        "slice6 = 5\r\n",
        "\r\n",
        "loser6 = [train_regret_loser_1[slice6],\r\n",
        "       train_regret_loser_2[slice6],\r\n",
        "       train_regret_loser_3[slice6],\r\n",
        "       train_regret_loser_4[slice6],\r\n",
        "       train_regret_loser_5[slice6],\r\n",
        "       train_regret_loser_6[slice6],\r\n",
        "       train_regret_loser_7[slice6],\r\n",
        "       train_regret_loser_8[slice6],\r\n",
        "       train_regret_loser_9[slice6],\r\n",
        "       train_regret_loser_10[slice6],\r\n",
        "       train_regret_loser_11[slice6],\r\n",
        "       train_regret_loser_12[slice6],\r\n",
        "       train_regret_loser_13[slice6],\r\n",
        "       train_regret_loser_14[slice6],\r\n",
        "       train_regret_loser_15[slice6],\r\n",
        "       train_regret_loser_16[slice6],\r\n",
        "       train_regret_loser_17[slice6],\r\n",
        "       train_regret_loser_18[slice6],\r\n",
        "       train_regret_loser_19[slice6],\r\n",
        "       train_regret_loser_20[slice6]]\r\n",
        "\r\n",
        "winner6 = [train_regret_winner_1[slice6],\r\n",
        "       train_regret_winner_2[slice6],\r\n",
        "       train_regret_winner_3[slice6],\r\n",
        "       train_regret_winner_4[slice6],\r\n",
        "       train_regret_winner_5[slice6],\r\n",
        "       train_regret_winner_6[slice6],\r\n",
        "       train_regret_winner_7[slice6],\r\n",
        "       train_regret_winner_8[slice6],\r\n",
        "       train_regret_winner_9[slice6],\r\n",
        "       train_regret_winner_10[slice6],\r\n",
        "       train_regret_winner_11[slice6],\r\n",
        "       train_regret_winner_12[slice6],\r\n",
        "       train_regret_winner_13[slice6],\r\n",
        "       train_regret_winner_14[slice6],\r\n",
        "       train_regret_winner_15[slice6],\r\n",
        "       train_regret_winner_16[slice6],\r\n",
        "       train_regret_winner_17[slice6],\r\n",
        "       train_regret_winner_18[slice6],\r\n",
        "       train_regret_winner_19[slice6],\r\n",
        "       train_regret_winner_20[slice6]]\r\n",
        "\r\n",
        "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\r\n",
        "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\r\n",
        "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\r\n",
        "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\r\n",
        "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\r\n",
        "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSwfNX5ExANt"
      },
      "source": [
        "# Iteration16 :\r\n",
        "\r\n",
        "slice16 = 15\r\n",
        "\r\n",
        "loser16 = [train_regret_loser_1[slice16],\r\n",
        "       train_regret_loser_2[slice16],\r\n",
        "       train_regret_loser_3[slice16],\r\n",
        "       train_regret_loser_4[slice16],\r\n",
        "       train_regret_loser_5[slice16],\r\n",
        "       train_regret_loser_6[slice16],\r\n",
        "       train_regret_loser_7[slice16],\r\n",
        "       train_regret_loser_8[slice16],\r\n",
        "       train_regret_loser_9[slice16],\r\n",
        "       train_regret_loser_10[slice16],\r\n",
        "       train_regret_loser_11[slice16],\r\n",
        "       train_regret_loser_12[slice16],\r\n",
        "       train_regret_loser_13[slice16],\r\n",
        "       train_regret_loser_14[slice16],\r\n",
        "       train_regret_loser_15[slice16],\r\n",
        "       train_regret_loser_16[slice16],\r\n",
        "       train_regret_loser_17[slice16],\r\n",
        "       train_regret_loser_18[slice16],\r\n",
        "       train_regret_loser_19[slice16],\r\n",
        "       train_regret_loser_20[slice16]]\r\n",
        "\r\n",
        "winner16 = [train_regret_winner_1[slice16],\r\n",
        "       train_regret_winner_2[slice16],\r\n",
        "       train_regret_winner_3[slice16],\r\n",
        "       train_regret_winner_4[slice16],\r\n",
        "       train_regret_winner_5[slice16],\r\n",
        "       train_regret_winner_6[slice16],\r\n",
        "       train_regret_winner_7[slice16],\r\n",
        "       train_regret_winner_8[slice16],\r\n",
        "       train_regret_winner_9[slice16],\r\n",
        "       train_regret_winner_10[slice16],\r\n",
        "       train_regret_winner_11[slice16],\r\n",
        "       train_regret_winner_12[slice16],\r\n",
        "       train_regret_winner_13[slice16],\r\n",
        "       train_regret_winner_14[slice16],\r\n",
        "       train_regret_winner_15[slice16],\r\n",
        "       train_regret_winner_16[slice16],\r\n",
        "       train_regret_winner_17[slice16],\r\n",
        "       train_regret_winner_18[slice16],\r\n",
        "       train_regret_winner_19[slice16],\r\n",
        "       train_regret_winner_20[slice16]]\r\n",
        "\r\n",
        "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\r\n",
        "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\r\n",
        "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\r\n",
        "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\r\n",
        "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\r\n",
        "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOB3hOLIxAQy"
      },
      "source": [
        "# Iteration26 :\r\n",
        "\r\n",
        "slice26 = 25\r\n",
        "\r\n",
        "loser26 = [train_regret_loser_1[slice26],\r\n",
        "       train_regret_loser_2[slice26],\r\n",
        "       train_regret_loser_3[slice26],\r\n",
        "       train_regret_loser_4[slice26],\r\n",
        "       train_regret_loser_5[slice26],\r\n",
        "       train_regret_loser_6[slice26],\r\n",
        "       train_regret_loser_7[slice26],\r\n",
        "       train_regret_loser_8[slice26],\r\n",
        "       train_regret_loser_9[slice26],\r\n",
        "       train_regret_loser_10[slice26],\r\n",
        "       train_regret_loser_11[slice26],\r\n",
        "       train_regret_loser_12[slice26],\r\n",
        "       train_regret_loser_13[slice26],\r\n",
        "       train_regret_loser_14[slice26],\r\n",
        "       train_regret_loser_15[slice26],\r\n",
        "       train_regret_loser_16[slice26],\r\n",
        "       train_regret_loser_17[slice26],\r\n",
        "       train_regret_loser_18[slice26],\r\n",
        "       train_regret_loser_19[slice26],\r\n",
        "       train_regret_loser_20[slice26]]\r\n",
        "\r\n",
        "winner26 = [train_regret_winner_1[slice26],\r\n",
        "       train_regret_winner_2[slice26],\r\n",
        "       train_regret_winner_3[slice26],\r\n",
        "       train_regret_winner_4[slice26],\r\n",
        "       train_regret_winner_5[slice26],\r\n",
        "       train_regret_winner_6[slice26],\r\n",
        "       train_regret_winner_7[slice26],\r\n",
        "       train_regret_winner_8[slice26],\r\n",
        "       train_regret_winner_9[slice26],\r\n",
        "       train_regret_winner_10[slice26],\r\n",
        "       train_regret_winner_11[slice26],\r\n",
        "       train_regret_winner_12[slice26],\r\n",
        "       train_regret_winner_13[slice26],\r\n",
        "       train_regret_winner_14[slice26],\r\n",
        "       train_regret_winner_15[slice26],\r\n",
        "       train_regret_winner_16[slice26],\r\n",
        "       train_regret_winner_17[slice26],\r\n",
        "       train_regret_winner_18[slice26],\r\n",
        "       train_regret_winner_19[slice26],\r\n",
        "       train_regret_winner_20[slice26]]\r\n",
        "\r\n",
        "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\r\n",
        "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\r\n",
        "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\r\n",
        "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\r\n",
        "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\r\n",
        "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-0hQMtOxATh"
      },
      "source": [
        "# Iteration36 :\r\n",
        "\r\n",
        "slice36 = 35\r\n",
        "\r\n",
        "loser36 = [train_regret_loser_1[slice36],\r\n",
        "       train_regret_loser_2[slice36],\r\n",
        "       train_regret_loser_3[slice36],\r\n",
        "       train_regret_loser_4[slice36],\r\n",
        "       train_regret_loser_5[slice36],\r\n",
        "       train_regret_loser_6[slice36],\r\n",
        "       train_regret_loser_7[slice36],\r\n",
        "       train_regret_loser_8[slice36],\r\n",
        "       train_regret_loser_9[slice36],\r\n",
        "       train_regret_loser_10[slice36],\r\n",
        "       train_regret_loser_11[slice36],\r\n",
        "       train_regret_loser_12[slice36],\r\n",
        "       train_regret_loser_13[slice36],\r\n",
        "       train_regret_loser_14[slice36],\r\n",
        "       train_regret_loser_15[slice36],\r\n",
        "       train_regret_loser_16[slice36],\r\n",
        "       train_regret_loser_17[slice36],\r\n",
        "       train_regret_loser_18[slice36],\r\n",
        "       train_regret_loser_19[slice36],\r\n",
        "       train_regret_loser_20[slice36]]\r\n",
        "\r\n",
        "winner36 = [train_regret_winner_1[slice36],\r\n",
        "       train_regret_winner_2[slice36],\r\n",
        "       train_regret_winner_3[slice36],\r\n",
        "       train_regret_winner_4[slice36],\r\n",
        "       train_regret_winner_5[slice36],\r\n",
        "       train_regret_winner_6[slice36],\r\n",
        "       train_regret_winner_7[slice36],\r\n",
        "       train_regret_winner_8[slice36],\r\n",
        "       train_regret_winner_9[slice36],\r\n",
        "       train_regret_winner_10[slice36],\r\n",
        "       train_regret_winner_11[slice36],\r\n",
        "       train_regret_winner_12[slice36],\r\n",
        "       train_regret_winner_13[slice36],\r\n",
        "       train_regret_winner_14[slice36],\r\n",
        "       train_regret_winner_15[slice36],\r\n",
        "       train_regret_winner_16[slice36],\r\n",
        "       train_regret_winner_17[slice36],\r\n",
        "       train_regret_winner_18[slice36],\r\n",
        "       train_regret_winner_19[slice36],\r\n",
        "       train_regret_winner_20[slice36]]\r\n",
        "\r\n",
        "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\r\n",
        "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\r\n",
        "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\r\n",
        "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\r\n",
        "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\r\n",
        "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSqBLqTgxAWs"
      },
      "source": [
        "# Iteration46 :\r\n",
        "\r\n",
        "slice46 = 45\r\n",
        "\r\n",
        "loser46 = [train_regret_loser_1[slice46],\r\n",
        "       train_regret_loser_2[slice46],\r\n",
        "       train_regret_loser_3[slice46],\r\n",
        "       train_regret_loser_4[slice46],\r\n",
        "       train_regret_loser_5[slice46],\r\n",
        "       train_regret_loser_6[slice46],\r\n",
        "       train_regret_loser_7[slice46],\r\n",
        "       train_regret_loser_8[slice46],\r\n",
        "       train_regret_loser_9[slice46],\r\n",
        "       train_regret_loser_10[slice46],\r\n",
        "       train_regret_loser_11[slice46],\r\n",
        "       train_regret_loser_12[slice46],\r\n",
        "       train_regret_loser_13[slice46],\r\n",
        "       train_regret_loser_14[slice46],\r\n",
        "       train_regret_loser_15[slice46],\r\n",
        "       train_regret_loser_16[slice46],\r\n",
        "       train_regret_loser_17[slice46],\r\n",
        "       train_regret_loser_18[slice46],\r\n",
        "       train_regret_loser_19[slice46],\r\n",
        "       train_regret_loser_20[slice46]]\r\n",
        "\r\n",
        "winner46 = [train_regret_winner_1[slice46],\r\n",
        "       train_regret_winner_2[slice46],\r\n",
        "       train_regret_winner_3[slice46],\r\n",
        "       train_regret_winner_4[slice46],\r\n",
        "       train_regret_winner_5[slice46],\r\n",
        "       train_regret_winner_6[slice46],\r\n",
        "       train_regret_winner_7[slice46],\r\n",
        "       train_regret_winner_8[slice46],\r\n",
        "       train_regret_winner_9[slice46],\r\n",
        "       train_regret_winner_10[slice46],\r\n",
        "       train_regret_winner_11[slice46],\r\n",
        "       train_regret_winner_12[slice46],\r\n",
        "       train_regret_winner_13[slice46],\r\n",
        "       train_regret_winner_14[slice46],\r\n",
        "       train_regret_winner_15[slice46],\r\n",
        "       train_regret_winner_16[slice46],\r\n",
        "       train_regret_winner_17[slice46],\r\n",
        "       train_regret_winner_18[slice46],\r\n",
        "       train_regret_winner_19[slice46],\r\n",
        "       train_regret_winner_20[slice46]]\r\n",
        "\r\n",
        "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\r\n",
        "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\r\n",
        "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\r\n",
        "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\r\n",
        "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\r\n",
        "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFwO80w6xAZZ"
      },
      "source": [
        "# Iteration56 :\r\n",
        "\r\n",
        "slice56 = 55\r\n",
        "\r\n",
        "loser56 = [train_regret_loser_1[slice56],\r\n",
        "       train_regret_loser_2[slice56],\r\n",
        "       train_regret_loser_3[slice56],\r\n",
        "       train_regret_loser_4[slice56],\r\n",
        "       train_regret_loser_5[slice56],\r\n",
        "       train_regret_loser_6[slice56],\r\n",
        "       train_regret_loser_7[slice56],\r\n",
        "       train_regret_loser_8[slice56],\r\n",
        "       train_regret_loser_9[slice56],\r\n",
        "       train_regret_loser_10[slice56],\r\n",
        "       train_regret_loser_11[slice56],\r\n",
        "       train_regret_loser_12[slice56],\r\n",
        "       train_regret_loser_13[slice56],\r\n",
        "       train_regret_loser_14[slice56],\r\n",
        "       train_regret_loser_15[slice56],\r\n",
        "       train_regret_loser_16[slice56],\r\n",
        "       train_regret_loser_17[slice56],\r\n",
        "       train_regret_loser_18[slice56],\r\n",
        "       train_regret_loser_19[slice56],\r\n",
        "       train_regret_loser_20[slice56]]\r\n",
        "\r\n",
        "winner56 = [train_regret_winner_1[slice56],\r\n",
        "       train_regret_winner_2[slice56],\r\n",
        "       train_regret_winner_3[slice56],\r\n",
        "       train_regret_winner_4[slice56],\r\n",
        "       train_regret_winner_5[slice56],\r\n",
        "       train_regret_winner_6[slice56],\r\n",
        "       train_regret_winner_7[slice56],\r\n",
        "       train_regret_winner_8[slice56],\r\n",
        "       train_regret_winner_9[slice56],\r\n",
        "       train_regret_winner_10[slice56],\r\n",
        "       train_regret_winner_11[slice56],\r\n",
        "       train_regret_winner_12[slice56],\r\n",
        "       train_regret_winner_13[slice56],\r\n",
        "       train_regret_winner_14[slice56],\r\n",
        "       train_regret_winner_15[slice56],\r\n",
        "       train_regret_winner_16[slice56],\r\n",
        "       train_regret_winner_17[slice56],\r\n",
        "       train_regret_winner_18[slice56],\r\n",
        "       train_regret_winner_19[slice56],\r\n",
        "       train_regret_winner_20[slice56]]\r\n",
        "\r\n",
        "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\r\n",
        "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\r\n",
        "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\r\n",
        "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\r\n",
        "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\r\n",
        "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfVZBDP8xAcC"
      },
      "source": [
        "# Iteration66 :\r\n",
        "\r\n",
        "slice66 = 65\r\n",
        "\r\n",
        "loser66 = [train_regret_loser_1[slice66],\r\n",
        "       train_regret_loser_2[slice66],\r\n",
        "       train_regret_loser_3[slice66],\r\n",
        "       train_regret_loser_4[slice66],\r\n",
        "       train_regret_loser_5[slice66],\r\n",
        "       train_regret_loser_6[slice66],\r\n",
        "       train_regret_loser_7[slice66],\r\n",
        "       train_regret_loser_8[slice66],\r\n",
        "       train_regret_loser_9[slice66],\r\n",
        "       train_regret_loser_10[slice66],\r\n",
        "       train_regret_loser_11[slice66],\r\n",
        "       train_regret_loser_12[slice66],\r\n",
        "       train_regret_loser_13[slice66],\r\n",
        "       train_regret_loser_14[slice66],\r\n",
        "       train_regret_loser_15[slice66],\r\n",
        "       train_regret_loser_16[slice66],\r\n",
        "       train_regret_loser_17[slice66],\r\n",
        "       train_regret_loser_18[slice66],\r\n",
        "       train_regret_loser_19[slice66],\r\n",
        "       train_regret_loser_20[slice66]]\r\n",
        "\r\n",
        "winner66 = [train_regret_winner_1[slice66],\r\n",
        "       train_regret_winner_2[slice66],\r\n",
        "       train_regret_winner_3[slice66],\r\n",
        "       train_regret_winner_4[slice66],\r\n",
        "       train_regret_winner_5[slice66],\r\n",
        "       train_regret_winner_6[slice66],\r\n",
        "       train_regret_winner_7[slice66],\r\n",
        "       train_regret_winner_8[slice66],\r\n",
        "       train_regret_winner_9[slice66],\r\n",
        "       train_regret_winner_10[slice66],\r\n",
        "       train_regret_winner_11[slice66],\r\n",
        "       train_regret_winner_12[slice66],\r\n",
        "       train_regret_winner_13[slice66],\r\n",
        "       train_regret_winner_14[slice66],\r\n",
        "       train_regret_winner_15[slice66],\r\n",
        "       train_regret_winner_16[slice66],\r\n",
        "       train_regret_winner_17[slice66],\r\n",
        "       train_regret_winner_18[slice66],\r\n",
        "       train_regret_winner_19[slice66],\r\n",
        "       train_regret_winner_20[slice66]]\r\n",
        "\r\n",
        "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\r\n",
        "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\r\n",
        "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\r\n",
        "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\r\n",
        "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\r\n",
        "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnSYoG7bxAfL"
      },
      "source": [
        "# Iteration76 :\r\n",
        "\r\n",
        "slice76 = 75\r\n",
        "\r\n",
        "loser76 = [train_regret_loser_1[slice76],\r\n",
        "       train_regret_loser_2[slice76],\r\n",
        "       train_regret_loser_3[slice76],\r\n",
        "       train_regret_loser_4[slice76],\r\n",
        "       train_regret_loser_5[slice76],\r\n",
        "       train_regret_loser_6[slice76],\r\n",
        "       train_regret_loser_7[slice76],\r\n",
        "       train_regret_loser_8[slice76],\r\n",
        "       train_regret_loser_9[slice76],\r\n",
        "       train_regret_loser_10[slice76],\r\n",
        "       train_regret_loser_11[slice76],\r\n",
        "       train_regret_loser_12[slice76],\r\n",
        "       train_regret_loser_13[slice76],\r\n",
        "       train_regret_loser_14[slice76],\r\n",
        "       train_regret_loser_15[slice76],\r\n",
        "       train_regret_loser_16[slice76],\r\n",
        "       train_regret_loser_17[slice76],\r\n",
        "       train_regret_loser_18[slice76],\r\n",
        "       train_regret_loser_19[slice76],\r\n",
        "       train_regret_loser_20[slice76]]\r\n",
        "\r\n",
        "winner76 = [train_regret_winner_1[slice76],\r\n",
        "       train_regret_winner_2[slice76],\r\n",
        "       train_regret_winner_3[slice76],\r\n",
        "       train_regret_winner_4[slice76],\r\n",
        "       train_regret_winner_5[slice76],\r\n",
        "       train_regret_winner_6[slice76],\r\n",
        "       train_regret_winner_7[slice76],\r\n",
        "       train_regret_winner_8[slice76],\r\n",
        "       train_regret_winner_9[slice76],\r\n",
        "       train_regret_winner_10[slice76],\r\n",
        "       train_regret_winner_11[slice76],\r\n",
        "       train_regret_winner_12[slice76],\r\n",
        "       train_regret_winner_13[slice76],\r\n",
        "       train_regret_winner_14[slice76],\r\n",
        "       train_regret_winner_15[slice76],\r\n",
        "       train_regret_winner_16[slice76],\r\n",
        "       train_regret_winner_17[slice76],\r\n",
        "       train_regret_winner_18[slice76],\r\n",
        "       train_regret_winner_19[slice76],\r\n",
        "       train_regret_winner_20[slice76]]\r\n",
        "\r\n",
        "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\r\n",
        "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\r\n",
        "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\r\n",
        "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\r\n",
        "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\r\n",
        "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ze_Oy9mxAiO"
      },
      "source": [
        "# Iteration86 :\r\n",
        "\r\n",
        "slice86 = 85\r\n",
        "\r\n",
        "loser86 = [train_regret_loser_1[slice86],\r\n",
        "       train_regret_loser_2[slice86],\r\n",
        "       train_regret_loser_3[slice86],\r\n",
        "       train_regret_loser_4[slice86],\r\n",
        "       train_regret_loser_5[slice86],\r\n",
        "       train_regret_loser_6[slice86],\r\n",
        "       train_regret_loser_7[slice86],\r\n",
        "       train_regret_loser_8[slice86],\r\n",
        "       train_regret_loser_9[slice86],\r\n",
        "       train_regret_loser_10[slice86],\r\n",
        "       train_regret_loser_11[slice86],\r\n",
        "       train_regret_loser_12[slice86],\r\n",
        "       train_regret_loser_13[slice86],\r\n",
        "       train_regret_loser_14[slice86],\r\n",
        "       train_regret_loser_15[slice86],\r\n",
        "       train_regret_loser_16[slice86],\r\n",
        "       train_regret_loser_17[slice86],\r\n",
        "       train_regret_loser_18[slice86],\r\n",
        "       train_regret_loser_19[slice86],\r\n",
        "       train_regret_loser_20[slice86]]\r\n",
        "\r\n",
        "winner86 = [train_regret_winner_1[slice86],\r\n",
        "       train_regret_winner_2[slice86],\r\n",
        "       train_regret_winner_3[slice86],\r\n",
        "       train_regret_winner_4[slice86],\r\n",
        "       train_regret_winner_5[slice86],\r\n",
        "       train_regret_winner_6[slice86],\r\n",
        "       train_regret_winner_7[slice86],\r\n",
        "       train_regret_winner_8[slice86],\r\n",
        "       train_regret_winner_9[slice86],\r\n",
        "       train_regret_winner_10[slice86],\r\n",
        "       train_regret_winner_11[slice86],\r\n",
        "       train_regret_winner_12[slice86],\r\n",
        "       train_regret_winner_13[slice86],\r\n",
        "       train_regret_winner_14[slice86],\r\n",
        "       train_regret_winner_15[slice86],\r\n",
        "       train_regret_winner_16[slice86],\r\n",
        "       train_regret_winner_17[slice86],\r\n",
        "       train_regret_winner_18[slice86],\r\n",
        "       train_regret_winner_19[slice86],\r\n",
        "       train_regret_winner_20[slice86]]\r\n",
        "\r\n",
        "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\r\n",
        "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\r\n",
        "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\r\n",
        "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\r\n",
        "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\r\n",
        "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glsE9_RPxAlS"
      },
      "source": [
        "# Iteration96 :\r\n",
        "\r\n",
        "slice96 = 95\r\n",
        "\r\n",
        "loser96 = [train_regret_loser_1[slice96],\r\n",
        "       train_regret_loser_2[slice96],\r\n",
        "       train_regret_loser_3[slice96],\r\n",
        "       train_regret_loser_4[slice96],\r\n",
        "       train_regret_loser_5[slice96],\r\n",
        "       train_regret_loser_6[slice96],\r\n",
        "       train_regret_loser_7[slice96],\r\n",
        "       train_regret_loser_8[slice96],\r\n",
        "       train_regret_loser_9[slice96],\r\n",
        "       train_regret_loser_10[slice96],\r\n",
        "       train_regret_loser_11[slice96],\r\n",
        "       train_regret_loser_12[slice96],\r\n",
        "       train_regret_loser_13[slice96],\r\n",
        "       train_regret_loser_14[slice96],\r\n",
        "       train_regret_loser_15[slice96],\r\n",
        "       train_regret_loser_16[slice96],\r\n",
        "       train_regret_loser_17[slice96],\r\n",
        "       train_regret_loser_18[slice96],\r\n",
        "       train_regret_loser_19[slice96],\r\n",
        "       train_regret_loser_20[slice96]]\r\n",
        "\r\n",
        "winner96 = [train_regret_winner_1[slice96],\r\n",
        "       train_regret_winner_2[slice96],\r\n",
        "       train_regret_winner_3[slice96],\r\n",
        "       train_regret_winner_4[slice96],\r\n",
        "       train_regret_winner_5[slice96],\r\n",
        "       train_regret_winner_6[slice96],\r\n",
        "       train_regret_winner_7[slice96],\r\n",
        "       train_regret_winner_8[slice96],\r\n",
        "       train_regret_winner_9[slice96],\r\n",
        "       train_regret_winner_10[slice96],\r\n",
        "       train_regret_winner_11[slice96],\r\n",
        "       train_regret_winner_12[slice96],\r\n",
        "       train_regret_winner_13[slice96],\r\n",
        "       train_regret_winner_14[slice96],\r\n",
        "       train_regret_winner_15[slice96],\r\n",
        "       train_regret_winner_16[slice96],\r\n",
        "       train_regret_winner_17[slice96],\r\n",
        "       train_regret_winner_18[slice96],\r\n",
        "       train_regret_winner_19[slice96],\r\n",
        "       train_regret_winner_20[slice96]]\r\n",
        "\r\n",
        "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\r\n",
        "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\r\n",
        "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\r\n",
        "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\r\n",
        "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\r\n",
        "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhsEfl1RxAoA"
      },
      "source": [
        "# Iteration7 :\r\n",
        "\r\n",
        "slice7 = 6\r\n",
        "\r\n",
        "loser7 = [train_regret_loser_1[slice7],\r\n",
        "       train_regret_loser_2[slice7],\r\n",
        "       train_regret_loser_3[slice7],\r\n",
        "       train_regret_loser_4[slice7],\r\n",
        "       train_regret_loser_5[slice7],\r\n",
        "       train_regret_loser_6[slice7],\r\n",
        "       train_regret_loser_7[slice7],\r\n",
        "       train_regret_loser_8[slice7],\r\n",
        "       train_regret_loser_9[slice7],\r\n",
        "       train_regret_loser_10[slice7],\r\n",
        "       train_regret_loser_11[slice7],\r\n",
        "       train_regret_loser_12[slice7],\r\n",
        "       train_regret_loser_13[slice7],\r\n",
        "       train_regret_loser_14[slice7],\r\n",
        "       train_regret_loser_15[slice7],\r\n",
        "       train_regret_loser_16[slice7],\r\n",
        "       train_regret_loser_17[slice7],\r\n",
        "       train_regret_loser_18[slice7],\r\n",
        "       train_regret_loser_19[slice7],\r\n",
        "       train_regret_loser_20[slice7]]\r\n",
        "\r\n",
        "winner7 = [train_regret_winner_1[slice7],\r\n",
        "       train_regret_winner_2[slice7],\r\n",
        "       train_regret_winner_3[slice7],\r\n",
        "       train_regret_winner_4[slice7],\r\n",
        "       train_regret_winner_5[slice7],\r\n",
        "       train_regret_winner_6[slice7],\r\n",
        "       train_regret_winner_7[slice7],\r\n",
        "       train_regret_winner_8[slice7],\r\n",
        "       train_regret_winner_9[slice7],\r\n",
        "       train_regret_winner_10[slice7],\r\n",
        "       train_regret_winner_11[slice7],\r\n",
        "       train_regret_winner_12[slice7],\r\n",
        "       train_regret_winner_13[slice7],\r\n",
        "       train_regret_winner_14[slice7],\r\n",
        "       train_regret_winner_15[slice7],\r\n",
        "       train_regret_winner_16[slice7],\r\n",
        "       train_regret_winner_17[slice7],\r\n",
        "       train_regret_winner_18[slice7],\r\n",
        "       train_regret_winner_19[slice7],\r\n",
        "       train_regret_winner_20[slice7]]\r\n",
        "\r\n",
        "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\r\n",
        "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\r\n",
        "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\r\n",
        "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\r\n",
        "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\r\n",
        "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUCMheLKxArL"
      },
      "source": [
        "# Iteration17 :\r\n",
        "\r\n",
        "slice17 = 16\r\n",
        "\r\n",
        "loser17 = [train_regret_loser_1[slice17],\r\n",
        "       train_regret_loser_2[slice17],\r\n",
        "       train_regret_loser_3[slice17],\r\n",
        "       train_regret_loser_4[slice17],\r\n",
        "       train_regret_loser_5[slice17],\r\n",
        "       train_regret_loser_6[slice17],\r\n",
        "       train_regret_loser_7[slice17],\r\n",
        "       train_regret_loser_8[slice17],\r\n",
        "       train_regret_loser_9[slice17],\r\n",
        "       train_regret_loser_10[slice17],\r\n",
        "       train_regret_loser_11[slice17],\r\n",
        "       train_regret_loser_12[slice17],\r\n",
        "       train_regret_loser_13[slice17],\r\n",
        "       train_regret_loser_14[slice17],\r\n",
        "       train_regret_loser_15[slice17],\r\n",
        "       train_regret_loser_16[slice17],\r\n",
        "       train_regret_loser_17[slice17],\r\n",
        "       train_regret_loser_18[slice17],\r\n",
        "       train_regret_loser_19[slice17],\r\n",
        "       train_regret_loser_20[slice17]]\r\n",
        "\r\n",
        "winner17 = [train_regret_winner_1[slice17],\r\n",
        "       train_regret_winner_2[slice17],\r\n",
        "       train_regret_winner_3[slice17],\r\n",
        "       train_regret_winner_4[slice17],\r\n",
        "       train_regret_winner_5[slice17],\r\n",
        "       train_regret_winner_6[slice17],\r\n",
        "       train_regret_winner_7[slice17],\r\n",
        "       train_regret_winner_8[slice17],\r\n",
        "       train_regret_winner_9[slice17],\r\n",
        "       train_regret_winner_10[slice17],\r\n",
        "       train_regret_winner_11[slice17],\r\n",
        "       train_regret_winner_12[slice17],\r\n",
        "       train_regret_winner_13[slice17],\r\n",
        "       train_regret_winner_14[slice17],\r\n",
        "       train_regret_winner_15[slice17],\r\n",
        "       train_regret_winner_16[slice17],\r\n",
        "       train_regret_winner_17[slice17],\r\n",
        "       train_regret_winner_18[slice17],\r\n",
        "       train_regret_winner_19[slice17],\r\n",
        "       train_regret_winner_20[slice17]]\r\n",
        "\r\n",
        "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\r\n",
        "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\r\n",
        "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\r\n",
        "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\r\n",
        "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\r\n",
        "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYnBmq7_xAuB"
      },
      "source": [
        "# Iteration27 :\r\n",
        "\r\n",
        "slice27 = 26\r\n",
        "\r\n",
        "loser27 = [train_regret_loser_1[slice27],\r\n",
        "       train_regret_loser_2[slice27],\r\n",
        "       train_regret_loser_3[slice27],\r\n",
        "       train_regret_loser_4[slice27],\r\n",
        "       train_regret_loser_5[slice27],\r\n",
        "       train_regret_loser_6[slice27],\r\n",
        "       train_regret_loser_7[slice27],\r\n",
        "       train_regret_loser_8[slice27],\r\n",
        "       train_regret_loser_9[slice27],\r\n",
        "       train_regret_loser_10[slice27],\r\n",
        "       train_regret_loser_11[slice27],\r\n",
        "       train_regret_loser_12[slice27],\r\n",
        "       train_regret_loser_13[slice27],\r\n",
        "       train_regret_loser_14[slice27],\r\n",
        "       train_regret_loser_15[slice27],\r\n",
        "       train_regret_loser_16[slice27],\r\n",
        "       train_regret_loser_17[slice27],\r\n",
        "       train_regret_loser_18[slice27],\r\n",
        "       train_regret_loser_19[slice27],\r\n",
        "       train_regret_loser_20[slice27]]\r\n",
        "\r\n",
        "winner27 = [train_regret_winner_1[slice27],\r\n",
        "       train_regret_winner_2[slice27],\r\n",
        "       train_regret_winner_3[slice27],\r\n",
        "       train_regret_winner_4[slice27],\r\n",
        "       train_regret_winner_5[slice27],\r\n",
        "       train_regret_winner_6[slice27],\r\n",
        "       train_regret_winner_7[slice27],\r\n",
        "       train_regret_winner_8[slice27],\r\n",
        "       train_regret_winner_9[slice27],\r\n",
        "       train_regret_winner_10[slice27],\r\n",
        "       train_regret_winner_11[slice27],\r\n",
        "       train_regret_winner_12[slice27],\r\n",
        "       train_regret_winner_13[slice27],\r\n",
        "       train_regret_winner_14[slice27],\r\n",
        "       train_regret_winner_15[slice27],\r\n",
        "       train_regret_winner_16[slice27],\r\n",
        "       train_regret_winner_17[slice27],\r\n",
        "       train_regret_winner_18[slice27],\r\n",
        "       train_regret_winner_19[slice27],\r\n",
        "       train_regret_winner_20[slice27]]\r\n",
        "\r\n",
        "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\r\n",
        "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\r\n",
        "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\r\n",
        "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\r\n",
        "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\r\n",
        "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ChA8KOUxAw6"
      },
      "source": [
        "# Iteration37 :\r\n",
        "\r\n",
        "slice37 = 36\r\n",
        "\r\n",
        "loser37 = [train_regret_loser_1[slice37],\r\n",
        "       train_regret_loser_2[slice37],\r\n",
        "       train_regret_loser_3[slice37],\r\n",
        "       train_regret_loser_4[slice37],\r\n",
        "       train_regret_loser_5[slice37],\r\n",
        "       train_regret_loser_6[slice37],\r\n",
        "       train_regret_loser_7[slice37],\r\n",
        "       train_regret_loser_8[slice37],\r\n",
        "       train_regret_loser_9[slice37],\r\n",
        "       train_regret_loser_10[slice37],\r\n",
        "       train_regret_loser_11[slice37],\r\n",
        "       train_regret_loser_12[slice37],\r\n",
        "       train_regret_loser_13[slice37],\r\n",
        "       train_regret_loser_14[slice37],\r\n",
        "       train_regret_loser_15[slice37],\r\n",
        "       train_regret_loser_16[slice37],\r\n",
        "       train_regret_loser_17[slice37],\r\n",
        "       train_regret_loser_18[slice37],\r\n",
        "       train_regret_loser_19[slice37],\r\n",
        "       train_regret_loser_20[slice37]]\r\n",
        "\r\n",
        "winner37 = [train_regret_winner_1[slice37],\r\n",
        "       train_regret_winner_2[slice37],\r\n",
        "       train_regret_winner_3[slice37],\r\n",
        "       train_regret_winner_4[slice37],\r\n",
        "       train_regret_winner_5[slice37],\r\n",
        "       train_regret_winner_6[slice37],\r\n",
        "       train_regret_winner_7[slice37],\r\n",
        "       train_regret_winner_8[slice37],\r\n",
        "       train_regret_winner_9[slice37],\r\n",
        "       train_regret_winner_10[slice37],\r\n",
        "       train_regret_winner_11[slice37],\r\n",
        "       train_regret_winner_12[slice37],\r\n",
        "       train_regret_winner_13[slice37],\r\n",
        "       train_regret_winner_14[slice37],\r\n",
        "       train_regret_winner_15[slice37],\r\n",
        "       train_regret_winner_16[slice37],\r\n",
        "       train_regret_winner_17[slice37],\r\n",
        "       train_regret_winner_18[slice37],\r\n",
        "       train_regret_winner_19[slice37],\r\n",
        "       train_regret_winner_20[slice37]]\r\n",
        "\r\n",
        "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\r\n",
        "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\r\n",
        "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\r\n",
        "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\r\n",
        "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\r\n",
        "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVsY1buDxAzx"
      },
      "source": [
        "# Iteration47 :\r\n",
        "\r\n",
        "slice47 = 46\r\n",
        "\r\n",
        "loser47 = [train_regret_loser_1[slice47],\r\n",
        "       train_regret_loser_2[slice47],\r\n",
        "       train_regret_loser_3[slice47],\r\n",
        "       train_regret_loser_4[slice47],\r\n",
        "       train_regret_loser_5[slice47],\r\n",
        "       train_regret_loser_6[slice47],\r\n",
        "       train_regret_loser_7[slice47],\r\n",
        "       train_regret_loser_8[slice47],\r\n",
        "       train_regret_loser_9[slice47],\r\n",
        "       train_regret_loser_10[slice47],\r\n",
        "       train_regret_loser_11[slice47],\r\n",
        "       train_regret_loser_12[slice47],\r\n",
        "       train_regret_loser_13[slice47],\r\n",
        "       train_regret_loser_14[slice47],\r\n",
        "       train_regret_loser_15[slice47],\r\n",
        "       train_regret_loser_16[slice47],\r\n",
        "       train_regret_loser_17[slice47],\r\n",
        "       train_regret_loser_18[slice47],\r\n",
        "       train_regret_loser_19[slice47],\r\n",
        "       train_regret_loser_20[slice47]]\r\n",
        "\r\n",
        "winner47 = [train_regret_winner_1[slice47],\r\n",
        "       train_regret_winner_2[slice47],\r\n",
        "       train_regret_winner_3[slice47],\r\n",
        "       train_regret_winner_4[slice47],\r\n",
        "       train_regret_winner_5[slice47],\r\n",
        "       train_regret_winner_6[slice47],\r\n",
        "       train_regret_winner_7[slice47],\r\n",
        "       train_regret_winner_8[slice47],\r\n",
        "       train_regret_winner_9[slice47],\r\n",
        "       train_regret_winner_10[slice47],\r\n",
        "       train_regret_winner_11[slice47],\r\n",
        "       train_regret_winner_12[slice47],\r\n",
        "       train_regret_winner_13[slice47],\r\n",
        "       train_regret_winner_14[slice47],\r\n",
        "       train_regret_winner_15[slice47],\r\n",
        "       train_regret_winner_16[slice47],\r\n",
        "       train_regret_winner_17[slice47],\r\n",
        "       train_regret_winner_18[slice47],\r\n",
        "       train_regret_winner_19[slice47],\r\n",
        "       train_regret_winner_20[slice47]]\r\n",
        "\r\n",
        "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\r\n",
        "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\r\n",
        "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\r\n",
        "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\r\n",
        "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\r\n",
        "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fIy_EyTxA20"
      },
      "source": [
        "# Iteration57 :\r\n",
        "\r\n",
        "slice57 = 56\r\n",
        "\r\n",
        "loser57 = [train_regret_loser_1[slice57],\r\n",
        "       train_regret_loser_2[slice57],\r\n",
        "       train_regret_loser_3[slice57],\r\n",
        "       train_regret_loser_4[slice57],\r\n",
        "       train_regret_loser_5[slice57],\r\n",
        "       train_regret_loser_6[slice57],\r\n",
        "       train_regret_loser_7[slice57],\r\n",
        "       train_regret_loser_8[slice57],\r\n",
        "       train_regret_loser_9[slice57],\r\n",
        "       train_regret_loser_10[slice57],\r\n",
        "       train_regret_loser_11[slice57],\r\n",
        "       train_regret_loser_12[slice57],\r\n",
        "       train_regret_loser_13[slice57],\r\n",
        "       train_regret_loser_14[slice57],\r\n",
        "       train_regret_loser_15[slice57],\r\n",
        "       train_regret_loser_16[slice57],\r\n",
        "       train_regret_loser_17[slice57],\r\n",
        "       train_regret_loser_18[slice57],\r\n",
        "       train_regret_loser_19[slice57],\r\n",
        "       train_regret_loser_20[slice57]]\r\n",
        "\r\n",
        "winner57 = [train_regret_winner_1[slice57],\r\n",
        "       train_regret_winner_2[slice57],\r\n",
        "       train_regret_winner_3[slice57],\r\n",
        "       train_regret_winner_4[slice57],\r\n",
        "       train_regret_winner_5[slice57],\r\n",
        "       train_regret_winner_6[slice57],\r\n",
        "       train_regret_winner_7[slice57],\r\n",
        "       train_regret_winner_8[slice57],\r\n",
        "       train_regret_winner_9[slice57],\r\n",
        "       train_regret_winner_10[slice57],\r\n",
        "       train_regret_winner_11[slice57],\r\n",
        "       train_regret_winner_12[slice57],\r\n",
        "       train_regret_winner_13[slice57],\r\n",
        "       train_regret_winner_14[slice57],\r\n",
        "       train_regret_winner_15[slice57],\r\n",
        "       train_regret_winner_16[slice57],\r\n",
        "       train_regret_winner_17[slice57],\r\n",
        "       train_regret_winner_18[slice57],\r\n",
        "       train_regret_winner_19[slice57],\r\n",
        "       train_regret_winner_20[slice57]]\r\n",
        "\r\n",
        "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\r\n",
        "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\r\n",
        "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\r\n",
        "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\r\n",
        "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\r\n",
        "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umwdBl8XxA5r"
      },
      "source": [
        "# Iteration67 :\r\n",
        "\r\n",
        "slice67 = 66\r\n",
        "\r\n",
        "loser67 = [train_regret_loser_1[slice67],\r\n",
        "       train_regret_loser_2[slice67],\r\n",
        "       train_regret_loser_3[slice67],\r\n",
        "       train_regret_loser_4[slice67],\r\n",
        "       train_regret_loser_5[slice67],\r\n",
        "       train_regret_loser_6[slice67],\r\n",
        "       train_regret_loser_7[slice67],\r\n",
        "       train_regret_loser_8[slice67],\r\n",
        "       train_regret_loser_9[slice67],\r\n",
        "       train_regret_loser_10[slice67],\r\n",
        "       train_regret_loser_11[slice67],\r\n",
        "       train_regret_loser_12[slice67],\r\n",
        "       train_regret_loser_13[slice67],\r\n",
        "       train_regret_loser_14[slice67],\r\n",
        "       train_regret_loser_15[slice67],\r\n",
        "       train_regret_loser_16[slice67],\r\n",
        "       train_regret_loser_17[slice67],\r\n",
        "       train_regret_loser_18[slice67],\r\n",
        "       train_regret_loser_19[slice67],\r\n",
        "       train_regret_loser_20[slice67]]\r\n",
        "\r\n",
        "winner67 = [train_regret_winner_1[slice67],\r\n",
        "       train_regret_winner_2[slice67],\r\n",
        "       train_regret_winner_3[slice67],\r\n",
        "       train_regret_winner_4[slice67],\r\n",
        "       train_regret_winner_5[slice67],\r\n",
        "       train_regret_winner_6[slice67],\r\n",
        "       train_regret_winner_7[slice67],\r\n",
        "       train_regret_winner_8[slice67],\r\n",
        "       train_regret_winner_9[slice67],\r\n",
        "       train_regret_winner_10[slice67],\r\n",
        "       train_regret_winner_11[slice67],\r\n",
        "       train_regret_winner_12[slice67],\r\n",
        "       train_regret_winner_13[slice67],\r\n",
        "       train_regret_winner_14[slice67],\r\n",
        "       train_regret_winner_15[slice67],\r\n",
        "       train_regret_winner_16[slice67],\r\n",
        "       train_regret_winner_17[slice67],\r\n",
        "       train_regret_winner_18[slice67],\r\n",
        "       train_regret_winner_19[slice67],\r\n",
        "       train_regret_winner_20[slice67]]\r\n",
        "\r\n",
        "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\r\n",
        "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\r\n",
        "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\r\n",
        "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\r\n",
        "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\r\n",
        "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF3KFX-ZxA8q"
      },
      "source": [
        "# Iteration77 :\r\n",
        "\r\n",
        "slice77 = 76\r\n",
        "\r\n",
        "loser77 = [train_regret_loser_1[slice77],\r\n",
        "       train_regret_loser_2[slice77],\r\n",
        "       train_regret_loser_3[slice77],\r\n",
        "       train_regret_loser_4[slice77],\r\n",
        "       train_regret_loser_5[slice77],\r\n",
        "       train_regret_loser_6[slice77],\r\n",
        "       train_regret_loser_7[slice77],\r\n",
        "       train_regret_loser_8[slice77],\r\n",
        "       train_regret_loser_9[slice77],\r\n",
        "       train_regret_loser_10[slice77],\r\n",
        "       train_regret_loser_11[slice77],\r\n",
        "       train_regret_loser_12[slice77],\r\n",
        "       train_regret_loser_13[slice77],\r\n",
        "       train_regret_loser_14[slice77],\r\n",
        "       train_regret_loser_15[slice77],\r\n",
        "       train_regret_loser_16[slice77],\r\n",
        "       train_regret_loser_17[slice77],\r\n",
        "       train_regret_loser_18[slice77],\r\n",
        "       train_regret_loser_19[slice77],\r\n",
        "       train_regret_loser_20[slice77]]\r\n",
        "\r\n",
        "winner77 = [train_regret_winner_1[slice77],\r\n",
        "       train_regret_winner_2[slice77],\r\n",
        "       train_regret_winner_3[slice77],\r\n",
        "       train_regret_winner_4[slice77],\r\n",
        "       train_regret_winner_5[slice77],\r\n",
        "       train_regret_winner_6[slice77],\r\n",
        "       train_regret_winner_7[slice77],\r\n",
        "       train_regret_winner_8[slice77],\r\n",
        "       train_regret_winner_9[slice77],\r\n",
        "       train_regret_winner_10[slice77],\r\n",
        "       train_regret_winner_11[slice77],\r\n",
        "       train_regret_winner_12[slice77],\r\n",
        "       train_regret_winner_13[slice77],\r\n",
        "       train_regret_winner_14[slice77],\r\n",
        "       train_regret_winner_15[slice77],\r\n",
        "       train_regret_winner_16[slice77],\r\n",
        "       train_regret_winner_17[slice77],\r\n",
        "       train_regret_winner_18[slice77],\r\n",
        "       train_regret_winner_19[slice77],\r\n",
        "       train_regret_winner_20[slice77]]\r\n",
        "\r\n",
        "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\r\n",
        "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\r\n",
        "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\r\n",
        "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\r\n",
        "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\r\n",
        "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVYft39xA_t"
      },
      "source": [
        "# Iteration87 :\r\n",
        "\r\n",
        "slice87 = 86\r\n",
        "\r\n",
        "loser87 = [train_regret_loser_1[slice87],\r\n",
        "       train_regret_loser_2[slice87],\r\n",
        "       train_regret_loser_3[slice87],\r\n",
        "       train_regret_loser_4[slice87],\r\n",
        "       train_regret_loser_5[slice87],\r\n",
        "       train_regret_loser_6[slice87],\r\n",
        "       train_regret_loser_7[slice87],\r\n",
        "       train_regret_loser_8[slice87],\r\n",
        "       train_regret_loser_9[slice87],\r\n",
        "       train_regret_loser_10[slice87],\r\n",
        "       train_regret_loser_11[slice87],\r\n",
        "       train_regret_loser_12[slice87],\r\n",
        "       train_regret_loser_13[slice87],\r\n",
        "       train_regret_loser_14[slice87],\r\n",
        "       train_regret_loser_15[slice87],\r\n",
        "       train_regret_loser_16[slice87],\r\n",
        "       train_regret_loser_17[slice87],\r\n",
        "       train_regret_loser_18[slice87],\r\n",
        "       train_regret_loser_19[slice87],\r\n",
        "       train_regret_loser_20[slice87]]\r\n",
        "\r\n",
        "winner87 = [train_regret_winner_1[slice87],\r\n",
        "       train_regret_winner_2[slice87],\r\n",
        "       train_regret_winner_3[slice87],\r\n",
        "       train_regret_winner_4[slice87],\r\n",
        "       train_regret_winner_5[slice87],\r\n",
        "       train_regret_winner_6[slice87],\r\n",
        "       train_regret_winner_7[slice87],\r\n",
        "       train_regret_winner_8[slice87],\r\n",
        "       train_regret_winner_9[slice87],\r\n",
        "       train_regret_winner_10[slice87],\r\n",
        "       train_regret_winner_11[slice87],\r\n",
        "       train_regret_winner_12[slice87],\r\n",
        "       train_regret_winner_13[slice87],\r\n",
        "       train_regret_winner_14[slice87],\r\n",
        "       train_regret_winner_15[slice87],\r\n",
        "       train_regret_winner_16[slice87],\r\n",
        "       train_regret_winner_17[slice87],\r\n",
        "       train_regret_winner_18[slice87],\r\n",
        "       train_regret_winner_19[slice87],\r\n",
        "       train_regret_winner_20[slice87]]\r\n",
        "\r\n",
        "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\r\n",
        "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\r\n",
        "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\r\n",
        "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\r\n",
        "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\r\n",
        "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gPkLC0yxBCs"
      },
      "source": [
        "# Iteration97 :\r\n",
        "\r\n",
        "slice97 = 96\r\n",
        "\r\n",
        "loser97 = [train_regret_loser_1[slice97],\r\n",
        "       train_regret_loser_2[slice97],\r\n",
        "       train_regret_loser_3[slice97],\r\n",
        "       train_regret_loser_4[slice97],\r\n",
        "       train_regret_loser_5[slice97],\r\n",
        "       train_regret_loser_6[slice97],\r\n",
        "       train_regret_loser_7[slice97],\r\n",
        "       train_regret_loser_8[slice97],\r\n",
        "       train_regret_loser_9[slice97],\r\n",
        "       train_regret_loser_10[slice97],\r\n",
        "       train_regret_loser_11[slice97],\r\n",
        "       train_regret_loser_12[slice97],\r\n",
        "       train_regret_loser_13[slice97],\r\n",
        "       train_regret_loser_14[slice97],\r\n",
        "       train_regret_loser_15[slice97],\r\n",
        "       train_regret_loser_16[slice97],\r\n",
        "       train_regret_loser_17[slice97],\r\n",
        "       train_regret_loser_18[slice97],\r\n",
        "       train_regret_loser_19[slice97],\r\n",
        "       train_regret_loser_20[slice97]]\r\n",
        "\r\n",
        "winner97 = [train_regret_winner_1[slice97],\r\n",
        "       train_regret_winner_2[slice97],\r\n",
        "       train_regret_winner_3[slice97],\r\n",
        "       train_regret_winner_4[slice97],\r\n",
        "       train_regret_winner_5[slice97],\r\n",
        "       train_regret_winner_6[slice97],\r\n",
        "       train_regret_winner_7[slice97],\r\n",
        "       train_regret_winner_8[slice97],\r\n",
        "       train_regret_winner_9[slice97],\r\n",
        "       train_regret_winner_10[slice97],\r\n",
        "       train_regret_winner_11[slice97],\r\n",
        "       train_regret_winner_12[slice97],\r\n",
        "       train_regret_winner_13[slice97],\r\n",
        "       train_regret_winner_14[slice97],\r\n",
        "       train_regret_winner_15[slice97],\r\n",
        "       train_regret_winner_16[slice97],\r\n",
        "       train_regret_winner_17[slice97],\r\n",
        "       train_regret_winner_18[slice97],\r\n",
        "       train_regret_winner_19[slice97],\r\n",
        "       train_regret_winner_20[slice97]]\r\n",
        "\r\n",
        "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\r\n",
        "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\r\n",
        "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\r\n",
        "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\r\n",
        "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\r\n",
        "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbgYEpYUxBFr"
      },
      "source": [
        "# Iteration8 :\r\n",
        "\r\n",
        "slice8 = 7\r\n",
        "\r\n",
        "loser8 = [train_regret_loser_1[slice8],\r\n",
        "       train_regret_loser_2[slice8],\r\n",
        "       train_regret_loser_3[slice8],\r\n",
        "       train_regret_loser_4[slice8],\r\n",
        "       train_regret_loser_5[slice8],\r\n",
        "       train_regret_loser_6[slice8],\r\n",
        "       train_regret_loser_7[slice8],\r\n",
        "       train_regret_loser_8[slice8],\r\n",
        "       train_regret_loser_9[slice8],\r\n",
        "       train_regret_loser_10[slice8],\r\n",
        "       train_regret_loser_11[slice8],\r\n",
        "       train_regret_loser_12[slice8],\r\n",
        "       train_regret_loser_13[slice8],\r\n",
        "       train_regret_loser_14[slice8],\r\n",
        "       train_regret_loser_15[slice8],\r\n",
        "       train_regret_loser_16[slice8],\r\n",
        "       train_regret_loser_17[slice8],\r\n",
        "       train_regret_loser_18[slice8],\r\n",
        "       train_regret_loser_19[slice8],\r\n",
        "       train_regret_loser_20[slice8]]\r\n",
        "\r\n",
        "winner8 = [train_regret_winner_1[slice8],\r\n",
        "       train_regret_winner_2[slice8],\r\n",
        "       train_regret_winner_3[slice8],\r\n",
        "       train_regret_winner_4[slice8],\r\n",
        "       train_regret_winner_5[slice8],\r\n",
        "       train_regret_winner_6[slice8],\r\n",
        "       train_regret_winner_7[slice8],\r\n",
        "       train_regret_winner_8[slice8],\r\n",
        "       train_regret_winner_9[slice8],\r\n",
        "       train_regret_winner_10[slice8],\r\n",
        "       train_regret_winner_11[slice8],\r\n",
        "       train_regret_winner_12[slice8],\r\n",
        "       train_regret_winner_13[slice8],\r\n",
        "       train_regret_winner_14[slice8],\r\n",
        "       train_regret_winner_15[slice8],\r\n",
        "       train_regret_winner_16[slice8],\r\n",
        "       train_regret_winner_17[slice8],\r\n",
        "       train_regret_winner_18[slice8],\r\n",
        "       train_regret_winner_19[slice8],\r\n",
        "       train_regret_winner_20[slice8]]\r\n",
        "\r\n",
        "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\r\n",
        "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\r\n",
        "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\r\n",
        "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\r\n",
        "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\r\n",
        "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im9dHUqvxBH9"
      },
      "source": [
        "# Iteration18 :\r\n",
        "\r\n",
        "slice18 = 17\r\n",
        "\r\n",
        "loser18 = [train_regret_loser_1[slice18],\r\n",
        "       train_regret_loser_2[slice18],\r\n",
        "       train_regret_loser_3[slice18],\r\n",
        "       train_regret_loser_4[slice18],\r\n",
        "       train_regret_loser_5[slice18],\r\n",
        "       train_regret_loser_6[slice18],\r\n",
        "       train_regret_loser_7[slice18],\r\n",
        "       train_regret_loser_8[slice18],\r\n",
        "       train_regret_loser_9[slice18],\r\n",
        "       train_regret_loser_10[slice18],\r\n",
        "       train_regret_loser_11[slice18],\r\n",
        "       train_regret_loser_12[slice18],\r\n",
        "       train_regret_loser_13[slice18],\r\n",
        "       train_regret_loser_14[slice18],\r\n",
        "       train_regret_loser_15[slice18],\r\n",
        "       train_regret_loser_16[slice18],\r\n",
        "       train_regret_loser_17[slice18],\r\n",
        "       train_regret_loser_18[slice18],\r\n",
        "       train_regret_loser_19[slice18],\r\n",
        "       train_regret_loser_20[slice18]]\r\n",
        "\r\n",
        "winner18 = [train_regret_winner_1[slice18],\r\n",
        "       train_regret_winner_2[slice18],\r\n",
        "       train_regret_winner_3[slice18],\r\n",
        "       train_regret_winner_4[slice18],\r\n",
        "       train_regret_winner_5[slice18],\r\n",
        "       train_regret_winner_6[slice18],\r\n",
        "       train_regret_winner_7[slice18],\r\n",
        "       train_regret_winner_8[slice18],\r\n",
        "       train_regret_winner_9[slice18],\r\n",
        "       train_regret_winner_10[slice18],\r\n",
        "       train_regret_winner_11[slice18],\r\n",
        "       train_regret_winner_12[slice18],\r\n",
        "       train_regret_winner_13[slice18],\r\n",
        "       train_regret_winner_14[slice18],\r\n",
        "       train_regret_winner_15[slice18],\r\n",
        "       train_regret_winner_16[slice18],\r\n",
        "       train_regret_winner_17[slice18],\r\n",
        "       train_regret_winner_18[slice18],\r\n",
        "       train_regret_winner_19[slice18],\r\n",
        "       train_regret_winner_20[slice18]]\r\n",
        "\r\n",
        "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\r\n",
        "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\r\n",
        "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\r\n",
        "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\r\n",
        "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\r\n",
        "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvqFjlBDxBK4"
      },
      "source": [
        "# Iteration28 :\r\n",
        "\r\n",
        "slice28 = 27\r\n",
        "\r\n",
        "loser28 = [train_regret_loser_1[slice28],\r\n",
        "       train_regret_loser_2[slice28],\r\n",
        "       train_regret_loser_3[slice28],\r\n",
        "       train_regret_loser_4[slice28],\r\n",
        "       train_regret_loser_5[slice28],\r\n",
        "       train_regret_loser_6[slice28],\r\n",
        "       train_regret_loser_7[slice28],\r\n",
        "       train_regret_loser_8[slice28],\r\n",
        "       train_regret_loser_9[slice28],\r\n",
        "       train_regret_loser_10[slice28],\r\n",
        "       train_regret_loser_11[slice28],\r\n",
        "       train_regret_loser_12[slice28],\r\n",
        "       train_regret_loser_13[slice28],\r\n",
        "       train_regret_loser_14[slice28],\r\n",
        "       train_regret_loser_15[slice28],\r\n",
        "       train_regret_loser_16[slice28],\r\n",
        "       train_regret_loser_17[slice28],\r\n",
        "       train_regret_loser_18[slice28],\r\n",
        "       train_regret_loser_19[slice28],\r\n",
        "       train_regret_loser_20[slice28]]\r\n",
        "\r\n",
        "winner28 = [train_regret_winner_1[slice28],\r\n",
        "       train_regret_winner_2[slice28],\r\n",
        "       train_regret_winner_3[slice28],\r\n",
        "       train_regret_winner_4[slice28],\r\n",
        "       train_regret_winner_5[slice28],\r\n",
        "       train_regret_winner_6[slice28],\r\n",
        "       train_regret_winner_7[slice28],\r\n",
        "       train_regret_winner_8[slice28],\r\n",
        "       train_regret_winner_9[slice28],\r\n",
        "       train_regret_winner_10[slice28],\r\n",
        "       train_regret_winner_11[slice28],\r\n",
        "       train_regret_winner_12[slice28],\r\n",
        "       train_regret_winner_13[slice28],\r\n",
        "       train_regret_winner_14[slice28],\r\n",
        "       train_regret_winner_15[slice28],\r\n",
        "       train_regret_winner_16[slice28],\r\n",
        "       train_regret_winner_17[slice28],\r\n",
        "       train_regret_winner_18[slice28],\r\n",
        "       train_regret_winner_19[slice28],\r\n",
        "       train_regret_winner_20[slice28]]\r\n",
        "\r\n",
        "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\r\n",
        "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\r\n",
        "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\r\n",
        "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\r\n",
        "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\r\n",
        "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It5daJCEted3"
      },
      "source": [
        "# Iteration38 :\r\n",
        "\r\n",
        "slice38 = 37\r\n",
        "\r\n",
        "loser38 = [train_regret_loser_1[slice38],\r\n",
        "       train_regret_loser_2[slice38],\r\n",
        "       train_regret_loser_3[slice38],\r\n",
        "       train_regret_loser_4[slice38],\r\n",
        "       train_regret_loser_5[slice38],\r\n",
        "       train_regret_loser_6[slice38],\r\n",
        "       train_regret_loser_7[slice38],\r\n",
        "       train_regret_loser_8[slice38],\r\n",
        "       train_regret_loser_9[slice38],\r\n",
        "       train_regret_loser_10[slice38],\r\n",
        "       train_regret_loser_11[slice38],\r\n",
        "       train_regret_loser_12[slice38],\r\n",
        "       train_regret_loser_13[slice38],\r\n",
        "       train_regret_loser_14[slice38],\r\n",
        "       train_regret_loser_15[slice38],\r\n",
        "       train_regret_loser_16[slice38],\r\n",
        "       train_regret_loser_17[slice38],\r\n",
        "       train_regret_loser_18[slice38],\r\n",
        "       train_regret_loser_19[slice38],\r\n",
        "       train_regret_loser_20[slice38]]\r\n",
        "\r\n",
        "winner38 = [train_regret_winner_1[slice38],\r\n",
        "       train_regret_winner_2[slice38],\r\n",
        "       train_regret_winner_3[slice38],\r\n",
        "       train_regret_winner_4[slice38],\r\n",
        "       train_regret_winner_5[slice38],\r\n",
        "       train_regret_winner_6[slice38],\r\n",
        "       train_regret_winner_7[slice38],\r\n",
        "       train_regret_winner_8[slice38],\r\n",
        "       train_regret_winner_9[slice38],\r\n",
        "       train_regret_winner_10[slice38],\r\n",
        "       train_regret_winner_11[slice38],\r\n",
        "       train_regret_winner_12[slice38],\r\n",
        "       train_regret_winner_13[slice38],\r\n",
        "       train_regret_winner_14[slice38],\r\n",
        "       train_regret_winner_15[slice38],\r\n",
        "       train_regret_winner_16[slice38],\r\n",
        "       train_regret_winner_17[slice38],\r\n",
        "       train_regret_winner_18[slice38],\r\n",
        "       train_regret_winner_19[slice38],\r\n",
        "       train_regret_winner_20[slice38]]\r\n",
        "\r\n",
        "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\r\n",
        "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\r\n",
        "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\r\n",
        "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\r\n",
        "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\r\n",
        "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GekXOf_gtegw"
      },
      "source": [
        "# Iteration48 :\r\n",
        "\r\n",
        "slice48 = 47\r\n",
        "\r\n",
        "loser48 = [train_regret_loser_1[slice48],\r\n",
        "       train_regret_loser_2[slice48],\r\n",
        "       train_regret_loser_3[slice48],\r\n",
        "       train_regret_loser_4[slice48],\r\n",
        "       train_regret_loser_5[slice48],\r\n",
        "       train_regret_loser_6[slice48],\r\n",
        "       train_regret_loser_7[slice48],\r\n",
        "       train_regret_loser_8[slice48],\r\n",
        "       train_regret_loser_9[slice48],\r\n",
        "       train_regret_loser_10[slice48],\r\n",
        "       train_regret_loser_11[slice48],\r\n",
        "       train_regret_loser_12[slice48],\r\n",
        "       train_regret_loser_13[slice48],\r\n",
        "       train_regret_loser_14[slice48],\r\n",
        "       train_regret_loser_15[slice48],\r\n",
        "       train_regret_loser_16[slice48],\r\n",
        "       train_regret_loser_17[slice48],\r\n",
        "       train_regret_loser_18[slice48],\r\n",
        "       train_regret_loser_19[slice48],\r\n",
        "       train_regret_loser_20[slice48]]\r\n",
        "\r\n",
        "winner48 = [train_regret_winner_1[slice48],\r\n",
        "       train_regret_winner_2[slice48],\r\n",
        "       train_regret_winner_3[slice48],\r\n",
        "       train_regret_winner_4[slice48],\r\n",
        "       train_regret_winner_5[slice48],\r\n",
        "       train_regret_winner_6[slice48],\r\n",
        "       train_regret_winner_7[slice48],\r\n",
        "       train_regret_winner_8[slice48],\r\n",
        "       train_regret_winner_9[slice48],\r\n",
        "       train_regret_winner_10[slice48],\r\n",
        "       train_regret_winner_11[slice48],\r\n",
        "       train_regret_winner_12[slice48],\r\n",
        "       train_regret_winner_13[slice48],\r\n",
        "       train_regret_winner_14[slice48],\r\n",
        "       train_regret_winner_15[slice48],\r\n",
        "       train_regret_winner_16[slice48],\r\n",
        "       train_regret_winner_17[slice48],\r\n",
        "       train_regret_winner_18[slice48],\r\n",
        "       train_regret_winner_19[slice48],\r\n",
        "       train_regret_winner_20[slice48]]\r\n",
        "\r\n",
        "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\r\n",
        "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\r\n",
        "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\r\n",
        "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\r\n",
        "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\r\n",
        "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNpV6knOtejv"
      },
      "source": [
        "# Iteration58 :\r\n",
        "\r\n",
        "slice58 = 57\r\n",
        "\r\n",
        "loser58 = [train_regret_loser_1[slice58],\r\n",
        "       train_regret_loser_2[slice58],\r\n",
        "       train_regret_loser_3[slice58],\r\n",
        "       train_regret_loser_4[slice58],\r\n",
        "       train_regret_loser_5[slice58],\r\n",
        "       train_regret_loser_6[slice58],\r\n",
        "       train_regret_loser_7[slice58],\r\n",
        "       train_regret_loser_8[slice58],\r\n",
        "       train_regret_loser_9[slice58],\r\n",
        "       train_regret_loser_10[slice58],\r\n",
        "       train_regret_loser_11[slice58],\r\n",
        "       train_regret_loser_12[slice58],\r\n",
        "       train_regret_loser_13[slice58],\r\n",
        "       train_regret_loser_14[slice58],\r\n",
        "       train_regret_loser_15[slice58],\r\n",
        "       train_regret_loser_16[slice58],\r\n",
        "       train_regret_loser_17[slice58],\r\n",
        "       train_regret_loser_18[slice58],\r\n",
        "       train_regret_loser_19[slice58],\r\n",
        "       train_regret_loser_20[slice58]]\r\n",
        "\r\n",
        "winner58 = [train_regret_winner_1[slice58],\r\n",
        "       train_regret_winner_2[slice58],\r\n",
        "       train_regret_winner_3[slice58],\r\n",
        "       train_regret_winner_4[slice58],\r\n",
        "       train_regret_winner_5[slice58],\r\n",
        "       train_regret_winner_6[slice58],\r\n",
        "       train_regret_winner_7[slice58],\r\n",
        "       train_regret_winner_8[slice58],\r\n",
        "       train_regret_winner_9[slice58],\r\n",
        "       train_regret_winner_10[slice58],\r\n",
        "       train_regret_winner_11[slice58],\r\n",
        "       train_regret_winner_12[slice58],\r\n",
        "       train_regret_winner_13[slice58],\r\n",
        "       train_regret_winner_14[slice58],\r\n",
        "       train_regret_winner_15[slice58],\r\n",
        "       train_regret_winner_16[slice58],\r\n",
        "       train_regret_winner_17[slice58],\r\n",
        "       train_regret_winner_18[slice58],\r\n",
        "       train_regret_winner_19[slice58],\r\n",
        "       train_regret_winner_20[slice58]]\r\n",
        "\r\n",
        "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\r\n",
        "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\r\n",
        "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\r\n",
        "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\r\n",
        "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\r\n",
        "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VFsqaBqtemn"
      },
      "source": [
        "# Iteration68 :\r\n",
        "\r\n",
        "slice68 = 67\r\n",
        "\r\n",
        "loser68 = [train_regret_loser_1[slice68],\r\n",
        "       train_regret_loser_2[slice68],\r\n",
        "       train_regret_loser_3[slice68],\r\n",
        "       train_regret_loser_4[slice68],\r\n",
        "       train_regret_loser_5[slice68],\r\n",
        "       train_regret_loser_6[slice68],\r\n",
        "       train_regret_loser_7[slice68],\r\n",
        "       train_regret_loser_8[slice68],\r\n",
        "       train_regret_loser_9[slice68],\r\n",
        "       train_regret_loser_10[slice68],\r\n",
        "       train_regret_loser_11[slice68],\r\n",
        "       train_regret_loser_12[slice68],\r\n",
        "       train_regret_loser_13[slice68],\r\n",
        "       train_regret_loser_14[slice68],\r\n",
        "       train_regret_loser_15[slice68],\r\n",
        "       train_regret_loser_16[slice68],\r\n",
        "       train_regret_loser_17[slice68],\r\n",
        "       train_regret_loser_18[slice68],\r\n",
        "       train_regret_loser_19[slice68],\r\n",
        "       train_regret_loser_20[slice68]]\r\n",
        "\r\n",
        "winner68 = [train_regret_winner_1[slice68],\r\n",
        "       train_regret_winner_2[slice68],\r\n",
        "       train_regret_winner_3[slice68],\r\n",
        "       train_regret_winner_4[slice68],\r\n",
        "       train_regret_winner_5[slice68],\r\n",
        "       train_regret_winner_6[slice68],\r\n",
        "       train_regret_winner_7[slice68],\r\n",
        "       train_regret_winner_8[slice68],\r\n",
        "       train_regret_winner_9[slice68],\r\n",
        "       train_regret_winner_10[slice68],\r\n",
        "       train_regret_winner_11[slice68],\r\n",
        "       train_regret_winner_12[slice68],\r\n",
        "       train_regret_winner_13[slice68],\r\n",
        "       train_regret_winner_14[slice68],\r\n",
        "       train_regret_winner_15[slice68],\r\n",
        "       train_regret_winner_16[slice68],\r\n",
        "       train_regret_winner_17[slice68],\r\n",
        "       train_regret_winner_18[slice68],\r\n",
        "       train_regret_winner_19[slice68],\r\n",
        "       train_regret_winner_20[slice68]]\r\n",
        "\r\n",
        "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\r\n",
        "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\r\n",
        "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\r\n",
        "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\r\n",
        "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\r\n",
        "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XlFH0Wxtepo"
      },
      "source": [
        "# Iteration78 :\r\n",
        "\r\n",
        "slice78 = 77\r\n",
        "\r\n",
        "loser78 = [train_regret_loser_1[slice78],\r\n",
        "       train_regret_loser_2[slice78],\r\n",
        "       train_regret_loser_3[slice78],\r\n",
        "       train_regret_loser_4[slice78],\r\n",
        "       train_regret_loser_5[slice78],\r\n",
        "       train_regret_loser_6[slice78],\r\n",
        "       train_regret_loser_7[slice78],\r\n",
        "       train_regret_loser_8[slice78],\r\n",
        "       train_regret_loser_9[slice78],\r\n",
        "       train_regret_loser_10[slice78],\r\n",
        "       train_regret_loser_11[slice78],\r\n",
        "       train_regret_loser_12[slice78],\r\n",
        "       train_regret_loser_13[slice78],\r\n",
        "       train_regret_loser_14[slice78],\r\n",
        "       train_regret_loser_15[slice78],\r\n",
        "       train_regret_loser_16[slice78],\r\n",
        "       train_regret_loser_17[slice78],\r\n",
        "       train_regret_loser_18[slice78],\r\n",
        "       train_regret_loser_19[slice78],\r\n",
        "       train_regret_loser_20[slice78]]\r\n",
        "\r\n",
        "winner78 = [train_regret_winner_1[slice78],\r\n",
        "       train_regret_winner_2[slice78],\r\n",
        "       train_regret_winner_3[slice78],\r\n",
        "       train_regret_winner_4[slice78],\r\n",
        "       train_regret_winner_5[slice78],\r\n",
        "       train_regret_winner_6[slice78],\r\n",
        "       train_regret_winner_7[slice78],\r\n",
        "       train_regret_winner_8[slice78],\r\n",
        "       train_regret_winner_9[slice78],\r\n",
        "       train_regret_winner_10[slice78],\r\n",
        "       train_regret_winner_11[slice78],\r\n",
        "       train_regret_winner_12[slice78],\r\n",
        "       train_regret_winner_13[slice78],\r\n",
        "       train_regret_winner_14[slice78],\r\n",
        "       train_regret_winner_15[slice78],\r\n",
        "       train_regret_winner_16[slice78],\r\n",
        "       train_regret_winner_17[slice78],\r\n",
        "       train_regret_winner_18[slice78],\r\n",
        "       train_regret_winner_19[slice78],\r\n",
        "       train_regret_winner_20[slice78]]\r\n",
        "\r\n",
        "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\r\n",
        "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\r\n",
        "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\r\n",
        "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\r\n",
        "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\r\n",
        "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsL0PJVetesC"
      },
      "source": [
        "# Iteration88 :\r\n",
        "\r\n",
        "slice88 = 87\r\n",
        "\r\n",
        "loser88 = [train_regret_loser_1[slice88],\r\n",
        "       train_regret_loser_2[slice88],\r\n",
        "       train_regret_loser_3[slice88],\r\n",
        "       train_regret_loser_4[slice88],\r\n",
        "       train_regret_loser_5[slice88],\r\n",
        "       train_regret_loser_6[slice88],\r\n",
        "       train_regret_loser_7[slice88],\r\n",
        "       train_regret_loser_8[slice88],\r\n",
        "       train_regret_loser_9[slice88],\r\n",
        "       train_regret_loser_10[slice88],\r\n",
        "       train_regret_loser_11[slice88],\r\n",
        "       train_regret_loser_12[slice88],\r\n",
        "       train_regret_loser_13[slice88],\r\n",
        "       train_regret_loser_14[slice88],\r\n",
        "       train_regret_loser_15[slice88],\r\n",
        "       train_regret_loser_16[slice88],\r\n",
        "       train_regret_loser_17[slice88],\r\n",
        "       train_regret_loser_18[slice88],\r\n",
        "       train_regret_loser_19[slice88],\r\n",
        "       train_regret_loser_20[slice88]]\r\n",
        "\r\n",
        "winner88 = [train_regret_winner_1[slice88],\r\n",
        "       train_regret_winner_2[slice88],\r\n",
        "       train_regret_winner_3[slice88],\r\n",
        "       train_regret_winner_4[slice88],\r\n",
        "       train_regret_winner_5[slice88],\r\n",
        "       train_regret_winner_6[slice88],\r\n",
        "       train_regret_winner_7[slice88],\r\n",
        "       train_regret_winner_8[slice88],\r\n",
        "       train_regret_winner_9[slice88],\r\n",
        "       train_regret_winner_10[slice88],\r\n",
        "       train_regret_winner_11[slice88],\r\n",
        "       train_regret_winner_12[slice88],\r\n",
        "       train_regret_winner_13[slice88],\r\n",
        "       train_regret_winner_14[slice88],\r\n",
        "       train_regret_winner_15[slice88],\r\n",
        "       train_regret_winner_16[slice88],\r\n",
        "       train_regret_winner_17[slice88],\r\n",
        "       train_regret_winner_18[slice88],\r\n",
        "       train_regret_winner_19[slice88],\r\n",
        "       train_regret_winner_20[slice88]]\r\n",
        "\r\n",
        "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\r\n",
        "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\r\n",
        "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\r\n",
        "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\r\n",
        "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\r\n",
        "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYWEWGku1o_9"
      },
      "source": [
        "# Iteration98 :\r\n",
        "\r\n",
        "slice98 = 97\r\n",
        "\r\n",
        "loser98 = [train_regret_loser_1[slice98],\r\n",
        "       train_regret_loser_2[slice98],\r\n",
        "       train_regret_loser_3[slice98],\r\n",
        "       train_regret_loser_4[slice98],\r\n",
        "       train_regret_loser_5[slice98],\r\n",
        "       train_regret_loser_6[slice98],\r\n",
        "       train_regret_loser_7[slice98],\r\n",
        "       train_regret_loser_8[slice98],\r\n",
        "       train_regret_loser_9[slice98],\r\n",
        "       train_regret_loser_10[slice98],\r\n",
        "       train_regret_loser_11[slice98],\r\n",
        "       train_regret_loser_12[slice98],\r\n",
        "       train_regret_loser_13[slice98],\r\n",
        "       train_regret_loser_14[slice98],\r\n",
        "       train_regret_loser_15[slice98],\r\n",
        "       train_regret_loser_16[slice98],\r\n",
        "       train_regret_loser_17[slice98],\r\n",
        "       train_regret_loser_18[slice98],\r\n",
        "       train_regret_loser_19[slice98],\r\n",
        "       train_regret_loser_20[slice98]]\r\n",
        "\r\n",
        "winner98 = [train_regret_winner_1[slice98],\r\n",
        "       train_regret_winner_2[slice98],\r\n",
        "       train_regret_winner_3[slice98],\r\n",
        "       train_regret_winner_4[slice98],\r\n",
        "       train_regret_winner_5[slice98],\r\n",
        "       train_regret_winner_6[slice98],\r\n",
        "       train_regret_winner_7[slice98],\r\n",
        "       train_regret_winner_8[slice98],\r\n",
        "       train_regret_winner_9[slice98],\r\n",
        "       train_regret_winner_10[slice98],\r\n",
        "       train_regret_winner_11[slice98],\r\n",
        "       train_regret_winner_12[slice98],\r\n",
        "       train_regret_winner_13[slice98],\r\n",
        "       train_regret_winner_14[slice98],\r\n",
        "       train_regret_winner_15[slice98],\r\n",
        "       train_regret_winner_16[slice98],\r\n",
        "       train_regret_winner_17[slice98],\r\n",
        "       train_regret_winner_18[slice98],\r\n",
        "       train_regret_winner_19[slice98],\r\n",
        "       train_regret_winner_20[slice98]]\r\n",
        "\r\n",
        "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\r\n",
        "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\r\n",
        "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\r\n",
        "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\r\n",
        "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\r\n",
        "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guLGgcX91pDK"
      },
      "source": [
        "# Iteration9 :\r\n",
        "\r\n",
        "slice9 = 8\r\n",
        "\r\n",
        "loser9 = [train_regret_loser_1[slice9],\r\n",
        "       train_regret_loser_2[slice9],\r\n",
        "       train_regret_loser_3[slice9],\r\n",
        "       train_regret_loser_4[slice9],\r\n",
        "       train_regret_loser_5[slice9],\r\n",
        "       train_regret_loser_6[slice9],\r\n",
        "       train_regret_loser_7[slice9],\r\n",
        "       train_regret_loser_8[slice9],\r\n",
        "       train_regret_loser_9[slice9],\r\n",
        "       train_regret_loser_10[slice9],\r\n",
        "       train_regret_loser_11[slice9],\r\n",
        "       train_regret_loser_12[slice9],\r\n",
        "       train_regret_loser_13[slice9],\r\n",
        "       train_regret_loser_14[slice9],\r\n",
        "       train_regret_loser_15[slice9],\r\n",
        "       train_regret_loser_16[slice9],\r\n",
        "       train_regret_loser_17[slice9],\r\n",
        "       train_regret_loser_18[slice9],\r\n",
        "       train_regret_loser_19[slice9],\r\n",
        "       train_regret_loser_20[slice9]]\r\n",
        "\r\n",
        "winner9 = [train_regret_winner_1[slice9],\r\n",
        "       train_regret_winner_2[slice9],\r\n",
        "       train_regret_winner_3[slice9],\r\n",
        "       train_regret_winner_4[slice9],\r\n",
        "       train_regret_winner_5[slice9],\r\n",
        "       train_regret_winner_6[slice9],\r\n",
        "       train_regret_winner_7[slice9],\r\n",
        "       train_regret_winner_8[slice9],\r\n",
        "       train_regret_winner_9[slice9],\r\n",
        "       train_regret_winner_10[slice9],\r\n",
        "       train_regret_winner_11[slice9],\r\n",
        "       train_regret_winner_12[slice9],\r\n",
        "       train_regret_winner_13[slice9],\r\n",
        "       train_regret_winner_14[slice9],\r\n",
        "       train_regret_winner_15[slice9],\r\n",
        "       train_regret_winner_16[slice9],\r\n",
        "       train_regret_winner_17[slice9],\r\n",
        "       train_regret_winner_18[slice9],\r\n",
        "       train_regret_winner_19[slice9],\r\n",
        "       train_regret_winner_20[slice9]]\r\n",
        "\r\n",
        "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\r\n",
        "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\r\n",
        "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\r\n",
        "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\r\n",
        "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\r\n",
        "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3_9-eoT1pGd"
      },
      "source": [
        "# Iteration19 :\r\n",
        "\r\n",
        "slice19 = 18\r\n",
        "\r\n",
        "loser19 = [train_regret_loser_1[slice19],\r\n",
        "       train_regret_loser_2[slice19],\r\n",
        "       train_regret_loser_3[slice19],\r\n",
        "       train_regret_loser_4[slice19],\r\n",
        "       train_regret_loser_5[slice19],\r\n",
        "       train_regret_loser_6[slice19],\r\n",
        "       train_regret_loser_7[slice19],\r\n",
        "       train_regret_loser_8[slice19],\r\n",
        "       train_regret_loser_9[slice19],\r\n",
        "       train_regret_loser_10[slice19],\r\n",
        "       train_regret_loser_11[slice19],\r\n",
        "       train_regret_loser_12[slice19],\r\n",
        "       train_regret_loser_13[slice19],\r\n",
        "       train_regret_loser_14[slice19],\r\n",
        "       train_regret_loser_15[slice19],\r\n",
        "       train_regret_loser_16[slice19],\r\n",
        "       train_regret_loser_17[slice19],\r\n",
        "       train_regret_loser_18[slice19],\r\n",
        "       train_regret_loser_19[slice19],\r\n",
        "       train_regret_loser_20[slice19]]\r\n",
        "\r\n",
        "winner19 = [train_regret_winner_1[slice19],\r\n",
        "       train_regret_winner_2[slice19],\r\n",
        "       train_regret_winner_3[slice19],\r\n",
        "       train_regret_winner_4[slice19],\r\n",
        "       train_regret_winner_5[slice19],\r\n",
        "       train_regret_winner_6[slice19],\r\n",
        "       train_regret_winner_7[slice19],\r\n",
        "       train_regret_winner_8[slice19],\r\n",
        "       train_regret_winner_9[slice19],\r\n",
        "       train_regret_winner_10[slice19],\r\n",
        "       train_regret_winner_11[slice19],\r\n",
        "       train_regret_winner_12[slice19],\r\n",
        "       train_regret_winner_13[slice19],\r\n",
        "       train_regret_winner_14[slice19],\r\n",
        "       train_regret_winner_15[slice19],\r\n",
        "       train_regret_winner_16[slice19],\r\n",
        "       train_regret_winner_17[slice19],\r\n",
        "       train_regret_winner_18[slice19],\r\n",
        "       train_regret_winner_19[slice19],\r\n",
        "       train_regret_winner_20[slice19]]\r\n",
        "\r\n",
        "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\r\n",
        "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\r\n",
        "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\r\n",
        "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\r\n",
        "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\r\n",
        "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ae5x13M1pRp"
      },
      "source": [
        "# Iteration29 :\r\n",
        "\r\n",
        "slice29 = 28\r\n",
        "\r\n",
        "loser29 = [train_regret_loser_1[slice29],\r\n",
        "       train_regret_loser_2[slice29],\r\n",
        "       train_regret_loser_3[slice29],\r\n",
        "       train_regret_loser_4[slice29],\r\n",
        "       train_regret_loser_5[slice29],\r\n",
        "       train_regret_loser_6[slice29],\r\n",
        "       train_regret_loser_7[slice29],\r\n",
        "       train_regret_loser_8[slice29],\r\n",
        "       train_regret_loser_9[slice29],\r\n",
        "       train_regret_loser_10[slice29],\r\n",
        "       train_regret_loser_11[slice29],\r\n",
        "       train_regret_loser_12[slice29],\r\n",
        "       train_regret_loser_13[slice29],\r\n",
        "       train_regret_loser_14[slice29],\r\n",
        "       train_regret_loser_15[slice29],\r\n",
        "       train_regret_loser_16[slice29],\r\n",
        "       train_regret_loser_17[slice29],\r\n",
        "       train_regret_loser_18[slice29],\r\n",
        "       train_regret_loser_19[slice29],\r\n",
        "       train_regret_loser_20[slice29]]\r\n",
        "\r\n",
        "winner29 = [train_regret_winner_1[slice29],\r\n",
        "       train_regret_winner_2[slice29],\r\n",
        "       train_regret_winner_3[slice29],\r\n",
        "       train_regret_winner_4[slice29],\r\n",
        "       train_regret_winner_5[slice29],\r\n",
        "       train_regret_winner_6[slice29],\r\n",
        "       train_regret_winner_7[slice29],\r\n",
        "       train_regret_winner_8[slice29],\r\n",
        "       train_regret_winner_9[slice29],\r\n",
        "       train_regret_winner_10[slice29],\r\n",
        "       train_regret_winner_11[slice29],\r\n",
        "       train_regret_winner_12[slice29],\r\n",
        "       train_regret_winner_13[slice29],\r\n",
        "       train_regret_winner_14[slice29],\r\n",
        "       train_regret_winner_15[slice29],\r\n",
        "       train_regret_winner_16[slice29],\r\n",
        "       train_regret_winner_17[slice29],\r\n",
        "       train_regret_winner_18[slice29],\r\n",
        "       train_regret_winner_19[slice29],\r\n",
        "       train_regret_winner_20[slice29]]\r\n",
        "\r\n",
        "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\r\n",
        "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\r\n",
        "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\r\n",
        "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\r\n",
        "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\r\n",
        "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR9aPvBy1pUc"
      },
      "source": [
        "# Iteration39 :\r\n",
        "\r\n",
        "slice39 = 38\r\n",
        "\r\n",
        "loser39 = [train_regret_loser_1[slice39],\r\n",
        "       train_regret_loser_2[slice39],\r\n",
        "       train_regret_loser_3[slice39],\r\n",
        "       train_regret_loser_4[slice39],\r\n",
        "       train_regret_loser_5[slice39],\r\n",
        "       train_regret_loser_6[slice39],\r\n",
        "       train_regret_loser_7[slice39],\r\n",
        "       train_regret_loser_8[slice39],\r\n",
        "       train_regret_loser_9[slice39],\r\n",
        "       train_regret_loser_10[slice39],\r\n",
        "       train_regret_loser_11[slice39],\r\n",
        "       train_regret_loser_12[slice39],\r\n",
        "       train_regret_loser_13[slice39],\r\n",
        "       train_regret_loser_14[slice39],\r\n",
        "       train_regret_loser_15[slice39],\r\n",
        "       train_regret_loser_16[slice39],\r\n",
        "       train_regret_loser_17[slice39],\r\n",
        "       train_regret_loser_18[slice39],\r\n",
        "       train_regret_loser_19[slice39],\r\n",
        "       train_regret_loser_20[slice39]]\r\n",
        "\r\n",
        "winner39 = [train_regret_winner_1[slice39],\r\n",
        "       train_regret_winner_2[slice39],\r\n",
        "       train_regret_winner_3[slice39],\r\n",
        "       train_regret_winner_4[slice39],\r\n",
        "       train_regret_winner_5[slice39],\r\n",
        "       train_regret_winner_6[slice39],\r\n",
        "       train_regret_winner_7[slice39],\r\n",
        "       train_regret_winner_8[slice39],\r\n",
        "       train_regret_winner_9[slice39],\r\n",
        "       train_regret_winner_10[slice39],\r\n",
        "       train_regret_winner_11[slice39],\r\n",
        "       train_regret_winner_12[slice39],\r\n",
        "       train_regret_winner_13[slice39],\r\n",
        "       train_regret_winner_14[slice39],\r\n",
        "       train_regret_winner_15[slice39],\r\n",
        "       train_regret_winner_16[slice39],\r\n",
        "       train_regret_winner_17[slice39],\r\n",
        "       train_regret_winner_18[slice39],\r\n",
        "       train_regret_winner_19[slice39],\r\n",
        "       train_regret_winner_20[slice39]]\r\n",
        "\r\n",
        "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\r\n",
        "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\r\n",
        "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\r\n",
        "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\r\n",
        "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\r\n",
        "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vb7vh2K1pXp"
      },
      "source": [
        "# Iteration49 :\r\n",
        "\r\n",
        "slice49 = 48\r\n",
        "\r\n",
        "loser49 = [train_regret_loser_1[slice49],\r\n",
        "       train_regret_loser_2[slice49],\r\n",
        "       train_regret_loser_3[slice49],\r\n",
        "       train_regret_loser_4[slice49],\r\n",
        "       train_regret_loser_5[slice49],\r\n",
        "       train_regret_loser_6[slice49],\r\n",
        "       train_regret_loser_7[slice49],\r\n",
        "       train_regret_loser_8[slice49],\r\n",
        "       train_regret_loser_9[slice49],\r\n",
        "       train_regret_loser_10[slice49],\r\n",
        "       train_regret_loser_11[slice49],\r\n",
        "       train_regret_loser_12[slice49],\r\n",
        "       train_regret_loser_13[slice49],\r\n",
        "       train_regret_loser_14[slice49],\r\n",
        "       train_regret_loser_15[slice49],\r\n",
        "       train_regret_loser_16[slice49],\r\n",
        "       train_regret_loser_17[slice49],\r\n",
        "       train_regret_loser_18[slice49],\r\n",
        "       train_regret_loser_19[slice49],\r\n",
        "       train_regret_loser_20[slice49]]\r\n",
        "\r\n",
        "winner49 = [train_regret_winner_1[slice49],\r\n",
        "       train_regret_winner_2[slice49],\r\n",
        "       train_regret_winner_3[slice49],\r\n",
        "       train_regret_winner_4[slice49],\r\n",
        "       train_regret_winner_5[slice49],\r\n",
        "       train_regret_winner_6[slice49],\r\n",
        "       train_regret_winner_7[slice49],\r\n",
        "       train_regret_winner_8[slice49],\r\n",
        "       train_regret_winner_9[slice49],\r\n",
        "       train_regret_winner_10[slice49],\r\n",
        "       train_regret_winner_11[slice49],\r\n",
        "       train_regret_winner_12[slice49],\r\n",
        "       train_regret_winner_13[slice49],\r\n",
        "       train_regret_winner_14[slice49],\r\n",
        "       train_regret_winner_15[slice49],\r\n",
        "       train_regret_winner_16[slice49],\r\n",
        "       train_regret_winner_17[slice49],\r\n",
        "       train_regret_winner_18[slice49],\r\n",
        "       train_regret_winner_19[slice49],\r\n",
        "       train_regret_winner_20[slice49]]\r\n",
        "\r\n",
        "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\r\n",
        "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\r\n",
        "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\r\n",
        "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\r\n",
        "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\r\n",
        "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wN7DWwt1paM"
      },
      "source": [
        "# Iteration59 :\r\n",
        "\r\n",
        "slice59 = 58\r\n",
        "\r\n",
        "loser59 = [train_regret_loser_1[slice59],\r\n",
        "       train_regret_loser_2[slice59],\r\n",
        "       train_regret_loser_3[slice59],\r\n",
        "       train_regret_loser_4[slice59],\r\n",
        "       train_regret_loser_5[slice59],\r\n",
        "       train_regret_loser_6[slice59],\r\n",
        "       train_regret_loser_7[slice59],\r\n",
        "       train_regret_loser_8[slice59],\r\n",
        "       train_regret_loser_9[slice59],\r\n",
        "       train_regret_loser_10[slice59],\r\n",
        "       train_regret_loser_11[slice59],\r\n",
        "       train_regret_loser_12[slice59],\r\n",
        "       train_regret_loser_13[slice59],\r\n",
        "       train_regret_loser_14[slice59],\r\n",
        "       train_regret_loser_15[slice59],\r\n",
        "       train_regret_loser_16[slice59],\r\n",
        "       train_regret_loser_17[slice59],\r\n",
        "       train_regret_loser_18[slice59],\r\n",
        "       train_regret_loser_19[slice59],\r\n",
        "       train_regret_loser_20[slice59]]\r\n",
        "\r\n",
        "winner59 = [train_regret_winner_1[slice59],\r\n",
        "       train_regret_winner_2[slice59],\r\n",
        "       train_regret_winner_3[slice59],\r\n",
        "       train_regret_winner_4[slice59],\r\n",
        "       train_regret_winner_5[slice59],\r\n",
        "       train_regret_winner_6[slice59],\r\n",
        "       train_regret_winner_7[slice59],\r\n",
        "       train_regret_winner_8[slice59],\r\n",
        "       train_regret_winner_9[slice59],\r\n",
        "       train_regret_winner_10[slice59],\r\n",
        "       train_regret_winner_11[slice59],\r\n",
        "       train_regret_winner_12[slice59],\r\n",
        "       train_regret_winner_13[slice59],\r\n",
        "       train_regret_winner_14[slice59],\r\n",
        "       train_regret_winner_15[slice59],\r\n",
        "       train_regret_winner_16[slice59],\r\n",
        "       train_regret_winner_17[slice59],\r\n",
        "       train_regret_winner_18[slice59],\r\n",
        "       train_regret_winner_19[slice59],\r\n",
        "       train_regret_winner_20[slice59]]\r\n",
        "\r\n",
        "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\r\n",
        "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\r\n",
        "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\r\n",
        "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\r\n",
        "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\r\n",
        "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqtgekml1pde"
      },
      "source": [
        "# Iteration69 :\r\n",
        "\r\n",
        "slice69 = 68\r\n",
        "\r\n",
        "loser69 = [train_regret_loser_1[slice69],\r\n",
        "       train_regret_loser_2[slice69],\r\n",
        "       train_regret_loser_3[slice69],\r\n",
        "       train_regret_loser_4[slice69],\r\n",
        "       train_regret_loser_5[slice69],\r\n",
        "       train_regret_loser_6[slice69],\r\n",
        "       train_regret_loser_7[slice69],\r\n",
        "       train_regret_loser_8[slice69],\r\n",
        "       train_regret_loser_9[slice69],\r\n",
        "       train_regret_loser_10[slice69],\r\n",
        "       train_regret_loser_11[slice69],\r\n",
        "       train_regret_loser_12[slice69],\r\n",
        "       train_regret_loser_13[slice69],\r\n",
        "       train_regret_loser_14[slice69],\r\n",
        "       train_regret_loser_15[slice69],\r\n",
        "       train_regret_loser_16[slice69],\r\n",
        "       train_regret_loser_17[slice69],\r\n",
        "       train_regret_loser_18[slice69],\r\n",
        "       train_regret_loser_19[slice69],\r\n",
        "       train_regret_loser_20[slice69]]\r\n",
        "\r\n",
        "winner69 = [train_regret_winner_1[slice69],\r\n",
        "       train_regret_winner_2[slice69],\r\n",
        "       train_regret_winner_3[slice69],\r\n",
        "       train_regret_winner_4[slice69],\r\n",
        "       train_regret_winner_5[slice69],\r\n",
        "       train_regret_winner_6[slice69],\r\n",
        "       train_regret_winner_7[slice69],\r\n",
        "       train_regret_winner_8[slice69],\r\n",
        "       train_regret_winner_9[slice69],\r\n",
        "       train_regret_winner_10[slice69],\r\n",
        "       train_regret_winner_11[slice69],\r\n",
        "       train_regret_winner_12[slice69],\r\n",
        "       train_regret_winner_13[slice69],\r\n",
        "       train_regret_winner_14[slice69],\r\n",
        "       train_regret_winner_15[slice69],\r\n",
        "       train_regret_winner_16[slice69],\r\n",
        "       train_regret_winner_17[slice69],\r\n",
        "       train_regret_winner_18[slice69],\r\n",
        "       train_regret_winner_19[slice69],\r\n",
        "       train_regret_winner_20[slice69]]\r\n",
        "\r\n",
        "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\r\n",
        "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\r\n",
        "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\r\n",
        "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\r\n",
        "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\r\n",
        "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPFTa1171pgF"
      },
      "source": [
        "# Iteration79 :\r\n",
        "\r\n",
        "slice79 = 78\r\n",
        "\r\n",
        "loser79 = [train_regret_loser_1[slice79],\r\n",
        "       train_regret_loser_2[slice79],\r\n",
        "       train_regret_loser_3[slice79],\r\n",
        "       train_regret_loser_4[slice79],\r\n",
        "       train_regret_loser_5[slice79],\r\n",
        "       train_regret_loser_6[slice79],\r\n",
        "       train_regret_loser_7[slice79],\r\n",
        "       train_regret_loser_8[slice79],\r\n",
        "       train_regret_loser_9[slice79],\r\n",
        "       train_regret_loser_10[slice79],\r\n",
        "       train_regret_loser_11[slice79],\r\n",
        "       train_regret_loser_12[slice79],\r\n",
        "       train_regret_loser_13[slice79],\r\n",
        "       train_regret_loser_14[slice79],\r\n",
        "       train_regret_loser_15[slice79],\r\n",
        "       train_regret_loser_16[slice79],\r\n",
        "       train_regret_loser_17[slice79],\r\n",
        "       train_regret_loser_18[slice79],\r\n",
        "       train_regret_loser_19[slice79],\r\n",
        "       train_regret_loser_20[slice79]]\r\n",
        "\r\n",
        "winner79 = [train_regret_winner_1[slice79],\r\n",
        "       train_regret_winner_2[slice79],\r\n",
        "       train_regret_winner_3[slice79],\r\n",
        "       train_regret_winner_4[slice79],\r\n",
        "       train_regret_winner_5[slice79],\r\n",
        "       train_regret_winner_6[slice79],\r\n",
        "       train_regret_winner_7[slice79],\r\n",
        "       train_regret_winner_8[slice79],\r\n",
        "       train_regret_winner_9[slice79],\r\n",
        "       train_regret_winner_10[slice79],\r\n",
        "       train_regret_winner_11[slice79],\r\n",
        "       train_regret_winner_12[slice79],\r\n",
        "       train_regret_winner_13[slice79],\r\n",
        "       train_regret_winner_14[slice79],\r\n",
        "       train_regret_winner_15[slice79],\r\n",
        "       train_regret_winner_16[slice79],\r\n",
        "       train_regret_winner_17[slice79],\r\n",
        "       train_regret_winner_18[slice79],\r\n",
        "       train_regret_winner_19[slice79],\r\n",
        "       train_regret_winner_20[slice79]]\r\n",
        "\r\n",
        "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\r\n",
        "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\r\n",
        "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\r\n",
        "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\r\n",
        "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\r\n",
        "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTnRF4lx1pio"
      },
      "source": [
        "# Iteration89 :\r\n",
        "\r\n",
        "slice89 = 88\r\n",
        "\r\n",
        "loser89 = [train_regret_loser_1[slice89],\r\n",
        "       train_regret_loser_2[slice89],\r\n",
        "       train_regret_loser_3[slice89],\r\n",
        "       train_regret_loser_4[slice89],\r\n",
        "       train_regret_loser_5[slice89],\r\n",
        "       train_regret_loser_6[slice89],\r\n",
        "       train_regret_loser_7[slice89],\r\n",
        "       train_regret_loser_8[slice89],\r\n",
        "       train_regret_loser_9[slice89],\r\n",
        "       train_regret_loser_10[slice89],\r\n",
        "       train_regret_loser_11[slice89],\r\n",
        "       train_regret_loser_12[slice89],\r\n",
        "       train_regret_loser_13[slice89],\r\n",
        "       train_regret_loser_14[slice89],\r\n",
        "       train_regret_loser_15[slice89],\r\n",
        "       train_regret_loser_16[slice89],\r\n",
        "       train_regret_loser_17[slice89],\r\n",
        "       train_regret_loser_18[slice89],\r\n",
        "       train_regret_loser_19[slice89],\r\n",
        "       train_regret_loser_20[slice89]]\r\n",
        "\r\n",
        "winner89 = [train_regret_winner_1[slice89],\r\n",
        "       train_regret_winner_2[slice89],\r\n",
        "       train_regret_winner_3[slice89],\r\n",
        "       train_regret_winner_4[slice89],\r\n",
        "       train_regret_winner_5[slice89],\r\n",
        "       train_regret_winner_6[slice89],\r\n",
        "       train_regret_winner_7[slice89],\r\n",
        "       train_regret_winner_8[slice89],\r\n",
        "       train_regret_winner_9[slice89],\r\n",
        "       train_regret_winner_10[slice89],\r\n",
        "       train_regret_winner_11[slice89],\r\n",
        "       train_regret_winner_12[slice89],\r\n",
        "       train_regret_winner_13[slice89],\r\n",
        "       train_regret_winner_14[slice89],\r\n",
        "       train_regret_winner_15[slice89],\r\n",
        "       train_regret_winner_16[slice89],\r\n",
        "       train_regret_winner_17[slice89],\r\n",
        "       train_regret_winner_18[slice89],\r\n",
        "       train_regret_winner_19[slice89],\r\n",
        "       train_regret_winner_20[slice89]]\r\n",
        "\r\n",
        "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\r\n",
        "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\r\n",
        "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\r\n",
        "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\r\n",
        "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\r\n",
        "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiLAF38i1plO"
      },
      "source": [
        "# Iteration99 :\r\n",
        "\r\n",
        "slice99 = 98\r\n",
        "\r\n",
        "loser99 = [train_regret_loser_1[slice99],\r\n",
        "       train_regret_loser_2[slice99],\r\n",
        "       train_regret_loser_3[slice99],\r\n",
        "       train_regret_loser_4[slice99],\r\n",
        "       train_regret_loser_5[slice99],\r\n",
        "       train_regret_loser_6[slice99],\r\n",
        "       train_regret_loser_7[slice99],\r\n",
        "       train_regret_loser_8[slice99],\r\n",
        "       train_regret_loser_9[slice99],\r\n",
        "       train_regret_loser_10[slice99],\r\n",
        "       train_regret_loser_11[slice99],\r\n",
        "       train_regret_loser_12[slice99],\r\n",
        "       train_regret_loser_13[slice99],\r\n",
        "       train_regret_loser_14[slice99],\r\n",
        "       train_regret_loser_15[slice99],\r\n",
        "       train_regret_loser_16[slice99],\r\n",
        "       train_regret_loser_17[slice99],\r\n",
        "       train_regret_loser_18[slice99],\r\n",
        "       train_regret_loser_19[slice99],\r\n",
        "       train_regret_loser_20[slice99]]\r\n",
        "\r\n",
        "winner99 = [train_regret_winner_1[slice99],\r\n",
        "       train_regret_winner_2[slice99],\r\n",
        "       train_regret_winner_3[slice99],\r\n",
        "       train_regret_winner_4[slice99],\r\n",
        "       train_regret_winner_5[slice99],\r\n",
        "       train_regret_winner_6[slice99],\r\n",
        "       train_regret_winner_7[slice99],\r\n",
        "       train_regret_winner_8[slice99],\r\n",
        "       train_regret_winner_9[slice99],\r\n",
        "       train_regret_winner_10[slice99],\r\n",
        "       train_regret_winner_11[slice99],\r\n",
        "       train_regret_winner_12[slice99],\r\n",
        "       train_regret_winner_13[slice99],\r\n",
        "       train_regret_winner_14[slice99],\r\n",
        "       train_regret_winner_15[slice99],\r\n",
        "       train_regret_winner_16[slice99],\r\n",
        "       train_regret_winner_17[slice99],\r\n",
        "       train_regret_winner_18[slice99],\r\n",
        "       train_regret_winner_19[slice99],\r\n",
        "       train_regret_winner_20[slice99]]\r\n",
        "\r\n",
        "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\r\n",
        "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\r\n",
        "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\r\n",
        "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\r\n",
        "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\r\n",
        "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBd_N0F21pn0"
      },
      "source": [
        "# Iteration10 :\r\n",
        "\r\n",
        "slice10 = 9\r\n",
        "\r\n",
        "loser10 = [train_regret_loser_1[slice10],\r\n",
        "       train_regret_loser_2[slice10],\r\n",
        "       train_regret_loser_3[slice10],\r\n",
        "       train_regret_loser_4[slice10],\r\n",
        "       train_regret_loser_5[slice10],\r\n",
        "       train_regret_loser_6[slice10],\r\n",
        "       train_regret_loser_7[slice10],\r\n",
        "       train_regret_loser_8[slice10],\r\n",
        "       train_regret_loser_9[slice10],\r\n",
        "       train_regret_loser_10[slice10],\r\n",
        "       train_regret_loser_11[slice10],\r\n",
        "       train_regret_loser_12[slice10],\r\n",
        "       train_regret_loser_13[slice10],\r\n",
        "       train_regret_loser_14[slice10],\r\n",
        "       train_regret_loser_15[slice10],\r\n",
        "       train_regret_loser_16[slice10],\r\n",
        "       train_regret_loser_17[slice10],\r\n",
        "       train_regret_loser_18[slice10],\r\n",
        "       train_regret_loser_19[slice10],\r\n",
        "       train_regret_loser_20[slice10]]\r\n",
        "\r\n",
        "winner10 = [train_regret_winner_1[slice10],\r\n",
        "       train_regret_winner_2[slice10],\r\n",
        "       train_regret_winner_3[slice10],\r\n",
        "       train_regret_winner_4[slice10],\r\n",
        "       train_regret_winner_5[slice10],\r\n",
        "       train_regret_winner_6[slice10],\r\n",
        "       train_regret_winner_7[slice10],\r\n",
        "       train_regret_winner_8[slice10],\r\n",
        "       train_regret_winner_9[slice10],\r\n",
        "       train_regret_winner_10[slice10],\r\n",
        "       train_regret_winner_11[slice10],\r\n",
        "       train_regret_winner_12[slice10],\r\n",
        "       train_regret_winner_13[slice10],\r\n",
        "       train_regret_winner_14[slice10],\r\n",
        "       train_regret_winner_15[slice10],\r\n",
        "       train_regret_winner_16[slice10],\r\n",
        "       train_regret_winner_17[slice10],\r\n",
        "       train_regret_winner_18[slice10],\r\n",
        "       train_regret_winner_19[slice10],\r\n",
        "       train_regret_winner_20[slice10]]\r\n",
        "\r\n",
        "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\r\n",
        "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\r\n",
        "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\r\n",
        "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\r\n",
        "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\r\n",
        "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXjdqc4s1pqb"
      },
      "source": [
        "# Iteration20 :\r\n",
        "\r\n",
        "slice20 = 19\r\n",
        "\r\n",
        "loser20 = [train_regret_loser_1[slice20],\r\n",
        "       train_regret_loser_2[slice20],\r\n",
        "       train_regret_loser_3[slice20],\r\n",
        "       train_regret_loser_4[slice20],\r\n",
        "       train_regret_loser_5[slice20],\r\n",
        "       train_regret_loser_6[slice20],\r\n",
        "       train_regret_loser_7[slice20],\r\n",
        "       train_regret_loser_8[slice20],\r\n",
        "       train_regret_loser_9[slice20],\r\n",
        "       train_regret_loser_10[slice20],\r\n",
        "       train_regret_loser_11[slice20],\r\n",
        "       train_regret_loser_12[slice20],\r\n",
        "       train_regret_loser_13[slice20],\r\n",
        "       train_regret_loser_14[slice20],\r\n",
        "       train_regret_loser_15[slice20],\r\n",
        "       train_regret_loser_16[slice20],\r\n",
        "       train_regret_loser_17[slice20],\r\n",
        "       train_regret_loser_18[slice20],\r\n",
        "       train_regret_loser_19[slice20],\r\n",
        "       train_regret_loser_20[slice20]]\r\n",
        "\r\n",
        "winner20 = [train_regret_winner_1[slice20],\r\n",
        "       train_regret_winner_2[slice20],\r\n",
        "       train_regret_winner_3[slice20],\r\n",
        "       train_regret_winner_4[slice20],\r\n",
        "       train_regret_winner_5[slice20],\r\n",
        "       train_regret_winner_6[slice20],\r\n",
        "       train_regret_winner_7[slice20],\r\n",
        "       train_regret_winner_8[slice20],\r\n",
        "       train_regret_winner_9[slice20],\r\n",
        "       train_regret_winner_10[slice20],\r\n",
        "       train_regret_winner_11[slice20],\r\n",
        "       train_regret_winner_12[slice20],\r\n",
        "       train_regret_winner_13[slice20],\r\n",
        "       train_regret_winner_14[slice20],\r\n",
        "       train_regret_winner_15[slice20],\r\n",
        "       train_regret_winner_16[slice20],\r\n",
        "       train_regret_winner_17[slice20],\r\n",
        "       train_regret_winner_18[slice20],\r\n",
        "       train_regret_winner_19[slice20],\r\n",
        "       train_regret_winner_20[slice20]]\r\n",
        "\r\n",
        "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\r\n",
        "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\r\n",
        "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\r\n",
        "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\r\n",
        "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\r\n",
        "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQBVS6dx1ptB"
      },
      "source": [
        "# Iteration30 :\r\n",
        "\r\n",
        "slice30 = 29\r\n",
        "\r\n",
        "loser30 = [train_regret_loser_1[slice30],\r\n",
        "       train_regret_loser_2[slice30],\r\n",
        "       train_regret_loser_3[slice30],\r\n",
        "       train_regret_loser_4[slice30],\r\n",
        "       train_regret_loser_5[slice30],\r\n",
        "       train_regret_loser_6[slice30],\r\n",
        "       train_regret_loser_7[slice30],\r\n",
        "       train_regret_loser_8[slice30],\r\n",
        "       train_regret_loser_9[slice30],\r\n",
        "       train_regret_loser_10[slice30],\r\n",
        "       train_regret_loser_11[slice30],\r\n",
        "       train_regret_loser_12[slice30],\r\n",
        "       train_regret_loser_13[slice30],\r\n",
        "       train_regret_loser_14[slice30],\r\n",
        "       train_regret_loser_15[slice30],\r\n",
        "       train_regret_loser_16[slice30],\r\n",
        "       train_regret_loser_17[slice30],\r\n",
        "       train_regret_loser_18[slice30],\r\n",
        "       train_regret_loser_19[slice30],\r\n",
        "       train_regret_loser_20[slice30]]\r\n",
        "\r\n",
        "winner30 = [train_regret_winner_1[slice30],\r\n",
        "       train_regret_winner_2[slice30],\r\n",
        "       train_regret_winner_3[slice30],\r\n",
        "       train_regret_winner_4[slice30],\r\n",
        "       train_regret_winner_5[slice30],\r\n",
        "       train_regret_winner_6[slice30],\r\n",
        "       train_regret_winner_7[slice30],\r\n",
        "       train_regret_winner_8[slice30],\r\n",
        "       train_regret_winner_9[slice30],\r\n",
        "       train_regret_winner_10[slice30],\r\n",
        "       train_regret_winner_11[slice30],\r\n",
        "       train_regret_winner_12[slice30],\r\n",
        "       train_regret_winner_13[slice30],\r\n",
        "       train_regret_winner_14[slice30],\r\n",
        "       train_regret_winner_15[slice30],\r\n",
        "       train_regret_winner_16[slice30],\r\n",
        "       train_regret_winner_17[slice30],\r\n",
        "       train_regret_winner_18[slice30],\r\n",
        "       train_regret_winner_19[slice30],\r\n",
        "       train_regret_winner_20[slice30]]\r\n",
        "\r\n",
        "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\r\n",
        "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\r\n",
        "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\r\n",
        "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\r\n",
        "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\r\n",
        "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-iWfLkj1pvn"
      },
      "source": [
        "# Iteration40 :\r\n",
        "\r\n",
        "slice40 = 39\r\n",
        "\r\n",
        "loser40 = [train_regret_loser_1[slice40],\r\n",
        "       train_regret_loser_2[slice40],\r\n",
        "       train_regret_loser_3[slice40],\r\n",
        "       train_regret_loser_4[slice40],\r\n",
        "       train_regret_loser_5[slice40],\r\n",
        "       train_regret_loser_6[slice40],\r\n",
        "       train_regret_loser_7[slice40],\r\n",
        "       train_regret_loser_8[slice40],\r\n",
        "       train_regret_loser_9[slice40],\r\n",
        "       train_regret_loser_10[slice40],\r\n",
        "       train_regret_loser_11[slice40],\r\n",
        "       train_regret_loser_12[slice40],\r\n",
        "       train_regret_loser_13[slice40],\r\n",
        "       train_regret_loser_14[slice40],\r\n",
        "       train_regret_loser_15[slice40],\r\n",
        "       train_regret_loser_16[slice40],\r\n",
        "       train_regret_loser_17[slice40],\r\n",
        "       train_regret_loser_18[slice40],\r\n",
        "       train_regret_loser_19[slice40],\r\n",
        "       train_regret_loser_20[slice40]]\r\n",
        "\r\n",
        "winner40 = [train_regret_winner_1[slice40],\r\n",
        "       train_regret_winner_2[slice40],\r\n",
        "       train_regret_winner_3[slice40],\r\n",
        "       train_regret_winner_4[slice40],\r\n",
        "       train_regret_winner_5[slice40],\r\n",
        "       train_regret_winner_6[slice40],\r\n",
        "       train_regret_winner_7[slice40],\r\n",
        "       train_regret_winner_8[slice40],\r\n",
        "       train_regret_winner_9[slice40],\r\n",
        "       train_regret_winner_10[slice40],\r\n",
        "       train_regret_winner_11[slice40],\r\n",
        "       train_regret_winner_12[slice40],\r\n",
        "       train_regret_winner_13[slice40],\r\n",
        "       train_regret_winner_14[slice40],\r\n",
        "       train_regret_winner_15[slice40],\r\n",
        "       train_regret_winner_16[slice40],\r\n",
        "       train_regret_winner_17[slice40],\r\n",
        "       train_regret_winner_18[slice40],\r\n",
        "       train_regret_winner_19[slice40],\r\n",
        "       train_regret_winner_20[slice40]]\r\n",
        "\r\n",
        "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\r\n",
        "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\r\n",
        "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\r\n",
        "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\r\n",
        "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\r\n",
        "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G5i-Cjw2U7K"
      },
      "source": [
        "# Iteration50 :\r\n",
        "\r\n",
        "slice50 = 49\r\n",
        "\r\n",
        "loser50 = [train_regret_loser_1[slice50],\r\n",
        "       train_regret_loser_2[slice50],\r\n",
        "       train_regret_loser_3[slice50],\r\n",
        "       train_regret_loser_4[slice50],\r\n",
        "       train_regret_loser_5[slice50],\r\n",
        "       train_regret_loser_6[slice50],\r\n",
        "       train_regret_loser_7[slice50],\r\n",
        "       train_regret_loser_8[slice50],\r\n",
        "       train_regret_loser_9[slice50],\r\n",
        "       train_regret_loser_10[slice50],\r\n",
        "       train_regret_loser_11[slice50],\r\n",
        "       train_regret_loser_12[slice50],\r\n",
        "       train_regret_loser_13[slice50],\r\n",
        "       train_regret_loser_14[slice50],\r\n",
        "       train_regret_loser_15[slice50],\r\n",
        "       train_regret_loser_16[slice50],\r\n",
        "       train_regret_loser_17[slice50],\r\n",
        "       train_regret_loser_18[slice50],\r\n",
        "       train_regret_loser_19[slice50],\r\n",
        "       train_regret_loser_20[slice50]]\r\n",
        "\r\n",
        "winner50 = [train_regret_winner_1[slice50],\r\n",
        "       train_regret_winner_2[slice50],\r\n",
        "       train_regret_winner_3[slice50],\r\n",
        "       train_regret_winner_4[slice50],\r\n",
        "       train_regret_winner_5[slice50],\r\n",
        "       train_regret_winner_6[slice50],\r\n",
        "       train_regret_winner_7[slice50],\r\n",
        "       train_regret_winner_8[slice50],\r\n",
        "       train_regret_winner_9[slice50],\r\n",
        "       train_regret_winner_10[slice50],\r\n",
        "       train_regret_winner_11[slice50],\r\n",
        "       train_regret_winner_12[slice50],\r\n",
        "       train_regret_winner_13[slice50],\r\n",
        "       train_regret_winner_14[slice50],\r\n",
        "       train_regret_winner_15[slice50],\r\n",
        "       train_regret_winner_16[slice50],\r\n",
        "       train_regret_winner_17[slice50],\r\n",
        "       train_regret_winner_18[slice50],\r\n",
        "       train_regret_winner_19[slice50],\r\n",
        "       train_regret_winner_20[slice50]]\r\n",
        "\r\n",
        "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\r\n",
        "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\r\n",
        "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\r\n",
        "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\r\n",
        "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\r\n",
        "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY6qxZQA2U-r"
      },
      "source": [
        "# Iteration60 :\r\n",
        "\r\n",
        "slice60 = 59\r\n",
        "\r\n",
        "loser60 = [train_regret_loser_1[slice60],\r\n",
        "       train_regret_loser_2[slice60],\r\n",
        "       train_regret_loser_3[slice60],\r\n",
        "       train_regret_loser_4[slice60],\r\n",
        "       train_regret_loser_5[slice60],\r\n",
        "       train_regret_loser_6[slice60],\r\n",
        "       train_regret_loser_7[slice60],\r\n",
        "       train_regret_loser_8[slice60],\r\n",
        "       train_regret_loser_9[slice60],\r\n",
        "       train_regret_loser_10[slice60],\r\n",
        "       train_regret_loser_11[slice60],\r\n",
        "       train_regret_loser_12[slice60],\r\n",
        "       train_regret_loser_13[slice60],\r\n",
        "       train_regret_loser_14[slice60],\r\n",
        "       train_regret_loser_15[slice60],\r\n",
        "       train_regret_loser_16[slice60],\r\n",
        "       train_regret_loser_17[slice60],\r\n",
        "       train_regret_loser_18[slice60],\r\n",
        "       train_regret_loser_19[slice60],\r\n",
        "       train_regret_loser_20[slice60]]\r\n",
        "\r\n",
        "winner60 = [train_regret_winner_1[slice60],\r\n",
        "       train_regret_winner_2[slice60],\r\n",
        "       train_regret_winner_3[slice60],\r\n",
        "       train_regret_winner_4[slice60],\r\n",
        "       train_regret_winner_5[slice60],\r\n",
        "       train_regret_winner_6[slice60],\r\n",
        "       train_regret_winner_7[slice60],\r\n",
        "       train_regret_winner_8[slice60],\r\n",
        "       train_regret_winner_9[slice60],\r\n",
        "       train_regret_winner_10[slice60],\r\n",
        "       train_regret_winner_11[slice60],\r\n",
        "       train_regret_winner_12[slice60],\r\n",
        "       train_regret_winner_13[slice60],\r\n",
        "       train_regret_winner_14[slice60],\r\n",
        "       train_regret_winner_15[slice60],\r\n",
        "       train_regret_winner_16[slice60],\r\n",
        "       train_regret_winner_17[slice60],\r\n",
        "       train_regret_winner_18[slice60],\r\n",
        "       train_regret_winner_19[slice60],\r\n",
        "       train_regret_winner_20[slice60]]\r\n",
        "\r\n",
        "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\r\n",
        "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\r\n",
        "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\r\n",
        "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\r\n",
        "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\r\n",
        "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqgVxVH2VCU"
      },
      "source": [
        "# Iteration70 :\r\n",
        "\r\n",
        "slice70 = 69\r\n",
        "\r\n",
        "loser70 = [train_regret_loser_1[slice70],\r\n",
        "       train_regret_loser_2[slice70],\r\n",
        "       train_regret_loser_3[slice70],\r\n",
        "       train_regret_loser_4[slice70],\r\n",
        "       train_regret_loser_5[slice70],\r\n",
        "       train_regret_loser_6[slice70],\r\n",
        "       train_regret_loser_7[slice70],\r\n",
        "       train_regret_loser_8[slice70],\r\n",
        "       train_regret_loser_9[slice70],\r\n",
        "       train_regret_loser_10[slice70],\r\n",
        "       train_regret_loser_11[slice70],\r\n",
        "       train_regret_loser_12[slice70],\r\n",
        "       train_regret_loser_13[slice70],\r\n",
        "       train_regret_loser_14[slice70],\r\n",
        "       train_regret_loser_15[slice70],\r\n",
        "       train_regret_loser_16[slice70],\r\n",
        "       train_regret_loser_17[slice70],\r\n",
        "       train_regret_loser_18[slice70],\r\n",
        "       train_regret_loser_19[slice70],\r\n",
        "       train_regret_loser_20[slice70]]\r\n",
        "\r\n",
        "winner70 = [train_regret_winner_1[slice70],\r\n",
        "       train_regret_winner_2[slice70],\r\n",
        "       train_regret_winner_3[slice70],\r\n",
        "       train_regret_winner_4[slice70],\r\n",
        "       train_regret_winner_5[slice70],\r\n",
        "       train_regret_winner_6[slice70],\r\n",
        "       train_regret_winner_7[slice70],\r\n",
        "       train_regret_winner_8[slice70],\r\n",
        "       train_regret_winner_9[slice70],\r\n",
        "       train_regret_winner_10[slice70],\r\n",
        "       train_regret_winner_11[slice70],\r\n",
        "       train_regret_winner_12[slice70],\r\n",
        "       train_regret_winner_13[slice70],\r\n",
        "       train_regret_winner_14[slice70],\r\n",
        "       train_regret_winner_15[slice70],\r\n",
        "       train_regret_winner_16[slice70],\r\n",
        "       train_regret_winner_17[slice70],\r\n",
        "       train_regret_winner_18[slice70],\r\n",
        "       train_regret_winner_19[slice70],\r\n",
        "       train_regret_winner_20[slice70]]\r\n",
        "\r\n",
        "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\r\n",
        "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\r\n",
        "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\r\n",
        "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\r\n",
        "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\r\n",
        "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF7mlDZL1pyN"
      },
      "source": [
        "# Iteration80 :\r\n",
        "\r\n",
        "slice80 = 79\r\n",
        "\r\n",
        "loser80 = [train_regret_loser_1[slice80],\r\n",
        "       train_regret_loser_2[slice80],\r\n",
        "       train_regret_loser_3[slice80],\r\n",
        "       train_regret_loser_4[slice80],\r\n",
        "       train_regret_loser_5[slice80],\r\n",
        "       train_regret_loser_6[slice80],\r\n",
        "       train_regret_loser_7[slice80],\r\n",
        "       train_regret_loser_8[slice80],\r\n",
        "       train_regret_loser_9[slice80],\r\n",
        "       train_regret_loser_10[slice80],\r\n",
        "       train_regret_loser_11[slice80],\r\n",
        "       train_regret_loser_12[slice80],\r\n",
        "       train_regret_loser_13[slice80],\r\n",
        "       train_regret_loser_14[slice80],\r\n",
        "       train_regret_loser_15[slice80],\r\n",
        "       train_regret_loser_16[slice80],\r\n",
        "       train_regret_loser_17[slice80],\r\n",
        "       train_regret_loser_18[slice80],\r\n",
        "       train_regret_loser_19[slice80],\r\n",
        "       train_regret_loser_20[slice80]]\r\n",
        "\r\n",
        "winner80 = [train_regret_winner_1[slice80],\r\n",
        "       train_regret_winner_2[slice80],\r\n",
        "       train_regret_winner_3[slice80],\r\n",
        "       train_regret_winner_4[slice80],\r\n",
        "       train_regret_winner_5[slice80],\r\n",
        "       train_regret_winner_6[slice80],\r\n",
        "       train_regret_winner_7[slice80],\r\n",
        "       train_regret_winner_8[slice80],\r\n",
        "       train_regret_winner_9[slice80],\r\n",
        "       train_regret_winner_10[slice80],\r\n",
        "       train_regret_winner_11[slice80],\r\n",
        "       train_regret_winner_12[slice80],\r\n",
        "       train_regret_winner_13[slice80],\r\n",
        "       train_regret_winner_14[slice80],\r\n",
        "       train_regret_winner_15[slice80],\r\n",
        "       train_regret_winner_16[slice80],\r\n",
        "       train_regret_winner_17[slice80],\r\n",
        "       train_regret_winner_18[slice80],\r\n",
        "       train_regret_winner_19[slice80],\r\n",
        "       train_regret_winner_20[slice80]]\r\n",
        "\r\n",
        "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\r\n",
        "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\r\n",
        "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\r\n",
        "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\r\n",
        "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\r\n",
        "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Rw8IkD1p0z"
      },
      "source": [
        "# Iteration90 :\r\n",
        "\r\n",
        "slice90 = 89\r\n",
        "\r\n",
        "loser90 = [train_regret_loser_1[slice90],\r\n",
        "       train_regret_loser_2[slice90],\r\n",
        "       train_regret_loser_3[slice90],\r\n",
        "       train_regret_loser_4[slice90],\r\n",
        "       train_regret_loser_5[slice90],\r\n",
        "       train_regret_loser_6[slice90],\r\n",
        "       train_regret_loser_7[slice90],\r\n",
        "       train_regret_loser_8[slice90],\r\n",
        "       train_regret_loser_9[slice90],\r\n",
        "       train_regret_loser_10[slice90],\r\n",
        "       train_regret_loser_11[slice90],\r\n",
        "       train_regret_loser_12[slice90],\r\n",
        "       train_regret_loser_13[slice90],\r\n",
        "       train_regret_loser_14[slice90],\r\n",
        "       train_regret_loser_15[slice90],\r\n",
        "       train_regret_loser_16[slice90],\r\n",
        "       train_regret_loser_17[slice90],\r\n",
        "       train_regret_loser_18[slice90],\r\n",
        "       train_regret_loser_19[slice90],\r\n",
        "       train_regret_loser_20[slice90]]\r\n",
        "\r\n",
        "winner90 = [train_regret_winner_1[slice90],\r\n",
        "       train_regret_winner_2[slice90],\r\n",
        "       train_regret_winner_3[slice90],\r\n",
        "       train_regret_winner_4[slice90],\r\n",
        "       train_regret_winner_5[slice90],\r\n",
        "       train_regret_winner_6[slice90],\r\n",
        "       train_regret_winner_7[slice90],\r\n",
        "       train_regret_winner_8[slice90],\r\n",
        "       train_regret_winner_9[slice90],\r\n",
        "       train_regret_winner_10[slice90],\r\n",
        "       train_regret_winner_11[slice90],\r\n",
        "       train_regret_winner_12[slice90],\r\n",
        "       train_regret_winner_13[slice90],\r\n",
        "       train_regret_winner_14[slice90],\r\n",
        "       train_regret_winner_15[slice90],\r\n",
        "       train_regret_winner_16[slice90],\r\n",
        "       train_regret_winner_17[slice90],\r\n",
        "       train_regret_winner_18[slice90],\r\n",
        "       train_regret_winner_19[slice90],\r\n",
        "       train_regret_winner_20[slice90]]\r\n",
        "\r\n",
        "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\r\n",
        "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\r\n",
        "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\r\n",
        "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\r\n",
        "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\r\n",
        "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiljCrq21p3a"
      },
      "source": [
        "# Iteration100 :\r\n",
        "\r\n",
        "slice100 = 99\r\n",
        "\r\n",
        "loser100 = [train_regret_loser_1[slice100],\r\n",
        "       train_regret_loser_2[slice100],\r\n",
        "       train_regret_loser_3[slice100],\r\n",
        "       train_regret_loser_4[slice100],\r\n",
        "       train_regret_loser_5[slice100],\r\n",
        "       train_regret_loser_6[slice100],\r\n",
        "       train_regret_loser_7[slice100],\r\n",
        "       train_regret_loser_8[slice100],\r\n",
        "       train_regret_loser_9[slice100],\r\n",
        "       train_regret_loser_10[slice100],\r\n",
        "       train_regret_loser_11[slice100],\r\n",
        "       train_regret_loser_12[slice100],\r\n",
        "       train_regret_loser_13[slice100],\r\n",
        "       train_regret_loser_14[slice100],\r\n",
        "       train_regret_loser_15[slice100],\r\n",
        "       train_regret_loser_16[slice100],\r\n",
        "       train_regret_loser_17[slice100],\r\n",
        "       train_regret_loser_18[slice100],\r\n",
        "       train_regret_loser_19[slice100],\r\n",
        "       train_regret_loser_20[slice100]]\r\n",
        "\r\n",
        "winner100 = [train_regret_winner_1[slice100],\r\n",
        "       train_regret_winner_2[slice100],\r\n",
        "       train_regret_winner_3[slice100],\r\n",
        "       train_regret_winner_4[slice100],\r\n",
        "       train_regret_winner_5[slice100],\r\n",
        "       train_regret_winner_6[slice100],\r\n",
        "       train_regret_winner_7[slice100],\r\n",
        "       train_regret_winner_8[slice100],\r\n",
        "       train_regret_winner_9[slice100],\r\n",
        "       train_regret_winner_10[slice100],\r\n",
        "       train_regret_winner_11[slice100],\r\n",
        "       train_regret_winner_12[slice100],\r\n",
        "       train_regret_winner_13[slice100],\r\n",
        "       train_regret_winner_14[slice100],\r\n",
        "       train_regret_winner_15[slice100],\r\n",
        "       train_regret_winner_16[slice100],\r\n",
        "       train_regret_winner_17[slice100],\r\n",
        "       train_regret_winner_18[slice100],\r\n",
        "       train_regret_winner_19[slice100],\r\n",
        "       train_regret_winner_20[slice100]]\r\n",
        "\r\n",
        "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\r\n",
        "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\r\n",
        "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\r\n",
        "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\r\n",
        "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\r\n",
        "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OItJgqUT2oNb"
      },
      "source": [
        "### Summarize arrays: 'Loser'\r\n",
        "\r\n",
        "lower_loser = [lower_loser1,\r\n",
        "            lower_loser2,\r\n",
        "            lower_loser3,\r\n",
        "            lower_loser4,\r\n",
        "            lower_loser5,\r\n",
        "            lower_loser6,\r\n",
        "            lower_loser7,\r\n",
        "            lower_loser8,\r\n",
        "            lower_loser9,\r\n",
        "            lower_loser10,\r\n",
        "            lower_loser11,\r\n",
        "            lower_loser12,\r\n",
        "            lower_loser13,\r\n",
        "            lower_loser14,\r\n",
        "            lower_loser15,\r\n",
        "            lower_loser16,\r\n",
        "            lower_loser17,\r\n",
        "            lower_loser18,\r\n",
        "            lower_loser19,\r\n",
        "            lower_loser20,\r\n",
        "            lower_loser21,\r\n",
        "            lower_loser22,\r\n",
        "            lower_loser23,\r\n",
        "            lower_loser24,\r\n",
        "            lower_loser25,\r\n",
        "            lower_loser26,\r\n",
        "            lower_loser27,\r\n",
        "            lower_loser28,\r\n",
        "            lower_loser29,\r\n",
        "            lower_loser30,\r\n",
        "            lower_loser31,\r\n",
        "            lower_loser32,\r\n",
        "            lower_loser33,\r\n",
        "            lower_loser34,\r\n",
        "            lower_loser35,\r\n",
        "            lower_loser36,\r\n",
        "            lower_loser37,\r\n",
        "            lower_loser38,\r\n",
        "            lower_loser39,\r\n",
        "            lower_loser40,\r\n",
        "            lower_loser41,\r\n",
        "            lower_loser42,\r\n",
        "            lower_loser43,\r\n",
        "            lower_loser44,\r\n",
        "            lower_loser45,\r\n",
        "            lower_loser46,\r\n",
        "            lower_loser47,\r\n",
        "            lower_loser48,\r\n",
        "            lower_loser49,\r\n",
        "            lower_loser50,\r\n",
        "            lower_loser51,\r\n",
        "            lower_loser52,\r\n",
        "            lower_loser53,\r\n",
        "            lower_loser54,\r\n",
        "            lower_loser55,\r\n",
        "            lower_loser56,\r\n",
        "            lower_loser57,\r\n",
        "            lower_loser58,\r\n",
        "            lower_loser59,\r\n",
        "            lower_loser60,\r\n",
        "            lower_loser61,\r\n",
        "            lower_loser62,\r\n",
        "            lower_loser63,\r\n",
        "            lower_loser64,\r\n",
        "            lower_loser65,\r\n",
        "            lower_loser66,\r\n",
        "            lower_loser67,\r\n",
        "            lower_loser68,\r\n",
        "            lower_loser69,\r\n",
        "            lower_loser70,\r\n",
        "            lower_loser71,\r\n",
        "            lower_loser72,\r\n",
        "            lower_loser73,\r\n",
        "            lower_loser74,\r\n",
        "            lower_loser75,\r\n",
        "            lower_loser76,\r\n",
        "            lower_loser77,\r\n",
        "            lower_loser78,\r\n",
        "            lower_loser79,\r\n",
        "            lower_loser80,\r\n",
        "            lower_loser81,\r\n",
        "            lower_loser82,\r\n",
        "            lower_loser83,\r\n",
        "            lower_loser84,\r\n",
        "            lower_loser85,\r\n",
        "            lower_loser86,\r\n",
        "            lower_loser87,\r\n",
        "            lower_loser88,\r\n",
        "            lower_loser89,\r\n",
        "            lower_loser90,\r\n",
        "            lower_loser91,\r\n",
        "            lower_loser92,\r\n",
        "            lower_loser93,\r\n",
        "            lower_loser94,\r\n",
        "            lower_loser95,\r\n",
        "            lower_loser96,\r\n",
        "            lower_loser97,\r\n",
        "            lower_loser98,\r\n",
        "            lower_loser99,\r\n",
        "            lower_loser100,\r\n",
        "            lower_loser101]\r\n",
        "\r\n",
        "median_loser = [median_loser1,\r\n",
        "            median_loser2,\r\n",
        "            median_loser3,\r\n",
        "            median_loser4,\r\n",
        "            median_loser5,\r\n",
        "            median_loser6,\r\n",
        "            median_loser7,\r\n",
        "            median_loser8,\r\n",
        "            median_loser9,\r\n",
        "            median_loser10,\r\n",
        "            median_loser11,\r\n",
        "            median_loser12,\r\n",
        "            median_loser13,\r\n",
        "            median_loser14,\r\n",
        "            median_loser15,\r\n",
        "            median_loser16,\r\n",
        "            median_loser17,\r\n",
        "            median_loser18,\r\n",
        "            median_loser19,\r\n",
        "            median_loser20,\r\n",
        "            median_loser21,\r\n",
        "            median_loser22,\r\n",
        "            median_loser23,\r\n",
        "            median_loser24,\r\n",
        "            median_loser25,\r\n",
        "            median_loser26,\r\n",
        "            median_loser27,\r\n",
        "            median_loser28,\r\n",
        "            median_loser29,\r\n",
        "            median_loser30,\r\n",
        "            median_loser31,\r\n",
        "            median_loser32,\r\n",
        "            median_loser33,\r\n",
        "            median_loser34,\r\n",
        "            median_loser35,\r\n",
        "            median_loser36,\r\n",
        "            median_loser37,\r\n",
        "            median_loser38,\r\n",
        "            median_loser39,\r\n",
        "            median_loser40,\r\n",
        "            median_loser41,\r\n",
        "            median_loser42,\r\n",
        "            median_loser43,\r\n",
        "            median_loser44,\r\n",
        "            median_loser45,\r\n",
        "            median_loser46,\r\n",
        "            median_loser47,\r\n",
        "            median_loser48,\r\n",
        "            median_loser49,\r\n",
        "            median_loser50,\r\n",
        "            median_loser51,\r\n",
        "            median_loser52,\r\n",
        "            median_loser53,\r\n",
        "            median_loser54,\r\n",
        "            median_loser55,\r\n",
        "            median_loser56,\r\n",
        "            median_loser57,\r\n",
        "            median_loser58,\r\n",
        "            median_loser59,\r\n",
        "            median_loser60,\r\n",
        "            median_loser61,\r\n",
        "            median_loser62,\r\n",
        "            median_loser63,\r\n",
        "            median_loser64,\r\n",
        "            median_loser65,\r\n",
        "            median_loser66,\r\n",
        "            median_loser67,\r\n",
        "            median_loser68,\r\n",
        "            median_loser69,\r\n",
        "            median_loser70,\r\n",
        "            median_loser71,\r\n",
        "            median_loser72,\r\n",
        "            median_loser73,\r\n",
        "            median_loser74,\r\n",
        "            median_loser75,\r\n",
        "            median_loser76,\r\n",
        "            median_loser77,\r\n",
        "            median_loser78,\r\n",
        "            median_loser79,\r\n",
        "            median_loser80,\r\n",
        "            median_loser81,\r\n",
        "            median_loser82,\r\n",
        "            median_loser83,\r\n",
        "            median_loser84,\r\n",
        "            median_loser85,\r\n",
        "            median_loser86,\r\n",
        "            median_loser87,\r\n",
        "            median_loser88,\r\n",
        "            median_loser89,\r\n",
        "            median_loser90,\r\n",
        "            median_loser91,\r\n",
        "            median_loser92,\r\n",
        "            median_loser93,\r\n",
        "            median_loser94,\r\n",
        "            median_loser95,\r\n",
        "            median_loser96,\r\n",
        "            median_loser97,\r\n",
        "            median_loser98,\r\n",
        "            median_loser99,\r\n",
        "            median_loser100,\r\n",
        "            median_loser101]\r\n",
        "\r\n",
        "upper_loser = [upper_loser1,\r\n",
        "            upper_loser2,\r\n",
        "            upper_loser3,\r\n",
        "            upper_loser4,\r\n",
        "            upper_loser5,\r\n",
        "            upper_loser6,\r\n",
        "            upper_loser7,\r\n",
        "            upper_loser8,\r\n",
        "            upper_loser9,\r\n",
        "            upper_loser10,\r\n",
        "            upper_loser11,\r\n",
        "            upper_loser12,\r\n",
        "            upper_loser13,\r\n",
        "            upper_loser14,\r\n",
        "            upper_loser15,\r\n",
        "            upper_loser16,\r\n",
        "            upper_loser17,\r\n",
        "            upper_loser18,\r\n",
        "            upper_loser19,\r\n",
        "            upper_loser20,\r\n",
        "            upper_loser21,\r\n",
        "            upper_loser22,\r\n",
        "            upper_loser23,\r\n",
        "            upper_loser24,\r\n",
        "            upper_loser25,\r\n",
        "            upper_loser26,\r\n",
        "            upper_loser27,\r\n",
        "            upper_loser28,\r\n",
        "            upper_loser29,\r\n",
        "            upper_loser30,\r\n",
        "            upper_loser31,\r\n",
        "            upper_loser32,\r\n",
        "            upper_loser33,\r\n",
        "            upper_loser34,\r\n",
        "            upper_loser35,\r\n",
        "            upper_loser36,\r\n",
        "            upper_loser37,\r\n",
        "            upper_loser38,\r\n",
        "            upper_loser39,\r\n",
        "            upper_loser40,\r\n",
        "            upper_loser41,\r\n",
        "            upper_loser42,\r\n",
        "            upper_loser43,\r\n",
        "            upper_loser44,\r\n",
        "            upper_loser45,\r\n",
        "            upper_loser46,\r\n",
        "            upper_loser47,\r\n",
        "            upper_loser48,\r\n",
        "            upper_loser49,\r\n",
        "            upper_loser50,\r\n",
        "            upper_loser51,\r\n",
        "            upper_loser52,\r\n",
        "            upper_loser53,\r\n",
        "            upper_loser54,\r\n",
        "            upper_loser55,\r\n",
        "            upper_loser56,\r\n",
        "            upper_loser57,\r\n",
        "            upper_loser58,\r\n",
        "            upper_loser59,\r\n",
        "            upper_loser60,\r\n",
        "            upper_loser61,\r\n",
        "            upper_loser62,\r\n",
        "            upper_loser63,\r\n",
        "            upper_loser64,\r\n",
        "            upper_loser65,\r\n",
        "            upper_loser66,\r\n",
        "            upper_loser67,\r\n",
        "            upper_loser68,\r\n",
        "            upper_loser69,\r\n",
        "            upper_loser70,\r\n",
        "            upper_loser71,\r\n",
        "            upper_loser72,\r\n",
        "            upper_loser73,\r\n",
        "            upper_loser74,\r\n",
        "            upper_loser75,\r\n",
        "            upper_loser76,\r\n",
        "            upper_loser77,\r\n",
        "            upper_loser78,\r\n",
        "            upper_loser79,\r\n",
        "            upper_loser80,\r\n",
        "            upper_loser81,\r\n",
        "            upper_loser82,\r\n",
        "            upper_loser83,\r\n",
        "            upper_loser84,\r\n",
        "            upper_loser85,\r\n",
        "            upper_loser86,\r\n",
        "            upper_loser87,\r\n",
        "            upper_loser88,\r\n",
        "            upper_loser89,\r\n",
        "            upper_loser90,\r\n",
        "            upper_loser91,\r\n",
        "            upper_loser92,\r\n",
        "            upper_loser93,\r\n",
        "            upper_loser94,\r\n",
        "            upper_loser95,\r\n",
        "            upper_loser96,\r\n",
        "            upper_loser97,\r\n",
        "            upper_loser98,\r\n",
        "            upper_loser99,\r\n",
        "            upper_loser100,\r\n",
        "            upper_loser101]"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u19FndLT2oU3"
      },
      "source": [
        "### Summarize arrays: 'Winner'\r\n",
        "\r\n",
        "lower_winner = [lower_winner1,\r\n",
        "            lower_winner2,\r\n",
        "            lower_winner3,\r\n",
        "            lower_winner4,\r\n",
        "            lower_winner5,\r\n",
        "            lower_winner6,\r\n",
        "            lower_winner7,\r\n",
        "            lower_winner8,\r\n",
        "            lower_winner9,\r\n",
        "            lower_winner10,\r\n",
        "            lower_winner11,\r\n",
        "            lower_winner12,\r\n",
        "            lower_winner13,\r\n",
        "            lower_winner14,\r\n",
        "            lower_winner15,\r\n",
        "            lower_winner16,\r\n",
        "            lower_winner17,\r\n",
        "            lower_winner18,\r\n",
        "            lower_winner19,\r\n",
        "            lower_winner20,\r\n",
        "            lower_winner21,\r\n",
        "            lower_winner22,\r\n",
        "            lower_winner23,\r\n",
        "            lower_winner24,\r\n",
        "            lower_winner25,\r\n",
        "            lower_winner26,\r\n",
        "            lower_winner27,\r\n",
        "            lower_winner28,\r\n",
        "            lower_winner29,\r\n",
        "            lower_winner30,\r\n",
        "            lower_winner31,\r\n",
        "            lower_winner32,\r\n",
        "            lower_winner33,\r\n",
        "            lower_winner34,\r\n",
        "            lower_winner35,\r\n",
        "            lower_winner36,\r\n",
        "            lower_winner37,\r\n",
        "            lower_winner38,\r\n",
        "            lower_winner39,\r\n",
        "            lower_winner40,\r\n",
        "            lower_winner41,\r\n",
        "            lower_winner42,\r\n",
        "            lower_winner43,\r\n",
        "            lower_winner44,\r\n",
        "            lower_winner45,\r\n",
        "            lower_winner46,\r\n",
        "            lower_winner47,\r\n",
        "            lower_winner48,\r\n",
        "            lower_winner49,\r\n",
        "            lower_winner50,\r\n",
        "            lower_winner51,\r\n",
        "            lower_winner52,\r\n",
        "            lower_winner53,\r\n",
        "            lower_winner54,\r\n",
        "            lower_winner55,\r\n",
        "            lower_winner56,\r\n",
        "            lower_winner57,\r\n",
        "            lower_winner58,\r\n",
        "            lower_winner59,\r\n",
        "            lower_winner60,\r\n",
        "            lower_winner61,\r\n",
        "            lower_winner62,\r\n",
        "            lower_winner63,\r\n",
        "            lower_winner64,\r\n",
        "            lower_winner65,\r\n",
        "            lower_winner66,\r\n",
        "            lower_winner67,\r\n",
        "            lower_winner68,\r\n",
        "            lower_winner69,\r\n",
        "            lower_winner70,\r\n",
        "            lower_winner71,\r\n",
        "            lower_winner72,\r\n",
        "            lower_winner73,\r\n",
        "            lower_winner74,\r\n",
        "            lower_winner75,\r\n",
        "            lower_winner76,\r\n",
        "            lower_winner77,\r\n",
        "            lower_winner78,\r\n",
        "            lower_winner79,\r\n",
        "            lower_winner80,\r\n",
        "            lower_winner81,\r\n",
        "            lower_winner82,\r\n",
        "            lower_winner83,\r\n",
        "            lower_winner84,\r\n",
        "            lower_winner85,\r\n",
        "            lower_winner86,\r\n",
        "            lower_winner87,\r\n",
        "            lower_winner88,\r\n",
        "            lower_winner89,\r\n",
        "            lower_winner90,\r\n",
        "            lower_winner91,\r\n",
        "            lower_winner92,\r\n",
        "            lower_winner93,\r\n",
        "            lower_winner94,\r\n",
        "            lower_winner95,\r\n",
        "            lower_winner96,\r\n",
        "            lower_winner97,\r\n",
        "            lower_winner98,\r\n",
        "            lower_winner99,\r\n",
        "            lower_winner100,\r\n",
        "            lower_winner101]\r\n",
        "\r\n",
        "median_winner = [median_winner1,\r\n",
        "            median_winner2,\r\n",
        "            median_winner3,\r\n",
        "            median_winner4,\r\n",
        "            median_winner5,\r\n",
        "            median_winner6,\r\n",
        "            median_winner7,\r\n",
        "            median_winner8,\r\n",
        "            median_winner9,\r\n",
        "            median_winner10,\r\n",
        "            median_winner11,\r\n",
        "            median_winner12,\r\n",
        "            median_winner13,\r\n",
        "            median_winner14,\r\n",
        "            median_winner15,\r\n",
        "            median_winner16,\r\n",
        "            median_winner17,\r\n",
        "            median_winner18,\r\n",
        "            median_winner19,\r\n",
        "            median_winner20,\r\n",
        "            median_winner21,\r\n",
        "            median_winner22,\r\n",
        "            median_winner23,\r\n",
        "            median_winner24,\r\n",
        "            median_winner25,\r\n",
        "            median_winner26,\r\n",
        "            median_winner27,\r\n",
        "            median_winner28,\r\n",
        "            median_winner29,\r\n",
        "            median_winner30,\r\n",
        "            median_winner31,\r\n",
        "            median_winner32,\r\n",
        "            median_winner33,\r\n",
        "            median_winner34,\r\n",
        "            median_winner35,\r\n",
        "            median_winner36,\r\n",
        "            median_winner37,\r\n",
        "            median_winner38,\r\n",
        "            median_winner39,\r\n",
        "            median_winner40,\r\n",
        "            median_winner41,\r\n",
        "            median_winner42,\r\n",
        "            median_winner43,\r\n",
        "            median_winner44,\r\n",
        "            median_winner45,\r\n",
        "            median_winner46,\r\n",
        "            median_winner47,\r\n",
        "            median_winner48,\r\n",
        "            median_winner49,\r\n",
        "            median_winner50,\r\n",
        "            median_winner51,\r\n",
        "            median_winner52,\r\n",
        "            median_winner53,\r\n",
        "            median_winner54,\r\n",
        "            median_winner55,\r\n",
        "            median_winner56,\r\n",
        "            median_winner57,\r\n",
        "            median_winner58,\r\n",
        "            median_winner59,\r\n",
        "            median_winner60,\r\n",
        "            median_winner61,\r\n",
        "            median_winner62,\r\n",
        "            median_winner63,\r\n",
        "            median_winner64,\r\n",
        "            median_winner65,\r\n",
        "            median_winner66,\r\n",
        "            median_winner67,\r\n",
        "            median_winner68,\r\n",
        "            median_winner69,\r\n",
        "            median_winner70,\r\n",
        "            median_winner71,\r\n",
        "            median_winner72,\r\n",
        "            median_winner73,\r\n",
        "            median_winner74,\r\n",
        "            median_winner75,\r\n",
        "            median_winner76,\r\n",
        "            median_winner77,\r\n",
        "            median_winner78,\r\n",
        "            median_winner79,\r\n",
        "            median_winner80,\r\n",
        "            median_winner81,\r\n",
        "            median_winner82,\r\n",
        "            median_winner83,\r\n",
        "            median_winner84,\r\n",
        "            median_winner85,\r\n",
        "            median_winner86,\r\n",
        "            median_winner87,\r\n",
        "            median_winner88,\r\n",
        "            median_winner89,\r\n",
        "            median_winner90,\r\n",
        "            median_winner91,\r\n",
        "            median_winner92,\r\n",
        "            median_winner93,\r\n",
        "            median_winner94,\r\n",
        "            median_winner95,\r\n",
        "            median_winner96,\r\n",
        "            median_winner97,\r\n",
        "            median_winner98,\r\n",
        "            median_winner99,\r\n",
        "            median_winner100,\r\n",
        "            median_winner101]\r\n",
        "\r\n",
        "upper_winner = [upper_winner1,\r\n",
        "            upper_winner2,\r\n",
        "            upper_winner3,\r\n",
        "            upper_winner4,\r\n",
        "            upper_winner5,\r\n",
        "            upper_winner6,\r\n",
        "            upper_winner7,\r\n",
        "            upper_winner8,\r\n",
        "            upper_winner9,\r\n",
        "            upper_winner10,\r\n",
        "            upper_winner11,\r\n",
        "            upper_winner12,\r\n",
        "            upper_winner13,\r\n",
        "            upper_winner14,\r\n",
        "            upper_winner15,\r\n",
        "            upper_winner16,\r\n",
        "            upper_winner17,\r\n",
        "            upper_winner18,\r\n",
        "            upper_winner19,\r\n",
        "            upper_winner20,\r\n",
        "            upper_winner21,\r\n",
        "            upper_winner22,\r\n",
        "            upper_winner23,\r\n",
        "            upper_winner24,\r\n",
        "            upper_winner25,\r\n",
        "            upper_winner26,\r\n",
        "            upper_winner27,\r\n",
        "            upper_winner28,\r\n",
        "            upper_winner29,\r\n",
        "            upper_winner30,\r\n",
        "            upper_winner31,\r\n",
        "            upper_winner32,\r\n",
        "            upper_winner33,\r\n",
        "            upper_winner34,\r\n",
        "            upper_winner35,\r\n",
        "            upper_winner36,\r\n",
        "            upper_winner37,\r\n",
        "            upper_winner38,\r\n",
        "            upper_winner39,\r\n",
        "            upper_winner40,\r\n",
        "            upper_winner41,\r\n",
        "            upper_winner42,\r\n",
        "            upper_winner43,\r\n",
        "            upper_winner44,\r\n",
        "            upper_winner45,\r\n",
        "            upper_winner46,\r\n",
        "            upper_winner47,\r\n",
        "            upper_winner48,\r\n",
        "            upper_winner49,\r\n",
        "            upper_winner50,\r\n",
        "            upper_winner51,\r\n",
        "            upper_winner52,\r\n",
        "            upper_winner53,\r\n",
        "            upper_winner54,\r\n",
        "            upper_winner55,\r\n",
        "            upper_winner56,\r\n",
        "            upper_winner57,\r\n",
        "            upper_winner58,\r\n",
        "            upper_winner59,\r\n",
        "            upper_winner60,\r\n",
        "            upper_winner61,\r\n",
        "            upper_winner62,\r\n",
        "            upper_winner63,\r\n",
        "            upper_winner64,\r\n",
        "            upper_winner65,\r\n",
        "            upper_winner66,\r\n",
        "            upper_winner67,\r\n",
        "            upper_winner68,\r\n",
        "            upper_winner69,\r\n",
        "            upper_winner70,\r\n",
        "            upper_winner71,\r\n",
        "            upper_winner72,\r\n",
        "            upper_winner73,\r\n",
        "            upper_winner74,\r\n",
        "            upper_winner75,\r\n",
        "            upper_winner76,\r\n",
        "            upper_winner77,\r\n",
        "            upper_winner78,\r\n",
        "            upper_winner79,\r\n",
        "            upper_winner80,\r\n",
        "            upper_winner81,\r\n",
        "            upper_winner82,\r\n",
        "            upper_winner83,\r\n",
        "            upper_winner84,\r\n",
        "            upper_winner85,\r\n",
        "            upper_winner86,\r\n",
        "            upper_winner87,\r\n",
        "            upper_winner88,\r\n",
        "            upper_winner89,\r\n",
        "            upper_winner90,\r\n",
        "            upper_winner91,\r\n",
        "            upper_winner92,\r\n",
        "            upper_winner93,\r\n",
        "            upper_winner94,\r\n",
        "            upper_winner95,\r\n",
        "            upper_winner96,\r\n",
        "            upper_winner97,\r\n",
        "            upper_winner98,\r\n",
        "            upper_winner99,\r\n",
        "            upper_winner100,\r\n",
        "            upper_winner101]"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84XIzmWD2oba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "a6784f8d-c9af-44dd-d700-587d0ec35034"
      },
      "source": [
        "### Visualize!\r\n",
        "\r\n",
        "title = obj_func\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "\r\n",
        "plt.plot(median_loser, color = 'Red')\r\n",
        "plt.plot(median_winner, color = 'Yellow')\r\n",
        "\r\n",
        "xstar = np.arange(0, max_iter+1, step=1)\r\n",
        "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Yellow', alpha=0.4, label='GP EI Regret IQR: L-BFGS-B')\r\n",
        "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Red', alpha=0.4, label='GP EI Regret IQR: Newton-CG with GP d$^{2}$EI')\r\n",
        "\r\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\r\n",
        "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') \r\n",
        "plt.ylabel('log(Regret)', weight = 'bold', family = 'Arial') \r\n",
        "plt.legend(loc=1) # add plot legend\r\n",
        "\r\n",
        "plt.show() "
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5dkR2Id2oiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b4a03e-a829-4cbf-e1f5-1208faf85df4"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3600.127376317978, 2280.01766705513)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNLBF57p2opi"
      },
      "source": [
        ""
      ],
      "execution_count": 179,
      "outputs": []
    }
  ]
}