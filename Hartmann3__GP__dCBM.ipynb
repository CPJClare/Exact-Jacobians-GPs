{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hartmann3__GP__dCBM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQo_XsusPbm"
      },
      "source": [
        "Hartmann3 synthetic function:\r\n",
        "\r\n",
        "GP EI: (exact GP EI gradients) vs. GP CBM: (exact GP CBM gradients)\r\n",
        "\r\n",
        "https://www.sfu.ca/~ssurjano/hart3.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhgucjJgsYfm",
        "outputId": "9731c87f-9101-46d6-d79d-31f34a2a258d"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyGPGO\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/87/a113c91ba014708114f7635d5c0f6a5e5c773480c5f0a537b257a02d180d/pyGPGO-0.4.0.dev1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n",
            "Building wheels for collected packages: pyGPGO\n",
            "  Building wheel for pyGPGO (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyGPGO: filename=pyGPGO-0.4.0.dev1-cp37-none-any.whl size=19866 sha256=9174a2e0b10460de7f30d0300f219f45f3a2c93b7a9134a4237107e42aeeabf7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/27/04/c4fa3bfe194d36e3cd51542132f43415a6813114a5e8301acb\n",
            "Successfully built pyGPGO\n",
            "Installing collected packages: pyGPGO\n",
            "Successfully installed pyGPGO-0.4.0.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E99qMZverfrP"
      },
      "source": [
        "### Import modules:\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import scipy as sp\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import warnings\r\n",
        "import time\r\n",
        "\r\n",
        "from pyGPGO.logger import EventLogger\r\n",
        "from pyGPGO.GPGO import GPGO\r\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\r\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\r\n",
        "from pyGPGO.acquisition import Acquisition\r\n",
        "from pyGPGO.covfunc import squaredExponential\r\n",
        "\r\n",
        "from collections import OrderedDict\r\n",
        "from joblib import Parallel, delayed\r\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\r\n",
        "from scipy.optimize import minimize\r\n",
        "from scipy.spatial.distance import cdist\r\n",
        "from scipy.special import gamma\r\n",
        "from scipy.stats import norm, t\r\n",
        "from matplotlib.pyplot import rc\r\n",
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiJSpz2P9qXK"
      },
      "source": [
        "n_start_AcqFunc = 250 #multi-start iterations to avoid local optima in AcqFunc optimization"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzPUbCs3s9nu"
      },
      "source": [
        "### Inputs:\r\n",
        "\r\n",
        "obj_func = 'Hartmann3'\r\n",
        "n_test = n_start_AcqFunc # test points\r\n",
        "df = 3 # nu\r\n",
        "Beta_CBM = 1.5 # Default UCB Acquisition function parameter in pyGPGO https://github.com/josejimenezluna/pyGPGO/blob/master/pyGPGO/acquisition.py#L83\r\n",
        "\r\n",
        "util_loser = 'dEI_GP'\r\n",
        "util_winner = 'dCBM_GP'\r\n",
        "n_init = 5 # random initialisations"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AMqjmpbtAb1"
      },
      "source": [
        "### Objective function:\r\n",
        "\r\n",
        "if obj_func == 'Hartmann3': # 3-D\r\n",
        "            \r\n",
        "    # True y bounds:\r\n",
        "    y_lb = -3.86278\r\n",
        "    operator = -1 # targets global minimum \r\n",
        "    y_global_orig = y_lb * operator # targets global minimum\r\n",
        "            \r\n",
        "    # Constraints:\r\n",
        "    lb = 0\r\n",
        "    ub = 1\r\n",
        "    \r\n",
        "    # Input array dimension(s):\r\n",
        "    dim = 3\r\n",
        "\r\n",
        "    # 3-D inputs' parameter bounds:\r\n",
        "    param = {'x1_training': ('cont', [lb, ub]),\r\n",
        "             'x2_training': ('cont', [lb, ub]),\r\n",
        "             'x3_training': ('cont', [lb, ub])}\r\n",
        "    \r\n",
        "    max_iter = (10 * dim)*0 + 100  # iterations of Bayesian optimisation\r\n",
        "    \r\n",
        "    # Test data:\r\n",
        "    x1_test = np.linspace(lb, ub, n_test) \r\n",
        "    x2_test = np.linspace(lb, ub, n_test)\r\n",
        "    x3_test = np.linspace(lb, ub, n_test)\r\n",
        "    Xstar_d = np.column_stack((x1_test, x2_test, x3_test))\r\n",
        "    \r\n",
        "    def f_syn_polarity(x1_training, x2_training, x3_training):\r\n",
        "       \r\n",
        "        value = np.array([x1_training, x2_training, x3_training])\r\n",
        "      \r\n",
        "        a = np.array([[3.0, 10, 30],\r\n",
        "                      [0.1, 10, 35],\r\n",
        "                      [3.0, 10, 30],\r\n",
        "                      [0.1, 10, 35]])\r\n",
        "        \r\n",
        "        alpha = np.array([1.0, 1.2, 3.0, 3.2])\r\n",
        "      \r\n",
        "        p = np.array([[.3689, .1170, .2673],\r\n",
        "                      [.4699, .4387, .7470],\r\n",
        "                      [.1091, .8732, .5547],\r\n",
        "                      [.3810, .5743, .8828]])\r\n",
        "  \r\n",
        "        s = 0\r\n",
        "        for i in [0,1,2,3]:\r\n",
        "            sm = a[i,0]*(value[0]-p[i,0])**2\r\n",
        "            sm += a[i,1]*(value[1]-p[i,1])**2\r\n",
        "            sm += a[i,2]*(value[2]-p[i,2])**2\r\n",
        "            s += alpha[i]*np.exp(-sm)\r\n",
        "        result = -s\r\n",
        "        \r\n",
        "        return operator * result"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyQX-59DtC7-"
      },
      "source": [
        "### Cumulative Regret Calculator:\r\n",
        "\r\n",
        "def min_max_array(x):\r\n",
        "    new_list = []\r\n",
        "    for i, num in enumerate(x):\r\n",
        "            new_list.append(np.min(x[0:i+1]))\r\n",
        "    return new_list"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE_I_YSntHxl"
      },
      "source": [
        "### Set-seeds:\r\n",
        "\r\n",
        "run_num_1 = 111\r\n",
        "run_num_2 = 113\r\n",
        "run_num_3 = 3333\r\n",
        "run_num_4 = 444\r\n",
        "run_num_5 = 5555\r\n",
        "run_num_6 = 6\r\n",
        "run_num_7 = 777\r\n",
        "run_num_8 = 887\r\n",
        "run_num_9 = 999\r\n",
        "run_num_10 = 1000\r\n",
        "run_num_11 = 1113\r\n",
        "run_num_12 = 1234\r\n",
        "run_num_13 = 2345\r\n",
        "run_num_14 = 88\r\n",
        "run_num_15 = 1556\r\n",
        "run_num_16 = 1666\r\n",
        "run_num_17 = 717\r\n",
        "run_num_18 = 8\r\n",
        "run_num_19 = 1998\r\n",
        "run_num_20 = 2000"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe29v5N5tKjF"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\r\n",
        "\r\n",
        "def l2norm_(X, Xstar):\r\n",
        "    \r\n",
        "    return cdist(X, Xstar)\r\n",
        "\r\n",
        "def kronDelta(X, Xstar):\r\n",
        "\r\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\r\n",
        "\r\n",
        "class squaredExponentialDeriv(squaredExponential):\r\n",
        "    \r\n",
        "    def K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\r\n",
        "        return K\r\n",
        "    \r\n",
        "    def dK(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\r\n",
        "        return dK\r\n",
        "    \r\n",
        "        \r\n",
        "    def d2K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (r**2-1)\r\n",
        "        return d2K\r\n",
        "    \r\n",
        "cov_func = squaredExponential()\r\n",
        "d_cov_func = squaredExponentialDeriv()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEBX5l7TtNSN"
      },
      "source": [
        "class Acquisition_new(Acquisition):    \r\n",
        "    def __init__(self, mode, eps=1e-08, **params):\r\n",
        "        \r\n",
        "        self.params = params\r\n",
        "        self.eps = eps\r\n",
        "\r\n",
        "        mode_dict = {\r\n",
        "            'dEI_GP': self.dEI_GP,\r\n",
        "            'dCBM_GP': self.dCBM_GP\r\n",
        "        }\r\n",
        "\r\n",
        "        self.f = mode_dict[mode]\r\n",
        "    \r\n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\r\n",
        "        \r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\r\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\r\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\r\n",
        "        \r\n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\r\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\r\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\r\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\r\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\r\n",
        "            \r\n",
        "        return f, df, d2f\r\n",
        "\r\n",
        "    def dCBM_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "    \r\n",
        "        gamma = -1 * (y_global_orig - mean - self.eps) / (std + self.eps)\r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        dmdx = (dm - gamma * dsdx) / (std + self.eps)\r\n",
        "\r\n",
        "        f = (std + self.eps) * (gamma + np.sqrt(Beta_CBM))\r\n",
        "        df = dsdx * (gamma + np.sqrt(Beta_CBM)) + (std + self.eps) * (dmdx + np.sqrt(Beta_CBM))\r\n",
        "        return f, df\r\n",
        "\r\n",
        "    def _eval(self, tau, mean, std):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, **self.params)\r\n",
        "    \r\n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, **self.params)\r\n",
        "\r\n",
        "    def d_eval_stp(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0, **self.params)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SdY8rHwtRJ_"
      },
      "source": [
        "### Surrogate derivatives: \r\n",
        "\r\n",
        "from scipy.linalg import cholesky, solve\r\n",
        "\r\n",
        "class dGaussianProcess(GaussianProcess):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1\r\n",
        "    sigman = 1e-6\r\n",
        "\r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\r\n",
        "        L = cholesky(K).T\r\n",
        "        alpha = solve(L.T, solve(L, self.y))\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(self.L, Kstar.T)\r\n",
        "        dv = solve(self.L, dKstar.T)\r\n",
        "        d2v = solve(self.L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -2 * np.dot(dv.T, v)\r\n",
        "        dvdv = np.dot(dv.T, dv)\r\n",
        "        d2s = -2 * (dvdv + d2v)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha)\r\n",
        "        d2m = np.dot(d2Kstar, alpha)\r\n",
        "        return ds, dm, dvdv, d2v, d2m\r\n",
        "\r\n",
        "class dtStudentProcess(tStudentProcess):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1\r\n",
        "    sigman = 1e-6\r\n",
        "    \r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\r\n",
        "        L = cholesky(self.K11).T\r\n",
        "        alpha = solve(L.T, solve(L, self.y))\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(L, Kstar.T)\r\n",
        "        dv = solve(L, dKstar.T)\r\n",
        "        d2v = solve(L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -2 * np.dot(dv.T, v)\r\n",
        "        dvdv = np.dot(dv.T, dv)\r\n",
        "        d2s = -2 * (dvdv + d2v)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha)\r\n",
        "        d2m = np.dot(d2Kstar, alpha)\r\n",
        "        return ds, dm, dvdv, d2v, d2m"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNv4Z0EqtUi2"
      },
      "source": [
        "class dGPGO(GPGO):  \r\n",
        "    n_start = n_start_AcqFunc\r\n",
        "    eps = 1e-08\r\n",
        "        \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7vea4uj-GOi"
      },
      "source": [
        "## dGPGO_stp - BayesOpt derivatives' class: Student's-t\r\n",
        "\r\n",
        "class dGPGO_stp(GPGO):  \r\n",
        "    n_start = 100\r\n",
        "        \r\n",
        "    def func_stp(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[0]\r\n",
        "        df = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq_stp(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func_stp,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq_stp()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expMSaSUtcB-"
      },
      "source": [
        "### d2GPGO - BayesOpt class: \r\n",
        "\r\n",
        "class d2GPGO(GPGO):  \r\n",
        "    n_start = 100\r\n",
        "    p = np.full((n_start,1),1)\r\n",
        "    \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2s, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "    \r\n",
        "    def hessp_nonzero(self, xnew, p):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2s, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        df2 = np.empty((self.n_start,))\r\n",
        "        df2 = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[2]\r\n",
        "        H2 = np.empty((self.n_start,))\r\n",
        "        df2 = np.asarray(df2)\r\n",
        "        p = np.asarray(p)\r\n",
        "        H2 = np.multiply(df2,p)\r\n",
        "        return H2\r\n",
        "    \r\n",
        "    def d_optimizeAcq(self, method='Newton-CG', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 hessp = self.hessp_nonzero,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        \r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.best = x_best[np.argmin(f_best)]     \r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umKPgDuEtcxH",
        "outputId": "b3cabe21-f8d6-4848-a61c-187759a69afe"
      },
      "source": [
        "start_lose = time.time()\r\n",
        "start_lose"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1615028014.0101392"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfEuNIqvtdSv",
        "outputId": "150c4d5c-6801-419f-ffb6-f3e6fa5fe3e9"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_loser_1 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_1 = dGPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.61217018 0.16906975 0.43605902]. \t  0.37345684462559386 \t 2.3951473341797507\n",
            "init   \t [0.76926247 0.2953253  0.14916296]. \t  0.296162062221701 \t 2.3951473341797507\n",
            "init   \t [0.02247832 0.42022449 0.23868214]. \t  0.2904113287153621 \t 2.3951473341797507\n",
            "init   \t [0.33765619 0.99071246 0.23772645]. \t  0.11013785080555143 \t 2.3951473341797507\n",
            "init   \t [0.08119266 0.66960024 0.62124292]. \t  2.3951473341797507 \t 2.3951473341797507\n",
            "1      \t [0.00485804 0.68286261 0.96018946]. \t  \u001b[92m2.420367869177603\u001b[0m \t 2.420367869177603\n",
            "2      \t [0.02880528 0.86879906 0.77145199]. \t  1.7601887902025246 \t 2.420367869177603\n",
            "3      \t [0.07492704 0.49363721 0.85736657]. \t  \u001b[92m3.6977257680054527\u001b[0m \t 3.6977257680054527\n",
            "4      \t [0.01690823 0.20303658 0.75373379]. \t  1.1282788852081818 \t 3.6977257680054527\n",
            "5      \t [0.8642259  0.77437694 0.99435991]. \t  1.4015971324220011 \t 3.6977257680054527\n",
            "6      \t [0.45635757 0.34106775 0.99893775]. \t  1.2764378468913773 \t 3.6977257680054527\n",
            "7      \t [0.51811283 0.98946856 0.87150225]. \t  0.67902146488023 \t 3.6977257680054527\n",
            "8      \t [0.99661639 0.78557754 0.7341533 ]. \t  1.357762610045871 \t 3.6977257680054527\n",
            "9      \t [0.23891206 0.52279384 0.744595  ]. \t  2.9890323710727635 \t 3.6977257680054527\n",
            "10     \t [0.81264104 0.02508176 0.05331337]. \t  0.1288747745884618 \t 3.6977257680054527\n",
            "11     \t [0.04939583 0.93739076 0.03475733]. \t  0.0010293495994891455 \t 3.6977257680054527\n",
            "12     \t [0.98025994 0.94493063 0.93523017]. \t  0.7397702819810867 \t 3.6977257680054527\n",
            "13     \t [0.98287372 0.41064655 0.75548703]. \t  2.5063594077603826 \t 3.6977257680054527\n",
            "14     \t [0.00356336 0.52795408 0.85605193]. \t  \u001b[92m3.784229583259046\u001b[0m \t 3.784229583259046\n",
            "15     \t [0.0224553  0.43458959 0.97377221]. \t  2.141981244272377 \t 3.784229583259046\n",
            "16     \t [0.89463363 0.84820422 0.04541421]. \t  0.0006702216295780916 \t 3.784229583259046\n",
            "17     \t [0.44206035 0.23963528 0.53817297]. \t  0.32404118535996623 \t 3.784229583259046\n",
            "18     \t [0.95149581 0.20190542 0.27573327]. \t  0.3360499969635434 \t 3.784229583259046\n",
            "19     \t [0.0256771  0.06257825 0.03723693]. \t  0.13933363855005157 \t 3.784229583259046\n",
            "20     \t [9.74869324e-01 5.78320164e-04 8.56021504e-01]. \t  0.2252628604478815 \t 3.784229583259046\n",
            "21     \t [0.0315056  0.47574841 0.74104847]. \t  2.7939271436222963 \t 3.784229583259046\n",
            "22     \t [0.28843827 0.47548806 0.0361873 ]. \t  0.05481802798153683 \t 3.784229583259046\n",
            "23     \t [0.96154876 0.6577064  0.49507711]. \t  0.28915685733921626 \t 3.784229583259046\n",
            "24     \t [0.07414375 0.54304308 0.82810303]. \t  3.77544594612987 \t 3.784229583259046\n",
            "25     \t [0.02938756 0.45830168 0.01424345]. \t  0.03241094855247772 \t 3.784229583259046\n",
            "26     \t [0.15112422 0.57169865 0.8695877 ]. \t  \u001b[92m3.8131284252695528\u001b[0m \t 3.8131284252695528\n",
            "27     \t [0.15043162 0.60970521 0.85780357]. \t  3.7470332461824105 \t 3.8131284252695528\n",
            "28     \t [0.74361257 0.16587207 0.9127195 ]. \t  0.7937697608354773 \t 3.8131284252695528\n",
            "29     \t [0.33554799 0.1960815  0.29285402]. \t  0.9219313064285296 \t 3.8131284252695528\n",
            "30     \t [0.11181592 0.64508821 0.86932481]. \t  3.552307619911147 \t 3.8131284252695528\n",
            "31     \t [0.34104117 0.1003848  0.35929984]. \t  0.7758886749514444 \t 3.8131284252695528\n",
            "32     \t [0.74081242 0.2689585  0.55926302]. \t  0.3561168332330422 \t 3.8131284252695528\n",
            "33     \t [0.78657803 0.00727757 0.24876982]. \t  0.5199892114983524 \t 3.8131284252695528\n",
            "34     \t [0.71467268 0.1858588  0.43815269]. \t  0.30645954895088945 \t 3.8131284252695528\n",
            "35     \t [0.35345103 0.44407979 0.5372887 ]. \t  0.7310458138475027 \t 3.8131284252695528\n",
            "36     \t [0.99817061 0.28845339 0.96459069]. \t  1.2542554717761243 \t 3.8131284252695528\n",
            "37     \t [0.13551095 0.6147752  0.86099234]. \t  3.7217456557246953 \t 3.8131284252695528\n",
            "38     \t [0.10902695 0.23131956 0.48553758]. \t  0.2882119729872925 \t 3.8131284252695528\n",
            "39     \t [0.02291614 0.84016591 0.0677154 ]. \t  0.0034917025704461944 \t 3.8131284252695528\n",
            "40     \t [0.73759773 0.4973644  0.27133928]. \t  0.17695216392903418 \t 3.8131284252695528\n",
            "41     \t [0.09340409 0.63120493 0.81880677]. \t  3.5500371462617144 \t 3.8131284252695528\n",
            "42     \t [0.15975379 0.0859061  0.84242869]. \t  0.5263639474321362 \t 3.8131284252695528\n",
            "43     \t [0.67406653 0.50991186 0.8188701 ]. \t  3.624086501118737 \t 3.8131284252695528\n",
            "44     \t [0.56610359 0.55232818 0.85012893]. \t  \u001b[92m3.825256006651254\u001b[0m \t 3.825256006651254\n",
            "45     \t [0.58436425 0.14790159 0.82402201]. \t  0.8773608434753184 \t 3.825256006651254\n",
            "46     \t [0.63902865 0.60026654 0.85043859]. \t  3.722030318046956 \t 3.825256006651254\n",
            "47     \t [0.17251606 0.20286351 0.62992262]. \t  0.5516254039236159 \t 3.825256006651254\n",
            "48     \t [0.17397627 0.42220867 0.13521323]. \t  0.2102484680473599 \t 3.825256006651254\n",
            "49     \t [0.01465029 0.57513231 0.85630011]. \t  3.801418874241632 \t 3.825256006651254\n",
            "50     \t [0.09463249 0.74337459 0.42976448]. \t  1.6086984706306757 \t 3.825256006651254\n",
            "51     \t [0.55394923 0.60701978 0.86924581]. \t  3.7135686672921917 \t 3.825256006651254\n",
            "52     \t [0.15384586 0.58434252 0.88508595]. \t  3.7217618326684545 \t 3.825256006651254\n",
            "53     \t [0.97344354 0.58114841 0.77316942]. \t  2.992698046283317 \t 3.825256006651254\n",
            "54     \t [0.5634305  0.72109247 0.84012619]. \t  2.9223035948746707 \t 3.825256006651254\n",
            "55     \t [0.94325517 0.68353852 0.09056374]. \t  0.006284268308091021 \t 3.825256006651254\n",
            "56     \t [0.52111688 0.40014383 0.78101946]. \t  2.817593833921735 \t 3.825256006651254\n",
            "57     \t [0.55152553 0.57153675 0.89655024]. \t  3.6490884963887 \t 3.825256006651254\n",
            "58     \t [0.50107907 0.74984541 0.40285632]. \t  0.8316034757573986 \t 3.825256006651254\n",
            "59     \t [0.6360143  0.17643848 0.6802233 ]. \t  0.6809071664843318 \t 3.825256006651254\n",
            "60     \t [0.80083048 0.87131534 0.70079956]. \t  0.954051341659558 \t 3.825256006651254\n",
            "61     \t [0.82945898 0.45413776 0.05193529]. \t  0.04229423308324152 \t 3.825256006651254\n",
            "62     \t [0.00645321 0.41674021 0.89383033]. \t  3.012380711862429 \t 3.825256006651254\n",
            "63     \t [0.03893406 0.41065296 0.22052005]. \t  0.29746538350005025 \t 3.825256006651254\n",
            "64     \t [0.41043802 0.7724817  0.31190719]. \t  0.36545761370116336 \t 3.825256006651254\n",
            "65     \t [0.10812468 0.72529452 0.25465347]. \t  0.18203581392260854 \t 3.825256006651254\n",
            "66     \t [0.02337902 0.57170695 0.84412219]. \t  3.802239441730874 \t 3.825256006651254\n",
            "67     \t [0.36281072 0.18520399 0.67829577]. \t  0.7170562744784025 \t 3.825256006651254\n",
            "68     \t [0.15911236 0.11059501 0.65089183]. \t  0.3669042997391726 \t 3.825256006651254\n",
            "69     \t [0.86350529 0.8473428  0.26259772]. \t  0.04416400359341738 \t 3.825256006651254\n",
            "70     \t [0.91551351 0.96186868 0.09233347]. \t  0.0007755993199785646 \t 3.825256006651254\n",
            "71     \t [0.54915703 0.56985442 0.26726008]. \t  0.173083197347087 \t 3.825256006651254\n",
            "72     \t [0.25627324 0.96587806 0.97601747]. \t  0.5330995829102597 \t 3.825256006651254\n",
            "73     \t [0.94115139 0.34761178 0.07356107]. \t  0.07136458178756505 \t 3.825256006651254\n",
            "74     \t [0.48760414 0.88948427 0.93965469]. \t  1.1227699240858733 \t 3.825256006651254\n",
            "75     \t [0.11304749 0.9094962  0.33213995]. \t  0.671613628713174 \t 3.825256006651254\n",
            "76     \t [0.27651602 0.0953211  0.81854326]. \t  0.5872636327758214 \t 3.825256006651254\n",
            "77     \t [0.85890336 0.09590625 0.90600383]. \t  0.4619459676508832 \t 3.825256006651254\n",
            "78     \t [0.23733868 0.82048291 0.73915572]. \t  2.124101861946425 \t 3.825256006651254\n",
            "79     \t [0.14510227 0.95514932 0.67105549]. \t  2.084284345766824 \t 3.825256006651254\n",
            "80     \t [0.62299085 0.06122236 0.99608914]. \t  0.17879623427674915 \t 3.825256006651254\n",
            "81     \t [0.29088308 0.7915848  0.41403377]. \t  1.4173589192745526 \t 3.825256006651254\n",
            "82     \t [0.04076394 0.57291442 0.54014621]. \t  1.474778439507063 \t 3.825256006651254\n",
            "83     \t [0.75370983 0.59648656 0.59667461]. \t  0.9825631878460681 \t 3.825256006651254\n",
            "84     \t [0.59983715 0.47183885 0.69463084]. \t  2.06949302073836 \t 3.825256006651254\n",
            "85     \t [0.33242802 0.83723309 0.23185022]. \t  0.11719684568063422 \t 3.825256006651254\n",
            "86     \t [0.07444371 0.91348219 0.27435046]. \t  0.27969467234132755 \t 3.825256006651254\n",
            "87     \t [0.12414229 0.18733278 0.12349369]. \t  0.4276920407976786 \t 3.825256006651254\n",
            "88     \t [0.04087888 0.24199679 0.91266509]. \t  1.323603308572735 \t 3.825256006651254\n",
            "89     \t [0.82437346 0.62993189 0.86780455]. \t  3.530389788348192 \t 3.825256006651254\n",
            "90     \t [0.51211929 0.25967277 0.9643693 ]. \t  1.1073875208142783 \t 3.825256006651254\n",
            "91     \t [0.37851489 0.68753796 0.86748997]. \t  3.2710525685540577 \t 3.825256006651254\n",
            "92     \t [0.91092111 0.42303455 0.93094604]. \t  2.642199576880523 \t 3.825256006651254\n",
            "93     \t [0.84348377 0.94747191 0.42779077]. \t  0.35056261602863853 \t 3.825256006651254\n",
            "94     \t [0.91441912 0.00287177 0.42191247]. \t  0.18004712031761186 \t 3.825256006651254\n",
            "95     \t [0.73026186 0.35943098 0.6904105 ]. \t  1.5865167617832956 \t 3.825256006651254\n",
            "96     \t [0.49324108 0.4044849  0.28058902]. \t  0.43862141669775573 \t 3.825256006651254\n",
            "97     \t [0.7886927  0.92922132 0.01214836]. \t  0.00022036803397238445 \t 3.825256006651254\n",
            "98     \t [0.80472456 0.53074051 0.03159763]. \t  0.01934647763876607 \t 3.825256006651254\n",
            "99     \t [0.39965895 0.49225993 0.45506877]. \t  0.5537815478508364 \t 3.825256006651254\n",
            "100    \t [0.32115229 0.55739422 0.943681  ]. \t  3.080532019803937 \t 3.825256006651254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j6UBhkdtdVe",
        "outputId": "f1897fc3-25b4-49a1-858b-4082605e3d88"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_loser_2 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_2 = dGPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.85198549 0.0739036  0.89493176]. \t  0.3999083566189884 \t 2.6229838112516717\n",
            "init   \t [0.43649355 0.12767773 0.57585787]. \t  0.24461577848211966 \t 2.6229838112516717\n",
            "init   \t [0.84047092 0.43512055 0.69591056]. \t  1.8897715258798413 \t 2.6229838112516717\n",
            "init   \t [0.6846381  0.70064837 0.77969426]. \t  2.6229838112516717 \t 2.6229838112516717\n",
            "init   \t [0.64274937 0.96102617 0.10846489]. \t  0.003309399496320042 \t 2.6229838112516717\n",
            "1      \t [0.84869923 0.93241717 0.93297728]. \t  0.833544515077231 \t 2.6229838112516717\n",
            "2      \t [0.02123335 0.81643687 0.92868758]. \t  1.7643065017177344 \t 2.6229838112516717\n",
            "3      \t [0.53772176 0.60822836 0.78939081]. \t  \u001b[92m3.334335833419579\u001b[0m \t 3.334335833419579\n",
            "4      \t [0.41955576 0.63153118 0.95115261]. \t  2.832879013202632 \t 3.334335833419579\n",
            "5      \t [0.04108525 0.77996289 0.25475702]. \t  0.19144274065115907 \t 3.334335833419579\n",
            "6      \t [0.33921093 0.6247264  0.68984899]. \t  2.4018311508594015 \t 3.334335833419579\n",
            "7      \t [0.4698131 1.        1.       ]. \t  0.3328206922837246 \t 3.334335833419579\n",
            "8      \t [0.5556955  0.49822112 0.91801822]. \t  3.306460837857002 \t 3.334335833419579\n",
            "9      \t [0.55486262 0.48000381 0.80443181]. \t  \u001b[92m3.45892051020631\u001b[0m \t 3.45892051020631\n",
            "10     \t [0.49254267 0.51666065 0.09211929]. \t  0.07789428322196056 \t 3.45892051020631\n",
            "11     \t [0.56427197 0.47315337 0.80769217]. \t  3.4524476308700676 \t 3.45892051020631\n",
            "12     \t [0.20076159 0.94802777 0.08215082]. \t  0.0037369434913272913 \t 3.45892051020631\n",
            "13     \t [0.59511958 0.52829156 0.7703814 ]. \t  3.2000191832757725 \t 3.45892051020631\n",
            "14     \t [0.88475754 0.03414516 0.00250664]. \t  0.05128069141159111 \t 3.45892051020631\n",
            "15     \t [0.93786595 0.53122158 0.05330172]. \t  0.017297868965155307 \t 3.45892051020631\n",
            "16     \t [0.55954194 0.47340423 0.8693643 ]. \t  \u001b[92m3.5813279149003128\u001b[0m \t 3.5813279149003128\n",
            "17     \t [0.04768557 0.32850307 0.06774003]. \t  0.14217133957014558 \t 3.5813279149003128\n",
            "18     \t [0.526396   0.39045094 0.86418002]. \t  2.984508448221251 \t 3.5813279149003128\n",
            "19     \t [0.43142122 0.02467197 0.00728288]. \t  0.11940469697941827 \t 3.5813279149003128\n",
            "20     \t [0.74314632 0.5090107  0.94480178]. \t  2.9362200818931545 \t 3.5813279149003128\n",
            "21     \t [0.22895586 0.32638851 0.44386193]. \t  0.38289739220068636 \t 3.5813279149003128\n",
            "22     \t [0.46913283 0.82262434 0.20066583]. \t  0.05199322975955362 \t 3.5813279149003128\n",
            "23     \t [0.09778912 0.09784369 0.99987747]. \t  0.24248164089538843 \t 3.5813279149003128\n",
            "24     \t [0.03443249 0.4132959  0.79333907]. \t  2.9934704340345526 \t 3.5813279149003128\n",
            "25     \t [0.38506754 0.63776631 0.47773801]. \t  1.2393662793255629 \t 3.5813279149003128\n",
            "26     \t [0.31206494 0.14585568 0.16799338]. \t  0.7307676260265302 \t 3.5813279149003128\n",
            "27     \t [0.01981209 0.39317072 0.99526576]. \t  1.5953649003670003 \t 3.5813279149003128\n",
            "28     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.5813279149003128\n",
            "29     \t [0.27452343 0.49067769 0.19135809]. \t  0.21490083226807472 \t 3.5813279149003128\n",
            "30     \t [0.41783706 0.46348636 0.08625525]. \t  0.11238386633298683 \t 3.5813279149003128\n",
            "31     \t [0.00644994 0.58259736 0.77864353]. \t  3.3564867701344787 \t 3.5813279149003128\n",
            "32     \t [0.44338236 0.20986102 0.51086629]. \t  0.28470762022492946 \t 3.5813279149003128\n",
            "33     \t [0.85132587 0.65806184 0.68313386]. \t  1.5772789254037718 \t 3.5813279149003128\n",
            "34     \t [0.72357086 0.17284508 0.5742109 ]. \t  0.2758162501943998 \t 3.5813279149003128\n",
            "35     \t [0.38871972 0.08593287 0.46946098]. \t  0.31822054482285544 \t 3.5813279149003128\n",
            "36     \t [2.55343109e-01 6.77527185e-01 3.57837085e-04]. \t  0.005091131363578613 \t 3.5813279149003128\n",
            "37     \t [0.03071868 0.06496602 0.57653961]. \t  0.15761028036633287 \t 3.5813279149003128\n",
            "38     \t [0.66009546 0.39983393 0.51405981]. \t  0.37461569752794827 \t 3.5813279149003128\n",
            "39     \t [0.59449511 0.41620195 0.91113527]. \t  2.8805701809722963 \t 3.5813279149003128\n",
            "40     \t [0.2256699  0.07992419 0.41748843]. \t  0.4819630201952553 \t 3.5813279149003128\n",
            "41     \t [0.97615885 0.05411726 0.32503146]. \t  0.28831141983778286 \t 3.5813279149003128\n",
            "42     \t [0.53328464 0.57774717 0.88782334]. \t  \u001b[92m3.7094133224093757\u001b[0m \t 3.7094133224093757\n",
            "43     \t [0.52246356 0.82266054 0.13911684]. \t  0.013759219160063875 \t 3.7094133224093757\n",
            "44     \t [0.72982604 0.28184031 0.86459949]. \t  1.9044776108865855 \t 3.7094133224093757\n",
            "45     \t [0.63818509 0.39198514 0.45536791]. \t  0.2892748188116343 \t 3.7094133224093757\n",
            "46     \t [0.73136775 0.30934836 0.92232944]. \t  1.8276447749873623 \t 3.7094133224093757\n",
            "47     \t [0.77047443 0.64214483 0.12903927]. \t  0.024102114086029674 \t 3.7094133224093757\n",
            "48     \t [0.4634485  0.65534339 0.23827586]. \t  0.11593556414242427 \t 3.7094133224093757\n",
            "49     \t [0.26527368 0.43158941 0.30440152]. \t  0.4071890908589547 \t 3.7094133224093757\n",
            "50     \t [0.98012153 0.29487901 0.54063701]. \t  0.273707971859134 \t 3.7094133224093757\n",
            "51     \t [0.20844235 0.33272232 0.11427776]. \t  0.2883861055346616 \t 3.7094133224093757\n",
            "52     \t [0.83243348 0.27176422 0.0498824 ]. \t  0.10004439130921053 \t 3.7094133224093757\n",
            "53     \t [0.31228377 0.9497917  0.78925978]. \t  1.1372129515685026 \t 3.7094133224093757\n",
            "54     \t [0.81204389 0.10745564 0.99319514]. \t  0.2793664207698889 \t 3.7094133224093757\n",
            "55     \t [0.30559737 0.74140756 0.20001864]. \t  0.0690608605636907 \t 3.7094133224093757\n",
            "56     \t [0.30355771 0.60105121 0.66701023]. \t  2.232680275582264 \t 3.7094133224093757\n",
            "57     \t [0.32655384 0.45283861 0.70914601]. \t  2.3157027494336946 \t 3.7094133224093757\n",
            "58     \t [0.28164058 0.91846989 0.13751527]. \t  0.015473546478944136 \t 3.7094133224093757\n",
            "59     \t [0.04941912 0.54241983 0.36607595]. \t  0.4385741473197778 \t 3.7094133224093757\n",
            "60     \t [0.17888481 0.27261652 0.57983518]. \t  0.5075266432155109 \t 3.7094133224093757\n",
            "61     \t [0.60451084 0.31746966 0.12296527]. \t  0.30343594735798185 \t 3.7094133224093757\n",
            "62     \t [0.69919355 0.08866689 0.64966157]. \t  0.30694751763473066 \t 3.7094133224093757\n",
            "63     \t [0.89315899 0.15618225 0.85935459]. \t  0.8735852961978772 \t 3.7094133224093757\n",
            "64     \t [0.1521253  0.35901651 0.22965336]. \t  0.4723852558929425 \t 3.7094133224093757\n",
            "65     \t [0.23231678 0.10972766 0.93641936]. \t  0.44882914638217647 \t 3.7094133224093757\n",
            "66     \t [0.84188226 0.70723005 0.54045426]. \t  0.6264810020583895 \t 3.7094133224093757\n",
            "67     \t [0.01124263 0.9495282  0.89764946]. \t  0.8860680765708808 \t 3.7094133224093757\n",
            "68     \t [0.64184498 0.26595085 0.80861442]. \t  1.7950464127622898 \t 3.7094133224093757\n",
            "69     \t [0.0229726  0.00499224 0.67044876]. \t  0.17729951913373557 \t 3.7094133224093757\n",
            "70     \t [0.51570772 0.65260972 0.10114952]. \t  0.025592110981376887 \t 3.7094133224093757\n",
            "71     \t [0.60347441 0.897619   0.45446129]. \t  1.06949486043582 \t 3.7094133224093757\n",
            "72     \t [0.95758577 0.64651273 0.9322479 ]. \t  2.9288989872413875 \t 3.7094133224093757\n",
            "73     \t [0.78857677 0.49823768 0.51203495]. \t  0.38734467751295526 \t 3.7094133224093757\n",
            "74     \t [0.43165387 0.02353938 0.45456709]. \t  0.3284207744430256 \t 3.7094133224093757\n",
            "75     \t [0.93712698 0.40753674 0.39799078]. \t  0.1357588290930862 \t 3.7094133224093757\n",
            "76     \t [0.75852101 0.56650411 0.29588659]. \t  0.12717945507032985 \t 3.7094133224093757\n",
            "77     \t [0.65699665 0.67754593 0.95225407]. \t  2.573110674137345 \t 3.7094133224093757\n",
            "78     \t [0.21411235 0.92073944 0.18720244]. \t  0.050558991438194395 \t 3.7094133224093757\n",
            "79     \t [0.81186963 0.95466875 0.19513996]. \t  0.013622693124886443 \t 3.7094133224093757\n",
            "80     \t [0.13321787 0.16889317 0.15110243]. \t  0.549728293542864 \t 3.7094133224093757\n",
            "81     \t [0.50055678 0.40552636 0.35889602]. \t  0.39460953216206407 \t 3.7094133224093757\n",
            "82     \t [0.68937348 0.74734445 0.93696005]. \t  2.2621603595408324 \t 3.7094133224093757\n",
            "83     \t [0.24422687 0.25258948 0.42558462]. \t  0.43458421843040185 \t 3.7094133224093757\n",
            "84     \t [0.33290521 0.92785349 0.70086949]. \t  1.7092276465727636 \t 3.7094133224093757\n",
            "85     \t [0.23066553 0.98634866 0.51216754]. \t  2.405127214497184 \t 3.7094133224093757\n",
            "86     \t [0.77062047 0.44235163 0.41154312]. \t  0.2070073279439252 \t 3.7094133224093757\n",
            "87     \t [0.89972384 0.5416144  0.91950053]. \t  3.316834149142124 \t 3.7094133224093757\n",
            "88     \t [0.35886225 0.27348523 0.98584931]. \t  1.0168568786796437 \t 3.7094133224093757\n",
            "89     \t [0.24008624 0.71862237 0.60323517]. \t  2.5243228209177246 \t 3.7094133224093757\n",
            "90     \t [0.70227469 0.71073892 0.38639049]. \t  0.3630229515433469 \t 3.7094133224093757\n",
            "91     \t [0.04630934 0.53729892 0.00134421]. \t  0.015083670432843367 \t 3.7094133224093757\n",
            "92     \t [0.68416238 0.83798606 0.92513195]. \t  1.5835707271787056 \t 3.7094133224093757\n",
            "93     \t [0.80857863 0.30322871 0.42341   ]. \t  0.23276557630011466 \t 3.7094133224093757\n",
            "94     \t [0.87240962 0.53505616 0.5612704 ]. \t  0.5766218070407124 \t 3.7094133224093757\n",
            "95     \t [0.37075559 0.35467445 0.40266831]. \t  0.4292705700245347 \t 3.7094133224093757\n",
            "96     \t [0.07623101 0.7603222  0.69765839]. \t  2.4874462002268505 \t 3.7094133224093757\n",
            "97     \t [0.52298903 0.07651244 0.54207474]. \t  0.17715826053064726 \t 3.7094133224093757\n",
            "98     \t [0.18967587 0.49028662 0.51106987]. \t  0.8680665472248712 \t 3.7094133224093757\n",
            "99     \t [0.77481162 0.28016538 0.68979909]. \t  1.2006144140525907 \t 3.7094133224093757\n",
            "100    \t [0.34253365 0.56155319 0.19047772]. \t  0.13390384213700912 \t 3.7094133224093757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWnL1rwztdYG",
        "outputId": "40f1e04c-1b72-4681-cadc-1634e1e6da73"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_loser_3 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_3 = dGPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.75157561 0.10925128 0.48612128]. \t  0.19395982560567754 \t 0.5647137279144399\n",
            "init   \t [0.49983118 0.65711    0.23588471]. \t  0.10633623603471726 \t 0.5647137279144399\n",
            "init   \t [0.61279489 0.1196524  0.7122023 ]. \t  0.5647137279144399 \t 0.5647137279144399\n",
            "init   \t [0.37256054 0.52476827 0.26328708]. \t  0.24645583811739444 \t 0.5647137279144399\n",
            "init   \t [0.62163122 0.44909976 0.21906361]. \t  0.2633149120115605 \t 0.5647137279144399\n",
            "1      \t [0.03146046 0.20692188 0.93064958]. \t  \u001b[92m0.9684227062893344\u001b[0m \t 0.9684227062893344\n",
            "2      \t [0.22074887 0.05835947 0.90763313]. \t  0.3319318657983171 \t 0.9684227062893344\n",
            "3      \t [0.80398301 0.59732101 0.97755957]. \t  \u001b[92m2.4285425171285553\u001b[0m \t 2.4285425171285553\n",
            "4      \t [0.5136305  0.6806365  0.98913253]. \t  2.0107525433877536 \t 2.4285425171285553\n",
            "5      \t [0.96047188 0.84635755 0.98454941]. \t  1.059610295269561 \t 2.4285425171285553\n",
            "6      \t [0.73881103 0.2852016  0.92866174]. \t  1.569439758670843 \t 2.4285425171285553\n",
            "7      \t [0.71874866 0.52230578 0.95817827]. \t  \u001b[92m2.7596541737687836\u001b[0m \t 2.7596541737687836\n",
            "8      \t [0.58383333 0.51359399 0.72217957]. \t  2.534851203896329 \t 2.7596541737687836\n",
            "9      \t [0.69948316 0.50135082 0.96939443]. \t  2.5149949368150564 \t 2.7596541737687836\n",
            "10     \t [2.21903490e-15 3.33185517e-14 2.60365980e-15]. \t  0.06797411659014074 \t 2.7596541737687836\n",
            "11     \t [0.08297588 0.58449891 0.0494654 ]. \t  0.021801490436882155 \t 2.7596541737687836\n",
            "12     \t [0.72187606 0.64625932 0.88228903]. \t  \u001b[92m3.435042406543167\u001b[0m \t 3.435042406543167\n",
            "13     \t [0.69808759 0.73788415 0.86826237]. \t  2.744041156678493 \t 3.435042406543167\n",
            "14     \t [0.93887588 0.6613653  0.69546595]. \t  1.6275056193081119 \t 3.435042406543167\n",
            "15     \t [0.31568132 0.43634948 0.00142586]. \t  0.04293482559337013 \t 3.435042406543167\n",
            "16     \t [0.16842711 0.275181   0.2593233 ]. \t  0.6951641784284915 \t 3.435042406543167\n",
            "17     \t [0.         0.06579622 0.28490418]. \t  0.642256134812402 \t 3.435042406543167\n",
            "18     \t [0.71655162 0.49206118 0.79554346]. \t  3.3736815291998044 \t 3.435042406543167\n",
            "19     \t [0.75518888 0.55697264 0.75159371]. \t  2.855037242391764 \t 3.435042406543167\n",
            "20     \t [0.70228575 0.97471212 0.02582005]. \t  0.00029310748513941917 \t 3.435042406543167\n",
            "21     \t [0.99752384 0.064151   0.1703046 ]. \t  0.22410412246044556 \t 3.435042406543167\n",
            "22     \t [0.0228455  0.90537222 0.38683236]. \t  1.2492930756187308 \t 3.435042406543167\n",
            "23     \t [0.13378523 0.95916986 0.14208018]. \t  0.017267101982617633 \t 3.435042406543167\n",
            "24     \t [0.15772068 0.87127358 0.74372625]. \t  1.8724552511399 \t 3.435042406543167\n",
            "25     \t [0.14574635 0.38726702 0.24731617]. \t  0.42664930173697285 \t 3.435042406543167\n",
            "26     \t [0.04777267 0.86009005 0.91542791]. \t  1.4808874257581612 \t 3.435042406543167\n",
            "27     \t [0.63444916 0.36426605 0.85539499]. \t  2.7493518034550775 \t 3.435042406543167\n",
            "28     \t [0.73479632 0.49575709 0.51112471]. \t  0.4256175209454758 \t 3.435042406543167\n",
            "29     \t [0.30819472 0.01158196 0.12503412]. \t  0.48221799946723587 \t 3.435042406543167\n",
            "30     \t [0.74662962 0.58954966 0.84542697]. \t  \u001b[92m3.7070039032667954\u001b[0m \t 3.7070039032667954\n",
            "31     \t [0.42596168 0.35197751 0.56469788]. \t  0.5906940593598009 \t 3.7070039032667954\n",
            "32     \t [0.80411671 0.9277025  0.12495525]. \t  0.0031149831092267023 \t 3.7070039032667954\n",
            "33     \t [0.66688846 0.22554472 0.58509531]. \t  0.3956915884135331 \t 3.7070039032667954\n",
            "34     \t [0.96402316 0.80627495 0.02410483]. \t  0.0005753484237622942 \t 3.7070039032667954\n",
            "35     \t [0.46624611 0.63255562 0.52234503]. \t  1.2945035042624384 \t 3.7070039032667954\n",
            "36     \t [0.75738113 0.92448556 0.82374733]. \t  1.0051096970615077 \t 3.7070039032667954\n",
            "37     \t [0.13401842 0.59394694 0.07117342]. \t  0.02871568488814516 \t 3.7070039032667954\n",
            "38     \t [0.5921114  0.60036898 0.98206343]. \t  2.377598838026082 \t 3.7070039032667954\n",
            "39     \t [0.79172764 0.90784428 0.70914857]. \t  0.8431319054674943 \t 3.7070039032667954\n",
            "40     \t [0.1529027  0.26265257 0.99129722]. \t  0.9064177756037938 \t 3.7070039032667954\n",
            "41     \t [0.62518867 0.7121498  0.30259927]. \t  0.17816033134214168 \t 3.7070039032667954\n",
            "42     \t [0.24439409 0.39110835 0.57603432]. \t  0.8043123257896625 \t 3.7070039032667954\n",
            "43     \t [0.23812336 0.07427496 0.89625034]. \t  0.40560748042148276 \t 3.7070039032667954\n",
            "44     \t [0.04418732 0.53134902 0.61276949]. \t  1.6534094783377127 \t 3.7070039032667954\n",
            "45     \t [0.89190225 0.97916204 0.0016274 ]. \t  7.543539199836043e-05 \t 3.7070039032667954\n",
            "46     \t [0.8607458  0.29914426 0.76705565]. \t  1.8822648550157148 \t 3.7070039032667954\n",
            "47     \t [0.48787013 0.40950719 0.58412121]. \t  0.8188835841989224 \t 3.7070039032667954\n",
            "48     \t [0.6043061  0.76298401 0.62408145]. \t  1.5630631055365873 \t 3.7070039032667954\n",
            "49     \t [0.40684603 0.86167236 0.56044735]. \t  2.3905743122833742 \t 3.7070039032667954\n",
            "50     \t [0.62111368 0.52704732 0.84922528]. \t  \u001b[92m3.7896923749467715\u001b[0m \t 3.7896923749467715\n",
            "51     \t [0.08214015 0.55596698 0.52951788]. \t  1.3249204381796214 \t 3.7896923749467715\n",
            "52     \t [0.49073662 0.44830916 0.62947485]. \t  1.3035237788368443 \t 3.7896923749467715\n",
            "53     \t [0.61633848 0.39414783 0.60448986]. \t  0.8720175831755752 \t 3.7896923749467715\n",
            "54     \t [0.09632054 0.64536185 0.89614376]. \t  3.407552915077267 \t 3.7896923749467715\n",
            "55     \t [0.92134716 0.78477873 0.66050099]. \t  0.9011550630172612 \t 3.7896923749467715\n",
            "56     \t [0.57293523 0.53361841 0.30639247]. \t  0.22796920303046153 \t 3.7896923749467715\n",
            "57     \t [0.21923538 0.73686176 0.76555192]. \t  2.631803742382601 \t 3.7896923749467715\n",
            "58     \t [0.28290393 0.40273006 0.21517779]. \t  0.40796869278351705 \t 3.7896923749467715\n",
            "59     \t [0.7297733  0.40900862 0.17685063]. \t  0.2271653108899221 \t 3.7896923749467715\n",
            "60     \t [0.2230724  0.51649058 0.0899105 ]. \t  0.07523571607849378 \t 3.7896923749467715\n",
            "61     \t [0.47269005 0.57372622 0.30365754]. \t  0.24086025892892132 \t 3.7896923749467715\n",
            "62     \t [0.19179956 0.04726392 0.22522958]. \t  0.8223003040779117 \t 3.7896923749467715\n",
            "63     \t [0.00454622 0.61282583 0.98623381]. \t  2.2601194402741687 \t 3.7896923749467715\n",
            "64     \t [0.21059232 0.42055126 0.12793231]. \t  0.20770560044238096 \t 3.7896923749467715\n",
            "65     \t [0.34021681 0.82098355 0.4395948 ]. \t  1.6861927233521885 \t 3.7896923749467715\n",
            "66     \t [0.39788785 0.59683922 0.81430545]. \t  3.6427668029762037 \t 3.7896923749467715\n",
            "67     \t [0.46904421 0.31390676 0.81104597]. \t  2.257544468526626 \t 3.7896923749467715\n",
            "68     \t [0.84478817 0.17962491 0.48977645]. \t  0.17730670216691743 \t 3.7896923749467715\n",
            "69     \t [0.65164826 0.42992198 0.58646353]. \t  0.786818072813842 \t 3.7896923749467715\n",
            "70     \t [0.61884566 0.31746878 0.49688504]. \t  0.29554206320383436 \t 3.7896923749467715\n",
            "71     \t [0.61376377 0.70204201 0.35125962]. \t  0.3258708768234984 \t 3.7896923749467715\n",
            "72     \t [0.70209444 0.16663276 0.15167109]. \t  0.46828875679206056 \t 3.7896923749467715\n",
            "73     \t [0.26262437 0.44250019 0.03060716]. \t  0.06252148954452384 \t 3.7896923749467715\n",
            "74     \t [0.16253149 0.66009353 0.2770664 ]. \t  0.2333070716240491 \t 3.7896923749467715\n",
            "75     \t [0.54743938 0.95930507 0.37960054]. \t  0.6252381735775987 \t 3.7896923749467715\n",
            "76     \t [0.16019912 0.90675208 0.41598391]. \t  1.6567154722530448 \t 3.7896923749467715\n",
            "77     \t [0.14125313 0.56127449 0.28467157]. \t  0.24526769934086157 \t 3.7896923749467715\n",
            "78     \t [0.68087076 0.47864738 0.02789553]. \t  0.03623559685115433 \t 3.7896923749467715\n",
            "79     \t [0.84363628 0.39010202 0.36788822]. \t  0.20606981964993962 \t 3.7896923749467715\n",
            "80     \t [0.88060534 0.81553029 0.87952935]. \t  1.9182660692088596 \t 3.7896923749467715\n",
            "81     \t [0.89877928 0.3459699  0.20278669]. \t  0.22577900317369806 \t 3.7896923749467715\n",
            "82     \t [0.45857582 0.75096334 0.18121827]. \t  0.04132361494283127 \t 3.7896923749467715\n",
            "83     \t [0.63085465 0.39247245 0.10795824]. \t  0.1782535802299659 \t 3.7896923749467715\n",
            "84     \t [0.95316593 0.55188064 0.16964544]. \t  0.04218875364955372 \t 3.7896923749467715\n",
            "85     \t [0.85684935 0.75766339 0.033176  ]. \t  0.0017000811369795696 \t 3.7896923749467715\n",
            "86     \t [0.78716689 0.04587408 0.94080521]. \t  0.239651019612562 \t 3.7896923749467715\n",
            "87     \t [0.43911044 0.6981408  0.64567541]. \t  2.053757934020994 \t 3.7896923749467715\n",
            "88     \t [0.8694776  0.36424848 0.71022565]. \t  1.7943091250771404 \t 3.7896923749467715\n",
            "89     \t [0.13016603 0.16838239 0.61643476]. \t  0.4054373001335664 \t 3.7896923749467715\n",
            "90     \t [0.02729288 0.32701445 0.32086059]. \t  0.4466395844396876 \t 3.7896923749467715\n",
            "91     \t [0.37397447 0.81674824 0.92999278]. \t  1.7677722791097792 \t 3.7896923749467715\n",
            "92     \t [0.18208795 0.60697419 0.59659828]. \t  1.9675248424084217 \t 3.7896923749467715\n",
            "93     \t [0.23646816 0.25281473 0.09853109]. \t  0.3358113649828574 \t 3.7896923749467715\n",
            "94     \t [0.33836395 0.50746848 0.85995754]. \t  3.776428935520155 \t 3.7896923749467715\n",
            "95     \t [0.01149402 0.12150612 0.0622546 ]. \t  0.19307208285276578 \t 3.7896923749467715\n",
            "96     \t [0.54711447 0.76849605 0.56712612]. \t  1.70301075667317 \t 3.7896923749467715\n",
            "97     \t [0.89431243 0.73140871 0.6123873 ]. \t  0.803263834874783 \t 3.7896923749467715\n",
            "98     \t [0.18324495 0.1323387  0.43675612]. \t  0.404632292432565 \t 3.7896923749467715\n",
            "99     \t [0.38974735 0.09538962 0.96336092]. \t  0.3290183762762645 \t 3.7896923749467715\n",
            "100    \t [0.19820904 0.06434005 0.77592979]. \t  0.4449318315806874 \t 3.7896923749467715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zciLjgNltda2",
        "outputId": "5f72789a-1fac-4523-c42b-2ff3b81aa8b9"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_loser_4 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_4 = dGPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.83936105 0.82129497 0.64220716]. \t  0.8796727111154451 \t 1.9592421489197056\n",
            "init   \t [0.66722262 0.03913991 0.06567239]. \t  0.21283931850025994 \t 1.9592421489197056\n",
            "init   \t [0.27650064 0.3163605  0.19359241]. \t  0.5590312289203825 \t 1.9592421489197056\n",
            "init   \t [0.19709288 0.84920429 0.84235809]. \t  1.818649590001362 \t 1.9592421489197056\n",
            "init   \t [0.34736473 0.7397423  0.49340914]. \t  1.9592421489197056 \t 1.9592421489197056\n",
            "1      \t [0.22412884 0.99744654 0.22062981]. \t  0.08723427566372202 \t 1.9592421489197056\n",
            "2      \t [0.4057286  0.31242791 0.80779792]. \t  \u001b[92m2.2369433002019576\u001b[0m \t 2.2369433002019576\n",
            "3      \t [0.32214328 0.501755   0.69916371]. \t  \u001b[92m2.347550186167049\u001b[0m \t 2.347550186167049\n",
            "4      \t [0.39005329 0.54644944 0.69817317]. \t  \u001b[92m2.3848842680130833\u001b[0m \t 2.3848842680130833\n",
            "5      \t [0.34533898 0.52489446 0.79943823]. \t  \u001b[92m3.5836920400367456\u001b[0m \t 3.5836920400367456\n",
            "6      \t [0.63960296 0.91555258 0.0555591 ]. \t  0.0010744359811289052 \t 3.5836920400367456\n",
            "7      \t [0.22351983 0.28690065 0.99397278]. \t  1.0190572439287129 \t 3.5836920400367456\n",
            "8      \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.5836920400367456\n",
            "9      \t [0.77141999 0.13456388 0.98918372]. \t  0.3672430639616408 \t 3.5836920400367456\n",
            "10     \t [6.73506703e-15 9.08143925e-15 7.56593801e-15]. \t  0.06797411659014295 \t 3.5836920400367456\n",
            "11     \t [0.51437176 0.48035947 0.99743479]. \t  1.9786849166252316 \t 3.5836920400367456\n",
            "12     \t [0.84655016 0.99679085 0.6174574 ]. \t  0.5215294903522869 \t 3.5836920400367456\n",
            "13     \t [0.17365811 0.53915787 0.02836154]. \t  0.0273099934743758 \t 3.5836920400367456\n",
            "14     \t [0.29207719 0.024576   0.05725103]. \t  0.2400905457827503 \t 3.5836920400367456\n",
            "15     \t [0.98578944 0.13581165 0.27179492]. \t  0.3182599136266915 \t 3.5836920400367456\n",
            "16     \t [0.34593306 0.65138162 0.95430717]. \t  2.7032184610475998 \t 3.5836920400367456\n",
            "17     \t [0.99991374 0.24441493 0.03350525]. \t  0.0499541826127307 \t 3.5836920400367456\n",
            "18     \t [0.99429147 0.7531398  0.66991977]. \t  0.9772652638068244 \t 3.5836920400367456\n",
            "19     \t [0.27805725 0.59470274 0.85747296]. \t  \u001b[92m3.8051821097914003\u001b[0m \t 3.8051821097914003\n",
            "20     \t [0.94383316 0.48004807 0.41985768]. \t  0.12403116135075266 \t 3.8051821097914003\n",
            "21     \t [0.27516191 0.5050454  0.91115678]. \t  3.4233821782694847 \t 3.8051821097914003\n",
            "22     \t [0.88210647 0.22206517 0.85819382]. \t  1.3626645528941668 \t 3.8051821097914003\n",
            "23     \t [0.465022   0.28549004 0.02474828]. \t  0.12537651819404955 \t 3.8051821097914003\n",
            "24     \t [0.77736605 0.42924543 0.32339966]. \t  0.23232259737210692 \t 3.8051821097914003\n",
            "25     \t [0.48323297 0.97953335 0.58107143]. \t  1.7743492083378223 \t 3.8051821097914003\n",
            "26     \t [0.07390852 0.47026935 0.4418145 ]. \t  0.5388868547402469 \t 3.8051821097914003\n",
            "27     \t [0.43391942 0.96545355 0.6232254 ]. \t  1.8531866038798177 \t 3.8051821097914003\n",
            "28     \t [0.72373636 0.89800689 0.19750102]. \t  0.022219851170923014 \t 3.8051821097914003\n",
            "29     \t [0.23399885 0.62804062 0.83057931]. \t  3.6327766749305987 \t 3.8051821097914003\n",
            "30     \t [0.47646467 0.31690246 0.92015299]. \t  1.9335877861944526 \t 3.8051821097914003\n",
            "31     \t [0.26793312 0.20835569 0.39616386]. \t  0.5675650435515753 \t 3.8051821097914003\n",
            "32     \t [0.15449961 0.0739904  0.02383857]. \t  0.1444824248167775 \t 3.8051821097914003\n",
            "33     \t [0.22224128 0.58835294 0.54227105]. \t  1.5620019707976203 \t 3.8051821097914003\n",
            "34     \t [0.7624105  0.83740933 0.4174219 ]. \t  0.475673205419742 \t 3.8051821097914003\n",
            "35     \t [0.38226498 0.56576795 0.31580928]. \t  0.2940891935717929 \t 3.8051821097914003\n",
            "36     \t [0.42122065 0.03334281 0.87861693]. \t  0.2979774737400446 \t 3.8051821097914003\n",
            "37     \t [0.95577186 0.2331745  0.18906347]. \t  0.25888078497782724 \t 3.8051821097914003\n",
            "38     \t [0.77686775 0.19817622 0.50516741]. \t  0.20303404687426588 \t 3.8051821097914003\n",
            "39     \t [0.02572599 0.03736815 0.77135148]. \t  0.34568347538666316 \t 3.8051821097914003\n",
            "40     \t [0.53961985 0.04018105 0.36712348]. \t  0.6427412747817036 \t 3.8051821097914003\n",
            "41     \t [0.09117621 0.44826011 0.64659412]. \t  1.6004301056836554 \t 3.8051821097914003\n",
            "42     \t [0.69797215 0.92699324 0.60669458]. \t  1.068196523028918 \t 3.8051821097914003\n",
            "43     \t [0.14716524 0.6846356  0.55273114]. \t  2.331277322632874 \t 3.8051821097914003\n",
            "44     \t [0.94460183 0.39841865 0.20035792]. \t  0.14743471184946214 \t 3.8051821097914003\n",
            "45     \t [0.02849758 0.52533294 0.99914079]. \t  2.0416369587013725 \t 3.8051821097914003\n",
            "46     \t [0.59525019 0.69610244 0.04470115]. \t  0.0072205823831998734 \t 3.8051821097914003\n",
            "47     \t [0.18193296 0.12830195 0.57593137]. \t  0.24216357204494532 \t 3.8051821097914003\n",
            "48     \t [0.20414017 0.28332865 0.57594635]. \t  0.5157278007739652 \t 3.8051821097914003\n",
            "49     \t [0.51788579 0.81395536 0.34779211]. \t  0.4928912781654505 \t 3.8051821097914003\n",
            "50     \t [0.19029052 0.2022316  0.06057696]. \t  0.23450252136128893 \t 3.8051821097914003\n",
            "51     \t [0.31404645 0.34552945 0.79524926]. \t  2.489876466905594 \t 3.8051821097914003\n",
            "52     \t [0.38784961 0.87753762 0.74012079]. \t  1.6471620810320635 \t 3.8051821097914003\n",
            "53     \t [0.86772633 0.88541181 0.69753257]. \t  0.7934431092025374 \t 3.8051821097914003\n",
            "54     \t [0.46112737 0.30964653 0.05452543]. \t  0.17298332743033695 \t 3.8051821097914003\n",
            "55     \t [0.26064166 0.75821426 0.03807247]. \t  0.0040868577728126795 \t 3.8051821097914003\n",
            "56     \t [0.54614286 0.98388172 0.59399202]. \t  1.4879629764781568 \t 3.8051821097914003\n",
            "57     \t [0.45408303 0.80825061 0.6365093 ]. \t  2.0677163745827047 \t 3.8051821097914003\n",
            "58     \t [0.70741816 0.07928674 0.66920884]. \t  0.32737925886271296 \t 3.8051821097914003\n",
            "59     \t [0.06057931 0.81118412 0.58648205]. \t  2.9844350007817253 \t 3.8051821097914003\n",
            "60     \t [0.18077291 0.11516388 0.30186734]. \t  0.8693688111325402 \t 3.8051821097914003\n",
            "61     \t [0.78261206 0.08739428 0.30479699]. \t  0.569307619631024 \t 3.8051821097914003\n",
            "62     \t [0.20286778 0.48979907 0.2163436 ]. \t  0.2338871807716341 \t 3.8051821097914003\n",
            "63     \t [0.5386693  0.27612135 0.13522479]. \t  0.4221550183648063 \t 3.8051821097914003\n",
            "64     \t [0.22997948 0.52205122 0.23892099]. \t  0.220726732621446 \t 3.8051821097914003\n",
            "65     \t [0.85129755 0.30898539 0.42632346]. \t  0.20407290980443352 \t 3.8051821097914003\n",
            "66     \t [0.5290956  0.06631674 0.6350847 ]. \t  0.23937763515635238 \t 3.8051821097914003\n",
            "67     \t [0.32225612 0.58867405 0.66683327]. \t  2.187367589850706 \t 3.8051821097914003\n",
            "68     \t [0.70417805 0.87920468 0.75425424]. \t  1.185947839625982 \t 3.8051821097914003\n",
            "69     \t [0.68707938 0.97773471 0.3911807 ]. \t  0.44383116056510996 \t 3.8051821097914003\n",
            "70     \t [0.36894736 0.56217032 0.37926925]. \t  0.4739893548207636 \t 3.8051821097914003\n",
            "71     \t [0.91906465 0.12271966 0.32488807]. \t  0.3661618263517013 \t 3.8051821097914003\n",
            "72     \t [0.79562402 0.78140355 0.99146553]. \t  1.4024275536421427 \t 3.8051821097914003\n",
            "73     \t [0.19952009 0.46582031 0.97637385]. \t  2.2772004000300323 \t 3.8051821097914003\n",
            "74     \t [0.42921714 0.21734282 0.37283993]. \t  0.6569917965717333 \t 3.8051821097914003\n",
            "75     \t [0.58785692 0.23925236 0.34858759]. \t  0.6224387919467267 \t 3.8051821097914003\n",
            "76     \t [0.83910257 0.94330969 0.78787869]. \t  0.7862412827987283 \t 3.8051821097914003\n",
            "77     \t [0.56118624 0.59986808 0.49463569]. \t  0.8250381800002857 \t 3.8051821097914003\n",
            "78     \t [0.7662562  0.14790208 0.78952916]. \t  0.8577485172452937 \t 3.8051821097914003\n",
            "79     \t [0.21712483 0.17339281 0.04403016]. \t  0.20263511578268303 \t 3.8051821097914003\n",
            "80     \t [0.34812389 0.0410566  0.25745782]. \t  0.9402321835937894 \t 3.8051821097914003\n",
            "81     \t [0.80787489 0.49984609 0.19371546]. \t  0.11358820295056764 \t 3.8051821097914003\n",
            "82     \t [0.77992988 0.67256643 0.99579517]. \t  1.9093788624256178 \t 3.8051821097914003\n",
            "83     \t [0.74233078 0.51867866 0.07880477]. \t  0.04543925368190092 \t 3.8051821097914003\n",
            "84     \t [0.42375063 0.18059118 0.13005468]. \t  0.5409662204364257 \t 3.8051821097914003\n",
            "85     \t [0.82159906 0.2629594  0.99301144]. \t  0.8829369483160052 \t 3.8051821097914003\n",
            "86     \t [0.87004636 0.48103157 0.64604538]. \t  1.304640200884708 \t 3.8051821097914003\n",
            "87     \t [0.00611828 0.39700184 0.56633479]. \t  0.7582299138150631 \t 3.8051821097914003\n",
            "88     \t [0.66081675 0.24908963 0.36671335]. \t  0.49741112338686794 \t 3.8051821097914003\n",
            "89     \t [0.37145448 0.12239177 0.85745563]. \t  0.6941282348959765 \t 3.8051821097914003\n",
            "90     \t [0.50921909 0.11168804 0.67675224]. \t  0.44129452824360094 \t 3.8051821097914003\n",
            "91     \t [0.12022965 0.95893321 0.59043519]. \t  2.751577852315629 \t 3.8051821097914003\n",
            "92     \t [0.61692084 0.26266657 0.40584368]. \t  0.41060339066604384 \t 3.8051821097914003\n",
            "93     \t [0.38132105 0.65477671 0.33897365]. \t  0.41884361706401735 \t 3.8051821097914003\n",
            "94     \t [0.90274566 0.39309968 0.42848717]. \t  0.1538058248859996 \t 3.8051821097914003\n",
            "95     \t [0.3874207  0.91625586 0.45246377]. \t  1.7139535209488614 \t 3.8051821097914003\n",
            "96     \t [0.9037247  0.67953797 0.64761112]. \t  1.108383505018141 \t 3.8051821097914003\n",
            "97     \t [0.2129587  0.5704972  0.34719778]. \t  0.42129348276748696 \t 3.8051821097914003\n",
            "98     \t [0.5736689  0.22695589 0.48933553]. \t  0.27844968171053286 \t 3.8051821097914003\n",
            "99     \t [0.27718184 0.48726663 0.59624794]. \t  1.2942319378416134 \t 3.8051821097914003\n",
            "100    \t [0.77226186 0.20972114 0.79326813]. \t  1.2850883206415245 \t 3.8051821097914003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH76hlE2tddm",
        "outputId": "0078e33e-ea4a-409c-97b4-30ff51b851d1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_loser_5 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_5 = dGPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.55735327 0.54176063 0.56042378]. \t  0.9599610140567721 \t 0.9810564697651996\n",
            "init   \t [0.38204809 0.11909418 0.84599912]. \t  0.6913364116577386 \t 0.9810564697651996\n",
            "init   \t [0.73974161 0.93235756 0.85932801]. \t  0.9810564697651996 \t 0.9810564697651996\n",
            "init   \t [0.70810463 0.68096875 0.4086023 ]. \t  0.4017443293787304 \t 0.9810564697651996\n",
            "init   \t [0.77458462 0.9844762  0.96548486]. \t  0.4769494155906823 \t 0.9810564697651996\n",
            "1      \t [0.0655006  0.90661895 0.81035589]. \t  \u001b[92m1.403888408720423\u001b[0m \t 1.403888408720423\n",
            "2      \t [0.41215072 0.7779011  0.74763494]. \t  \u001b[92m2.1757126346478852\u001b[0m \t 2.1757126346478852\n",
            "3      \t [0.34717117 0.77095612 0.78484957]. \t  \u001b[92m2.3966577054831983\u001b[0m \t 2.3966577054831983\n",
            "4      \t [0.30370554 0.5420168  0.98229967]. \t  \u001b[92m2.396700843210034\u001b[0m \t 2.396700843210034\n",
            "5      \t [0.03830972 0.41319886 0.87948681]. \t  \u001b[92m3.0868801903159286\u001b[0m \t 3.0868801903159286\n",
            "6      \t [0.0550351  0.47493583 0.91186515]. \t  \u001b[92m3.247821453024615\u001b[0m \t 3.247821453024615\n",
            "7      \t [0.13007588 0.76889517 0.96291702]. \t  1.8354815792825336 \t 3.247821453024615\n",
            "8      \t [0.06020638 0.52263597 0.83656287]. \t  \u001b[92m3.7724345426814376\u001b[0m \t 3.7724345426814376\n",
            "9      \t [0.05709412 0.58527165 0.69772483]. \t  2.531288763638519 \t 3.7724345426814376\n",
            "10     \t [0.91025839 0.97028742 0.12792309]. \t  0.0018457322577184834 \t 3.7724345426814376\n",
            "11     \t [0.99573914 0.49153797 0.75591514]. \t  2.7894953486682232 \t 3.7724345426814376\n",
            "12     \t [0.10059082 0.55459117 0.835178  ]. \t  \u001b[92m3.8123132323574875\u001b[0m \t 3.8123132323574875\n",
            "13     \t [0.96087615 0.67807239 0.2789962 ]. \t  0.03903754399449888 \t 3.8123132323574875\n",
            "14     \t [0.85683623 0.07037409 0.99016484]. \t  0.20334698494229897 \t 3.8123132323574875\n",
            "15     \t [0.97447876 0.84968796 0.99975702]. \t  0.9204545784883199 \t 3.8123132323574875\n",
            "16     \t [0.61918291 0.63706631 0.36531518]. \t  0.31507380690837034 \t 3.8123132323574875\n",
            "17     \t [0.60892365 0.82768428 0.00560017]. \t  0.0008542513403068642 \t 3.8123132323574875\n",
            "18     \t [0.88794259 0.11985922 0.05976312]. \t  0.12240316278901528 \t 3.8123132323574875\n",
            "19     \t [0.18565789 0.41975985 0.79881607]. \t  3.104257557926961 \t 3.8123132323574875\n",
            "20     \t [0.54787183 0.252763   0.54415079]. \t  0.33307288848698713 \t 3.8123132323574875\n",
            "21     \t [0.20787217 0.61004851 0.74845814]. \t  3.0360388299352037 \t 3.8123132323574875\n",
            "22     \t [0.98303924 0.28190876 0.89179177]. \t  1.7480026944637124 \t 3.8123132323574875\n",
            "23     \t [0.61780318 0.02581508 0.05799669]. \t  0.20530916988886677 \t 3.8123132323574875\n",
            "24     \t [0.65433974 0.02262776 0.44688952]. \t  0.28211827697204606 \t 3.8123132323574875\n",
            "25     \t [0.02006712 0.54361265 0.85674407]. \t  3.8106114172417715 \t 3.8123132323574875\n",
            "26     \t [0.04408369 0.57798277 0.84008442]. \t  3.792111144405652 \t 3.8123132323574875\n",
            "27     \t [0.14501681 0.47075112 0.91461001]. \t  3.2110042352024024 \t 3.8123132323574875\n",
            "28     \t [0.03558969 0.66343753 0.93451965]. \t  2.8921565718998465 \t 3.8123132323574875\n",
            "29     \t [0.01404798 0.87419782 0.65725577]. \t  2.4792575908001058 \t 3.8123132323574875\n",
            "30     \t [0.18633038 0.99262283 0.31785779]. \t  0.4753520265355485 \t 3.8123132323574875\n",
            "31     \t [0.89852877 0.01203974 0.63453993]. \t  0.14481656909288962 \t 3.8123132323574875\n",
            "32     \t [0.97150247 0.69294377 0.22199652]. \t  0.019920135017441068 \t 3.8123132323574875\n",
            "33     \t [0.46085152 0.56645311 0.57553687]. \t  1.2863944466686195 \t 3.8123132323574875\n",
            "34     \t [0.82964158 0.57980175 0.14424181]. \t  0.041147105173632396 \t 3.8123132323574875\n",
            "35     \t [0.27448468 0.17200083 0.38396597]. \t  0.6423027740941983 \t 3.8123132323574875\n",
            "36     \t [0.26566642 0.00681334 0.42585266]. \t  0.40953511308472856 \t 3.8123132323574875\n",
            "37     \t [0.06013269 0.35026807 0.49586356]. \t  0.3954287956437817 \t 3.8123132323574875\n",
            "38     \t [0.65855735 0.7339546  0.17842301]. \t  0.027927969983747716 \t 3.8123132323574875\n",
            "39     \t [0.75833171 0.253738   0.0292391 ]. \t  0.09612694912065668 \t 3.8123132323574875\n",
            "40     \t [0.56494099 0.01288046 0.30065057]. \t  0.7736377060560666 \t 3.8123132323574875\n",
            "41     \t [0.44126691 0.35921091 0.76431648]. \t  2.3882885446221644 \t 3.8123132323574875\n",
            "42     \t [0.09772268 0.39730314 0.0571514 ]. \t  0.09736614631874568 \t 3.8123132323574875\n",
            "43     \t [0.04390447 0.99697316 0.73397787]. \t  1.2650016562313509 \t 3.8123132323574875\n",
            "44     \t [0.99299479 0.54594283 0.06288426]. \t  0.014164106367212341 \t 3.8123132323574875\n",
            "45     \t [0.83645947 0.11483328 0.79232853]. \t  0.671516469465977 \t 3.8123132323574875\n",
            "46     \t [0.25559256 0.4677096  0.20778315]. \t  0.26763763133381235 \t 3.8123132323574875\n",
            "47     \t [0.52134778 0.64412547 0.66476584]. \t  1.9387526401965522 \t 3.8123132323574875\n",
            "48     \t [0.97960237 0.58509312 0.39486473]. \t  0.0980438150330066 \t 3.8123132323574875\n",
            "49     \t [0.93425858 0.61751435 0.93636713]. \t  3.0007595366600857 \t 3.8123132323574875\n",
            "50     \t [0.93665032 0.83303117 0.3717566 ]. \t  0.14220193723144822 \t 3.8123132323574875\n",
            "51     \t [0.01043746 0.06645344 0.0582733 ]. \t  0.17874671309400159 \t 3.8123132323574875\n",
            "52     \t [0.42146344 0.43093162 0.21313343]. \t  0.34858766310202843 \t 3.8123132323574875\n",
            "53     \t [0.85740045 0.72358599 0.10595874]. \t  0.0067127424362161785 \t 3.8123132323574875\n",
            "54     \t [0.74536496 0.74030997 0.81142443]. \t  2.5236996121014146 \t 3.8123132323574875\n",
            "55     \t [0.48596649 0.05583247 0.71267398]. \t  0.34836328475177203 \t 3.8123132323574875\n",
            "56     \t [0.27943898 0.74737658 0.31328012]. \t  0.42639692017161174 \t 3.8123132323574875\n",
            "57     \t [0.7121061  0.70213367 0.23237757]. \t  0.05544100754439492 \t 3.8123132323574875\n",
            "58     \t [0.89629735 0.50531801 0.27715699]. \t  0.10830442188564005 \t 3.8123132323574875\n",
            "59     \t [0.81710177 0.67511305 0.7989285 ]. \t  2.909475046834837 \t 3.8123132323574875\n",
            "60     \t [0.27827065 0.70878759 0.22002966]. \t  0.10050040713970884 \t 3.8123132323574875\n",
            "61     \t [0.12217544 0.78496978 0.79026058]. \t  2.37095832378724 \t 3.8123132323574875\n",
            "62     \t [0.65576377 0.4536732  0.46632376]. \t  0.32528800321453955 \t 3.8123132323574875\n",
            "63     \t [0.78302288 0.13617894 0.70367363]. \t  0.5995147367720821 \t 3.8123132323574875\n",
            "64     \t [0.27565291 0.35679019 0.68211396]. \t  1.5721358806728192 \t 3.8123132323574875\n",
            "65     \t [0.50685866 0.05015652 0.57968208]. \t  0.1581913686140206 \t 3.8123132323574875\n",
            "66     \t [0.21613274 0.78748947 0.61979404]. \t  2.752520587780271 \t 3.8123132323574875\n",
            "67     \t [0.97683169 0.98537602 0.47034241]. \t  0.22879582575819743 \t 3.8123132323574875\n",
            "68     \t [0.8247913  0.00865746 0.77607339]. \t  0.2671791607087722 \t 3.8123132323574875\n",
            "69     \t [0.6154278  0.16102298 0.72380069]. \t  0.7868765266505973 \t 3.8123132323574875\n",
            "70     \t [0.44639332 0.52624246 0.94752527]. \t  2.9776268007255116 \t 3.8123132323574875\n",
            "71     \t [0.35265186 0.07195568 0.39129923]. \t  0.622943735926077 \t 3.8123132323574875\n",
            "72     \t [0.01915566 0.7514324  0.80350619]. \t  2.641516303394991 \t 3.8123132323574875\n",
            "73     \t [0.31166647 0.75193678 0.43966625]. \t  1.5656143444144268 \t 3.8123132323574875\n",
            "74     \t [0.057032   0.85634764 0.70514785]. \t  2.17220634986867 \t 3.8123132323574875\n",
            "75     \t [0.17449207 0.38170487 0.29543257]. \t  0.4687919094460895 \t 3.8123132323574875\n",
            "76     \t [0.18262366 0.40234101 0.78174162]. \t  2.853310454318601 \t 3.8123132323574875\n",
            "77     \t [0.23326724 0.12923415 0.21844802]. \t  0.8800126024126476 \t 3.8123132323574875\n",
            "78     \t [0.76291281 0.1939433  0.15790554]. \t  0.4132222812659086 \t 3.8123132323574875\n",
            "79     \t [0.09675512 0.75527026 0.79408307]. \t  2.6065132144451044 \t 3.8123132323574875\n",
            "80     \t [0.32685191 0.0862388  0.628197  ]. \t  0.2658594374175428 \t 3.8123132323574875\n",
            "81     \t [0.26988867 0.88735757 0.70824977]. \t  1.9302015782523108 \t 3.8123132323574875\n",
            "82     \t [0.84299487 0.1543878  0.18070156]. \t  0.4012743622864063 \t 3.8123132323574875\n",
            "83     \t [0.41130472 0.07171761 0.33969365]. \t  0.8345360731124587 \t 3.8123132323574875\n",
            "84     \t [7.35487395e-04 7.71143033e-01 7.78163031e-01]. \t  2.4185376828203515 \t 3.8123132323574875\n",
            "85     \t [0.27421342 0.3677449  0.62104297]. \t  1.042391069581968 \t 3.8123132323574875\n",
            "86     \t [0.02541217 0.25237601 0.45267658]. \t  0.29580436684080935 \t 3.8123132323574875\n",
            "87     \t [0.44317316 0.12260709 0.52398733]. \t  0.22575510537096177 \t 3.8123132323574875\n",
            "88     \t [0.57293859 0.99880182 0.4018766 ]. \t  0.6680010837087829 \t 3.8123132323574875\n",
            "89     \t [0.8022128  0.10624647 0.516911  ]. \t  0.15447209300219936 \t 3.8123132323574875\n",
            "90     \t [0.29358989 0.02299719 0.73290056]. \t  0.2828304241174419 \t 3.8123132323574875\n",
            "91     \t [0.54505805 0.52661682 0.47490715]. \t  0.5606793615622767 \t 3.8123132323574875\n",
            "92     \t [0.77323979 0.16021315 0.81059028]. \t  0.9486529552671926 \t 3.8123132323574875\n",
            "93     \t [0.45302664 0.21908675 0.15855595]. \t  0.618922530291222 \t 3.8123132323574875\n",
            "94     \t [0.82084058 0.40124898 0.96684758]. \t  2.0325833187159366 \t 3.8123132323574875\n",
            "95     \t [0.09455294 0.52870413 0.13348204]. \t  0.0900712093873404 \t 3.8123132323574875\n",
            "96     \t [0.44929156 0.89821783 0.27772   ]. \t  0.21315354397159983 \t 3.8123132323574875\n",
            "97     \t [0.28590341 0.55800049 0.51488452]. \t  1.1719963153531516 \t 3.8123132323574875\n",
            "98     \t [0.11525554 0.82021185 0.11016824]. \t  0.010565660616481393 \t 3.8123132323574875\n",
            "99     \t [0.13037669 0.19765145 0.80752148]. \t  1.2195795915290009 \t 3.8123132323574875\n",
            "100    \t [0.35073261 0.92595667 0.04860803]. \t  0.0014692202765782152 \t 3.8123132323574875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh_Amb8TtdgO",
        "outputId": "ca1a1bed-1bbe-44ba-f1bf-e89cd2f13fa1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_loser_6 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_6 = dGPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.89286015 0.33197981 0.82122912]. \t  2.3879127825536575 \t 2.5106636917702634\n",
            "init   \t [0.04169663 0.10765668 0.59505206]. \t  0.2321934630074273 \t 2.5106636917702634\n",
            "init   \t [0.52981736 0.41880743 0.33540785]. \t  0.37989557867087576 \t 2.5106636917702634\n",
            "init   \t [0.62251943 0.43814143 0.73588211]. \t  2.5106636917702634 \t 2.5106636917702634\n",
            "init   \t [0.51803641 0.5788586  0.6453551 ]. \t  1.7289203948034264 \t 2.5106636917702634\n",
            "1      \t [0.62339102 0.39276721 0.97743955]. \t  1.8557550530508597 \t 2.5106636917702634\n",
            "2      \t [0.81081535 0.27720993 0.64760969]. \t  0.8549986775341755 \t 2.5106636917702634\n",
            "3      \t [0.97311854 0.60304551 0.92068359]. \t  \u001b[92m3.2276797271364055\u001b[0m \t 3.2276797271364055\n",
            "4      \t [0.89264006 0.58864963 0.95652948]. \t  2.776092518207796 \t 3.2276797271364055\n",
            "5      \t [0.96507948 0.60085005 0.82529255]. \t  \u001b[92m3.479322232477214\u001b[0m \t 3.479322232477214\n",
            "6      \t [0.95842992 0.48714277 0.81520669]. \t  3.4274222523101296 \t 3.479322232477214\n",
            "7      \t [0.95032843 0.60164973 0.75457865]. \t  2.678691870274754 \t 3.479322232477214\n",
            "8      \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.479322232477214\n",
            "9      \t [0.01270824 0.5371693  0.06605772]. \t  0.0354315091022392 \t 3.479322232477214\n",
            "10     \t [0.20140086 0.25986668 0.00643016]. \t  0.09731674626066539 \t 3.479322232477214\n",
            "11     \t [0.9954704  0.56038921 0.8728146 ]. \t  \u001b[92m3.6483342701470103\u001b[0m \t 3.6483342701470103\n",
            "12     \t [0.08776956 0.94212277 0.01891421]. \t  0.0006566083725651407 \t 3.6483342701470103\n",
            "13     \t [0.4346958  0.60889655 0.8201429 ]. \t  3.631042399892129 \t 3.6483342701470103\n",
            "14     \t [0.01161386 0.66515041 0.99551219]. \t  1.9498416196162505 \t 3.6483342701470103\n",
            "15     \t [0.33254032 0.89414231 0.98713051]. \t  0.8152211979376042 \t 3.6483342701470103\n",
            "16     \t [0.22420658 0.65264203 0.9182928 ]. \t  3.176475020832948 \t 3.6483342701470103\n",
            "17     \t [0.38995695 0.50459032 0.97495208]. \t  2.4537818811544887 \t 3.6483342701470103\n",
            "18     \t [0.89731428 0.0644819  0.04485822]. \t  0.09540218126323545 \t 3.6483342701470103\n",
            "19     \t [0.5430287  0.62248852 0.87089541]. \t  3.6481824403125525 \t 3.6483342701470103\n",
            "20     \t [0.02245446 0.71257964 0.70832644]. \t  2.542735102304769 \t 3.6483342701470103\n",
            "21     \t [0.84916036 0.43499921 0.05964426]. \t  0.050000621163526424 \t 3.6483342701470103\n",
            "22     \t [0.72328122 0.78476196 0.91791194]. \t  2.0917540572107587 \t 3.6483342701470103\n",
            "23     \t [0.50671568 0.59326383 0.87247359]. \t  \u001b[92m3.7574239741369033\u001b[0m \t 3.7574239741369033\n",
            "24     \t [0.80771617 0.77253684 0.56420031]. \t  0.8074089229060448 \t 3.7574239741369033\n",
            "25     \t [0.82280098 0.98264258 0.03503644]. \t  0.00023447364217251603 \t 3.7574239741369033\n",
            "26     \t [0.36120819 0.01883246 0.12314037]. \t  0.48675579172091765 \t 3.7574239741369033\n",
            "27     \t [0.61004954 0.97795743 0.5380338 ]. \t  1.2796657235618687 \t 3.7574239741369033\n",
            "28     \t [0.50645476 0.26659896 0.96461242]. \t  1.1510964734131832 \t 3.7574239741369033\n",
            "29     \t [0.00523641 0.84890647 0.76701844]. \t  1.8904386191124445 \t 3.7574239741369033\n",
            "30     \t [0.07455069 0.71285656 0.57962799]. \t  2.5839320858154777 \t 3.7574239741369033\n",
            "31     \t [0.83401196 0.42203083 0.26259397]. \t  0.2125427448756169 \t 3.7574239741369033\n",
            "32     \t [0.00550112 0.79531694 0.3026345 ]. \t  0.4132763468583321 \t 3.7574239741369033\n",
            "33     \t [0.33510451 0.26649245 0.92074688]. \t  1.4902139984299103 \t 3.7574239741369033\n",
            "34     \t [0.69362147 0.34044285 0.45542876]. \t  0.2582487569586175 \t 3.7574239741369033\n",
            "35     \t [0.27408507 0.37107772 0.1495041 ]. \t  0.33823696322386365 \t 3.7574239741369033\n",
            "36     \t [0.14167772 0.6583676  0.23549375]. \t  0.13307732649588877 \t 3.7574239741369033\n",
            "37     \t [0.36784136 0.65945815 0.86312928]. \t  3.4849121667963368 \t 3.7574239741369033\n",
            "38     \t [0.50130668 0.46810664 0.92565854]. \t  3.071990928694588 \t 3.7574239741369033\n",
            "39     \t [0.11506585 0.08818824 0.01786167]. \t  0.1264166391740304 \t 3.7574239741369033\n",
            "40     \t [0.61125555 0.20673819 0.86929837]. \t  1.2342396193127003 \t 3.7574239741369033\n",
            "41     \t [0.11552913 0.937178   0.91319381]. \t  0.9229498004302739 \t 3.7574239741369033\n",
            "42     \t [0.73894287 0.33413814 0.6103939 ]. \t  0.7455446737025775 \t 3.7574239741369033\n",
            "43     \t [0.58721719 0.38510632 0.8374942 ]. \t  2.96017619594543 \t 3.7574239741369033\n",
            "44     \t [0.2924251  0.24847362 0.29458142]. \t  0.8162192034311126 \t 3.7574239741369033\n",
            "45     \t [0.9399308  0.69448975 0.89984114]. \t  2.934326827015603 \t 3.7574239741369033\n",
            "46     \t [0.49109484 0.33504597 0.0306225 ]. \t  0.11075140000922437 \t 3.7574239741369033\n",
            "47     \t [0.95946823 0.76273733 0.74217599]. \t  1.6013158015827202 \t 3.7574239741369033\n",
            "48     \t [0.00768901 0.57624884 0.45984971]. \t  1.006739146423282 \t 3.7574239741369033\n",
            "49     \t [0.7588967  0.6338851  0.46007792]. \t  0.4302286953977035 \t 3.7574239741369033\n",
            "50     \t [0.43681923 0.26771217 0.525546  ]. \t  0.3357713031640264 \t 3.7574239741369033\n",
            "51     \t [0.84650032 0.32968555 0.67756352]. \t  1.3030194973795366 \t 3.7574239741369033\n",
            "52     \t [0.29116669 0.1959375  0.57236862]. \t  0.33835299363392896 \t 3.7574239741369033\n",
            "53     \t [0.49086338 0.2402136  0.66310662]. \t  0.8580888362708491 \t 3.7574239741369033\n",
            "54     \t [0.67174643 0.13687066 0.29850672]. \t  0.7358259286026887 \t 3.7574239741369033\n",
            "55     \t [0.90824711 0.51686926 0.94068471]. \t  2.9773967891772 \t 3.7574239741369033\n",
            "56     \t [0.96313273 0.38871968 0.58126843]. \t  0.56783893931115 \t 3.7574239741369033\n",
            "57     \t [0.47678022 0.58904368 0.60675061]. \t  1.527985140761224 \t 3.7574239741369033\n",
            "58     \t [0.61939634 0.49537322 0.21904615]. \t  0.19585169709126193 \t 3.7574239741369033\n",
            "59     \t [0.89125536 0.52848028 0.37504456]. \t  0.12161102080979641 \t 3.7574239741369033\n",
            "60     \t [0.2492909  0.99861842 0.46834362]. \t  1.9369879716347638 \t 3.7574239741369033\n",
            "61     \t [0.89311458 0.0260461  0.41271687]. \t  0.21862146592906784 \t 3.7574239741369033\n",
            "62     \t [0.75365534 0.64361142 0.56627093]. \t  0.8495271886852758 \t 3.7574239741369033\n",
            "63     \t [0.10353124 0.02964342 0.15018766]. \t  0.4970848309262672 \t 3.7574239741369033\n",
            "64     \t [0.38652703 0.09628612 0.01688402]. \t  0.15160735034832432 \t 3.7574239741369033\n",
            "65     \t [0.90753459 0.45836172 0.5257814 ]. \t  0.3378386372294143 \t 3.7574239741369033\n",
            "66     \t [0.97786691 0.23969996 0.30313812]. \t  0.2737430610896056 \t 3.7574239741369033\n",
            "67     \t [0.2985376  0.45515157 0.72666153]. \t  2.5519283059711437 \t 3.7574239741369033\n",
            "68     \t [0.71962825 0.37640665 0.92594521]. \t  2.3788782644785234 \t 3.7574239741369033\n",
            "69     \t [0.91095124 0.26026786 0.08100662]. \t  0.11910272542970807 \t 3.7574239741369033\n",
            "70     \t [0.2007796  0.40410126 0.04414278]. \t  0.09057280874521183 \t 3.7574239741369033\n",
            "71     \t [0.98066823 0.17618395 0.30887276]. \t  0.29940407941322894 \t 3.7574239741369033\n",
            "72     \t [0.26486167 0.03322341 0.3011884 ]. \t  0.8724455649378886 \t 3.7574239741369033\n",
            "73     \t [0.19341254 0.9115757  0.8193853 ]. \t  1.3477910671526903 \t 3.7574239741369033\n",
            "74     \t [0.85383639 0.303693   0.58484458]. \t  0.4983868149187136 \t 3.7574239741369033\n",
            "75     \t [0.11261631 0.64295778 0.35291107]. \t  0.5654453165628954 \t 3.7574239741369033\n",
            "76     \t [0.25255531 0.58296163 0.5355288 ]. \t  1.4636644975046686 \t 3.7574239741369033\n",
            "77     \t [0.69044902 0.1928956  0.86895197]. \t  1.1234012728940739 \t 3.7574239741369033\n",
            "78     \t [0.06808589 0.44438348 0.31510407]. \t  0.33025696989363273 \t 3.7574239741369033\n",
            "79     \t [0.1535016  0.47575467 0.38377442]. \t  0.4277339308242742 \t 3.7574239741369033\n",
            "80     \t [0.88950651 0.15575891 0.5753877 ]. \t  0.23678865995367532 \t 3.7574239741369033\n",
            "81     \t [0.60563334 0.94996264 0.53201211]. \t  1.357208075848746 \t 3.7574239741369033\n",
            "82     \t [0.03640684 0.90685607 0.13933935]. \t  0.017359262999842685 \t 3.7574239741369033\n",
            "83     \t [0.83622549 0.00479024 0.15238126]. \t  0.30811922728605484 \t 3.7574239741369033\n",
            "84     \t [0.03047936 0.45130896 0.09831493]. \t  0.0994418319705768 \t 3.7574239741369033\n",
            "85     \t [0.56096791 0.42054039 0.53609475]. \t  0.5375442750723753 \t 3.7574239741369033\n",
            "86     \t [0.69089327 0.41434753 0.52576339]. \t  0.41222705943601856 \t 3.7574239741369033\n",
            "87     \t [0.33303682 0.85963986 0.88246611]. \t  1.6270249322213164 \t 3.7574239741369033\n",
            "88     \t [0.76694449 0.44906852 0.62817908]. \t  1.1229951985849844 \t 3.7574239741369033\n",
            "89     \t [0.38506973 0.2254537  0.89158306]. \t  1.3124017684339946 \t 3.7574239741369033\n",
            "90     \t [0.94760925 0.81277586 0.24814905]. \t  0.02384324621445827 \t 3.7574239741369033\n",
            "91     \t [0.22321952 0.85183336 0.71042249]. \t  2.116278041457136 \t 3.7574239741369033\n",
            "92     \t [0.63232264 0.79212972 0.83959709]. \t  2.2158432287888115 \t 3.7574239741369033\n",
            "93     \t [0.89311012 0.27852264 0.10452753]. \t  0.15260228430094222 \t 3.7574239741369033\n",
            "94     \t [0.65225194 0.56959118 0.85237454]. \t  \u001b[92m3.7923072368609834\u001b[0m \t 3.7923072368609834\n",
            "95     \t [0.45988746 0.15648535 0.31912992]. \t  0.889225446032996 \t 3.7923072368609834\n",
            "96     \t [0.91897039 0.23558463 0.23921164]. \t  0.34277234372304527 \t 3.7923072368609834\n",
            "97     \t [0.84404778 0.5475815  0.87225978]. \t  3.714655484967224 \t 3.7923072368609834\n",
            "98     \t [0.22581407 0.36439774 0.63953126]. \t  1.194698309876963 \t 3.7923072368609834\n",
            "99     \t [0.36129501 0.29655172 0.46130896]. \t  0.3620060373190372 \t 3.7923072368609834\n",
            "100    \t [0.21910047 0.55932934 0.67552251]. \t  2.2680994506603955 \t 3.7923072368609834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNmoDTuUtdi3",
        "outputId": "df14b884-2d66-442d-d49d-f98041242eee"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_loser_7 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_7 = dGPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.15266373 0.30235661 0.06203641]. \t  0.1742288077468587 \t 1.6237282255098657\n",
            "init   \t [0.45986034 0.83525338 0.92699705]. \t  1.6237282255098657 \t 1.6237282255098657\n",
            "init   \t [0.72698898 0.76849622 0.26920507]. \t  0.08405715787489784 \t 1.6237282255098657\n",
            "init   \t [0.64402929 0.09337326 0.07968589]. \t  0.2756481743251193 \t 1.6237282255098657\n",
            "init   \t [0.58961375 0.34334054 0.98887615]. \t  1.4020548914621052 \t 1.6237282255098657\n",
            "1      \t [0.32658881 0.90655956 0.99955954]. \t  0.6793796294845356 \t 1.6237282255098657\n",
            "2      \t [0.97122775 0.93199907 0.87171651]. \t  0.9308657054768225 \t 1.6237282255098657\n",
            "3      \t [0.57909004 0.58214012 0.79783263]. \t  \u001b[92m3.478089696581643\u001b[0m \t 3.478089696581643\n",
            "4      \t [0.49507021 0.39113672 0.52467305]. \t  0.478096743295257 \t 3.478089696581643\n",
            "5      \t [0.96188519 0.49079304 0.94964007]. \t  2.739322132683216 \t 3.478089696581643\n",
            "6      \t [0.90946938 0.60810834 0.78435192]. \t  3.0774531590511494 \t 3.478089696581643\n",
            "7      \t [0.06261914 0.66505939 0.93875608]. \t  2.8328149285258104 \t 3.478089696581643\n",
            "8      \t [0.63975981 0.37291359 0.00880596]. \t  0.05617173979970511 \t 3.478089696581643\n",
            "9      \t [0.76047284 0.60730235 0.87877328]. \t  \u001b[92m3.623712367146717\u001b[0m \t 3.623712367146717\n",
            "10     \t [0.6876877  0.55744025 0.93478593]. \t  3.1829751880829815 \t 3.623712367146717\n",
            "11     \t [0.08676052 0.79807152 0.87850186]. \t  2.2208290897125167 \t 3.623712367146717\n",
            "12     \t [0.28916277 0.04583535 0.00261902]. \t  0.11401832354434507 \t 3.623712367146717\n",
            "13     \t [0.71667679 0.57664501 0.81395165]. \t  3.5779368756411625 \t 3.623712367146717\n",
            "14     \t [0.11508561 0.71452061 0.02534668]. \t  0.004527603857494236 \t 3.623712367146717\n",
            "15     \t [0.7138761  0.68377075 0.77379694]. \t  2.655945564466352 \t 3.623712367146717\n",
            "16     \t [0.62595126 0.94264969 0.00840659]. \t  0.00028614683822522104 \t 3.623712367146717\n",
            "17     \t [0.99588291 0.81961183 0.15742965]. \t  0.003958213795357282 \t 3.623712367146717\n",
            "18     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.623712367146717\n",
            "19     \t [0.87761517 0.03556698 0.76435623]. \t  0.3351948214917791 \t 3.623712367146717\n",
            "20     \t [0.97857549 0.00446178 0.16841397]. \t  0.2154343354374641 \t 3.623712367146717\n",
            "21     \t [0.81221252 0.525542   0.85463545]. \t  \u001b[92m3.7301762932112674\u001b[0m \t 3.7301762932112674\n",
            "22     \t [0.900185   0.75296151 0.1168608 ]. \t  0.005072749512409333 \t 3.7301762932112674\n",
            "23     \t [0.81353052 0.51280105 0.82744971]. \t  3.6314788950239585 \t 3.7301762932112674\n",
            "24     \t [0.96913426 0.69361638 0.55129864]. \t  0.45446883683488787 \t 3.7301762932112674\n",
            "25     \t [0.45611335 0.03406024 0.61887374]. \t  0.1704663937816772 \t 3.7301762932112674\n",
            "26     \t [0.59223331 0.04267828 0.1338271 ]. \t  0.47744590290110217 \t 3.7301762932112674\n",
            "27     \t [0.83943545 0.77914392 0.08044774]. \t  0.0029026804888624066 \t 3.7301762932112674\n",
            "28     \t [0.20846012 0.53348377 0.51992205]. \t  1.1202841961222842 \t 3.7301762932112674\n",
            "29     \t [0.67681719 0.52661603 0.85870288]. \t  \u001b[92m3.7745252416454917\u001b[0m \t 3.7745252416454917\n",
            "30     \t [0.12509163 0.87757482 0.5510732 ]. \t  3.0681829219005508 \t 3.7745252416454917\n",
            "31     \t [0.14369607 0.95138149 0.31002503]. \t  0.4675814613706676 \t 3.7745252416454917\n",
            "32     \t [0.03528407 0.40941311 0.95717746]. \t  2.236590588082838 \t 3.7745252416454917\n",
            "33     \t [0.44280206 0.63711349 0.86091196]. \t  3.611710810406915 \t 3.7745252416454917\n",
            "34     \t [0.26180585 0.73568205 0.73537597]. \t  2.5128286579035257 \t 3.7745252416454917\n",
            "35     \t [0.00487776 0.79900419 0.58464229]. \t  2.8879111571102887 \t 3.7745252416454917\n",
            "36     \t [0.62391776 0.3903563  0.68428808]. \t  1.6717627713433323 \t 3.7745252416454917\n",
            "37     \t [0.22203301 0.58076122 0.99938986]. \t  2.090782773049574 \t 3.7745252416454917\n",
            "38     \t [0.0497901  0.70348162 0.40504456]. \t  1.1607301643572914 \t 3.7745252416454917\n",
            "39     \t [0.08902865 0.97575397 0.57315788]. \t  2.714889081910986 \t 3.7745252416454917\n",
            "40     \t [0.71455558 0.69017883 0.69063379]. \t  1.73744604695138 \t 3.7745252416454917\n",
            "41     \t [0.0498949  0.78122685 0.55605748]. \t  2.879384773660573 \t 3.7745252416454917\n",
            "42     \t [0.45781108 0.26174588 0.94199186]. \t  1.2974802200354303 \t 3.7745252416454917\n",
            "43     \t [0.80913553 0.35598418 0.04574654]. \t  0.07245009830167098 \t 3.7745252416454917\n",
            "44     \t [0.39885319 0.30427766 0.15303229]. \t  0.4753994581941886 \t 3.7745252416454917\n",
            "45     \t [0.83497665 0.25408047 0.74194063]. \t  1.4079264679802885 \t 3.7745252416454917\n",
            "46     \t [0.05275221 0.73612313 0.71259125]. \t  2.516696422081942 \t 3.7745252416454917\n",
            "47     \t [0.03962876 0.04670778 0.97027546]. \t  0.19387205368595167 \t 3.7745252416454917\n",
            "48     \t [0.86384956 0.12288378 0.74114512]. \t  0.6383704123542232 \t 3.7745252416454917\n",
            "49     \t [0.79372708 0.35345592 0.53009712]. \t  0.32798013380319835 \t 3.7745252416454917\n",
            "50     \t [0.97791284 0.38396962 0.41962348]. \t  0.1246402598256812 \t 3.7745252416454917\n",
            "51     \t [0.64270236 0.75777217 0.68904066]. \t  1.6447147713498858 \t 3.7745252416454917\n",
            "52     \t [0.59139013 0.74227825 0.5157312 ]. \t  1.2993767437115655 \t 3.7745252416454917\n",
            "53     \t [0.0323653 0.3292039 0.1426402]. \t  0.2856535148959515 \t 3.7745252416454917\n",
            "54     \t [0.14383193 0.24141099 0.57065719]. \t  0.40695734141747664 \t 3.7745252416454917\n",
            "55     \t [0.89946978 0.08606762 0.85056182]. \t  0.5104618044370799 \t 3.7745252416454917\n",
            "56     \t [0.51976017 0.22889251 0.35232833]. \t  0.6751039301685013 \t 3.7745252416454917\n",
            "57     \t [0.34184268 0.52468794 0.68221237]. \t  2.1899374017129496 \t 3.7745252416454917\n",
            "58     \t [0.8893635  0.6882076  0.46426269]. \t  0.3181762541715076 \t 3.7745252416454917\n",
            "59     \t [0.24686786 0.27282795 0.58364829]. \t  0.5249811572852917 \t 3.7745252416454917\n",
            "60     \t [0.70830618 0.04660946 0.32106337]. \t  0.6183033089960998 \t 3.7745252416454917\n",
            "61     \t [0.65447025 0.27748361 0.42481929]. \t  0.33403303258545897 \t 3.7745252416454917\n",
            "62     \t [0.37683677 0.11525415 0.83630861]. \t  0.6800501825362579 \t 3.7745252416454917\n",
            "63     \t [0.6491765  0.27384865 0.4162992 ]. \t  0.3572171488569962 \t 3.7745252416454917\n",
            "64     \t [0.56966596 0.46622878 0.01058221]. \t  0.036280012296470564 \t 3.7745252416454917\n",
            "65     \t [0.45698753 0.27685337 0.04188856]. \t  0.16480651689762704 \t 3.7745252416454917\n",
            "66     \t [0.78088958 0.25464311 0.24869187]. \t  0.49327180578225444 \t 3.7745252416454917\n",
            "67     \t [0.61253894 0.36137669 0.63838783]. \t  1.0866471780065128 \t 3.7745252416454917\n",
            "68     \t [0.73972948 0.36316492 0.22201953]. \t  0.3420994600975986 \t 3.7745252416454917\n",
            "69     \t [0.598202   0.82681672 0.4747132 ]. \t  1.208649785030718 \t 3.7745252416454917\n",
            "70     \t [0.46091194 0.43140288 0.24909247]. \t  0.377258888183895 \t 3.7745252416454917\n",
            "71     \t [0.30980585 0.49272807 0.36154795]. \t  0.3955171481806316 \t 3.7745252416454917\n",
            "72     \t [0.96726563 0.89399728 0.96826019]. \t  0.8901370600589371 \t 3.7745252416454917\n",
            "73     \t [0.94643135 0.6245116  0.67643341]. \t  1.505106316787355 \t 3.7745252416454917\n",
            "74     \t [0.60589302 0.39114068 0.79202229]. \t  2.8228352062288518 \t 3.7745252416454917\n",
            "75     \t [0.82920723 0.53231563 0.33423074]. \t  0.13146137802733474 \t 3.7745252416454917\n",
            "76     \t [0.40343961 0.54806134 0.28058575]. \t  0.23948447099976833 \t 3.7745252416454917\n",
            "77     \t [0.69097311 0.11912294 0.61050187]. \t  0.27857093327605525 \t 3.7745252416454917\n",
            "78     \t [0.25755065 0.94546291 0.61372001]. \t  2.5138857218938946 \t 3.7745252416454917\n",
            "79     \t [0.09052689 0.51850689 0.63509928]. \t  1.7795558465731527 \t 3.7745252416454917\n",
            "80     \t [0.08479463 0.43788564 0.85789907]. \t  3.3742250385770323 \t 3.7745252416454917\n",
            "81     \t [0.70973134 0.52561147 0.93043662]. \t  3.200528830327633 \t 3.7745252416454917\n",
            "82     \t [0.07887098 0.62444953 0.75322688]. \t  3.047981909596093 \t 3.7745252416454917\n",
            "83     \t [0.43462338 0.85841275 0.40296966]. \t  1.097896948788411 \t 3.7745252416454917\n",
            "84     \t [0.07294121 0.01268299 0.75749395]. \t  0.27074762988187523 \t 3.7745252416454917\n",
            "85     \t [0.44020526 0.37645287 0.00968987]. \t  0.06863582851943462 \t 3.7745252416454917\n",
            "86     \t [0.96569043 0.0641876  0.43360957]. \t  0.15547161313291824 \t 3.7745252416454917\n",
            "87     \t [0.9198246  0.75940701 0.3705377 ]. \t  0.14052092081423212 \t 3.7745252416454917\n",
            "88     \t [0.22721022 0.26302799 0.52202655]. \t  0.3376790780837995 \t 3.7745252416454917\n",
            "89     \t [0.49843932 0.33475206 0.39853827]. \t  0.4193223537377658 \t 3.7745252416454917\n",
            "90     \t [0.88988416 0.29059567 0.72660669]. \t  1.5339975868716995 \t 3.7745252416454917\n",
            "91     \t [0.72225574 0.33315413 0.03442952]. \t  0.08471335964138589 \t 3.7745252416454917\n",
            "92     \t [0.43095451 0.12771229 0.9532665 ]. \t  0.46890119473138564 \t 3.7745252416454917\n",
            "93     \t [0.89322154 0.53536312 0.32746133]. \t  0.10282724257581978 \t 3.7745252416454917\n",
            "94     \t [0.77737971 0.27780371 0.64434659]. \t  0.8377082090623511 \t 3.7745252416454917\n",
            "95     \t [0.35604936 0.29752458 0.55250137]. \t  0.44763709345367153 \t 3.7745252416454917\n",
            "96     \t [0.30570847 0.99814714 0.78796821]. \t  0.8833022204035772 \t 3.7745252416454917\n",
            "97     \t [0.00304443 0.23869801 0.80155107]. \t  1.5294429905123614 \t 3.7745252416454917\n",
            "98     \t [0.23894336 0.95513749 0.52767956]. \t  2.6336435430210194 \t 3.7745252416454917\n",
            "99     \t [0.96334931 0.53329215 0.96711868]. \t  2.5687415955445196 \t 3.7745252416454917\n",
            "100    \t [0.07733048 0.16211484 0.24032112]. \t  0.7439261134149052 \t 3.7745252416454917\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_21yVprtdlu",
        "outputId": "54b39f2c-c13b-4f1c-d77d-26d31e03a4c2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_loser_8 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_8 = dGPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.10949987 0.04221501 0.59969258]. \t  0.15732379460832624 \t 0.8830091449513892\n",
            "init   \t [0.29987071 0.79661178 0.36619613]. \t  0.8830091449513892 \t 0.8830091449513892\n",
            "init   \t [0.88060834 0.29784163 0.32910326]. \t  0.2992873424905509 \t 0.8830091449513892\n",
            "init   \t [0.56871692 0.74392742 0.05385289]. \t  0.0051668955191694075 \t 0.8830091449513892\n",
            "init   \t [0.60207437 0.42931858 0.20547361]. \t  0.2908249205255991 \t 0.8830091449513892\n",
            "1      \t [0.19816249 0.92572195 0.98825083]. \t  0.653164991364588 \t 0.8830091449513892\n",
            "2      \t [0.03762084 0.88738442 0.32955494]. \t  0.6464733859302134 \t 0.8830091449513892\n",
            "3      \t [0.59298739 0.68240117 0.69116296]. \t  \u001b[92m1.9678379589841422\u001b[0m \t 1.9678379589841422\n",
            "4      \t [0.99676486 0.73834616 0.85474135]. \t  \u001b[92m2.622597831543382\u001b[0m \t 2.622597831543382\n",
            "5      \t [0.93909608 0.58044829 0.98333517]. \t  2.313423464876202 \t 2.622597831543382\n",
            "6      \t [0.99228465 0.88181182 0.94936817]. \t  1.067160942216205 \t 2.622597831543382\n",
            "7      \t [0.99710209 0.50230383 0.74747112]. \t  \u001b[92m2.685143360380625\u001b[0m \t 2.685143360380625\n",
            "8      \t [0.9896991  0.65501731 0.64538838]. \t  1.0539030471951414 \t 2.685143360380625\n",
            "9      \t [0.7541667  0.03842059 0.03250692]. \t  0.11522480748951529 \t 2.685143360380625\n",
            "10     \t [0.07190656 0.53540888 0.79364133]. \t  \u001b[92m3.5327551500263175\u001b[0m \t 3.5327551500263175\n",
            "11     \t [0.05636162 0.68631151 0.69982546]. \t  2.571862442260311 \t 3.5327551500263175\n",
            "12     \t [0.01897474 0.65584092 0.8751561 ]. \t  3.4459419056992657 \t 3.5327551500263175\n",
            "13     \t [0.2026131  0.90973368 0.02599246]. \t  0.0009568874318973229 \t 3.5327551500263175\n",
            "14     \t [0.02015943 0.17546544 0.86007435]. \t  1.0094731274153572 \t 3.5327551500263175\n",
            "15     \t [0.19305313 0.4204086  0.91237006]. \t  2.90434651604119 \t 3.5327551500263175\n",
            "16     \t [0.01195417 0.52698999 0.94314404]. \t  3.0097456026265337 \t 3.5327551500263175\n",
            "17     \t [0.25596225 0.39703781 0.71712214]. \t  2.163721458320439 \t 3.5327551500263175\n",
            "18     \t [0.25710802 0.65710627 0.8981127 ]. \t  3.3435551924240294 \t 3.5327551500263175\n",
            "19     \t [0.16048547 0.50023439 0.88621337]. \t  \u001b[92m3.6214125818552776\u001b[0m \t 3.6214125818552776\n",
            "20     \t [0.1447954  0.99892044 0.43056541]. \t  1.609250144807879 \t 3.6214125818552776\n",
            "21     \t [0.11841363 0.58660831 0.83267808]. \t  \u001b[92m3.772577454290076\u001b[0m \t 3.772577454290076\n",
            "22     \t [0.97987785 0.13714279 0.00248463]. \t  0.039646571425264975 \t 3.772577454290076\n",
            "23     \t [0.92705413 0.12267879 0.87792272]. \t  0.6413473884501208 \t 3.772577454290076\n",
            "24     \t [0.54587708 0.31433642 0.57322102]. \t  0.5247106459907515 \t 3.772577454290076\n",
            "25     \t [0.65244562 0.21314933 0.18035824]. \t  0.5712228373905991 \t 3.772577454290076\n",
            "26     \t [0.04984912 0.5167762  0.42371492]. \t  0.6001261842172103 \t 3.772577454290076\n",
            "27     \t [0.49843812 0.60544155 0.73993733]. \t  2.788419670954484 \t 3.772577454290076\n",
            "28     \t [0.00250769 0.18540452 0.04799546]. \t  0.15072785648040404 \t 3.772577454290076\n",
            "29     \t [0.14683982 0.04030608 0.72601804]. \t  0.3195768296436863 \t 3.772577454290076\n",
            "30     \t [0.6154916  0.22834525 0.33458037]. \t  0.649724837300166 \t 3.772577454290076\n",
            "31     \t [0.94447738 0.03541163 0.59211034]. \t  0.12339061425489205 \t 3.772577454290076\n",
            "32     \t [0.80518828 0.21820939 0.23047591]. \t  0.4900798351740443 \t 3.772577454290076\n",
            "33     \t [0.49180021 0.67289655 0.09640609]. \t  0.020477956936715228 \t 3.772577454290076\n",
            "34     \t [0.18869189 0.24753615 0.03695648]. \t  0.15575914337109642 \t 3.772577454290076\n",
            "35     \t [0.2574242  0.34725895 0.79140007]. \t  2.4829744875783115 \t 3.772577454290076\n",
            "36     \t [0.56853046 0.35320536 0.40936881]. \t  0.3551657529796507 \t 3.772577454290076\n",
            "37     \t [0.45222453 0.34502697 0.26254166]. \t  0.5921939442113907 \t 3.772577454290076\n",
            "38     \t [0.37352425 0.49742226 0.6706283 ]. \t  1.9661671443461928 \t 3.772577454290076\n",
            "39     \t [0.33441288 0.55681958 0.67261602]. \t  2.1628781305061278 \t 3.772577454290076\n",
            "40     \t [0.8265361  0.4549186  0.12056968]. \t  0.08966259473256484 \t 3.772577454290076\n",
            "41     \t [0.01969536 0.35498542 0.88037469]. \t  2.548617770891581 \t 3.772577454290076\n",
            "42     \t [0.37671893 0.37852214 0.4858566 ]. \t  0.41712957272792933 \t 3.772577454290076\n",
            "43     \t [0.94598866 0.17874027 0.78746887]. \t  1.0359074807983142 \t 3.772577454290076\n",
            "44     \t [0.49561461 0.44173046 0.18714477]. \t  0.27897904173413063 \t 3.772577454290076\n",
            "45     \t [0.65245155 0.41173725 0.90895695]. \t  2.858627770566837 \t 3.772577454290076\n",
            "46     \t [0.32817392 0.43930842 0.76576021]. \t  2.93773136755562 \t 3.772577454290076\n",
            "47     \t [0.47310704 0.73225946 0.77967682]. \t  2.567374504345819 \t 3.772577454290076\n",
            "48     \t [0.9492419  0.6437365  0.83270298]. \t  3.3208203778497323 \t 3.772577454290076\n",
            "49     \t [0.37866648 0.48483831 0.11681013]. \t  0.13267064162900638 \t 3.772577454290076\n",
            "50     \t [0.63776864 0.13811317 0.10232408]. \t  0.3542316780422566 \t 3.772577454290076\n",
            "51     \t [0.0226272  0.46039584 0.39306004]. \t  0.39248366552278097 \t 3.772577454290076\n",
            "52     \t [0.74226553 0.68447382 0.74109946]. \t  2.258201061656978 \t 3.772577454290076\n",
            "53     \t [0.73671863 0.51163238 0.43451977]. \t  0.2618360570450024 \t 3.772577454290076\n",
            "54     \t [0.28537774 0.79787034 0.66787496]. \t  2.4079245833904652 \t 3.772577454290076\n",
            "55     \t [0.52755091 0.87057573 0.11727033]. \t  0.007314436154976636 \t 3.772577454290076\n",
            "56     \t [0.29346589 0.65910737 0.11163608]. \t  0.029896622802024427 \t 3.772577454290076\n",
            "57     \t [0.2895177  0.92186079 0.05601142]. \t  0.0019237374875095135 \t 3.772577454290076\n",
            "58     \t [0.88082954 0.71100467 0.07799923]. \t  0.004986407593590679 \t 3.772577454290076\n",
            "59     \t [0.73226542 0.65192332 0.47391306]. \t  0.5463663067102064 \t 3.772577454290076\n",
            "60     \t [0.02333221 0.35878397 0.34093755]. \t  0.38738388979188215 \t 3.772577454290076\n",
            "61     \t [0.98356167 0.53763874 0.63483676]. \t  1.1181476260591983 \t 3.772577454290076\n",
            "62     \t [0.156046   0.59655081 0.86958024]. \t  3.7669941228717163 \t 3.772577454290076\n",
            "63     \t [0.88684696 0.12634136 0.08895455]. \t  0.17206619872067863 \t 3.772577454290076\n",
            "64     \t [0.18218602 0.45036847 0.09175881]. \t  0.1184082809363772 \t 3.772577454290076\n",
            "65     \t [0.34292031 0.68519737 0.09262511]. \t  0.018785249890164495 \t 3.772577454290076\n",
            "66     \t [0.7317639  0.74224622 0.95671144]. \t  2.077065612238471 \t 3.772577454290076\n",
            "67     \t [0.87137829 0.5695713  0.62329453]. \t  1.0607784657356756 \t 3.772577454290076\n",
            "68     \t [0.40862621 0.14201035 0.85863338]. \t  0.8060903991006787 \t 3.772577454290076\n",
            "69     \t [0.16367498 0.61225246 0.02678576]. \t  0.013724982809582762 \t 3.772577454290076\n",
            "70     \t [0.22148134 0.37110448 0.85048554]. \t  2.8364247037422916 \t 3.772577454290076\n",
            "71     \t [0.13958531 0.2856406  0.7196961 ]. \t  1.5031167436047488 \t 3.772577454290076\n",
            "72     \t [0.07848445 0.86996977 0.15270764]. \t  0.02527200462831359 \t 3.772577454290076\n",
            "73     \t [0.58561207 0.76780412 0.07594845]. \t  0.0055932062345625125 \t 3.772577454290076\n",
            "74     \t [0.43456286 0.62971159 0.41523235]. \t  0.7294054153517803 \t 3.772577454290076\n",
            "75     \t [0.70086648 0.12372512 0.64641762]. \t  0.38204950898303597 \t 3.772577454290076\n",
            "76     \t [0.91643672 0.5876769  0.04810105]. \t  0.010587036778547476 \t 3.772577454290076\n",
            "77     \t [0.51723626 0.13900936 0.15596207]. \t  0.6423475702077569 \t 3.772577454290076\n",
            "78     \t [0.08522595 0.01635146 0.36754278]. \t  0.5270746580713294 \t 3.772577454290076\n",
            "79     \t [0.78306988 0.08787704 0.37528958]. \t  0.4211244425314013 \t 3.772577454290076\n",
            "80     \t [0.19974885 0.85121287 0.97113527]. \t  1.1809105868252145 \t 3.772577454290076\n",
            "81     \t [0.90777519 0.74850197 0.25926154]. \t  0.03547903149967979 \t 3.772577454290076\n",
            "82     \t [0.1885716  0.12356481 0.45988309]. \t  0.3315845871223555 \t 3.772577454290076\n",
            "83     \t [0.63074269 0.83889432 0.31506937]. \t  0.23857930741350988 \t 3.772577454290076\n",
            "84     \t [0.7426569  0.81448563 0.09728448]. \t  0.003764932984621751 \t 3.772577454290076\n",
            "85     \t [0.15288894 0.84947162 0.48361687]. \t  2.5748938282286176 \t 3.772577454290076\n",
            "86     \t [0.14127014 0.02953937 0.32421009]. \t  0.7204917512255963 \t 3.772577454290076\n",
            "87     \t [0.89069861 0.32297623 0.05610986]. \t  0.07585405621148908 \t 3.772577454290076\n",
            "88     \t [0.89113566 0.81003517 0.79241111]. \t  1.7044650571138988 \t 3.772577454290076\n",
            "89     \t [0.29976282 0.72376121 0.38408178]. \t  0.9206494684935415 \t 3.772577454290076\n",
            "90     \t [0.21801703 0.69174281 0.33881151]. \t  0.5459252904266572 \t 3.772577454290076\n",
            "91     \t [0.32550266 0.96841791 0.94725396]. \t  0.6263481766596568 \t 3.772577454290076\n",
            "92     \t [0.67032488 0.96302862 0.87442259]. \t  0.7919522796466973 \t 3.772577454290076\n",
            "93     \t [0.16572149 0.0161577  0.12188675]. \t  0.4232172729959182 \t 3.772577454290076\n",
            "94     \t [0.31948346 0.05319824 0.01070769]. \t  0.13222931057489118 \t 3.772577454290076\n",
            "95     \t [0.8957484  0.2366864  0.66129404]. \t  0.7942865476787239 \t 3.772577454290076\n",
            "96     \t [0.6343747  0.42482422 0.42699233]. \t  0.28859725211452913 \t 3.772577454290076\n",
            "97     \t [0.09414835 0.47320232 0.48502237]. \t  0.6944438440132831 \t 3.772577454290076\n",
            "98     \t [0.35682058 0.62212976 0.54162852]. \t  1.5788029312392569 \t 3.772577454290076\n",
            "99     \t [0.48811289 0.59955038 0.18383557]. \t  0.09066010487259671 \t 3.772577454290076\n",
            "100    \t [0.52697289 0.4467923  0.3546839 ]. \t  0.34114377715747707 \t 3.772577454290076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkq32q1utdoO",
        "outputId": "6257b3dc-41e2-42ac-ca24-889848e1f2d2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_loser_9 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_9 = dGPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.80342804 0.5275223  0.11911147]. \t  0.05516938924925502 \t 1.6536488994056173\n",
            "init   \t [0.63968144 0.09092526 0.33222568]. \t  0.7039339223433662 \t 1.6536488994056173\n",
            "init   \t [0.42738095 0.55438581 0.62812652]. \t  1.6536488994056173 \t 1.6536488994056173\n",
            "init   \t [0.69739294 0.78994969 0.13189035]. \t  0.009151170523361311 \t 1.6536488994056173\n",
            "init   \t [0.34277045 0.20155961 0.70732423]. \t  0.9342632521680911 \t 1.6536488994056173\n",
            "1      \t [0.21003505 0.96880326 0.94623317]. \t  0.629144655843092 \t 1.6536488994056173\n",
            "2      \t [0.88822247 0.64717505 0.93325184]. \t  \u001b[92m2.936038130897624\u001b[0m \t 2.936038130897624\n",
            "3      \t [0.92309585 0.63136935 0.89713949]. \t  \u001b[92m3.36145188082496\u001b[0m \t 3.36145188082496\n",
            "4      \t [0.97078368 0.37827572 0.60407706]. \t  0.7223897216338022 \t 3.36145188082496\n",
            "5      \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.36145188082496\n",
            "6      \t [1. 1. 1.]. \t  0.31688362070415665 \t 3.36145188082496\n",
            "7      \t [0.00148228 0.77610523 0.75410658]. \t  2.350920832258359 \t 3.36145188082496\n",
            "8      \t [0.94693831 0.61155431 0.92293946]. \t  3.1864363691427835 \t 3.36145188082496\n",
            "9      \t [0.98424851 0.89958172 0.51609175]. \t  0.31770001519740687 \t 3.36145188082496\n",
            "10     \t [0.93586997 0.71443262 0.87120637]. \t  2.8725659805480657 \t 3.36145188082496\n",
            "11     \t [0.77658981 0.35780449 0.82640336]. \t  2.6628596109839844 \t 3.36145188082496\n",
            "12     \t [0.03186592 0.79569369 0.94704453]. \t  1.7844095329589087 \t 3.36145188082496\n",
            "13     \t [0.86200319 0.56979155 0.74442387]. \t  2.6684475637901897 \t 3.36145188082496\n",
            "14     \t [0.41031621 0.0265955  0.03250833]. \t  0.17539991132785313 \t 3.36145188082496\n",
            "15     \t [0.99776157 0.64222621 0.87827287]. \t  \u001b[92m3.368594517628543\u001b[0m \t 3.368594517628543\n",
            "16     \t [0.01878772 0.99728483 0.09035177]. \t  0.004010356586168865 \t 3.368594517628543\n",
            "17     \t [0.97557082 0.95671533 0.08182595]. \t  0.0004614956657524075 \t 3.368594517628543\n",
            "18     \t [0.09961053 0.42373985 0.04458018]. \t  0.07105609479834335 \t 3.368594517628543\n",
            "19     \t [0.99727223 0.54122305 0.93669611]. \t  3.0519998910271773 \t 3.368594517628543\n",
            "20     \t [0.00683062 0.90479811 0.88456725]. \t  1.2373877556091002 \t 3.368594517628543\n",
            "21     \t [0.69338885 0.08627401 0.11065029]. \t  0.3459448233643284 \t 3.368594517628543\n",
            "22     \t [0.18895708 0.29736843 0.64769767]. \t  0.9953405067578573 \t 3.368594517628543\n",
            "23     \t [0.35292396 0.7284181  0.45097298]. \t  1.5102668359383205 \t 3.368594517628543\n",
            "24     \t [0.01964039 0.59122522 0.4011626 ]. \t  0.7098580300055227 \t 3.368594517628543\n",
            "25     \t [0.04125021 0.48258325 0.94515344]. \t  2.837141442408747 \t 3.368594517628543\n",
            "26     \t [0.00398344 0.15635549 0.93625284]. \t  0.6489238761176981 \t 3.368594517628543\n",
            "27     \t [0.21856344 0.9647262  0.58236059]. \t  2.6596853529896296 \t 3.368594517628543\n",
            "28     \t [0.65705179 0.74970073 0.56481824]. \t  1.2542485516850428 \t 3.368594517628543\n",
            "29     \t [0.43860812 0.29570706 0.13229548]. \t  0.41484876224588224 \t 3.368594517628543\n",
            "30     \t [0.81559219 0.47198787 0.91297578]. \t  3.189328460116302 \t 3.368594517628543\n",
            "31     \t [0.17228086 0.74038486 0.67007788]. \t  2.552227852029529 \t 3.368594517628543\n",
            "32     \t [0.82225239 0.45869837 0.27291106]. \t  0.17904623993483504 \t 3.368594517628543\n",
            "33     \t [0.52240711 0.30817363 0.33234368]. \t  0.5886953591025507 \t 3.368594517628543\n",
            "34     \t [0.03322266 0.99462012 0.91350755]. \t  0.5965323152717071 \t 3.368594517628543\n",
            "35     \t [0.48284383 0.60851186 0.30178257]. \t  0.2274639721561767 \t 3.368594517628543\n",
            "36     \t [0.45149601 0.13284041 0.40736616]. \t  0.5555766641190214 \t 3.368594517628543\n",
            "37     \t [0.02936286 0.50790832 0.37360011]. \t  0.40799229236381035 \t 3.368594517628543\n",
            "38     \t [0.21818567 0.06360072 0.46128035]. \t  0.31401638914978247 \t 3.368594517628543\n",
            "39     \t [0.34508795 0.46351135 0.42728305]. \t  0.4658177758162148 \t 3.368594517628543\n",
            "40     \t [0.80047097 0.20977166 0.93964803]. \t  0.9353536138601612 \t 3.368594517628543\n",
            "41     \t [0.56338529 0.40970159 0.87966518]. \t  3.0816785177566506 \t 3.368594517628543\n",
            "42     \t [0.62898453 0.15398445 0.99820052]. \t  0.3995217044582213 \t 3.368594517628543\n",
            "43     \t [0.0953194  0.12889631 0.9974975 ]. \t  0.3258998308602872 \t 3.368594517628543\n",
            "44     \t [0.74579503 0.9702184  0.92701043]. \t  0.6504016745915788 \t 3.368594517628543\n",
            "45     \t [0.82514889 0.47849315 0.0029895 ]. \t  0.01784129005495299 \t 3.368594517628543\n",
            "46     \t [0.9188371  0.24794378 0.49868342]. \t  0.1764476238190525 \t 3.368594517628543\n",
            "47     \t [0.54600719 0.5851521  0.59197347]. \t  1.2946530885733964 \t 3.368594517628543\n",
            "48     \t [0.89827105 0.52168441 0.30821513]. \t  0.10285301763675626 \t 3.368594517628543\n",
            "49     \t [0.81484174 0.21135661 0.82836122]. \t  1.3201867902358642 \t 3.368594517628543\n",
            "50     \t [0.04297924 0.14489111 0.80397184]. \t  0.8487009222388502 \t 3.368594517628543\n",
            "51     \t [0.44974115 0.01155191 0.59013511]. \t  0.12822442104215265 \t 3.368594517628543\n",
            "52     \t [0.56402957 0.15836296 0.81883483]. \t  0.9473434746978434 \t 3.368594517628543\n",
            "53     \t [0.05298445 0.79463477 0.40427988]. \t  1.4275269907996242 \t 3.368594517628543\n",
            "54     \t [0.32126796 0.23252415 0.7501739 ]. \t  1.3344198696765737 \t 3.368594517628543\n",
            "55     \t [0.73366163 0.85545537 0.01165897]. \t  0.0005379679255610535 \t 3.368594517628543\n",
            "56     \t [0.54033511 0.08211717 0.30454888]. \t  0.8685149917997927 \t 3.368594517628543\n",
            "57     \t [0.32280099 0.4509323  0.4857151 ]. \t  0.5797761220824894 \t 3.368594517628543\n",
            "58     \t [0.48645451 0.20290869 0.15632495]. \t  0.6160579007606918 \t 3.368594517628543\n",
            "59     \t [0.88657382 0.0909298  0.49189932]. \t  0.13633706368341955 \t 3.368594517628543\n",
            "60     \t [0.68439215 0.37020731 0.72050967]. \t  1.9833113696201004 \t 3.368594517628543\n",
            "61     \t [0.51141405 0.09418599 0.10047457]. \t  0.4061489572814179 \t 3.368594517628543\n",
            "62     \t [0.31198933 0.70608232 0.40206586]. \t  1.0247202121973575 \t 3.368594517628543\n",
            "63     \t [0.33741312 0.5207626  0.95663166]. \t  2.8153421842141757 \t 3.368594517628543\n",
            "64     \t [0.03344787 0.8869391  0.50351697]. \t  2.7488240138915274 \t 3.368594517628543\n",
            "65     \t [0.82589378 0.70949834 0.42831321]. \t  0.32973691478035394 \t 3.368594517628543\n",
            "66     \t [0.10956208 0.51557668 0.38425112]. \t  0.47166276244020716 \t 3.368594517628543\n",
            "67     \t [0.10601178 0.96121496 0.08958546]. \t  0.004469271519085479 \t 3.368594517628543\n",
            "68     \t [0.84485844 0.91589973 0.97923942]. \t  0.7251132535909458 \t 3.368594517628543\n",
            "69     \t [0.00710543 0.95040531 0.98600257]. \t  0.550192477085139 \t 3.368594517628543\n",
            "70     \t [0.85744293 0.25559295 0.24291441]. \t  0.3969420390316026 \t 3.368594517628543\n",
            "71     \t [0.26519166 0.0887628  0.46017331]. \t  0.3395092315991357 \t 3.368594517628543\n",
            "72     \t [0.90395729 0.3567291  0.37436871]. \t  0.18962674617183883 \t 3.368594517628543\n",
            "73     \t [0.05179127 0.23950252 0.93598409]. \t  1.1625248997381643 \t 3.368594517628543\n",
            "74     \t [0.20215366 0.23546802 0.21831932]. \t  0.74571077099277 \t 3.368594517628543\n",
            "75     \t [0.32964147 0.6739022  0.07081528]. \t  0.015614063513558365 \t 3.368594517628543\n",
            "76     \t [0.09664869 0.29738318 0.92270834]. \t  1.7252518603526286 \t 3.368594517628543\n",
            "77     \t [0.74898839 0.10693057 0.11347135]. \t  0.3184501015130546 \t 3.368594517628543\n",
            "78     \t [0.11971233 0.3297402  0.11591569]. \t  0.26592145385109955 \t 3.368594517628543\n",
            "79     \t [0.68666318 0.69387016 0.76306807]. \t  2.498359242000209 \t 3.368594517628543\n",
            "80     \t [0.21155027 0.908299   0.69610042]. \t  2.0048917301760367 \t 3.368594517628543\n",
            "81     \t [0.72726961 0.84442141 0.54620606]. \t  1.0287107371753577 \t 3.368594517628543\n",
            "82     \t [0.76275115 0.91814487 0.47922723]. \t  0.7010143297199012 \t 3.368594517628543\n",
            "83     \t [0.77735567 0.78843499 0.0312755 ]. \t  0.0014528375337805194 \t 3.368594517628543\n",
            "84     \t [0.75981294 0.37752905 0.47924214]. \t  0.2445766115717162 \t 3.368594517628543\n",
            "85     \t [0.32076759 0.78466335 0.89882487]. \t  2.267498681703352 \t 3.368594517628543\n",
            "86     \t [0.75002834 0.19657723 0.90564032]. \t  1.018964296156073 \t 3.368594517628543\n",
            "87     \t [0.89324058 0.30707588 0.98512393]. \t  1.1942851913691062 \t 3.368594517628543\n",
            "88     \t [0.22373454 0.24845131 0.00914723]. \t  0.10696785384108524 \t 3.368594517628543\n",
            "89     \t [0.80565659 0.50885834 0.7450086 ]. \t  2.7413748713011907 \t 3.368594517628543\n",
            "90     \t [0.48535488 0.48539508 0.44074141]. \t  0.4427910360182567 \t 3.368594517628543\n",
            "91     \t [0.70727185 0.052759   0.75253142]. \t  0.3848868135768166 \t 3.368594517628543\n",
            "92     \t [0.65551647 0.61255679 0.24423964]. \t  0.10058266221322114 \t 3.368594517628543\n",
            "93     \t [0.3117153  0.57133763 0.91604566]. \t  \u001b[92m3.467293904848227\u001b[0m \t 3.467293904848227\n",
            "94     \t [0.97957236 0.53099367 0.29827555]. \t  0.07146247475221883 \t 3.467293904848227\n",
            "95     \t [0.07078675 0.7785934  0.05120101]. \t  0.0037300677157083425 \t 3.467293904848227\n",
            "96     \t [0.61396047 0.44728502 0.34116334]. \t  0.29998891684476914 \t 3.467293904848227\n",
            "97     \t [0.65055637 0.16667546 0.59492917]. \t  0.32597291361489167 \t 3.467293904848227\n",
            "98     \t [0.14447002 0.2820046  0.62544465]. \t  0.7791423355461649 \t 3.467293904848227\n",
            "99     \t [0.42241034 0.37140917 0.48479234]. \t  0.3928200094367406 \t 3.467293904848227\n",
            "100    \t [0.94137119 0.19156    0.80927874]. \t  1.1500040915581455 \t 3.467293904848227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Aur-aRtdq-",
        "outputId": "834b79a9-3a39-4b5d-e6cc-d303d8ace60a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_loser_10 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_10 = dGPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.65358959 0.11500694 0.95028286]. \t  0.42730315147591735 \t 1.1029187088185965\n",
            "init   \t [0.4821914  0.87247454 0.21233268]. \t  0.06161964400032635 \t 1.1029187088185965\n",
            "init   \t [0.04070962 0.39719446 0.2331322 ]. \t  0.33269334660262956 \t 1.1029187088185965\n",
            "init   \t [0.84174072 0.20708234 0.74246953]. \t  1.1029187088185965 \t 1.1029187088185965\n",
            "init   \t [0.39215413 0.18225652 0.74353941]. \t  0.9779763535009853 \t 1.1029187088185965\n",
            "1      \t [0.86210958 0.05779215 0.34314786]. \t  0.3927532126432764 \t 1.1029187088185965\n",
            "2      \t [0.92518579 0.78050514 0.87881645]. \t  \u001b[92m2.2443644989048672\u001b[0m \t 2.2443644989048672\n",
            "3      \t [0.98338289 0.93392737 0.81033881]. \t  0.8330483590093913 \t 2.2443644989048672\n",
            "4      \t [0.24984974 0.81883701 0.99874938]. \t  1.135383544316759 \t 2.2443644989048672\n",
            "5      \t [0.84241011 0.46743734 0.97175067]. \t  \u001b[92m2.3195199773796715\u001b[0m \t 2.3195199773796715\n",
            "6      \t [0.77577588 0.62900507 0.95536967]. \t  \u001b[92m2.727536856652607\u001b[0m \t 2.727536856652607\n",
            "7      \t [0.65288565 0.62205817 0.86074034]. \t  \u001b[92m3.6353071006528688\u001b[0m \t 3.6353071006528688\n",
            "8      \t [0.66640478 0.61877439 0.58484337]. \t  1.0883761860789831 \t 3.6353071006528688\n",
            "9      \t [0.11836853 0.92421572 0.40077358]. \t  1.4382047023912417 \t 3.6353071006528688\n",
            "10     \t [6.24314263e-15 1.31537357e-14 1.20310445e-14]. \t  0.06797411659014845 \t 3.6353071006528688\n",
            "11     \t [0.91498214 0.04875353 0.00402212]. \t  0.048769558323538735 \t 3.6353071006528688\n",
            "12     \t [0.28501363 0.56069794 0.81105969]. \t  \u001b[92m3.7011399387928163\u001b[0m \t 3.7011399387928163\n",
            "13     \t [0.06191779 0.75578134 0.02041316]. \t  0.0025418667181896612 \t 3.7011399387928163\n",
            "14     \t [0.44724157 0.86154826 0.74155042]. \t  1.6436280199400723 \t 3.7011399387928163\n",
            "15     \t [0.32900204 0.5222541  0.95157111]. \t  2.9035028561512806 \t 3.7011399387928163\n",
            "16     \t [0.20016925 0.69643574 0.79177916]. \t  3.0240737795597763 \t 3.7011399387928163\n",
            "17     \t [0.42243564 0.66380211 0.71533146]. \t  2.4692385729881137 \t 3.7011399387928163\n",
            "18     \t [0.09177594 0.9678026  0.06127449]. \t  0.0020036168886189963 \t 3.7011399387928163\n",
            "19     \t [0.68362641 0.98533124 0.05418554]. \t  0.0006363090741918352 \t 3.7011399387928163\n",
            "20     \t [0.1121814  0.57163451 0.79125054]. \t  3.5216749058979495 \t 3.7011399387928163\n",
            "21     \t [0.5783509  0.49008829 0.81913902]. \t  3.5934784878492065 \t 3.7011399387928163\n",
            "22     \t [0.51884018 0.57083199 0.85833946]. \t  \u001b[92m3.825950075211796\u001b[0m \t 3.825950075211796\n",
            "23     \t [0.0093238  0.61751695 0.70112407]. \t  2.564612796557479 \t 3.825950075211796\n",
            "24     \t [0.0306889  0.97870848 0.73586423]. \t  1.337128891077002 \t 3.825950075211796\n",
            "25     \t [0.27980633 0.46148506 0.1752227 ]. \t  0.2378344066527602 \t 3.825950075211796\n",
            "26     \t [0.32699639 0.02315428 0.06128359]. \t  0.2549683775569132 \t 3.825950075211796\n",
            "27     \t [0.73893811 0.68093479 0.04192289]. \t  0.006243422255177505 \t 3.825950075211796\n",
            "28     \t [0.20139786 0.88767482 0.66395525]. \t  2.388005815326538 \t 3.825950075211796\n",
            "29     \t [0.02551235 0.3423593  0.75184502]. \t  2.1388202357199315 \t 3.825950075211796\n",
            "30     \t [0.6657422  0.53602642 0.87915674]. \t  3.7327606277135947 \t 3.825950075211796\n",
            "31     \t [0.53783974 0.37586609 0.11422795]. \t  0.23297846283288623 \t 3.825950075211796\n",
            "32     \t [0.17733175 0.19713417 0.53632996]. \t  0.27802306505192764 \t 3.825950075211796\n",
            "33     \t [0.03047332 0.0352073  0.14699465]. \t  0.42970513491749057 \t 3.825950075211796\n",
            "34     \t [0.92196866 0.20235519 0.22032218]. \t  0.34779631994302274 \t 3.825950075211796\n",
            "35     \t [0.41268804 0.21677931 0.07668479]. \t  0.3026346220661753 \t 3.825950075211796\n",
            "36     \t [0.28033917 0.9940199  0.44363132]. \t  1.6428151604376122 \t 3.825950075211796\n",
            "37     \t [0.91502671 0.69756974 0.91725515]. \t  2.787194121186799 \t 3.825950075211796\n",
            "38     \t [0.04794957 0.42614372 0.76621089]. \t  2.846087590501406 \t 3.825950075211796\n",
            "39     \t [0.84554329 0.19749171 0.53590731]. \t  0.21072762937007228 \t 3.825950075211796\n",
            "40     \t [0.10248424 0.96645507 0.94996451]. \t  0.6251604128490882 \t 3.825950075211796\n",
            "41     \t [0.54897872 0.42031727 0.66842942]. \t  1.6163134760994067 \t 3.825950075211796\n",
            "42     \t [0.94325126 0.54732528 0.85763938]. \t  3.69811452912248 \t 3.825950075211796\n",
            "43     \t [0.90677211 0.67028093 0.75452163]. \t  2.372125722503001 \t 3.825950075211796\n",
            "44     \t [0.25070052 0.63827166 0.40552541]. \t  0.884706677204184 \t 3.825950075211796\n",
            "45     \t [0.32103194 0.78721674 0.28324319]. \t  0.2781771042452093 \t 3.825950075211796\n",
            "46     \t [0.73990251 0.38721673 0.73946956]. \t  2.273626035083554 \t 3.825950075211796\n",
            "47     \t [0.61276184 0.23705712 0.22676888]. \t  0.6905085975860578 \t 3.825950075211796\n",
            "48     \t [0.79204942 0.18643486 0.93466089]. \t  0.8194979038041548 \t 3.825950075211796\n",
            "49     \t [0.2182822  0.82045885 0.34387207]. \t  0.7485559328161449 \t 3.825950075211796\n",
            "50     \t [0.79434093 0.75456143 0.80972959]. \t  2.3576544924574 \t 3.825950075211796\n",
            "51     \t [0.99685024 0.77141333 0.03923149]. \t  0.0009762900711944165 \t 3.825950075211796\n",
            "52     \t [0.33678804 0.22014992 0.00684788]. \t  0.11712495717363505 \t 3.825950075211796\n",
            "53     \t [0.41028139 0.5719381  0.26311632]. \t  0.19774787863788867 \t 3.825950075211796\n",
            "54     \t [0.60077419 0.77319619 0.97632717]. \t  1.6470965209265724 \t 3.825950075211796\n",
            "55     \t [0.95857662 0.47617844 0.80837472]. \t  3.338517307679115 \t 3.825950075211796\n",
            "56     \t [0.16839819 0.09440242 0.80593045]. \t  0.5819460844192055 \t 3.825950075211796\n",
            "57     \t [0.77126344 0.27787038 0.57095094]. \t  0.40678651464373805 \t 3.825950075211796\n",
            "58     \t [0.23622091 0.68363914 0.77638665]. \t  2.9986286541218723 \t 3.825950075211796\n",
            "59     \t [0.98691583 0.16799015 0.96810489]. \t  0.5602547142275782 \t 3.825950075211796\n",
            "60     \t [0.09323872 0.98788564 0.33246815]. \t  0.5978453646759858 \t 3.825950075211796\n",
            "61     \t [0.20585617 0.57446848 0.03650038]. \t  0.02341783038978431 \t 3.825950075211796\n",
            "62     \t [0.91977899 0.02827927 0.32915799]. \t  0.33213211617093846 \t 3.825950075211796\n",
            "63     \t [0.1744179  0.49781195 0.54077387]. \t  1.051091442156995 \t 3.825950075211796\n",
            "64     \t [0.99869586 0.95597948 0.00719737]. \t  6.747924827195337e-05 \t 3.825950075211796\n",
            "65     \t [0.05923735 0.65355236 0.73897894]. \t  2.847661076986637 \t 3.825950075211796\n",
            "66     \t [0.01269454 0.01430481 0.53274047]. \t  0.11687636895554003 \t 3.825950075211796\n",
            "67     \t [0.83162222 0.16189688 0.6821508 ]. \t  0.6204044967331243 \t 3.825950075211796\n",
            "68     \t [0.10307635 0.98704216 0.62230977]. \t  2.3851373682146537 \t 3.825950075211796\n",
            "69     \t [0.81310371 0.96461204 0.89116056]. \t  0.7398602162679827 \t 3.825950075211796\n",
            "70     \t [0.96392299 0.0732633  0.82991453]. \t  0.4700895298354437 \t 3.825950075211796\n",
            "71     \t [0.72813753 0.72226023 0.02144692]. \t  0.0029895386586784 \t 3.825950075211796\n",
            "72     \t [0.20147301 0.83427847 0.02792526]. \t  0.0016588547472213234 \t 3.825950075211796\n",
            "73     \t [0.96753452 0.40978965 0.46139521]. \t  0.14787896680659463 \t 3.825950075211796\n",
            "74     \t [0.24613548 0.25790695 0.36722679]. \t  0.6088691497095003 \t 3.825950075211796\n",
            "75     \t [0.61869238 0.30960007 0.75805921]. \t  1.9422037888060717 \t 3.825950075211796\n",
            "76     \t [0.06499414 0.21232159 0.37895357]. \t  0.497457472021624 \t 3.825950075211796\n",
            "77     \t [0.03885248 0.33913528 0.45254913]. \t  0.33612032404781683 \t 3.825950075211796\n",
            "78     \t [0.55379685 0.80032251 0.19912658]. \t  0.04278284053876489 \t 3.825950075211796\n",
            "79     \t [0.02021057 0.15528238 0.76939561]. \t  0.8700524306842632 \t 3.825950075211796\n",
            "80     \t [0.322192   0.09348805 0.37668228]. \t  0.6953889615688063 \t 3.825950075211796\n",
            "81     \t [0.09095127 0.93642432 0.61762491]. \t  2.685359651785647 \t 3.825950075211796\n",
            "82     \t [0.10159781 0.36047065 0.96154357]. \t  1.8417153323280768 \t 3.825950075211796\n",
            "83     \t [0.57007758 0.64436134 0.09782204]. \t  0.02497798409309806 \t 3.825950075211796\n",
            "84     \t [0.07585703 0.84169618 0.31840343]. \t  0.5586293380163749 \t 3.825950075211796\n",
            "85     \t [0.26852638 0.32867212 0.25219684]. \t  0.625031988431732 \t 3.825950075211796\n",
            "86     \t [0.03915346 0.47901975 0.39966145]. \t  0.4367926667230772 \t 3.825950075211796\n",
            "87     \t [0.79613901 0.22285573 0.8464027 ]. \t  1.401327231034421 \t 3.825950075211796\n",
            "88     \t [0.78634327 0.08272775 0.40411308]. \t  0.34048301845184237 \t 3.825950075211796\n",
            "89     \t [0.25075283 0.42774475 0.05874369]. \t  0.09926859078580283 \t 3.825950075211796\n",
            "90     \t [0.52020758 0.06642892 0.61019363]. \t  0.20301745333843152 \t 3.825950075211796\n",
            "91     \t [0.04145178 0.9982077  0.04650104]. \t  0.0011636459714390384 \t 3.825950075211796\n",
            "92     \t [0.6463775  0.3034504  0.95923746]. \t  1.4498903641763436 \t 3.825950075211796\n",
            "93     \t [0.21348632 0.41152009 0.08247436]. \t  0.14062703500889415 \t 3.825950075211796\n",
            "94     \t [0.52393133 0.71775999 0.69267821]. \t  2.0244943228113934 \t 3.825950075211796\n",
            "95     \t [0.92048307 0.02856064 0.60107968]. \t  0.12706036817059735 \t 3.825950075211796\n",
            "96     \t [0.55966008 0.00459803 0.75177789]. \t  0.2510841678353941 \t 3.825950075211796\n",
            "97     \t [0.06118166 0.51334582 0.17268385]. \t  0.12985995150060514 \t 3.825950075211796\n",
            "98     \t [0.67207969 0.29327723 0.95465068]. \t  1.416516516636988 \t 3.825950075211796\n",
            "99     \t [0.21093958 0.37233381 0.00276439]. \t  0.05926409704295324 \t 3.825950075211796\n",
            "100    \t [0.06026911 0.76609599 0.98141921]. \t  1.630173690362098 \t 3.825950075211796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMu7U9NNtdtn",
        "outputId": "53f98370-5850-4bb8-9d40-3acba02a51a3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_loser_11 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_11 = dGPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.98103566 0.82124785 0.64589361]. \t  0.6570240879762975 \t 0.687459437576373\n",
            "init   \t [0.42368801 0.20231098 0.49190677]. \t  0.29901296656621285 \t 0.687459437576373\n",
            "init   \t [0.13855833 0.45252104 0.11373463]. \t  0.1378633538075966 \t 0.687459437576373\n",
            "init   \t [0.00292449 0.52342617 0.18997116]. \t  0.12299763462541902 \t 0.687459437576373\n",
            "init   \t [0.14171979 0.33586637 0.58369219]. \t  0.687459437576373 \t 0.687459437576373\n",
            "1      \t [0.23267662 0.55632866 0.98247629]. \t  \u001b[92m2.401417505244251\u001b[0m \t 2.401417505244251\n",
            "2      \t [0.1018956  0.95528749 0.95450529]. \t  0.6625688909370097 \t 2.401417505244251\n",
            "3      \t [0.49069589 0.32394686 0.99166679]. \t  1.2576491073928158 \t 2.401417505244251\n",
            "4      \t [0.99622896 0.50984357 0.00987988]. \t  0.008998496185923171 \t 2.401417505244251\n",
            "5      \t [0.17147829 0.44852386 0.99855404]. \t  1.8327027793895603 \t 2.401417505244251\n",
            "6      \t [0.52755702 0.61099275 0.96085651]. \t  \u001b[92m2.731594261003499\u001b[0m \t 2.731594261003499\n",
            "7      \t [0.50454698 0.68368804 0.91827122]. \t  \u001b[92m2.973325813164906\u001b[0m \t 2.973325813164906\n",
            "8      \t [0.45418709 0.73924568 0.73077065]. \t  2.2589019118704425 \t 2.973325813164906\n",
            "9      \t [0.53897689 0.95646638 0.88685629]. \t  0.8406578977033827 \t 2.973325813164906\n",
            "10     \t [0.60547932 0.56968479 0.81474854]. \t  \u001b[92m3.6411377654076196\u001b[0m \t 3.6411377654076196\n",
            "11     \t [0.9286639  0.28303617 0.64074614]. \t  0.8065492930206374 \t 3.6411377654076196\n",
            "12     \t [0.6863489  0.52266533 0.67959972]. \t  1.8802784961869148 \t 3.6411377654076196\n",
            "13     \t [0.81516659 0.04230252 0.0144339 ]. \t  0.07642119456660208 \t 3.6411377654076196\n",
            "14     \t [3.56002803e-02 9.17723598e-01 3.28278754e-04]. \t  0.0004253659195000631 \t 3.6411377654076196\n",
            "15     \t [0.93259193 0.59135126 0.8998208 ]. \t  3.4797007688347623 \t 3.6411377654076196\n",
            "16     \t [0.03180589 0.80324037 0.77495056]. \t  2.203713570050467 \t 3.6411377654076196\n",
            "17     \t [0.11708033 0.14671029 0.01423623]. \t  0.11999746361891822 \t 3.6411377654076196\n",
            "18     \t [0.50657018 0.49089223 0.85138264]. \t  \u001b[92m3.707332576217637\u001b[0m \t 3.707332576217637\n",
            "19     \t [0.80737161 0.0155614  0.66176037]. \t  0.18365620244113123 \t 3.707332576217637\n",
            "20     \t [0.37567124 0.48150598 0.85948258]. \t  3.6688440344570115 \t 3.707332576217637\n",
            "21     \t [0.48075247 0.4868558  0.84329412]. \t  3.6882293140094626 \t 3.707332576217637\n",
            "22     \t [0.41981534 0.53971767 0.86767719]. \t  \u001b[92m3.82609236093791\u001b[0m \t 3.82609236093791\n",
            "23     \t [0.02516017 0.58371562 0.30762436]. \t  0.28033234832751935 \t 3.82609236093791\n",
            "24     \t [0.48944405 0.6559092  0.83877525]. \t  3.459362722987617 \t 3.82609236093791\n",
            "25     \t [0.42029516 0.21429986 0.30920755]. \t  0.8618548387131872 \t 3.82609236093791\n",
            "26     \t [0.96711039 0.14576491 0.91452994]. \t  0.6616327147356602 \t 3.82609236093791\n",
            "27     \t [0.38311766 0.57716223 0.84585103]. \t  \u001b[92m3.832003952745011\u001b[0m \t 3.832003952745011\n",
            "28     \t [0.77891632 0.65166351 0.98580516]. \t  2.150894348089524 \t 3.832003952745011\n",
            "29     \t [0.9881304  0.83507513 0.9180588 ]. \t  1.588643416372116 \t 3.832003952745011\n",
            "30     \t [0.7384121  0.63332243 0.15380586]. \t  0.03551552148442237 \t 3.832003952745011\n",
            "31     \t [0.89618434 0.95859692 0.01322118]. \t  0.00011836310433250633 \t 3.832003952745011\n",
            "32     \t [0.3222753  0.04010627 0.03332996]. \t  0.18124540626821045 \t 3.832003952745011\n",
            "33     \t [0.88887881 0.39281112 0.92796296]. \t  2.4565050300574933 \t 3.832003952745011\n",
            "34     \t [0.31470288 0.14953304 0.93760765]. \t  0.6195643860286129 \t 3.832003952745011\n",
            "35     \t [0.75331242 0.27518024 0.74191406]. \t  1.5632474996030026 \t 3.832003952745011\n",
            "36     \t [0.01390206 0.79841187 0.82989979]. \t  2.270327964643763 \t 3.832003952745011\n",
            "37     \t [0.41503592 0.54752483 0.84909805]. \t  \u001b[92m3.8511165079641128\u001b[0m \t 3.8511165079641128\n",
            "38     \t [0.93771621 0.40085258 0.90840629]. \t  2.710212038381022 \t 3.8511165079641128\n",
            "39     \t [0.14965119 0.83004101 0.03678273]. \t  0.0020265420497056937 \t 3.8511165079641128\n",
            "40     \t [0.65825822 0.03070688 0.99081436]. \t  0.13820090658858034 \t 3.8511165079641128\n",
            "41     \t [0.81284622 0.54746547 0.93318464]. \t  3.1695915074968855 \t 3.8511165079641128\n",
            "42     \t [0.06732819 0.25070027 0.34887756]. \t  0.5420588033558534 \t 3.8511165079641128\n",
            "43     \t [0.53629411 0.64714885 0.64894173]. \t  1.7993921357365534 \t 3.8511165079641128\n",
            "44     \t [0.76632994 0.38500206 0.54507039]. \t  0.42354185795531757 \t 3.8511165079641128\n",
            "45     \t [0.86768148 0.71598166 0.27425688]. \t  0.05270532364446077 \t 3.8511165079641128\n",
            "46     \t [0.23880265 0.48264459 0.03340302]. \t  0.04854418552914468 \t 3.8511165079641128\n",
            "47     \t [0.57775579 0.25342925 0.84516579]. \t  1.6930206158719372 \t 3.8511165079641128\n",
            "48     \t [0.64910131 0.22567323 0.61880267]. \t  0.5435876480199389 \t 3.8511165079641128\n",
            "49     \t [0.01604554 0.33670665 0.18461822]. \t  0.3487165227529128 \t 3.8511165079641128\n",
            "50     \t [0.79408129 0.56766075 0.71696715]. \t  2.3064305635288314 \t 3.8511165079641128\n",
            "51     \t [0.7402549  0.3655645  0.54355901]. \t  0.406519646040674 \t 3.8511165079641128\n",
            "52     \t [0.3419478  0.37953654 0.70966463]. \t  1.978436778247392 \t 3.8511165079641128\n",
            "53     \t [0.88811151 0.16272661 0.61333689]. \t  0.3545873365654224 \t 3.8511165079641128\n",
            "54     \t [0.12063825 0.28654585 0.99348593]. \t  1.0168883573826288 \t 3.8511165079641128\n",
            "55     \t [0.60034281 0.33793455 0.11347122]. \t  0.2572321192658406 \t 3.8511165079641128\n",
            "56     \t [0.39737068 0.82353818 0.94749653]. \t  1.5740956107911792 \t 3.8511165079641128\n",
            "57     \t [0.41435179 0.284656   0.94117003]. \t  1.4810304390715325 \t 3.8511165079641128\n",
            "58     \t [0.28734283 0.70631795 0.44272901]. \t  1.4552116958940275 \t 3.8511165079641128\n",
            "59     \t [0.64582064 0.27048736 0.60267387]. \t  0.5685328343977809 \t 3.8511165079641128\n",
            "60     \t [0.00961796 0.99646683 0.43966302]. \t  1.6845678787749219 \t 3.8511165079641128\n",
            "61     \t [0.79067284 0.4889186  0.34277043]. \t  0.1720656681141432 \t 3.8511165079641128\n",
            "62     \t [0.73614964 0.08873081 0.08123764]. \t  0.23429503064823837 \t 3.8511165079641128\n",
            "63     \t [0.4074256  0.8972777  0.15489097]. \t  0.020428079250579424 \t 3.8511165079641128\n",
            "64     \t [0.37505358 0.98290616 0.57097691]. \t  2.1753048118193075 \t 3.8511165079641128\n",
            "65     \t [0.95340121 0.51222226 0.23034747]. \t  0.07641901044959246 \t 3.8511165079641128\n",
            "66     \t [0.66780369 0.70433721 0.20511346]. \t  0.04426141336992313 \t 3.8511165079641128\n",
            "67     \t [0.04098637 0.00125381 0.04984295]. \t  0.1533279300561005 \t 3.8511165079641128\n",
            "68     \t [0.00668649 0.08940948 0.4725365 ]. \t  0.21997486144380612 \t 3.8511165079641128\n",
            "69     \t [0.50161296 0.95814489 0.13143229]. \t  0.008605766789104212 \t 3.8511165079641128\n",
            "70     \t [0.89116531 0.99014832 0.88225675]. \t  0.5995239394899143 \t 3.8511165079641128\n",
            "71     \t [0.52987778 0.82918509 0.07807094]. \t  0.003878824742720693 \t 3.8511165079641128\n",
            "72     \t [0.46976662 0.07514661 0.19917063]. \t  0.8292924932649325 \t 3.8511165079641128\n",
            "73     \t [0.69407614 0.03869317 0.24545797]. \t  0.67522371915888 \t 3.8511165079641128\n",
            "74     \t [0.8785674  0.28201439 0.88658166]. \t  1.7952803792715935 \t 3.8511165079641128\n",
            "75     \t [0.11661158 0.57658081 0.89141671]. \t  3.6830251509630694 \t 3.8511165079641128\n",
            "76     \t [0.59244815 0.46942989 0.82981494]. \t  3.5506139509376005 \t 3.8511165079641128\n",
            "77     \t [0.15000929 0.83777228 0.81145404]. \t  1.947239797927471 \t 3.8511165079641128\n",
            "78     \t [0.03822197 0.16401539 0.05399344]. \t  0.17994921078638834 \t 3.8511165079641128\n",
            "79     \t [0.47399276 0.3863694  0.51186274]. \t  0.4426504618250182 \t 3.8511165079641128\n",
            "80     \t [0.07164216 0.1899446  0.51708782]. \t  0.2455051318790618 \t 3.8511165079641128\n",
            "81     \t [0.38550927 0.95537209 0.6460422 ]. \t  1.8994677417985926 \t 3.8511165079641128\n",
            "82     \t [0.62526033 0.68415627 0.34354801]. \t  0.2776421583690714 \t 3.8511165079641128\n",
            "83     \t [0.26733556 0.17048883 0.76671184]. \t  0.9703752437022648 \t 3.8511165079641128\n",
            "84     \t [0.0319908  0.90880231 0.79468209]. \t  1.423155602228559 \t 3.8511165079641128\n",
            "85     \t [0.89718587 0.84522385 0.93800549]. \t  1.412830789733427 \t 3.8511165079641128\n",
            "86     \t [0.45140418 0.1109365  0.8919736 ]. \t  0.5691786665236267 \t 3.8511165079641128\n",
            "87     \t [0.88238899 0.29590736 0.9282799 ]. \t  1.6420849187800834 \t 3.8511165079641128\n",
            "88     \t [0.51218717 0.00332253 0.90375347]. \t  0.1970479087957312 \t 3.8511165079641128\n",
            "89     \t [0.93373041 0.30008258 0.2868365 ]. \t  0.2738027942787708 \t 3.8511165079641128\n",
            "90     \t [0.82243595 0.81083886 0.19101491]. \t  0.015539666017811682 \t 3.8511165079641128\n",
            "91     \t [0.41487269 0.86355672 0.87969103]. \t  1.5871950857407389 \t 3.8511165079641128\n",
            "92     \t [0.85695397 0.31416628 0.88882694]. \t  2.089645940612437 \t 3.8511165079641128\n",
            "93     \t [0.30654317 0.68735427 0.25532736]. \t  0.1666041551928666 \t 3.8511165079641128\n",
            "94     \t [0.67643792 0.00669018 0.92556499]. \t  0.17923468057010708 \t 3.8511165079641128\n",
            "95     \t [0.41761799 0.16804838 0.19966897]. \t  0.8437008962970893 \t 3.8511165079641128\n",
            "96     \t [0.79041122 0.13903144 0.95899516]. \t  0.4865923987032969 \t 3.8511165079641128\n",
            "97     \t [0.27664727 0.08700359 0.81163344]. \t  0.5498838079004507 \t 3.8511165079641128\n",
            "98     \t [0.29130459 0.70467093 0.66824017]. \t  2.401895065242285 \t 3.8511165079641128\n",
            "99     \t [0.60433714 0.04489796 0.1987617 ]. \t  0.6982724795764844 \t 3.8511165079641128\n",
            "100    \t [0.92270353 0.34908719 0.75873467]. \t  2.179299016401862 \t 3.8511165079641128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjKllHqtdwW",
        "outputId": "a393e2a7-8cc6-46be-bc91-0d2c082789a0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_loser_12 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_12 = dGPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.19151945 0.62210877 0.43772774]. \t  1.1006281843679786 \t 1.6482992955272024\n",
            "init   \t [0.78535858 0.77997581 0.27259261]. \t  0.07153771680480671 \t 1.6482992955272024\n",
            "init   \t [0.27646426 0.80187218 0.95813935]. \t  1.6482992955272024 \t 1.6482992955272024\n",
            "init   \t [0.87593263 0.35781727 0.50099513]. \t  0.2282556248207173 \t 1.6482992955272024\n",
            "init   \t [0.68346294 0.71270203 0.37025075]. \t  0.33032494760932407 \t 1.6482992955272024\n",
            "1      \t [0.01398415 0.60897714 0.98856638]. \t  \u001b[92m2.228026380269567\u001b[0m \t 2.228026380269567\n",
            "2      \t [0.03494356 0.11089546 0.89536938]. \t  0.5535687532914756 \t 2.228026380269567\n",
            "3      \t [0.00761147 0.89633798 0.99755295]. \t  0.7296905183544732 \t 2.228026380269567\n",
            "4      \t [0.42621529 0.52201143 0.94674436]. \t  \u001b[92m2.981393016278439\u001b[0m \t 2.981393016278439\n",
            "5      \t [0.62997572 0.44202894 0.99618939]. \t  1.8391155649095643 \t 2.981393016278439\n",
            "6      \t [0.32429872 0.48995801 0.97777927]. \t  2.3563074830909687 \t 2.981393016278439\n",
            "7      \t [0.99206005 0.87479422 0.59276177]. \t  0.4186473279209974 \t 2.981393016278439\n",
            "8      \t [0.05476847 0.53285389 0.64325851]. \t  1.8981988576929787 \t 2.981393016278439\n",
            "9      \t [1.36465618e-14 4.78754555e-14 1.19124674e-14]. \t  0.06797411659015497 \t 2.981393016278439\n",
            "10     \t [0.0322103  0.83671651 0.03102825]. \t  0.0015280586580840726 \t 2.981393016278439\n",
            "11     \t [0.7698327  0.69612096 0.89058708]. \t  \u001b[92m3.029485017097235\u001b[0m \t 3.029485017097235\n",
            "12     \t [0.43926768 0.5817004  0.86293835]. \t  \u001b[92m3.8177647120619196\u001b[0m \t 3.8177647120619196\n",
            "13     \t [0.94895416 0.92227183 0.04577195]. \t  0.00027661898739457985 \t 3.8177647120619196\n",
            "14     \t [0.96716075 0.55712453 0.89266817]. \t  3.560435248735423 \t 3.8177647120619196\n",
            "15     \t [0.9867346  0.37982781 0.09809661]. \t  0.06760397508437056 \t 3.8177647120619196\n",
            "16     \t [0.90017763 0.51696429 0.78507178]. \t  3.237173868723097 \t 3.8177647120619196\n",
            "17     \t [0.27326543 0.62896984 0.83586619]. \t  3.6455175745982786 \t 3.8177647120619196\n",
            "18     \t [0.41927477 0.56180083 0.80926929]. \t  3.6659304271249677 \t 3.8177647120619196\n",
            "19     \t [0.35657319 0.69697249 0.11065344]. \t  0.021505444175504602 \t 3.8177647120619196\n",
            "20     \t [0.19309422 0.2903732  0.41339479]. \t  0.4298102396014697 \t 3.8177647120619196\n",
            "21     \t [0.20463233 0.04324562 0.65697062]. \t  0.23108242657891664 \t 3.8177647120619196\n",
            "22     \t [1.57842901e-02 8.20918142e-01 7.03733502e-04]. \t  0.0008601507897846089 \t 3.8177647120619196\n",
            "23     \t [0.60248017 0.3231838  0.54524734]. \t  0.40827501689705414 \t 3.8177647120619196\n",
            "24     \t [0.95934255 0.13483385 0.58978918]. \t  0.23503660187818645 \t 3.8177647120619196\n",
            "25     \t [0.944549   0.01940069 0.0303243 ]. \t  0.062405931694092066 \t 3.8177647120619196\n",
            "26     \t [0.55017482 0.2753227  0.00227453]. \t  0.08574825192552826 \t 3.8177647120619196\n",
            "27     \t [0.6303606  0.91787344 0.54542479]. \t  1.3455171557446375 \t 3.8177647120619196\n",
            "28     \t [0.41436159 0.5956259  0.79046636]. \t  3.439291049398898 \t 3.8177647120619196\n",
            "29     \t [0.43817054 0.11696402 0.3778372 ]. \t  0.6896644481940144 \t 3.8177647120619196\n",
            "30     \t [0.06346045 0.63738689 0.33134359]. \t  0.4292681917320401 \t 3.8177647120619196\n",
            "31     \t [0.02637106 0.98958525 0.7432182 ]. \t  1.225135757871817 \t 3.8177647120619196\n",
            "32     \t [0.82033973 0.79716428 0.08591558]. \t  0.0028307305418085256 \t 3.8177647120619196\n",
            "33     \t [0.45938261 0.75250997 0.30932276]. \t  0.3116825248599445 \t 3.8177647120619196\n",
            "34     \t [0.14857774 0.26493928 0.00777946]. \t  0.09209710617458892 \t 3.8177647120619196\n",
            "35     \t [0.60832238 0.55700038 0.81874419]. \t  3.683568939368378 \t 3.8177647120619196\n",
            "36     \t [0.17079159 0.75188383 0.20204625]. \t  0.07527308098946896 \t 3.8177647120619196\n",
            "37     \t [0.88857253 0.13831501 0.99785855]. \t  0.3461011837569411 \t 3.8177647120619196\n",
            "38     \t [0.43495971 0.82343517 0.56848465]. \t  2.260262249034812 \t 3.8177647120619196\n",
            "39     \t [0.05922251 0.19985131 0.18276069]. \t  0.5656147149471785 \t 3.8177647120619196\n",
            "40     \t [0.17134421 0.86725833 0.08159063]. \t  0.004730173871140647 \t 3.8177647120619196\n",
            "41     \t [0.42662425 0.69689899 0.79793902]. \t  2.9774239054093257 \t 3.8177647120619196\n",
            "42     \t [0.18872382 0.51514048 0.22488052]. \t  0.20745664175205186 \t 3.8177647120619196\n",
            "43     \t [0.06999489 0.27249309 0.77138987]. \t  1.7229732668908813 \t 3.8177647120619196\n",
            "44     \t [0.42434339 0.83062154 0.07494507]. \t  0.004198432986741109 \t 3.8177647120619196\n",
            "45     \t [0.52873134 0.48130385 0.74748806]. \t  2.8460017521193435 \t 3.8177647120619196\n",
            "46     \t [0.85544522 0.01913392 0.08899941]. \t  0.17209892135155425 \t 3.8177647120619196\n",
            "47     \t [0.94509336 0.52652727 0.52765881]. \t  0.355756562588529 \t 3.8177647120619196\n",
            "48     \t [0.21204997 0.52165853 0.10561627]. \t  0.08444321506372225 \t 3.8177647120619196\n",
            "49     \t [0.49512097 0.96958735 0.07743071]. \t  0.0021082109387776248 \t 3.8177647120619196\n",
            "50     \t [0.86690057 0.3390747  0.56690559]. \t  0.4488104603551687 \t 3.8177647120619196\n",
            "51     \t [0.43918792 0.76714066 0.46667066]. \t  1.5678915085471918 \t 3.8177647120619196\n",
            "52     \t [0.98846208 0.47663677 0.92545313]. \t  3.0092802227686923 \t 3.8177647120619196\n",
            "53     \t [0.47038939 0.8521396  0.9974098 ]. \t  0.9628504485489497 \t 3.8177647120619196\n",
            "54     \t [0.77998072 0.34352677 0.73868039]. \t  1.9942376878878827 \t 3.8177647120619196\n",
            "55     \t [0.68813783 0.09659201 0.92520935]. \t  0.4257988157189452 \t 3.8177647120619196\n",
            "56     \t [0.50350165 0.82297981 0.52319174]. \t  1.8476111133622661 \t 3.8177647120619196\n",
            "57     \t [0.70155505 0.48281139 0.71915125]. \t  2.381860570908207 \t 3.8177647120619196\n",
            "58     \t [0.55925288 0.44245054 0.32789276]. \t  0.3357905619753193 \t 3.8177647120619196\n",
            "59     \t [0.49978886 0.66873457 0.25703783]. \t  0.13282998981277314 \t 3.8177647120619196\n",
            "60     \t [0.84105879 0.5629506  0.34627985]. \t  0.1243490147263995 \t 3.8177647120619196\n",
            "61     \t [0.12603402 0.63989199 0.71126356]. \t  2.6777207524288986 \t 3.8177647120619196\n",
            "62     \t [0.39964433 0.99658814 0.45459576]. \t  1.484373314649264 \t 3.8177647120619196\n",
            "63     \t [0.44241317 0.17151992 0.84918487]. \t  1.0160410107130888 \t 3.8177647120619196\n",
            "64     \t [0.18133544 0.67887524 0.60018155]. \t  2.3929382970522894 \t 3.8177647120619196\n",
            "65     \t [0.81359004 0.76129887 0.49975164]. \t  0.6095207961373732 \t 3.8177647120619196\n",
            "66     \t [0.06363343 0.5422766  0.31167274]. \t  0.2877952793680176 \t 3.8177647120619196\n",
            "67     \t [0.63145594 0.62557868 0.9352136 ]. \t  3.0671100960742277 \t 3.8177647120619196\n",
            "68     \t [0.50259193 0.833834   0.27032893]. \t  0.16973072351208715 \t 3.8177647120619196\n",
            "69     \t [0.86423622 0.32921701 0.78258116]. \t  2.2149879950678066 \t 3.8177647120619196\n",
            "70     \t [0.09070466 0.39075461 0.30718995]. \t  0.40511813609791275 \t 3.8177647120619196\n",
            "71     \t [0.92027701 0.90035824 0.55900314]. \t  0.4813358852570495 \t 3.8177647120619196\n",
            "72     \t [0.07396428 0.72569041 0.44195972]. \t  1.6725069055792048 \t 3.8177647120619196\n",
            "73     \t [0.38433519 0.8620133  0.47753883]. \t  2.017900529756957 \t 3.8177647120619196\n",
            "74     \t [0.70212302 0.08661498 0.22402478]. \t  0.671412664775161 \t 3.8177647120619196\n",
            "75     \t [0.43888827 0.73929972 0.25561194]. \t  0.14414037023096218 \t 3.8177647120619196\n",
            "76     \t [0.75182032 0.06531014 0.26174813]. \t  0.626724796742964 \t 3.8177647120619196\n",
            "77     \t [0.09764391 0.93899129 0.53823406]. \t  2.882858654763731 \t 3.8177647120619196\n",
            "78     \t [0.77977615 0.7150441  0.1566065 ]. \t  0.01689399623485657 \t 3.8177647120619196\n",
            "79     \t [0.1475432  0.68460258 0.02741935]. \t  0.006626606260917234 \t 3.8177647120619196\n",
            "80     \t [0.10978135 0.16938244 0.0893402 ]. \t  0.30763139312569293 \t 3.8177647120619196\n",
            "81     \t [0.91764548 0.85015027 0.7222717 ]. \t  0.9821056508060743 \t 3.8177647120619196\n",
            "82     \t [0.36101895 0.51007173 0.34447252]. \t  0.35858889577856073 \t 3.8177647120619196\n",
            "83     \t [0.55837414 0.61348561 0.73980716]. \t  2.7160211996294548 \t 3.8177647120619196\n",
            "84     \t [0.00460694 0.52167147 0.3961229 ]. \t  0.49170667493883247 \t 3.8177647120619196\n",
            "85     \t [0.31677985 0.2602402  0.8693305 ]. \t  1.7042213605703371 \t 3.8177647120619196\n",
            "86     \t [0.18255073 0.2406186  0.76028492]. \t  1.433703322006356 \t 3.8177647120619196\n",
            "87     \t [0.41735198 0.83179659 0.40232147]. \t  1.1129711832789138 \t 3.8177647120619196\n",
            "88     \t [0.79543712 0.53168433 0.72799723]. \t  2.503079766226982 \t 3.8177647120619196\n",
            "89     \t [0.53838599 0.95932856 0.43034474]. \t  1.0109630170128627 \t 3.8177647120619196\n",
            "90     \t [0.25203354 0.22257782 0.88271513]. \t  1.3217363185741255 \t 3.8177647120619196\n",
            "91     \t [0.60582574 0.32749757 0.65001159]. \t  1.0835379561877838 \t 3.8177647120619196\n",
            "92     \t [0.90664635 0.39572821 0.15957417]. \t  0.13677434322670798 \t 3.8177647120619196\n",
            "93     \t [0.0285213  0.76993271 0.50713399]. \t  2.540627034382265 \t 3.8177647120619196\n",
            "94     \t [0.55089586 0.71819762 0.38418863]. \t  0.5712272229635553 \t 3.8177647120619196\n",
            "95     \t [0.90917994 0.50707355 0.56146547]. \t  0.5385477977482964 \t 3.8177647120619196\n",
            "96     \t [0.41922349 0.57565327 0.70638927]. \t  2.4807373000665214 \t 3.8177647120619196\n",
            "97     \t [0.92676762 0.83831109 0.71541354]. \t  0.9938525433078214 \t 3.8177647120619196\n",
            "98     \t [0.67512551 0.55953668 0.47623847]. \t  0.47444785276726686 \t 3.8177647120619196\n",
            "99     \t [0.14963312 0.34455413 0.24765072]. \t  0.5208219028417973 \t 3.8177647120619196\n",
            "100    \t [0.72125536 0.92491891 0.11602707]. \t  0.0034592068050073 \t 3.8177647120619196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoONg4VEtdy-",
        "outputId": "190b7c90-0f14-4411-82a8-c3665971dfc2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_loser_13 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_13 = dGPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.66064431 0.41360065 0.66810256]. \t  1.5457680899590245 \t 2.6697919207500047\n",
            "init   \t [0.22994342 0.80767834 0.63681846]. \t  2.6697919207500047 \t 2.6697919207500047\n",
            "init   \t [0.17219385 0.26038587 0.91531999]. \t  1.4685599870430508 \t 2.6697919207500047\n",
            "init   \t [0.46281551 0.12019095 0.88088551]. \t  0.639297028201682 \t 2.6697919207500047\n",
            "init   \t [0.22621895 0.81144033 0.44587892]. \t  1.960293029067079 \t 2.6697919207500047\n",
            "1      \t [0.54200533 0.91610447 0.94072107]. \t  0.9345749070922738 \t 2.6697919207500047\n",
            "2      \t [0.02210984 0.92408402 0.84182838]. \t  1.1986876793966825 \t 2.6697919207500047\n",
            "3      \t [0.2551603  0.64940006 0.63764251]. \t  2.2608836438455646 \t 2.6697919207500047\n",
            "4      \t [0.66842278 0.8625381  0.47685558]. \t  0.9979464612823808 \t 2.6697919207500047\n",
            "5      \t [0.28295395 0.99213626 0.55559161]. \t  2.407164166226089 \t 2.6697919207500047\n",
            "6      \t [0.88103078 0.40276708 0.88651999]. \t  \u001b[92m2.915655348161583\u001b[0m \t 2.915655348161583\n",
            "7      \t [0.85186124 0.38309496 0.96481362]. \t  1.9341935747459023 \t 2.915655348161583\n",
            "8      \t [0.99269926 0.32021245 0.76400007]. \t  1.9945111951258503 \t 2.915655348161583\n",
            "9      \t [0.9725296  0.74703252 0.78270881]. \t  2.1046312358842396 \t 2.915655348161583\n",
            "10     \t [0.64921845 0.58807206 0.82377664]. \t  \u001b[92m3.6487778595796403\u001b[0m \t 3.6487778595796403\n",
            "11     \t [0.67136868 0.65587793 0.78364563]. \t  2.96634855335022 \t 3.6487778595796403\n",
            "12     \t [0.93628178 0.98787552 0.00269434]. \t  5.9873549531252935e-05 \t 3.6487778595796403\n",
            "13     \t [0.20452643 0.95991774 0.00283118]. \t  0.0003842936989728932 \t 3.6487778595796403\n",
            "14     \t [0.28742108 0.58414624 0.97025443]. \t  2.6197401819259363 \t 3.6487778595796403\n",
            "15     \t [0.00918113 0.97131325 0.42350288]. \t  1.5801725863686558 \t 3.6487778595796403\n",
            "16     \t [0.48726769 0.41055569 0.89191979]. \t  3.0159354059023435 \t 3.6487778595796403\n",
            "17     \t [0.96241204 0.4977143  0.74078178]. \t  2.599080513436684 \t 3.6487778595796403\n",
            "18     \t [0.74037132 0.56122025 0.89563392]. \t  3.6190597808260043 \t 3.6487778595796403\n",
            "19     \t [0.43409753 0.96598791 0.28398903]. \t  0.22325574543686724 \t 3.6487778595796403\n",
            "20     \t [6.20647375e-16 8.28149418e-14 1.08553013e-14]. \t  0.06797411659015741 \t 3.6487778595796403\n",
            "21     \t [0.07733708 0.51291034 0.03301615]. \t  0.03137606726735763 \t 3.6487778595796403\n",
            "22     \t [0.00463129 0.16486818 0.46556882]. \t  0.2530080813283786 \t 3.6487778595796403\n",
            "23     \t [0.00320505 0.76662497 0.0974604 ]. \t  0.009031250496607012 \t 3.6487778595796403\n",
            "24     \t [0.94297442 0.99829129 0.44240974]. \t  0.22086974728088565 \t 3.6487778595796403\n",
            "25     \t [0.21806645 0.88497201 0.6999076 ]. \t  2.063426247291155 \t 3.6487778595796403\n",
            "26     \t [0.20999599 0.09189476 0.71383494]. \t  0.4644809892599904 \t 3.6487778595796403\n",
            "27     \t [0.74413859 0.50011464 0.86989513]. \t  \u001b[92m3.6589177774387114\u001b[0m \t 3.6589177774387114\n",
            "28     \t [0.73999113 0.51658583 0.89231812]. \t  3.5895994430253797 \t 3.6589177774387114\n",
            "29     \t [2.42898347e-04 6.38186582e-01 9.27581282e-01]. \t  3.1001233077485315 \t 3.6589177774387114\n",
            "30     \t [0.59203947 0.80806188 0.30643635]. \t  0.23209380349430725 \t 3.6589177774387114\n",
            "31     \t [0.99851265 0.8170673  0.06665683]. \t  0.000890527374873991 \t 3.6589177774387114\n",
            "32     \t [0.68939258 0.52375207 0.88165166]. \t  \u001b[92m3.691592425297216\u001b[0m \t 3.691592425297216\n",
            "33     \t [0.71862106 0.57869395 0.48791097]. \t  0.4877635725523503 \t 3.691592425297216\n",
            "34     \t [0.71015982 0.06240761 0.63233308]. \t  0.22214275190933627 \t 3.691592425297216\n",
            "35     \t [0.29537222 0.35230617 0.91184422]. \t  2.3294800034017893 \t 3.691592425297216\n",
            "36     \t [0.12149322 0.05671003 0.94459882]. \t  0.26115399667729156 \t 3.691592425297216\n",
            "37     \t [0.99370289 0.94780616 0.6772205 ]. \t  0.4206480841228158 \t 3.691592425297216\n",
            "38     \t [0.79155526 0.75895341 0.35415099]. \t  0.20442672556046407 \t 3.691592425297216\n",
            "39     \t [0.41819026 0.01110224 0.42672782]. \t  0.42020265486539904 \t 3.691592425297216\n",
            "40     \t [0.78397085 0.65300818 0.7031766 ]. \t  1.901811970012256 \t 3.691592425297216\n",
            "41     \t [0.77590574 0.03251059 0.70689557]. \t  0.27454686586139604 \t 3.691592425297216\n",
            "42     \t [0.60493648 0.51070951 0.91896704]. \t  3.3321496123694563 \t 3.691592425297216\n",
            "43     \t [0.04359117 0.24712361 0.53820461]. \t  0.3207940100321773 \t 3.691592425297216\n",
            "44     \t [0.92602388 0.994073   0.32371181]. \t  0.07090802659903218 \t 3.691592425297216\n",
            "45     \t [0.57344005 0.61197114 0.18922437]. \t  0.07785242306504275 \t 3.691592425297216\n",
            "46     \t [0.12012265 0.14761798 0.26838165]. \t  0.8242627741310308 \t 3.691592425297216\n",
            "47     \t [0.27362221 0.36267462 0.50258601]. \t  0.44167122163239614 \t 3.691592425297216\n",
            "48     \t [0.46186667 0.12699026 0.51178451]. \t  0.23846173888788497 \t 3.691592425297216\n",
            "49     \t [0.76667116 0.6392036  0.80705852]. \t  3.2443713047496106 \t 3.691592425297216\n",
            "50     \t [0.13767661 0.66241837 0.82792657]. \t  3.425531063299217 \t 3.691592425297216\n",
            "51     \t [0.95793867 0.66615194 0.60097035]. \t  0.7192507904766757 \t 3.691592425297216\n",
            "52     \t [0.05145473 0.64851336 0.21348459]. \t  0.09474949345676345 \t 3.691592425297216\n",
            "53     \t [0.48623021 0.77377589 0.411413  ]. \t  0.9734191014544429 \t 3.691592425297216\n",
            "54     \t [0.78882542 0.13270961 0.90543571]. \t  0.6333344930577843 \t 3.691592425297216\n",
            "55     \t [0.24220402 0.25654683 0.99013943]. \t  0.8858897160755633 \t 3.691592425297216\n",
            "56     \t [0.22829701 0.05339116 0.54151275]. \t  0.16354623570708623 \t 3.691592425297216\n",
            "57     \t [0.76016192 0.74881222 0.56770784]. \t  0.9373503703113361 \t 3.691592425297216\n",
            "58     \t [0.2795376  0.58991046 0.2037006 ]. \t  0.12301042643614361 \t 3.691592425297216\n",
            "59     \t [0.31980344 0.82750827 0.11581836]. \t  0.011157555616638023 \t 3.691592425297216\n",
            "60     \t [0.20118285 0.96464589 0.24263923]. \t  0.14556622456810533 \t 3.691592425297216\n",
            "61     \t [0.51970053 0.84549219 0.4922885 ]. \t  1.6293240662063382 \t 3.691592425297216\n",
            "62     \t [0.82379254 0.81675594 0.85550943]. \t  1.9278312884785196 \t 3.691592425297216\n",
            "63     \t [0.40139966 0.0505134  0.44444517]. \t  0.38492138667391107 \t 3.691592425297216\n",
            "64     \t [0.41306454 0.59975919 0.95354358]. \t  2.885245533239425 \t 3.691592425297216\n",
            "65     \t [0.90762895 0.09043539 0.35045986]. \t  0.33954753185910946 \t 3.691592425297216\n",
            "66     \t [0.56437488 0.00299382 0.07478562]. \t  0.2575720757158853 \t 3.691592425297216\n",
            "67     \t [0.04035773 0.82036326 0.80315336]. \t  2.0799583721374217 \t 3.691592425297216\n",
            "68     \t [0.17023444 0.85258575 0.91493466]. \t  1.5565545405274104 \t 3.691592425297216\n",
            "69     \t [0.87734285 0.27734693 0.62186276]. \t  0.6661400065856827 \t 3.691592425297216\n",
            "70     \t [0.85442665 0.36765057 0.29538047]. \t  0.2636416785367353 \t 3.691592425297216\n",
            "71     \t [0.65398105 0.18685371 0.2556733 ]. \t  0.7441875730102886 \t 3.691592425297216\n",
            "72     \t [0.16483906 0.02722539 0.07838903]. \t  0.27911575215119816 \t 3.691592425297216\n",
            "73     \t [0.88555469 0.94382983 0.28432556]. \t  0.05271556107938427 \t 3.691592425297216\n",
            "74     \t [0.68988585 0.09996215 0.53871476]. \t  0.1713866911101003 \t 3.691592425297216\n",
            "75     \t [0.0180464  0.85478503 0.24884179]. \t  0.17919690815782152 \t 3.691592425297216\n",
            "76     \t [0.78321123 0.04993389 0.03396936]. \t  0.11155547807075329 \t 3.691592425297216\n",
            "77     \t [0.83488428 0.17038079 0.32578126]. \t  0.4593399119267525 \t 3.691592425297216\n",
            "78     \t [0.00337352 0.81377532 0.47141789]. \t  2.3007426133611486 \t 3.691592425297216\n",
            "79     \t [0.44659059 0.34840865 0.05190856]. \t  0.14299750465571026 \t 3.691592425297216\n",
            "80     \t [0.82874866 0.46665737 0.88077643]. \t  3.426233378521397 \t 3.691592425297216\n",
            "81     \t [0.44960303 0.01595051 0.18324486]. \t  0.716359984072404 \t 3.691592425297216\n",
            "82     \t [0.24540929 0.96089703 0.58222289]. \t  2.6288618583053487 \t 3.691592425297216\n",
            "83     \t [0.60404809 0.72598803 0.35607217]. \t  0.37369385522183546 \t 3.691592425297216\n",
            "84     \t [0.48995829 0.47442484 0.90045138]. \t  3.391988206652351 \t 3.691592425297216\n",
            "85     \t [0.86483266 0.77760985 0.20264433]. \t  0.017360610657446037 \t 3.691592425297216\n",
            "86     \t [0.96077586 0.70534906 0.83842714]. \t  2.8847770418582708 \t 3.691592425297216\n",
            "87     \t [0.46111202 0.66460214 0.78780807]. \t  3.0911449545854603 \t 3.691592425297216\n",
            "88     \t [0.17321434 0.20357407 0.55873031]. \t  0.3164822550587725 \t 3.691592425297216\n",
            "89     \t [0.03865646 0.34254306 0.56508583]. \t  0.5978670329887741 \t 3.691592425297216\n",
            "90     \t [0.79060194 0.25759698 0.43351173]. \t  0.24940990598258378 \t 3.691592425297216\n",
            "91     \t [0.32236939 0.42199787 0.28337111]. \t  0.42707895490724906 \t 3.691592425297216\n",
            "92     \t [0.67223202 0.36264268 0.17064459]. \t  0.31461328905573827 \t 3.691592425297216\n",
            "93     \t [0.62734209 0.15873131 0.39466884]. \t  0.505366693689942 \t 3.691592425297216\n",
            "94     \t [0.86554748 0.19661497 0.24919996]. \t  0.4438912830916337 \t 3.691592425297216\n",
            "95     \t [0.98606367 0.93178489 0.99407399]. \t  0.5702372182084068 \t 3.691592425297216\n",
            "96     \t [0.13527758 0.02125589 0.12677246]. \t  0.42834497445761877 \t 3.691592425297216\n",
            "97     \t [0.6710944  0.88514049 0.09061597]. \t  0.0026314471908069825 \t 3.691592425297216\n",
            "98     \t [0.13400302 0.73596897 0.96113073]. \t  2.091712838707545 \t 3.691592425297216\n",
            "99     \t [0.8586582  0.53204264 0.85304921]. \t  \u001b[92m3.721611532889103\u001b[0m \t 3.721611532889103\n",
            "100    \t [0.86204299 0.60518289 0.98541934]. \t  2.265846033509685 \t 3.721611532889103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQHSe2Dtd1m",
        "outputId": "8e19d76e-e1e8-4c25-eb8d-e181d2597f2c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_loser_14 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_14 = dGPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.64755105 0.50714969 0.52834138]. \t  0.5963212059988954 \t 2.610000357863649\n",
            "init   \t [0.8962852  0.69999119 0.7142971 ]. \t  1.7197848290620104 \t 2.610000357863649\n",
            "init   \t [0.71733838 0.22281946 0.17515452]. \t  0.48166052848103497 \t 2.610000357863649\n",
            "init   \t [0.45684149 0.92873843 0.00988589]. \t  0.0004588015757462679 \t 2.610000357863649\n",
            "init   \t [0.08992219 0.85020027 0.48562106]. \t  2.610000357863649 \t 2.610000357863649\n",
            "1      \t [0.07426471 0.99752623 0.88021278]. \t  0.6630933245090485 \t 2.610000357863649\n",
            "2      \t [0.00180211 0.46271645 0.11240184]. \t  0.09986378420438345 \t 2.610000357863649\n",
            "3      \t [0.04945322 0.96746298 0.34336512]. \t  0.71192198606886 \t 2.610000357863649\n",
            "4      \t [0.95164465 0.9953009  0.00385032]. \t  5.429819046242939e-05 \t 2.610000357863649\n",
            "5      \t [0.9979422  0.99655151 0.98186954]. \t  0.37591226277898976 \t 2.610000357863649\n",
            "6      \t [0.95837713 0.62910703 0.12515201]. \t  0.014715862521621976 \t 2.610000357863649\n",
            "7      \t [0.05380284 0.69099212 0.61559705]. \t  2.477025978003247 \t 2.610000357863649\n",
            "8      \t [0.4099993  0.92486099 0.52611019]. \t  2.2038453653786894 \t 2.610000357863649\n",
            "9      \t [0.12502002 0.82889781 0.49304721]. \t  \u001b[92m2.6590433374920455\u001b[0m \t 2.6590433374920455\n",
            "10     \t [0.0432031  0.81394199 0.52604994]. \t  \u001b[92m2.863126326791752\u001b[0m \t 2.863126326791752\n",
            "11     \t [0.93728378 0.01172871 0.05842301]. \t  0.0917317165535674 \t 2.863126326791752\n",
            "12     \t [0.04830445 0.79599636 0.56926789]. \t  \u001b[92m2.94888161804253\u001b[0m \t 2.94888161804253\n",
            "13     \t [0.96219208 0.92550704 0.48515165]. \t  0.2980789003451671 \t 2.94888161804253\n",
            "14     \t [0.49305204 0.0469595  0.        ]. \t  0.10658903029135289 \t 2.94888161804253\n",
            "15     \t [0.9725237  0.48844653 0.87837032]. \t  \u001b[92m3.4955070864224314\u001b[0m \t 3.4955070864224314\n",
            "16     \t [0.97765512 0.28908983 0.8882185 ]. \t  1.8331935020921892 \t 3.4955070864224314\n",
            "17     \t [0.82336931 0.69094476 0.99535318]. \t  1.8317234802335283 \t 3.4955070864224314\n",
            "18     \t [0.96873907 0.56322882 0.9383951 ]. \t  3.050598972616365 \t 3.4955070864224314\n",
            "19     \t [0.98414681 0.41566923 0.49281313]. \t  0.19462008725546937 \t 3.4955070864224314\n",
            "20     \t [0.55403896 0.41378808 0.03051479]. \t  0.06960499772290545 \t 3.4955070864224314\n",
            "21     \t [0.82438027 0.50574454 0.9092123 ]. \t  3.376368871912553 \t 3.4955070864224314\n",
            "22     \t [0.50527634 0.4021167  0.905708  ]. \t  2.827435466911807 \t 3.4955070864224314\n",
            "23     \t [0.33973592 0.83048357 0.79975323]. \t  1.9524561078382523 \t 3.4955070864224314\n",
            "24     \t [0.65056615 0.4367664  0.9827399 ]. \t  2.025405392267641 \t 3.4955070864224314\n",
            "25     \t [0.15264283 0.02107091 0.5549017 ]. \t  0.12891552497090242 \t 3.4955070864224314\n",
            "26     \t [0.5351411  0.09445077 0.68338355]. \t  0.40519482656621714 \t 3.4955070864224314\n",
            "27     \t [0.28528739 0.46336557 0.26731631]. \t  0.33819052205087036 \t 3.4955070864224314\n",
            "28     \t [0.14266977 0.41799958 0.07239393]. \t  0.11124283833397586 \t 3.4955070864224314\n",
            "29     \t [0.55257834 0.99265052 0.26703074]. \t  0.1208697033520126 \t 3.4955070864224314\n",
            "30     \t [0.01806789 0.96107533 0.70194425]. \t  1.71026065747257 \t 3.4955070864224314\n",
            "31     \t [0.6096993  0.99899235 0.85987141]. \t  0.6218218838754161 \t 3.4955070864224314\n",
            "32     \t [0.47334064 0.29370186 0.6061317 ]. \t  0.6727834909940495 \t 3.4955070864224314\n",
            "33     \t [0.07255508 0.10238236 0.99487251]. \t  0.2646432290137243 \t 3.4955070864224314\n",
            "34     \t [0.07668192 0.4967238  0.99851593]. \t  1.9951281424451577 \t 3.4955070864224314\n",
            "35     \t [0.16316254 0.77607392 0.10225318]. \t  0.01087632488374639 \t 3.4955070864224314\n",
            "36     \t [0.17653826 0.21738289 0.96215749]. \t  0.8595982659733136 \t 3.4955070864224314\n",
            "37     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.4955070864224314\n",
            "38     \t [0.91865577 0.36275365 0.1912791 ]. \t  0.1862391981836753 \t 3.4955070864224314\n",
            "39     \t [0.41527166 0.64813462 0.16672562]. \t  0.058604953951955356 \t 3.4955070864224314\n",
            "40     \t [0.79917452 0.98775728 0.7286888 ]. \t  0.5596406347526663 \t 3.4955070864224314\n",
            "41     \t [0.39789933 0.57798059 0.91184543]. \t  \u001b[92m3.509310013992248\u001b[0m \t 3.509310013992248\n",
            "42     \t [0.07907219 0.0441213  0.49501061]. \t  0.18632550644606571 \t 3.509310013992248\n",
            "43     \t [0.43329081 0.40589903 0.72027082]. \t  2.2226473633261703 \t 3.509310013992248\n",
            "44     \t [0.12907142 0.51616038 0.51373335]. \t  1.0164915943293173 \t 3.509310013992248\n",
            "45     \t [0.4413456  0.07346512 0.83536804]. \t  0.48157224856520825 \t 3.509310013992248\n",
            "46     \t [0.42542469 0.33852438 0.46777118]. \t  0.35843440517809333 \t 3.509310013992248\n",
            "47     \t [0.45205372 0.86171738 0.8772896 ]. \t  1.6020784611943335 \t 3.509310013992248\n",
            "48     \t [0.06500231 0.74732896 0.95425446]. \t  2.0866199126866483 \t 3.509310013992248\n",
            "49     \t [0.40675235 0.20604412 0.32647895]. \t  0.835147485697324 \t 3.509310013992248\n",
            "50     \t [0.51629496 0.44238532 0.11428872]. \t  0.1618554098435086 \t 3.509310013992248\n",
            "51     \t [0.01841433 0.66213843 0.44030181]. \t  1.3100501508645412 \t 3.509310013992248\n",
            "52     \t [0.42750177 0.08095008 0.22881845]. \t  0.934718699821992 \t 3.509310013992248\n",
            "53     \t [0.99132776 0.56247312 0.87898254]. \t  \u001b[92m3.6264090638744766\u001b[0m \t 3.6264090638744766\n",
            "54     \t [0.19229006 0.27583495 0.78979266]. \t  1.8392142767687603 \t 3.6264090638744766\n",
            "55     \t [0.99607341 0.95060923 0.07884714]. \t  0.00040076247048857424 \t 3.6264090638744766\n",
            "56     \t [0.72236324 0.20397123 0.68392677]. \t  0.809154011084921 \t 3.6264090638744766\n",
            "57     \t [0.82726544 0.56984414 0.91488448]. \t  3.402460992571767 \t 3.6264090638744766\n",
            "58     \t [0.4496368  0.09232231 0.65260425]. \t  0.3286036761854578 \t 3.6264090638744766\n",
            "59     \t [0.66686898 0.36122784 0.22139213]. \t  0.3992469319683874 \t 3.6264090638744766\n",
            "60     \t [0.84343985 0.47811337 0.0260408 ]. \t  0.024123015022092978 \t 3.6264090638744766\n",
            "61     \t [0.50786571 0.30890895 0.09449962]. \t  0.26672971047047095 \t 3.6264090638744766\n",
            "62     \t [0.15524704 0.78149988 0.03254962]. \t  0.002786228877873154 \t 3.6264090638744766\n",
            "63     \t [0.29530089 0.8267771  0.50007408]. \t  2.4621184302121764 \t 3.6264090638744766\n",
            "64     \t [0.73916819 0.92720124 0.92738278]. \t  0.8970108912251867 \t 3.6264090638744766\n",
            "65     \t [0.67444173 0.44494752 0.87215447]. \t  3.372482011479299 \t 3.6264090638744766\n",
            "66     \t [0.56649389 0.50233033 0.51945521]. \t  0.6376902576687437 \t 3.6264090638744766\n",
            "67     \t [0.28076717 0.11678024 0.47315713]. \t  0.3132157828560709 \t 3.6264090638744766\n",
            "68     \t [0.34975716 0.55735162 0.44800429]. \t  0.7645979471726574 \t 3.6264090638744766\n",
            "69     \t [0.79661882 0.26738063 0.53155921]. \t  0.2656598661584797 \t 3.6264090638744766\n",
            "70     \t [0.14360166 0.87285564 0.22607183]. \t  0.11979374408608724 \t 3.6264090638744766\n",
            "71     \t [0.20273741 0.68270988 0.68133879]. \t  2.5067715263777925 \t 3.6264090638744766\n",
            "72     \t [0.10236774 0.30777171 0.08361548]. \t  0.2042348819686146 \t 3.6264090638744766\n",
            "73     \t [0.67858085 0.6219549  0.55325466]. \t  0.9070086533155038 \t 3.6264090638744766\n",
            "74     \t [0.14445184 0.18590658 0.93168239]. \t  0.8378508615508395 \t 3.6264090638744766\n",
            "75     \t [0.33188654 0.9244444  0.23513361]. \t  0.11906325846609561 \t 3.6264090638744766\n",
            "76     \t [0.48097666 0.13009226 0.29282477]. \t  0.9441138008571816 \t 3.6264090638744766\n",
            "77     \t [0.32990587 0.58124315 0.16500403]. \t  0.09588791985837758 \t 3.6264090638744766\n",
            "78     \t [0.55142752 0.00357512 0.70672671]. \t  0.21495598343963285 \t 3.6264090638744766\n",
            "79     \t [0.49708493 0.55071354 0.15142348]. \t  0.10212397980838571 \t 3.6264090638744766\n",
            "80     \t [0.83839049 0.61356518 0.90770868]. \t  3.37999894933908 \t 3.6264090638744766\n",
            "81     \t [0.10514481 0.86268084 0.29104502]. \t  0.3755481141119861 \t 3.6264090638744766\n",
            "82     \t [0.77926204 0.03190372 0.28453967]. \t  0.5564609157951522 \t 3.6264090638744766\n",
            "83     \t [0.0844148  0.96013624 0.30520214]. \t  0.42971674692981515 \t 3.6264090638744766\n",
            "84     \t [0.91312378 0.47041951 0.20526919]. \t  0.1073063230476932 \t 3.6264090638744766\n",
            "85     \t [0.90271173 0.30248265 0.60457918]. \t  0.6061828342528154 \t 3.6264090638744766\n",
            "86     \t [0.56818296 0.13607622 0.98090366]. \t  0.40439730435804294 \t 3.6264090638744766\n",
            "87     \t [0.02036094 0.91618381 0.91600987]. \t  1.0458528763005754 \t 3.6264090638744766\n",
            "88     \t [0.58408506 0.22139976 0.15178313]. \t  0.5231401031426556 \t 3.6264090638744766\n",
            "89     \t [0.7460948 0.9881278 0.4693557]. \t  0.6309118965643825 \t 3.6264090638744766\n",
            "90     \t [0.64502969 0.7036153  0.33497261]. \t  0.24718341667757182 \t 3.6264090638744766\n",
            "91     \t [0.81336943 0.7955296  0.2408704 ]. \t  0.038691247046843534 \t 3.6264090638744766\n",
            "92     \t [0.22908182 0.34958393 0.70349495]. \t  1.7538685921576378 \t 3.6264090638744766\n",
            "93     \t [0.02556752 0.98745888 0.72362373]. \t  1.3883167758583452 \t 3.6264090638744766\n",
            "94     \t [0.59237836 0.93434985 0.51741072]. \t  1.4000596558201313 \t 3.6264090638744766\n",
            "95     \t [0.07210507 0.7999767  0.69767257]. \t  2.4017222558308857 \t 3.6264090638744766\n",
            "96     \t [0.01540501 0.18169681 0.53582139]. \t  0.23750339174042914 \t 3.6264090638744766\n",
            "97     \t [0.38957153 0.32283297 0.30482957]. \t  0.6454892445601828 \t 3.6264090638744766\n",
            "98     \t [0.38533378 0.41887816 0.54780256]. \t  0.6877116008288234 \t 3.6264090638744766\n",
            "99     \t [0.14901495 0.8771433  0.04026063]. \t  0.001634100766366568 \t 3.6264090638744766\n",
            "100    \t [0.62120192 0.76496167 0.20277625]. \t  0.04053907768932854 \t 3.6264090638744766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq4J_FTytd4O",
        "outputId": "b5fd0ad6-0a51-4720-a001-1d0d6a97e5a7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_loser_15 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_15 = dGPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.57788186 0.25550133 0.25252687]. \t  0.7217338409961529 \t 1.540625560354162\n",
            "init   \t [0.70990435 0.44755236 0.22694296]. \t  0.23199690080875204 \t 1.540625560354162\n",
            "init   \t [0.40170957 0.88225774 0.43721347]. \t  1.540625560354162 \t 1.540625560354162\n",
            "init   \t [0.87842518 0.78052775 0.53421429]. \t  0.5653855252190516 \t 1.540625560354162\n",
            "init   \t [0.01173301 0.26575648 0.3311941 ]. \t  0.5019682819961726 \t 1.540625560354162\n",
            "1      \t [0.45870705 0.97826428 0.97993811]. \t  0.46749494069699443 \t 1.540625560354162\n",
            "2      \t [0.66075471 0.         0.61312367]. \t  0.12159642826997123 \t 1.540625560354162\n",
            "3      \t [0.46645428 0.99752683 0.41295545]. \t  0.9605705723922775 \t 1.540625560354162\n",
            "4      \t [0.34390964 0.64896828 0.49739212]. \t  1.5093166326574845 \t 1.540625560354162\n",
            "5      \t [0.05653148 0.81602006 0.23121467]. \t  0.13016751165670606 \t 1.540625560354162\n",
            "6      \t [0.29839982 0.94779509 0.61815286]. \t  \u001b[92m2.376883919188496\u001b[0m \t 2.376883919188496\n",
            "7      \t [0.20225734 0.91762416 0.5993738 ]. \t  \u001b[92m2.8143502335642547\u001b[0m \t 2.8143502335642547\n",
            "8      \t [0.05726953 0.95949547 0.73838485]. \t  1.4281001613955855 \t 2.8143502335642547\n",
            "9      \t [0.96497544 0.07697573 0.01139424]. \t  0.04752173213497603 \t 2.8143502335642547\n",
            "10     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.8143502335642547\n",
            "11     \t [0.06083111 0.95090286 0.46023173]. \t  2.1523413244692358 \t 2.8143502335642547\n",
            "12     \t [0.29544804 0.00749755 0.1138019 ]. \t  0.4304462537200079 \t 2.8143502335642547\n",
            "13     \t [0.9788367  0.13663849 0.46305662]. \t  0.1333115859356235 \t 2.8143502335642547\n",
            "14     \t [0.99248475 0.89393169 0.25379208]. \t  0.019773995845805473 \t 2.8143502335642547\n",
            "15     \t [0.12141814 0.77615604 0.59627157]. \t  \u001b[92m2.88255169201502\u001b[0m \t 2.88255169201502\n",
            "16     \t [0.09484896 0.05628241 0.76753651]. \t  0.40772636461818845 \t 2.88255169201502\n",
            "17     \t [0.98855647 0.64396695 0.96994006]. \t  2.387841970988428 \t 2.88255169201502\n",
            "18     \t [0.10907278 0.82392297 0.59561025]. \t  \u001b[92m3.000256288747202\u001b[0m \t 3.000256288747202\n",
            "19     \t [0.96921191 0.20641809 0.99642329]. \t  0.5856509656909898 \t 3.000256288747202\n",
            "20     \t [0.99071045 0.11175798 0.0756385 ]. \t  0.10411761062601754 \t 3.000256288747202\n",
            "21     \t [0.17049127 0.30955908 0.5032451 ]. \t  0.36574896563945947 \t 3.000256288747202\n",
            "22     \t [0.35626726 0.30263896 0.91762246]. \t  1.8275297317309531 \t 3.000256288747202\n",
            "23     \t [0.26082213 0.52658403 0.99305453]. \t  2.176598638221769 \t 3.000256288747202\n",
            "24     \t [0.75812964 0.4744     0.99163431]. \t  2.031412034788625 \t 3.000256288747202\n",
            "25     \t [0.30805774 0.03183635 0.97626546]. \t  0.1605391479077385 \t 3.000256288747202\n",
            "26     \t [0.67703224 0.08606007 0.20809089]. \t  0.6706776152497954 \t 3.000256288747202\n",
            "27     \t [0.22027058 0.82222382 0.71245758]. \t  2.222960151813486 \t 3.000256288747202\n",
            "28     \t [0.04551279 0.41571432 0.91993079]. \t  2.7629813981502034 \t 3.000256288747202\n",
            "29     \t [0.02701892 0.73247455 0.54921471]. \t  2.5873896378178856 \t 3.000256288747202\n",
            "30     \t [0.16202041 0.86821179 0.56604579]. \t  \u001b[92m3.062704393865933\u001b[0m \t 3.062704393865933\n",
            "31     \t [0.37857704 0.4008916  0.57095718]. \t  0.7635747625375403 \t 3.062704393865933\n",
            "32     \t [0.09185293 0.74550382 0.61485494]. \t  2.7270945552334847 \t 3.062704393865933\n",
            "33     \t [0.81464089 0.20410306 0.19092096]. \t  0.42888437233317794 \t 3.062704393865933\n",
            "34     \t [0.85842146 0.29924661 0.15859815]. \t  0.24543263868828105 \t 3.062704393865933\n",
            "35     \t [0.03667396 0.16455783 0.93639795]. \t  0.6920956476994375 \t 3.062704393865933\n",
            "36     \t [0.01042273 0.54901765 0.83205998]. \t  \u001b[92m3.77490607109067\u001b[0m \t 3.77490607109067\n",
            "37     \t [0.09235068 0.59453282 0.5299634 ]. \t  1.5829305238168287 \t 3.77490607109067\n",
            "38     \t [0.13771917 0.43744959 0.88943158]. \t  3.232777206779901 \t 3.77490607109067\n",
            "39     \t [0.05362766 0.30928959 0.06249417]. \t  0.14577172423795093 \t 3.77490607109067\n",
            "40     \t [0.03454882 0.5530963  0.43818732]. \t  0.7889596293821415 \t 3.77490607109067\n",
            "41     \t [0.06175869 0.84298002 0.17074262]. \t  0.03836987173902284 \t 3.77490607109067\n",
            "42     \t [0.17364572 0.64082997 0.5845864 ]. \t  2.1333882315447505 \t 3.77490607109067\n",
            "43     \t [0.19228284 0.34767431 0.35689909]. \t  0.48325991939194524 \t 3.77490607109067\n",
            "44     \t [0.23354636 0.46147258 0.26207807]. \t  0.32929384743697176 \t 3.77490607109067\n",
            "45     \t [0.55513085 0.78585576 0.17222904]. \t  0.026846057895913045 \t 3.77490607109067\n",
            "46     \t [0.32640468 0.99936566 0.57634998]. \t  2.227868485959561 \t 3.77490607109067\n",
            "47     \t [0.10589905 0.5469575  0.0158781 ]. \t  0.019376708817783993 \t 3.77490607109067\n",
            "48     \t [0.72025457 0.42821853 0.89108759]. \t  3.1298633504098006 \t 3.77490607109067\n",
            "49     \t [0.95041933 0.47369464 0.77930951]. \t  3.0573049834269135 \t 3.77490607109067\n",
            "50     \t [0.37463577 0.52589471 0.87634371]. \t  3.7726339942689924 \t 3.77490607109067\n",
            "51     \t [0.80954807 0.70749514 0.87223495]. \t  2.9788339110325452 \t 3.77490607109067\n",
            "52     \t [0.04936964 0.76643056 0.27365458]. \t  0.25866039619980574 \t 3.77490607109067\n",
            "53     \t [0.20524069 0.76502703 0.87035564]. \t  2.5771584611667055 \t 3.77490607109067\n",
            "54     \t [0.44579854 0.10044851 0.46713745]. \t  0.32543707520716264 \t 3.77490607109067\n",
            "55     \t [0.76353136 0.02023589 0.33067034]. \t  0.5065642586272455 \t 3.77490607109067\n",
            "56     \t [0.05689745 0.55302702 0.21793852]. \t  0.139296626823076 \t 3.77490607109067\n",
            "57     \t [0.93704072 0.31603255 0.75654058]. \t  1.9235605761300654 \t 3.77490607109067\n",
            "58     \t [0.92634453 0.55250125 0.37701181]. \t  0.10624015191215301 \t 3.77490607109067\n",
            "59     \t [0.98566888 0.93332484 0.38748483]. \t  0.12626546681121031 \t 3.77490607109067\n",
            "60     \t [0.52099922 0.16674173 0.54409066]. \t  0.2500879587637723 \t 3.77490607109067\n",
            "61     \t [0.45686738 0.7185135  0.97018309]. \t  2.0936312622869466 \t 3.77490607109067\n",
            "62     \t [0.38554434 0.52484819 0.3410793 ]. \t  0.3446974468763136 \t 3.77490607109067\n",
            "63     \t [0.48744492 0.75878045 0.40262775]. \t  0.8724685464007343 \t 3.77490607109067\n",
            "64     \t [0.32494146 0.55551894 0.11106917]. \t  0.07247095796871711 \t 3.77490607109067\n",
            "65     \t [0.22437916 0.18511023 0.39884955]. \t  0.554974513879724 \t 3.77490607109067\n",
            "66     \t [0.66195118 0.83031165 0.1263408 ]. \t  0.007416368719395564 \t 3.77490607109067\n",
            "67     \t [0.45277534 0.90837962 0.37948517]. \t  0.8302928560503484 \t 3.77490607109067\n",
            "68     \t [0.11030779 0.93358032 0.79663281]. \t  1.2673113215586038 \t 3.77490607109067\n",
            "69     \t [0.75689381 0.26626376 0.40769996]. \t  0.3093906641862753 \t 3.77490607109067\n",
            "70     \t [0.25249093 0.86169084 0.93775836]. \t  1.3487665227408552 \t 3.77490607109067\n",
            "71     \t [0.07417741 0.78342391 0.77384638]. \t  2.3549929045130416 \t 3.77490607109067\n",
            "72     \t [0.47843585 0.0947401  0.45417235]. \t  0.35892328654689665 \t 3.77490607109067\n",
            "73     \t [0.0028475  0.80972295 0.83880453]. \t  2.1616271984147324 \t 3.77490607109067\n",
            "74     \t [0.18412308 0.40611329 0.4560012 ]. \t  0.4478611342438572 \t 3.77490607109067\n",
            "75     \t [0.72015424 0.33382483 0.25526604]. \t  0.43357330396654686 \t 3.77490607109067\n",
            "76     \t [0.63260961 0.21704913 0.29129282]. \t  0.7245423555057698 \t 3.77490607109067\n",
            "77     \t [0.55578338 0.43008045 0.87967857]. \t  3.2470912244888375 \t 3.77490607109067\n",
            "78     \t [0.12989691 0.37828551 0.63240513]. \t  1.1871928238269298 \t 3.77490607109067\n",
            "79     \t [0.53736537 0.60544502 0.00838678]. \t  0.01141987847330488 \t 3.77490607109067\n",
            "80     \t [0.53955518 0.92113759 0.71411087]. \t  1.251299842896169 \t 3.77490607109067\n",
            "81     \t [0.78266961 0.80364241 0.95875221]. \t  1.5914169852142543 \t 3.77490607109067\n",
            "82     \t [0.98124241 0.27066251 0.49880464]. \t  0.16785373269959117 \t 3.77490607109067\n",
            "83     \t [0.86908715 0.7723944  0.27931136]. \t  0.05584375404926373 \t 3.77490607109067\n",
            "84     \t [0.18191085 0.87279776 0.06389839]. \t  0.0030067773074108337 \t 3.77490607109067\n",
            "85     \t [0.68267667 0.23806959 0.53615028]. \t  0.27687692743916364 \t 3.77490607109067\n",
            "86     \t [0.51580088 0.11030196 0.4589715 ]. \t  0.33837634771568686 \t 3.77490607109067\n",
            "87     \t [0.96852447 0.15481291 0.01517424]. \t  0.049788937359754426 \t 3.77490607109067\n",
            "88     \t [0.57673346 0.28305391 0.46648613]. \t  0.3037232717970031 \t 3.77490607109067\n",
            "89     \t [0.74398022 0.26145588 0.77234634]. \t  1.630304906430959 \t 3.77490607109067\n",
            "90     \t [0.9634963  0.27587429 0.56715608]. \t  0.35600452235267915 \t 3.77490607109067\n",
            "91     \t [0.73619882 0.71342445 0.18271428]. \t  0.026606237937590572 \t 3.77490607109067\n",
            "92     \t [0.57409829 0.41833827 0.3448306 ]. \t  0.3538976036382143 \t 3.77490607109067\n",
            "93     \t [0.56102639 0.06679605 0.64037298]. \t  0.2483887387972291 \t 3.77490607109067\n",
            "94     \t [0.93882985 0.30550525 0.23690212]. \t  0.25813917911444156 \t 3.77490607109067\n",
            "95     \t [0.05289148 0.90095206 0.18316854]. \t  0.04819088234293035 \t 3.77490607109067\n",
            "96     \t [0.47581993 0.98761237 0.71369248]. \t  1.0932033178536977 \t 3.77490607109067\n",
            "97     \t [0.79326914 0.88886517 0.95242994]. \t  1.0291534985796806 \t 3.77490607109067\n",
            "98     \t [0.90504706 0.25651419 0.73046914]. \t  1.3447868969921708 \t 3.77490607109067\n",
            "99     \t [0.35541618 0.87374982 0.17466265]. \t  0.03535399187079531 \t 3.77490607109067\n",
            "100    \t [0.61162121 0.13982352 0.67154308]. \t  0.513200668672658 \t 3.77490607109067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tJ-ta9Gtd7H",
        "outputId": "88c170a7-2b89-43a8-f04d-dbc80acdd092"
      },
      "source": [
        "\r\n",
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_loser_16 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_16 = dGPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.86394833 0.316366   0.67215078]. \t  1.1947633822631252 \t 3.8084053754826726\n",
            "init   \t [0.50791167 0.78166043 0.28368168]. \t  0.20052765717741963 \t 3.8084053754826726\n",
            "init   \t [0.23370878 0.56379969 0.87502436]. \t  3.8084053754826726 \t 3.8084053754826726\n",
            "init   \t [0.71894292 0.18213174 0.24380041]. \t  0.6532826728302027 \t 3.8084053754826726\n",
            "init   \t [0.08605673 0.53424539 0.83969965]. \t  3.8072565344663847 \t 3.8084053754826726\n",
            "1      \t [0.12378065 0.3168353  0.9881258 ]. \t  1.2451880833929392 \t 3.8084053754826726\n",
            "2      \t [0.27777137 0.94321124 0.96390235]. \t  0.6864412627036871 \t 3.8084053754826726\n",
            "3      \t [0.04467565 0.50685234 0.49507307]. \t  0.8672168540674181 \t 3.8084053754826726\n",
            "4      \t [0.00628562 0.62733695 0.98683809]. \t  2.216178855902818 \t 3.8084053754826726\n",
            "5      \t [0.24302952 0.56104258 0.71810923]. \t  2.714059998275844 \t 3.8084053754826726\n",
            "6      \t [0.96778771 0.7812586  0.04735995]. \t  0.0011023859383055771 \t 3.8084053754826726\n",
            "7      \t [0.94597768 0.01177013 0.21221742]. \t  0.30096480916717455 \t 3.8084053754826726\n",
            "8      \t [1. 1. 1.]. \t  0.3168836207042708 \t 3.8084053754826726\n",
            "9      \t [0.86553754 0.59748584 0.96361124]. \t  2.6531081511639076 \t 3.8084053754826726\n",
            "10     \t [0.64363474 0.74123114 0.01847141]. \t  0.002719251460432877 \t 3.8084053754826726\n",
            "11     \t [0.50952739 0.70630666 0.95920627]. \t  2.3194290643909032 \t 3.8084053754826726\n",
            "12     \t [0.85522449 0.97974245 0.20836416]. \t  0.014056808087911507 \t 3.8084053754826726\n",
            "13     \t [0.90774286 0.35181844 0.01944449]. \t  0.03818621659189494 \t 3.8084053754826726\n",
            "14     \t [0.10442144 0.99757424 0.15075461]. \t  0.019461290813885892 \t 3.8084053754826726\n",
            "15     \t [0.14617975 0.59194485 0.82341198]. \t  3.724750123953375 \t 3.8084053754826726\n",
            "16     \t [1.         0.50084316 0.99314177]. \t  2.040382279489931 \t 3.8084053754826726\n",
            "17     \t [0.98906708 0.49449814 0.55793286]. \t  0.4720653973501239 \t 3.8084053754826726\n",
            "18     \t [0.62536537 0.37354973 0.93829796]. \t  2.2286937559849958 \t 3.8084053754826726\n",
            "19     \t [0.18821051 0.55334529 0.97120305]. \t  2.600081052146005 \t 3.8084053754826726\n",
            "20     \t [6.94938151e-14 6.25738034e-16 6.44627663e-15]. \t  0.06797411659014986 \t 3.8084053754826726\n",
            "21     \t [0.04213704 0.63940159 0.82388213]. \t  3.5206016328446132 \t 3.8084053754826726\n",
            "22     \t [0.05848135 0.47135776 0.81994148]. \t  3.5213504013127426 \t 3.8084053754826726\n",
            "23     \t [0.02438986 0.40362393 0.08475438]. \t  0.11377877239053279 \t 3.8084053754826726\n",
            "24     \t [0.9949863  0.74522997 0.86488097]. \t  2.5691256948847254 \t 3.8084053754826726\n",
            "25     \t [0.0918845  0.35222564 0.4215209 ]. \t  0.3685063985974734 \t 3.8084053754826726\n",
            "26     \t [0.52488244 0.425859   0.11355576]. \t  0.17691971367879086 \t 3.8084053754826726\n",
            "27     \t [0.6074091  0.5651644  0.76113511]. \t  3.062024712880875 \t 3.8084053754826726\n",
            "28     \t [0.22418673 0.0710134  0.17935842]. \t  0.7291417604546632 \t 3.8084053754826726\n",
            "29     \t [0.23621156 0.77273158 0.72314536]. \t  2.3689595603050706 \t 3.8084053754826726\n",
            "30     \t [0.77679747 0.65878278 0.78907801]. \t  2.9409868761613063 \t 3.8084053754826726\n",
            "31     \t [0.45034395 0.31184179 0.49118223]. \t  0.34019795084950083 \t 3.8084053754826726\n",
            "32     \t [0.46134655 0.08631294 0.92137812]. \t  0.4002426920815167 \t 3.8084053754826726\n",
            "33     \t [0.19064436 0.41679247 0.29594489]. \t  0.4111794760569056 \t 3.8084053754826726\n",
            "34     \t [0.75430526 0.46631548 0.45988832]. \t  0.2589634023281027 \t 3.8084053754826726\n",
            "35     \t [0.13569583 0.80181312 0.26061919]. \t  0.2203440587860694 \t 3.8084053754826726\n",
            "36     \t [0.0210929  0.27206192 0.20612286]. \t  0.490978954096385 \t 3.8084053754826726\n",
            "37     \t [0.02629512 0.25758702 0.77117475]. \t  1.5962871548899078 \t 3.8084053754826726\n",
            "38     \t [0.0730912  0.3120373  0.68606544]. \t  1.373801062907681 \t 3.8084053754826726\n",
            "39     \t [0.60526256 0.23723161 0.17647999]. \t  0.5717865799947605 \t 3.8084053754826726\n",
            "40     \t [0.72925291 0.17985652 0.1038405 ]. \t  0.292122585001201 \t 3.8084053754826726\n",
            "41     \t [0.79270642 0.632309   0.21490531]. \t  0.050736446991021644 \t 3.8084053754826726\n",
            "42     \t [0.74023708 0.53345148 0.69811026]. \t  2.097988154667948 \t 3.8084053754826726\n",
            "43     \t [0.93793031 0.3497155  0.18397546]. \t  0.1792599539864782 \t 3.8084053754826726\n",
            "44     \t [0.86734098 0.14622375 0.46534638]. \t  0.1795843969354928 \t 3.8084053754826726\n",
            "45     \t [0.32042685 0.21356485 0.69082321]. \t  0.909072075181484 \t 3.8084053754826726\n",
            "46     \t [0.93574892 0.27989918 0.01027377]. \t  0.04031023584034215 \t 3.8084053754826726\n",
            "47     \t [0.81706479 0.81081815 0.35408109]. \t  0.1967663213350807 \t 3.8084053754826726\n",
            "48     \t [0.87105988 0.83328345 0.27765357]. \t  0.054612293237242315 \t 3.8084053754826726\n",
            "49     \t [0.80936618 0.41185184 0.98653548]. \t  1.8141680762717804 \t 3.8084053754826726\n",
            "50     \t [0.19768881 0.46814887 0.84863148]. \t  3.6004712755916812 \t 3.8084053754826726\n",
            "51     \t [0.96048891 0.86684945 0.3033962 ]. \t  0.05267070671118978 \t 3.8084053754826726\n",
            "52     \t [0.12271592 0.1474196  0.20314984]. \t  0.7305270817652886 \t 3.8084053754826726\n",
            "53     \t [0.17349131 0.99299808 0.26561668]. \t  0.20964188064412295 \t 3.8084053754826726\n",
            "54     \t [0.10685981 0.53430589 0.85641253]. \t  \u001b[92m3.823263664347254\u001b[0m \t 3.823263664347254\n",
            "55     \t [0.11058698 0.97447688 0.3993736 ]. \t  1.3143788435188235 \t 3.823263664347254\n",
            "56     \t [0.87576718 0.49481146 0.55244252]. \t  0.5009209227040916 \t 3.823263664347254\n",
            "57     \t [0.02274508 0.95137123 0.03512004]. \t  0.0009698546243275209 \t 3.823263664347254\n",
            "58     \t [0.38999471 0.77210159 0.1036472 ]. \t  0.010896907604143387 \t 3.823263664347254\n",
            "59     \t [0.21582292 0.48375857 0.98977518]. \t  2.1204190952489697 \t 3.823263664347254\n",
            "60     \t [0.73989052 0.66110466 0.04476188]. \t  0.007994844924721287 \t 3.823263664347254\n",
            "61     \t [0.6488681  0.22512697 0.23753612]. \t  0.6857983232125278 \t 3.823263664347254\n",
            "62     \t [0.14359178 0.42114215 0.06835466]. \t  0.10418286952472619 \t 3.823263664347254\n",
            "63     \t [0.00618998 0.96069648 0.70577431]. \t  1.666715409499641 \t 3.823263664347254\n",
            "64     \t [0.064971   0.79011856 0.66386657]. \t  2.5880114255305275 \t 3.823263664347254\n",
            "65     \t [0.13804909 0.39060505 0.22599701]. \t  0.3945123996524405 \t 3.823263664347254\n",
            "66     \t [0.97278609 0.91514663 0.22301319]. \t  0.01214201106358989 \t 3.823263664347254\n",
            "67     \t [0.47805976 0.44904643 0.69911163]. \t  2.1228685999717154 \t 3.823263664347254\n",
            "68     \t [0.77591688 0.99971211 0.4089057 ]. \t  0.3571896826106656 \t 3.823263664347254\n",
            "69     \t [0.20130951 0.18090192 0.01361808]. \t  0.1280031127309551 \t 3.823263664347254\n",
            "70     \t [0.17274771 0.89530659 0.84319324]. \t  1.4260445211381512 \t 3.823263664347254\n",
            "71     \t [0.44817697 0.83618646 0.21842817]. \t  0.07567909537410877 \t 3.823263664347254\n",
            "72     \t [0.63705785 0.06685462 0.37546693]. \t  0.5564580121519138 \t 3.823263664347254\n",
            "73     \t [0.34473282 0.70500744 0.23790905]. \t  0.12499189981633492 \t 3.823263664347254\n",
            "74     \t [0.01471892 0.52767853 0.83725391]. \t  3.770355627665735 \t 3.823263664347254\n",
            "75     \t [0.8195422  0.67247422 0.25780942]. \t  0.056289828958829925 \t 3.823263664347254\n",
            "76     \t [0.45033804 0.7720043  0.64231289]. \t  2.072030997389843 \t 3.823263664347254\n",
            "77     \t [0.95281713 0.4814589  0.13972786]. \t  0.05889852967393788 \t 3.823263664347254\n",
            "78     \t [0.21029326 0.08949757 0.61269778]. \t  0.24228973974732076 \t 3.823263664347254\n",
            "79     \t [0.97854859 0.24215962 0.51359038]. \t  0.17767339073826924 \t 3.823263664347254\n",
            "80     \t [0.27415217 0.58686992 0.96646935]. \t  2.6831589861332583 \t 3.823263664347254\n",
            "81     \t [0.38697907 0.29536366 0.09405804]. \t  0.29552336972369 \t 3.823263664347254\n",
            "82     \t [0.38041894 0.28113978 0.19138337]. \t  0.6436855569505461 \t 3.823263664347254\n",
            "83     \t [0.73644648 0.74409984 0.0593703 ]. \t  0.004067044093233711 \t 3.823263664347254\n",
            "84     \t [0.43974686 0.25576428 0.76727338]. \t  1.5860201622412489 \t 3.823263664347254\n",
            "85     \t [0.91716939 0.23206351 0.2338737 ]. \t  0.3441867231335853 \t 3.823263664347254\n",
            "86     \t [0.928106   0.36398451 0.2170875 ]. \t  0.19819281824603735 \t 3.823263664347254\n",
            "87     \t [0.55496943 0.67175689 0.39539542]. \t  0.5496100823873051 \t 3.823263664347254\n",
            "88     \t [0.10679858 0.88619389 0.87102049]. \t  1.436988468303008 \t 3.823263664347254\n",
            "89     \t [0.42338151 0.41695061 0.16612036]. \t  0.2995029456411814 \t 3.823263664347254\n",
            "90     \t [0.89484097 0.34203198 0.19629526]. \t  0.22656051317356457 \t 3.823263664347254\n",
            "91     \t [0.17349676 0.41012199 0.94005107]. \t  2.4938462280056983 \t 3.823263664347254\n",
            "92     \t [0.19771752 0.31239557 0.2045076 ]. \t  0.558662218988743 \t 3.823263664347254\n",
            "93     \t [0.33178305 0.02142237 0.89524347]. \t  0.24704410756571965 \t 3.823263664347254\n",
            "94     \t [0.33225438 0.53569611 0.07988812]. \t  0.06111318267713783 \t 3.823263664347254\n",
            "95     \t [0.65412864 0.03015545 0.90688896]. \t  0.2532532659952299 \t 3.823263664347254\n",
            "96     \t [0.96827564 0.09360594 0.34086681]. \t  0.28907922102817385 \t 3.823263664347254\n",
            "97     \t [0.50114949 0.9151915  0.28738893]. \t  0.21956923397295164 \t 3.823263664347254\n",
            "98     \t [0.4219465  0.87052368 0.10971162]. \t  0.007494253147581695 \t 3.823263664347254\n",
            "99     \t [0.64684064 0.68675623 0.41936934]. \t  0.5458447520385002 \t 3.823263664347254\n",
            "100    \t [0.28560638 0.15564314 0.33671172]. \t  0.8403431815373047 \t 3.823263664347254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqjDLo0otd9-",
        "outputId": "af78ee26-0e5a-4553-9ae6-4de4621aceb4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_loser_17 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_17 = dGPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.65182403 0.73051962 0.19336736]. \t  0.03562358530655929 \t 3.1179188940604616\n",
            "init   \t [0.51967724 0.67638327 0.80444487]. \t  3.1179188940604616 \t 3.1179188940604616\n",
            "init   \t [0.53351286 0.90803418 0.44348113]. \t  1.1984486017622078 \t 3.1179188940604616\n",
            "init   \t [0.25597221 0.27866502 0.04332186]. \t  0.1645724004847893 \t 3.1179188940604616\n",
            "init   \t [0.46897499 0.9416138  0.60040624]. \t  1.9192522196289552 \t 3.1179188940604616\n",
            "1      \t [0.24286651 0.38597556 0.96454773]. \t  1.996297956656514 \t 3.1179188940604616\n",
            "2      \t [0.78722    0.79439557 0.98615055]. \t  1.382220738619041 \t 3.1179188940604616\n",
            "3      \t [0.48019116 0.49032971 0.69126343]. \t  2.1360120328634014 \t 3.1179188940604616\n",
            "4      \t [0.4607031  0.72735048 0.97289599]. \t  2.000642401584277 \t 3.1179188940604616\n",
            "5      \t [0.00166169 0.44190378 0.82055392]. \t  \u001b[92m3.336924169425464\u001b[0m \t 3.336924169425464\n",
            "6      \t [0.84992296 0.83200048 0.62767412]. \t  0.8027149864585417 \t 3.336924169425464\n",
            "7      \t [0.0340026  0.68308541 0.48965537]. \t  1.8929631780470242 \t 3.336924169425464\n",
            "8      \t [0.02659308 0.43512057 0.74588651]. \t  2.6717059421663465 \t 3.336924169425464\n",
            "9      \t [0.05231581 0.5113275  0.93934183]. \t  3.03637678220694 \t 3.336924169425464\n",
            "10     \t [0.78110492 0.5287478  0.92088201]. \t  3.3164288980141534 \t 3.336924169425464\n",
            "11     \t [0.7581868  0.59054205 0.89327271]. \t  \u001b[92m3.5935769448692607\u001b[0m \t 3.5935769448692607\n",
            "12     \t [0.06540998 0.93970118 0.11022951]. \t  0.0080286044197451 \t 3.5935769448692607\n",
            "13     \t [0.00917304 0.02388783 0.90208087]. \t  0.24124246128516022 \t 3.5935769448692607\n",
            "14     \t [0.76092022 0.54245962 0.7960535 ]. \t  3.430452623032548 \t 3.5935769448692607\n",
            "15     \t [0.06151538 0.96523417 0.47660316]. \t  2.287919808243954 \t 3.5935769448692607\n",
            "16     \t [0.70429838 0.55004718 0.88602409]. \t  \u001b[92m3.696554861371928\u001b[0m \t 3.696554861371928\n",
            "17     \t [0.63769391 0.60924651 0.82227901]. \t  3.571667524633763 \t 3.696554861371928\n",
            "18     \t [0.84564485 0.31890278 0.00286951]. \t  0.04128989362046553 \t 3.696554861371928\n",
            "19     \t [0.68465033 0.98359513 0.32227619]. \t  0.19492681141269716 \t 3.696554861371928\n",
            "20     \t [0.63440592 0.59892124 0.85253504]. \t  \u001b[92m3.7306976893459707\u001b[0m \t 3.7306976893459707\n",
            "21     \t [0.82399351 0.15394836 0.64722767]. \t  0.4584541049637956 \t 3.7306976893459707\n",
            "22     \t [0.00093538 0.63875692 0.84784539]. \t  3.577111350024265 \t 3.7306976893459707\n",
            "23     \t [0.13352918 0.94357418 0.30028195]. \t  0.4097630621694289 \t 3.7306976893459707\n",
            "24     \t [0.03451205 0.12497542 0.02756137]. \t  0.1274157801200702 \t 3.7306976893459707\n",
            "25     \t [0.76576681 0.43251344 0.11100496]. \t  0.111032154670387 \t 3.7306976893459707\n",
            "26     \t [0.71302946 0.65428918 0.85710419]. \t  3.4312930079457056 \t 3.7306976893459707\n",
            "27     \t [0.71440553 0.77407405 0.36272561]. \t  0.3093169795364798 \t 3.7306976893459707\n",
            "28     \t [0.70784051 0.97993163 0.67028909]. \t  0.7892816451607718 \t 3.7306976893459707\n",
            "29     \t [0.51800861 0.54551968 0.26487654]. \t  0.19936679977656882 \t 3.7306976893459707\n",
            "30     \t [0.03243126 0.94999519 0.69930194]. \t  1.8007352842172186 \t 3.7306976893459707\n",
            "31     \t [0.48660742 0.02568772 0.01930355]. \t  0.1394555186161855 \t 3.7306976893459707\n",
            "32     \t [0.78903081 0.82775419 0.1624179 ]. \t  0.009969463208157872 \t 3.7306976893459707\n",
            "33     \t [0.34639487 0.50220626 0.23128726]. \t  0.24563542914317918 \t 3.7306976893459707\n",
            "34     \t [0.73921642 0.46775549 0.67290683]. \t  1.6954635060896477 \t 3.7306976893459707\n",
            "35     \t [0.78764978 0.06042556 0.8302592 ]. \t  0.4267724380537717 \t 3.7306976893459707\n",
            "36     \t [0.70383577 0.57096565 0.17725708]. \t  0.0771285228845155 \t 3.7306976893459707\n",
            "37     \t [0.08224507 0.52206253 0.92520623]. \t  3.274999790654435 \t 3.7306976893459707\n",
            "38     \t [0.0620436  0.42801267 0.25147661]. \t  0.3106822479949355 \t 3.7306976893459707\n",
            "39     \t [0.20240843 0.223458   0.35299039]. \t  0.6751471570789034 \t 3.7306976893459707\n",
            "40     \t [0.78411066 0.99428025 0.80895205]. \t  0.5882277001490892 \t 3.7306976893459707\n",
            "41     \t [0.90256261 0.37869663 0.54763987]. \t  0.3839181168949045 \t 3.7306976893459707\n",
            "42     \t [0.82027636 0.03502204 0.24575806]. \t  0.5004688146837654 \t 3.7306976893459707\n",
            "43     \t [0.85121567 0.22893522 0.27913839]. \t  0.438480566219128 \t 3.7306976893459707\n",
            "44     \t [0.26433048 0.1597651  0.85691673]. \t  0.9205910915577684 \t 3.7306976893459707\n",
            "45     \t [0.76691622 0.51337129 0.38827815]. \t  0.19408491849855966 \t 3.7306976893459707\n",
            "46     \t [0.55298012 0.12193723 0.8363212 ]. \t  0.7153673496834476 \t 3.7306976893459707\n",
            "47     \t [0.98453676 0.40768679 0.23852634]. \t  0.13626639106252103 \t 3.7306976893459707\n",
            "48     \t [0.25856198 0.52084654 0.77467825]. \t  3.3395602090894543 \t 3.7306976893459707\n",
            "49     \t [0.45296663 0.78267689 0.08376546]. \t  0.006740911098779007 \t 3.7306976893459707\n",
            "50     \t [0.58567443 0.59245542 0.05416328]. \t  0.02355784289594982 \t 3.7306976893459707\n",
            "51     \t [0.59592109 0.97796742 0.08147182]. \t  0.0017791627784233446 \t 3.7306976893459707\n",
            "52     \t [0.02516885 0.60819578 0.81362045]. \t  3.592382920232589 \t 3.7306976893459707\n",
            "53     \t [0.81432498 0.1020756  0.43962605]. \t  0.2412751609616757 \t 3.7306976893459707\n",
            "54     \t [0.64815103 0.29806231 0.44364782]. \t  0.2969447837152171 \t 3.7306976893459707\n",
            "55     \t [0.56051781 0.51183617 0.49888347]. \t  0.5888244819252172 \t 3.7306976893459707\n",
            "56     \t [0.48517218 0.99134974 0.49422206]. \t  1.5385809578318683 \t 3.7306976893459707\n",
            "57     \t [0.05816553 0.45763475 0.69376408]. \t  2.153215054180613 \t 3.7306976893459707\n",
            "58     \t [0.26909419 0.74643037 0.04197797]. \t  0.004915713440641152 \t 3.7306976893459707\n",
            "59     \t [0.25910482 0.33170441 0.73636218]. \t  1.9550234372979356 \t 3.7306976893459707\n",
            "60     \t [0.86860015 0.22907423 0.02897167]. \t  0.07587325391055157 \t 3.7306976893459707\n",
            "61     \t [0.26429574 0.80177263 0.59003997]. \t  2.7847853706041117 \t 3.7306976893459707\n",
            "62     \t [0.3464018  0.83103848 0.63003205]. \t  2.4356311999821503 \t 3.7306976893459707\n",
            "63     \t [0.33390264 0.9444325  0.98442306]. \t  0.5888996464899308 \t 3.7306976893459707\n",
            "64     \t [0.71600419 0.11410615 0.07313597]. \t  0.22480895578597396 \t 3.7306976893459707\n",
            "65     \t [0.86916049 0.615922   0.70458205]. \t  1.9599722542591864 \t 3.7306976893459707\n",
            "66     \t [0.53336509 0.84486317 0.59531914]. \t  1.8388020734686554 \t 3.7306976893459707\n",
            "67     \t [0.73978342 0.65624186 0.80203376]. \t  3.109129084832267 \t 3.7306976893459707\n",
            "68     \t [0.46908484 0.29016108 0.48622976]. \t  0.32457236537058454 \t 3.7306976893459707\n",
            "69     \t [0.8055574  0.75264993 0.77470985]. \t  2.0918877698505494 \t 3.7306976893459707\n",
            "70     \t [0.71930082 0.69451456 0.87152847]. \t  3.121077480785742 \t 3.7306976893459707\n",
            "71     \t [0.23683334 0.00284899 0.28194082]. \t  0.8279866835289357 \t 3.7306976893459707\n",
            "72     \t [0.1040646  0.38199153 0.39295203]. \t  0.3873126590831428 \t 3.7306976893459707\n",
            "73     \t [0.30928002 0.42841219 0.01410685]. \t  0.054880482195661766 \t 3.7306976893459707\n",
            "74     \t [0.97649113 0.50950127 0.40146832]. \t  0.1005319691968133 \t 3.7306976893459707\n",
            "75     \t [0.8338393  0.74114505 0.04995728]. \t  0.002826701835849269 \t 3.7306976893459707\n",
            "76     \t [0.03921711 0.24384318 0.59904097]. \t  0.5135943190545953 \t 3.7306976893459707\n",
            "77     \t [0.66007928 0.85332333 0.76544453]. \t  1.4290146320216675 \t 3.7306976893459707\n",
            "78     \t [0.89236848 0.35997672 0.13321747]. \t  0.14218906731735056 \t 3.7306976893459707\n",
            "79     \t [0.78621447 0.86215128 0.27846026]. \t  0.07912757823474001 \t 3.7306976893459707\n",
            "80     \t [0.80024897 0.32727763 0.05355603]. \t  0.09341169596865143 \t 3.7306976893459707\n",
            "81     \t [0.93128779 0.68743986 0.38818922]. \t  0.13885246555159952 \t 3.7306976893459707\n",
            "82     \t [0.47095339 0.96251407 0.29195505]. \t  0.23654792306024855 \t 3.7306976893459707\n",
            "83     \t [0.14768116 0.94840828 0.44541929]. \t  1.9774934950818177 \t 3.7306976893459707\n",
            "84     \t [0.86850914 0.70841359 0.22112106]. \t  0.027852132025478437 \t 3.7306976893459707\n",
            "85     \t [0.3953896  0.57065299 0.56860651]. \t  1.3740985016426173 \t 3.7306976893459707\n",
            "86     \t [0.45464463 0.93392166 0.39989463]. \t  0.9872177351317891 \t 3.7306976893459707\n",
            "87     \t [0.83370591 0.80028876 0.03974119]. \t  0.0012445130735553001 \t 3.7306976893459707\n",
            "88     \t [0.3954636 0.4760783 0.9305186]. \t  3.0539262472087465 \t 3.7306976893459707\n",
            "89     \t [0.20014603 0.64918314 0.30455259]. \t  0.32378189219673037 \t 3.7306976893459707\n",
            "90     \t [0.81105369 0.1425165  0.27709311]. \t  0.5516169326058925 \t 3.7306976893459707\n",
            "91     \t [0.51933244 0.17350192 0.01046866]. \t  0.12509653075594532 \t 3.7306976893459707\n",
            "92     \t [0.20641137 0.07008326 0.74426415]. \t  0.43692938093535527 \t 3.7306976893459707\n",
            "93     \t [0.26813454 0.33962522 0.85618197]. \t  2.5218159508499016 \t 3.7306976893459707\n",
            "94     \t [0.34518094 0.31846439 0.72401148]. \t  1.7566215036092712 \t 3.7306976893459707\n",
            "95     \t [0.72152361 0.5511943  0.11487202]. \t  0.05310531285038199 \t 3.7306976893459707\n",
            "96     \t [0.88422074 0.77691097 0.8119611 ]. \t  2.122276340715377 \t 3.7306976893459707\n",
            "97     \t [0.88363888 0.3470758  0.85211001]. \t  2.540494522342128 \t 3.7306976893459707\n",
            "98     \t [0.8354768  0.44173748 0.23095813]. \t  0.17848714396589418 \t 3.7306976893459707\n",
            "99     \t [0.04158365 0.95493571 0.1420074 ]. \t  0.01712143442770344 \t 3.7306976893459707\n",
            "100    \t [0.87034875 0.38981482 0.60959142]. \t  0.8117138118498288 \t 3.7306976893459707\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOqNmgSIteAw",
        "outputId": "b40d33db-ce51-4cd8-f0ca-d845941d7d97"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_loser_18 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_18 = dGPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.8734294  0.96854066 0.86919454]. \t  0.722531189286755 \t 1.1210522139432408\n",
            "init   \t [0.53085569 0.23272833 0.0113988 ]. \t  0.1133646794700679 \t 1.1210522139432408\n",
            "init   \t [0.43046882 0.40235136 0.52267467]. \t  0.5232646985053151 \t 1.1210522139432408\n",
            "init   \t [0.4783918  0.55535647 0.54338602]. \t  1.0390748854353227 \t 1.1210522139432408\n",
            "init   \t [0.76089558 0.71237457 0.6196821 ]. \t  1.1210522139432408 \t 1.1210522139432408\n",
            "1      \t [0.28982083 0.95201409 0.18404427]. \t  0.04220734680078784 \t 1.1210522139432408\n",
            "2      \t [0.12883542 0.98258307 0.90180597]. \t  0.6910738837771888 \t 1.1210522139432408\n",
            "3      \t [0.49148239 0.8099037  0.6976846 ]. \t  \u001b[92m1.8373634823765186\u001b[0m \t 1.8373634823765186\n",
            "4      \t [0.46136123 0.85569499 0.88610921]. \t  1.63167944843026 \t 1.8373634823765186\n",
            "5      \t [0.49454713 0.93328953 0.56170327]. \t  \u001b[92m1.9054625615284428\u001b[0m \t 1.9054625615284428\n",
            "6      \t [0.41435422 0.93277915 0.58517174]. \t  \u001b[92m2.210826503581639\u001b[0m \t 2.210826503581639\n",
            "7      \t [0.80890325 0.0109094  0.32544314]. \t  0.45214607547986396 \t 2.210826503581639\n",
            "8      \t [0.34819219 0.99739422 0.56097608]. \t  2.1934316775664433 \t 2.210826503581639\n",
            "9      \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.210826503581639\n",
            "10     \t [0.43963141 0.98143234 0.73039565]. \t  1.0945002305811715 \t 2.210826503581639\n",
            "11     \t [0.02531226 0.36418907 0.95948581]. \t  1.8848202094359148 \t 2.210826503581639\n",
            "12     \t [0.9798975  0.65782747 1.        ]. \t  1.8577503855843776 \t 2.210826503581639\n",
            "13     \t [0.99970731 0.70062909 0.0567806 ]. \t  0.0027811478146734293 \t 2.210826503581639\n",
            "14     \t [0.02495978 0.79442693 0.58527601]. \t  \u001b[92m2.9049684298182346\u001b[0m \t 2.9049684298182346\n",
            "15     \t [0.08533529 0.90729061 0.49773317]. \t  2.706790721132025 \t 2.9049684298182346\n",
            "16     \t [0.5421836  0.01602986 0.01723476]. \t  0.1264373214115636 \t 2.9049684298182346\n",
            "17     \t [0.13495713 0.71802934 0.77968976]. \t  2.822467058101586 \t 2.9049684298182346\n",
            "18     \t [0.03434562 0.05142528 0.46605694]. \t  0.22909651602977216 \t 2.9049684298182346\n",
            "19     \t [0.33535147 0.59732357 0.98448116]. \t  2.350249163604855 \t 2.9049684298182346\n",
            "20     \t [0.1926461  0.75814723 0.55309241]. \t  2.740424052375752 \t 2.9049684298182346\n",
            "21     \t [0.06859238 0.70957122 0.98703068]. \t  1.888144648409867 \t 2.9049684298182346\n",
            "22     \t [0.48419506 0.01549588 0.89987476]. \t  0.22772306298785155 \t 2.9049684298182346\n",
            "23     \t [0.10489585 0.81005677 0.60815803]. \t  \u001b[92m2.927816116398253\u001b[0m \t 2.927816116398253\n",
            "24     \t [0.04431098 0.71713489 0.71201417]. \t  2.554798272340415 \t 2.927816116398253\n",
            "25     \t [0.18247832 0.87344963 0.60022047]. \t  \u001b[92m2.938319453370552\u001b[0m \t 2.938319453370552\n",
            "26     \t [0.06030413 0.11138544 0.92178117]. \t  0.49124076807692607 \t 2.938319453370552\n",
            "27     \t [0.12645112 0.80261085 0.63044693]. \t  2.800126405517988 \t 2.938319453370552\n",
            "28     \t [0.95024252 0.95266361 0.40567717]. \t  0.17505838902345658 \t 2.938319453370552\n",
            "29     \t [0.77931996 0.70884583 0.55471014]. \t  0.8143693810560588 \t 2.938319453370552\n",
            "30     \t [0.98265583 0.00159317 0.96059378]. \t  0.1290059718409104 \t 2.938319453370552\n",
            "31     \t [0.01807392 0.81368688 0.98082843]. \t  1.3266707709398322 \t 2.938319453370552\n",
            "32     \t [0.91169582 0.26398139 0.02446877]. \t  0.056762094606577974 \t 2.938319453370552\n",
            "33     \t [0.80542036 0.93073344 0.58505498]. \t  0.7409700208735187 \t 2.938319453370552\n",
            "34     \t [0.08887828 0.80400539 0.6141851 ]. \t  2.8865270941268943 \t 2.938319453370552\n",
            "35     \t [0.64262746 0.50518779 0.79424085]. \t  \u001b[92m3.4202876002328155\u001b[0m \t 3.4202876002328155\n",
            "36     \t [0.86634869 0.14229106 0.77209624]. \t  0.7956734891111156 \t 3.4202876002328155\n",
            "37     \t [0.79991674 0.35631883 0.97903321]. \t  1.5825716542459531 \t 3.4202876002328155\n",
            "38     \t [0.52897407 0.54925181 0.88068636]. \t  \u001b[92m3.7659958974425827\u001b[0m \t 3.7659958974425827\n",
            "39     \t [0.96662041 0.17014501 0.54127832]. \t  0.17694808258495845 \t 3.7659958974425827\n",
            "40     \t [0.61165389 0.07896505 0.17290464]. \t  0.6322229024122338 \t 3.7659958974425827\n",
            "41     \t [0.15815988 0.67675116 0.59988159]. \t  2.396099008710307 \t 3.7659958974425827\n",
            "42     \t [0.55307155 0.83113468 0.65867287]. \t  1.659746827776479 \t 3.7659958974425827\n",
            "43     \t [0.10993072 0.74821578 0.46707571]. \t  2.077590778069325 \t 3.7659958974425827\n",
            "44     \t [0.62939252 0.56615106 0.881394  ]. \t  3.7398318014237706 \t 3.7659958974425827\n",
            "45     \t [0.792842   0.95433978 0.03948485]. \t  0.0003512453288885632 \t 3.7659958974425827\n",
            "46     \t [0.45353138 0.15999598 0.48404493]. \t  0.29717680074648767 \t 3.7659958974425827\n",
            "47     \t [0.22384738 0.1550626  0.19208477]. \t  0.7812168459064405 \t 3.7659958974425827\n",
            "48     \t [0.48298156 0.28940149 0.37160245]. \t  0.5464451043545906 \t 3.7659958974425827\n",
            "49     \t [0.70331652 0.53653374 0.90614469]. \t  3.518286282461717 \t 3.7659958974425827\n",
            "50     \t [0.60261663 0.44378932 0.49433609]. \t  0.40890372618870746 \t 3.7659958974425827\n",
            "51     \t [0.5967522  0.98799453 0.06163179]. \t  0.0009981258980179624 \t 3.7659958974425827\n",
            "52     \t [0.71091254 0.69983393 0.23784189]. \t  0.059899164199538016 \t 3.7659958974425827\n",
            "53     \t [0.20273128 0.58872086 0.10767453]. \t  0.04954895770137224 \t 3.7659958974425827\n",
            "54     \t [0.66317934 0.93525471 0.68586192]. \t  0.9970043927416965 \t 3.7659958974425827\n",
            "55     \t [0.58764246 0.33637294 0.31484846]. \t  0.5169073769604036 \t 3.7659958974425827\n",
            "56     \t [0.86629224 0.04252853 0.05942087]. \t  0.1231852269548608 \t 3.7659958974425827\n",
            "57     \t [0.57283024 0.38759573 0.9278396 ]. \t  2.4697606835699473 \t 3.7659958974425827\n",
            "58     \t [0.27813955 0.37995999 0.75926053]. \t  2.5025501735589653 \t 3.7659958974425827\n",
            "59     \t [0.41186934 0.23513372 0.39403079]. \t  0.562511567165831 \t 3.7659958974425827\n",
            "60     \t [0.32108626 0.38704859 0.13114269]. \t  0.27577883497062416 \t 3.7659958974425827\n",
            "61     \t [0.30102575 0.18571067 0.69279238]. \t  0.7864759815082917 \t 3.7659958974425827\n",
            "62     \t [0.65089824 0.92138798 0.79668312]. \t  1.0511102182778886 \t 3.7659958974425827\n",
            "63     \t [0.0107059  0.76416342 0.50836861]. \t  2.499686990361062 \t 3.7659958974425827\n",
            "64     \t [0.77860221 0.52231521 0.39471157]. \t  0.19287222570968438 \t 3.7659958974425827\n",
            "65     \t [0.36787302 0.17657212 0.1280795 ]. \t  0.5396635722306211 \t 3.7659958974425827\n",
            "66     \t [0.79104461 0.10531206 0.92057209]. \t  0.4680412116273659 \t 3.7659958974425827\n",
            "67     \t [0.81682994 0.62089582 0.08571168]. \t  0.01656009674352917 \t 3.7659958974425827\n",
            "68     \t [0.88480013 0.54827671 0.58088944]. \t  0.6976109027858441 \t 3.7659958974425827\n",
            "69     \t [0.25229893 0.85664729 0.91213272]. \t  1.5378716112082156 \t 3.7659958974425827\n",
            "70     \t [0.50164155 0.6093058  0.23688983]. \t  0.12733227359775642 \t 3.7659958974425827\n",
            "71     \t [0.68805769 0.04971805 0.07463285]. \t  0.2312012001698145 \t 3.7659958974425827\n",
            "72     \t [0.16447824 0.05546928 0.91134554]. \t  0.31617261110468226 \t 3.7659958974425827\n",
            "73     \t [0.67234226 0.0349185  0.15057444]. \t  0.4712658901470614 \t 3.7659958974425827\n",
            "74     \t [0.3134     0.67794811 0.1465898 ]. \t  0.03974076975009795 \t 3.7659958974425827\n",
            "75     \t [0.30640295 0.92612678 0.86974016]. \t  1.119825071673948 \t 3.7659958974425827\n",
            "76     \t [0.17382888 0.31318679 0.10265048]. \t  0.26947136899597673 \t 3.7659958974425827\n",
            "77     \t [0.75990013 0.21311832 0.99415522]. \t  0.6388463637914399 \t 3.7659958974425827\n",
            "78     \t [0.89047902 0.64491292 0.32921552]. \t  0.08812881948180289 \t 3.7659958974425827\n",
            "79     \t [0.98696351 0.18431559 0.75848459]. \t  1.0022842918950319 \t 3.7659958974425827\n",
            "80     \t [0.35343701 0.18275473 0.59440178]. \t  0.3720688142423593 \t 3.7659958974425827\n",
            "81     \t [0.31539192 0.7792746  0.79955247]. \t  2.3896674916440004 \t 3.7659958974425827\n",
            "82     \t [0.14508814 0.18143484 0.64846608]. \t  0.5653094985500519 \t 3.7659958974425827\n",
            "83     \t [0.40737253 0.82678737 0.820015  ]. \t  1.9662984601658695 \t 3.7659958974425827\n",
            "84     \t [0.92766229 0.18593304 0.06503856]. \t  0.10954397157902122 \t 3.7659958974425827\n",
            "85     \t [0.32869146 0.05835603 0.47108057]. \t  0.2996102459165014 \t 3.7659958974425827\n",
            "86     \t [0.02684041 0.70539556 0.80612601]. \t  3.0104249940434444 \t 3.7659958974425827\n",
            "87     \t [0.05193446 0.06965373 0.43434944]. \t  0.3262841943372766 \t 3.7659958974425827\n",
            "88     \t [0.81004504 0.44320829 0.10766096]. \t  0.0898625192921795 \t 3.7659958974425827\n",
            "89     \t [0.61289151 0.38869684 0.43681509]. \t  0.29952453091027376 \t 3.7659958974425827\n",
            "90     \t [0.86832659 0.12322599 0.86359209]. \t  0.6746635330832248 \t 3.7659958974425827\n",
            "91     \t [0.86847025 0.1642101  0.21030744]. \t  0.4197267969060901 \t 3.7659958974425827\n",
            "92     \t [0.09465095 0.7470169  0.71808077]. \t  2.503304736154796 \t 3.7659958974425827\n",
            "93     \t [0.29236438 0.26645893 0.82895882]. \t  1.829975409816589 \t 3.7659958974425827\n",
            "94     \t [0.92417251 0.24429314 0.62430697]. \t  0.5906192293138568 \t 3.7659958974425827\n",
            "95     \t [0.55557028 0.3141471  0.73791255]. \t  1.8287978040338055 \t 3.7659958974425827\n",
            "96     \t [0.90070846 0.18634617 0.11644003]. \t  0.20613038331249936 \t 3.7659958974425827\n",
            "97     \t [0.92852018 0.18809886 0.72942986]. \t  0.9291753024647236 \t 3.7659958974425827\n",
            "98     \t [0.98478221 0.01874909 0.39303239]. \t  0.18371725118779617 \t 3.7659958974425827\n",
            "99     \t [0.5440732  0.8788577  0.07824594]. \t  0.002815235884962438 \t 3.7659958974425827\n",
            "100    \t [0.12195859 0.52278805 0.62882059]. \t  1.749290705274293 \t 3.7659958974425827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4HnKuoqteDo",
        "outputId": "a6da40ac-525f-4c3f-9cfe-98210cd740a1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_loser_19 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_19 = dGPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.55944558 0.76149199 0.24453155]. \t  0.09433080659504385 \t 2.524990008735946\n",
            "init   \t [0.77168947 0.35447208 0.7966974 ]. \t  2.524990008735946 \t 2.524990008735946\n",
            "init   \t [0.34600002 0.44387444 0.30330359]. \t  0.3914194506273641 \t 2.524990008735946\n",
            "init   \t [0.44211392 0.57320936 0.06566402]. \t  0.036928944981920925 \t 2.524990008735946\n",
            "init   \t [0.02408889 0.82062263 0.36513765]. \t  0.9772047942100802 \t 2.524990008735946\n",
            "1      \t [0.96499061 0.19185662 0.98563058]. \t  0.5815385407774518 \t 2.524990008735946\n",
            "2      \t [0.35021227 0.89949578 0.95419986]. \t  0.9824452595236306 \t 2.524990008735946\n",
            "3      \t [0.8364035  0.47078086 0.70932116]. \t  2.156884519592091 \t 2.524990008735946\n",
            "4      \t [0.46047313 0.25352007 0.98033154]. \t  0.9460814398415793 \t 2.524990008735946\n",
            "5      \t [0.87336482 0.09118839 0.45818428]. \t  0.17563272101768643 \t 2.524990008735946\n",
            "6      \t [0.70411181 0.42279097 0.83675882]. \t  \u001b[92m3.2477068869485466\u001b[0m \t 3.2477068869485466\n",
            "7      \t [0.74803474 0.60533411 0.93779535]. \t  3.0703841077028233 \t 3.2477068869485466\n",
            "8      \t [0.57560304 0.47085767 0.99523987]. \t  1.978350792868597 \t 3.2477068869485466\n",
            "9      \t [0.8489015  0.05888081 0.        ]. \t  0.05678630591260754 \t 3.2477068869485466\n",
            "10     \t [0.57851022 0.66686718 0.73448346]. \t  2.4468307948849954 \t 3.2477068869485466\n",
            "11     \t [0.01381987 0.98972179 0.81693901]. \t  0.8543229768829992 \t 3.2477068869485466\n",
            "12     \t [0.23320797 0.98878694 0.03639898]. \t  0.0008881824743265614 \t 3.2477068869485466\n",
            "13     \t [0.98986012 0.68494222 0.00835794]. \t  0.0016982651496915456 \t 3.2477068869485466\n",
            "14     \t [0.49101384 0.3773169  0.69597041]. \t  1.7869436007858153 \t 3.2477068869485466\n",
            "15     \t [0.55409487 0.02109967 0.87866785]. \t  0.26380236860584677 \t 3.2477068869485466\n",
            "16     \t [0.01606225 0.5779807  0.00526043]. \t  0.01062082046094439 \t 3.2477068869485466\n",
            "17     \t [0.99767452 0.26448242 0.08286597]. \t  0.08857016652594893 \t 3.2477068869485466\n",
            "18     \t [0.74305045 0.94567471 0.91399927]. \t  0.8206436917127765 \t 3.2477068869485466\n",
            "19     \t [0.00140369 0.98879627 0.22223029]. \t  0.09232469302045357 \t 3.2477068869485466\n",
            "20     \t [0.01163211 0.08638709 0.6501099 ]. \t  0.30126735795355003 \t 3.2477068869485466\n",
            "21     \t [0.79458708 0.54059858 0.95886999]. \t  2.764205728180439 \t 3.2477068869485466\n",
            "22     \t [0.68655261 0.47258864 0.84533407]. \t  \u001b[92m3.58096390901099\u001b[0m \t 3.58096390901099\n",
            "23     \t [0.09645332 0.95171827 0.70824437]. \t  1.7335812415162595 \t 3.58096390901099\n",
            "24     \t [0.29698544 0.17077213 0.37241439]. \t  0.6982054319324602 \t 3.58096390901099\n",
            "25     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.58096390901099\n",
            "26     \t [0.05937236 0.02006886 0.20927426]. \t  0.617373006092504 \t 3.58096390901099\n",
            "27     \t [0.56925757 0.90150448 0.25170707]. \t  0.10227822235163753 \t 3.58096390901099\n",
            "28     \t [0.05486895 0.31756584 0.13401811]. \t  0.2926632371936038 \t 3.58096390901099\n",
            "29     \t [0.50329508 0.22209549 0.64275971]. \t  0.6699701094542871 \t 3.58096390901099\n",
            "30     \t [0.61332035 0.09483347 0.84810284]. \t  0.563234333851149 \t 3.58096390901099\n",
            "31     \t [0.8527929  0.94871274 0.00283516]. \t  0.00011822558074294612 \t 3.58096390901099\n",
            "32     \t [0.56351103 0.61250042 0.93764616]. \t  3.0875063529706157 \t 3.58096390901099\n",
            "33     \t [0.2471381  0.01618808 0.14396125]. \t  0.547463513490101 \t 3.58096390901099\n",
            "34     \t [0.97547624 0.00351099 0.13726235]. \t  0.17554002587598122 \t 3.58096390901099\n",
            "35     \t [0.01895849 0.29888338 0.15063556]. \t  0.33152174386330074 \t 3.58096390901099\n",
            "36     \t [0.04503366 0.22594075 0.7378816 ]. \t  1.2153461880632406 \t 3.58096390901099\n",
            "37     \t [0.28985676 0.99895607 0.97563001]. \t  0.40933801513369666 \t 3.58096390901099\n",
            "38     \t [0.82309201 0.72860314 0.42102276]. \t  0.3286530780947166 \t 3.58096390901099\n",
            "39     \t [0.42547655 0.0600468  0.47533445]. \t  0.28650314809862054 \t 3.58096390901099\n",
            "40     \t [0.12523936 0.71993174 0.23134444]. \t  0.12419919234462012 \t 3.58096390901099\n",
            "41     \t [0.96160152 0.36463765 0.76840526]. \t  2.3586836952155927 \t 3.58096390901099\n",
            "42     \t [0.19076268 0.78430101 0.32881315]. \t  0.5982041585220237 \t 3.58096390901099\n",
            "43     \t [0.40813466 0.66810217 0.94607042]. \t  2.739423609810528 \t 3.58096390901099\n",
            "44     \t [0.9962501  0.90959763 0.21364914]. \t  0.009053813831887955 \t 3.58096390901099\n",
            "45     \t [0.89691926 0.09320847 0.15243687]. \t  0.2900082957339025 \t 3.58096390901099\n",
            "46     \t [0.08587334 0.74904582 0.5635006 ]. \t  2.7671860688773733 \t 3.58096390901099\n",
            "47     \t [0.08933768 0.62692057 0.7161244 ]. \t  2.717688340274245 \t 3.58096390901099\n",
            "48     \t [0.77119201 0.03859479 0.34265034]. \t  0.4890468547987831 \t 3.58096390901099\n",
            "49     \t [0.23163943 0.83329578 0.03876278]. \t  0.0021264459665956 \t 3.58096390901099\n",
            "50     \t [0.22967869 0.13702735 0.05796596]. \t  0.2523975141805462 \t 3.58096390901099\n",
            "51     \t [0.11237253 0.65966214 0.42607608]. \t  1.1995499965462453 \t 3.58096390901099\n",
            "52     \t [0.40230869 0.63370855 0.44560545]. \t  0.9785715601738311 \t 3.58096390901099\n",
            "53     \t [0.63854359 0.86124614 0.40485685]. \t  0.6643958568560102 \t 3.58096390901099\n",
            "54     \t [0.08013076 0.54133813 0.42400428]. \t  0.6870736161624138 \t 3.58096390901099\n",
            "55     \t [0.47446429 0.75289979 0.96713271]. \t  1.9044832473946198 \t 3.58096390901099\n",
            "56     \t [0.63207101 0.35509364 0.00158214]. \t  0.055427795107536884 \t 3.58096390901099\n",
            "57     \t [0.01274147 0.03203166 0.9689726 ]. \t  0.16871854597066624 \t 3.58096390901099\n",
            "58     \t [0.96799084 0.6911544  0.91876818]. \t  2.8023998550294 \t 3.58096390901099\n",
            "59     \t [0.9030704  0.69926889 0.30648406]. \t  0.06705165906432335 \t 3.58096390901099\n",
            "60     \t [0.37250422 0.30495954 0.09332783]. \t  0.28344377600126297 \t 3.58096390901099\n",
            "61     \t [0.1500338  0.2151008  0.13895462]. \t  0.48014866379633925 \t 3.58096390901099\n",
            "62     \t [0.72429142 0.87225717 0.0389365 ]. \t  0.0008069837359671329 \t 3.58096390901099\n",
            "63     \t [0.86377943 0.88216093 0.87175809]. \t  1.329082134525974 \t 3.58096390901099\n",
            "64     \t [0.52780961 0.26488685 0.19576341]. \t  0.6398531580442676 \t 3.58096390901099\n",
            "65     \t [0.60830622 0.53552546 0.48498891]. \t  0.5388088371927547 \t 3.58096390901099\n",
            "66     \t [0.43756068 0.48625546 0.57947842]. \t  1.0478624634428502 \t 3.58096390901099\n",
            "67     \t [0.5947457  0.26791511 0.43075178]. \t  0.3584817684980289 \t 3.58096390901099\n",
            "68     \t [0.3857746  0.28585579 0.81923471]. \t  2.0091479654578412 \t 3.58096390901099\n",
            "69     \t [0.36378236 0.43626916 0.27521403]. \t  0.3957733306512729 \t 3.58096390901099\n",
            "70     \t [0.31011784 0.11476409 0.94703201]. \t  0.4384406165894201 \t 3.58096390901099\n",
            "71     \t [0.96106242 0.25484353 0.2852708 ]. \t  0.28735363514336076 \t 3.58096390901099\n",
            "72     \t [0.67275161 0.90584988 0.19440821]. \t  0.024581555798376124 \t 3.58096390901099\n",
            "73     \t [0.42856543 0.24123473 0.06572751]. \t  0.2506139230144343 \t 3.58096390901099\n",
            "74     \t [0.11795803 0.82308563 0.88447371]. \t  1.9627816582310302 \t 3.58096390901099\n",
            "75     \t [0.63786628 0.39515356 0.74653427]. \t  2.4223442758118603 \t 3.58096390901099\n",
            "76     \t [0.24354281 0.26780065 0.85639567]. \t  1.8094612962111003 \t 3.58096390901099\n",
            "77     \t [0.66767182 0.18181134 0.57205183]. \t  0.28984354895333153 \t 3.58096390901099\n",
            "78     \t [0.30974426 0.6692461  0.01863442]. \t  0.007649779620783134 \t 3.58096390901099\n",
            "79     \t [0.39396999 0.01359379 0.42209846]. \t  0.44288926847849175 \t 3.58096390901099\n",
            "80     \t [0.00622778 0.70489465 0.76489267]. \t  2.78904023721274 \t 3.58096390901099\n",
            "81     \t [0.18417019 0.65820939 0.99740273]. \t  1.9631796307825014 \t 3.58096390901099\n",
            "82     \t [0.58584054 0.3550544  0.10439691]. \t  0.2224736789213607 \t 3.58096390901099\n",
            "83     \t [0.87431571 0.63954261 0.41195434]. \t  0.19574496210540726 \t 3.58096390901099\n",
            "84     \t [0.69349169 0.58534044 0.62972631]. \t  1.3302088711861788 \t 3.58096390901099\n",
            "85     \t [0.15209794 0.15202414 0.68155881]. \t  0.5943639086588272 \t 3.58096390901099\n",
            "86     \t [0.79200461 0.63794397 0.74960302]. \t  2.5584025352025037 \t 3.58096390901099\n",
            "87     \t [0.34459962 0.41262583 0.27340987]. \t  0.4449259337296729 \t 3.58096390901099\n",
            "88     \t [0.20535494 0.65975039 0.7603884 ]. \t  3.0016424961664874 \t 3.58096390901099\n",
            "89     \t [0.46426209 0.42973631 0.24545965]. \t  0.3772305907675988 \t 3.58096390901099\n",
            "90     \t [0.19736085 0.98220752 0.1427098 ]. \t  0.016314629956979514 \t 3.58096390901099\n",
            "91     \t [0.66980287 0.41687428 0.04706623]. \t  0.07243416592273004 \t 3.58096390901099\n",
            "92     \t [0.04429268 0.08927876 0.88714029]. \t  0.47583883365716306 \t 3.58096390901099\n",
            "93     \t [0.79558746 0.93614399 0.12043335]. \t  0.0028177863330248122 \t 3.58096390901099\n",
            "94     \t [0.41132162 0.13810393 0.80963589]. \t  0.8209804264571146 \t 3.58096390901099\n",
            "95     \t [0.6393976  0.57288184 0.79072809]. \t  3.3957518147380443 \t 3.58096390901099\n",
            "96     \t [0.40173052 0.16642951 0.80461023]. \t  1.0008823398027678 \t 3.58096390901099\n",
            "97     \t [0.91045104 0.13395119 0.72920788]. \t  0.6572687768834146 \t 3.58096390901099\n",
            "98     \t [0.98245798 0.35708702 0.41811415]. \t  0.12973870484146707 \t 3.58096390901099\n",
            "99     \t [0.38571383 0.86707224 0.2595399 ]. \t  0.17830519849965276 \t 3.58096390901099\n",
            "100    \t [0.00128665 0.16274988 0.01869641]. \t  0.10224037162700153 \t 3.58096390901099\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfiDmH_bteGg",
        "outputId": "daf2dd3c-b8ae-4960-afb4-a4b38f2b6c95"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_loser_20 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_20 = dGPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.57051729 0.56452876 0.48844183]. \t  0.675391399411646 \t 0.675391399411646\n",
            "init   \t [0.33647775 0.37586818 0.53203587]. \t  0.5331349538596052 \t 0.675391399411646\n",
            "init   \t [0.06810629 0.58452906 0.23789776]. \t  0.14747335095307315 \t 0.675391399411646\n",
            "init   \t [0.16075658 0.15211915 0.12706922]. \t  0.48089725804912437 \t 0.675391399411646\n",
            "init   \t [0.32744117 0.69415387 0.35896647]. \t  0.6289408047543804 \t 0.675391399411646\n",
            "1      \t [0.837611   0.97862966 0.10314233]. \t  0.0013418779384681767 \t 0.675391399411646\n",
            "2      \t [0.4192146  0.99013113 0.80759551]. \t  \u001b[92m0.8040311121195758\u001b[0m \t 0.8040311121195758\n",
            "3      \t [0.58490171 0.         0.        ]. \t  0.08889155882157919 \t 0.8040311121195758\n",
            "4      \t [0.56803068 0.99098437 0.50767036]. \t  \u001b[92m1.310904866486966\u001b[0m \t 1.310904866486966\n",
            "5      \t [0.84822682 0.9788853  0.85466061]. \t  0.6702188994918671 \t 1.310904866486966\n",
            "6      \t [0.84509259 0.         0.89862994]. \t  0.19194384543565085 \t 1.310904866486966\n",
            "7      \t [0.67407812 0.99571082 0.58414561]. \t  1.010438400626982 \t 1.310904866486966\n",
            "8      \t [0.25706606 0.99173014 0.15932733]. \t  0.02275764294278305 \t 1.310904866486966\n",
            "9      \t [0.         0.         0.31158648]. \t  0.5470921834405232 \t 1.310904866486966\n",
            "10     \t [0.97947894 0.27434002 0.93560419]. \t  \u001b[92m1.395891124057239\u001b[0m \t 1.395891124057239\n",
            "11     \t [0.87557865 0.50601557 0.95614793]. \t  \u001b[92m2.7139777944223322\u001b[0m \t 2.7139777944223322\n",
            "12     \t [0.75793425 0.69772209 0.98756189]. \t  1.927540683132728 \t 2.7139777944223322\n",
            "13     \t [0.12414169 0.99832635 0.576206  ]. \t  2.566437553023501 \t 2.7139777944223322\n",
            "14     \t [0.75104891 0.2645525  0.9551541 ]. \t  1.199937865485019 \t 2.7139777944223322\n",
            "15     \t [0.97780808 0.58615943 0.81664382]. \t  \u001b[92m3.457183993275378\u001b[0m \t 3.457183993275378\n",
            "16     \t [0.98736861 0.72749542 0.80627068]. \t  2.471770357321162 \t 3.457183993275378\n",
            "17     \t [0.89238206 0.48690832 0.72463641]. \t  2.3802531328000907 \t 3.457183993275378\n",
            "18     \t [0.0105289  0.05708276 0.89134199]. \t  0.34912691446217564 \t 3.457183993275378\n",
            "19     \t [0.37352692 0.01567742 0.7968013 ]. \t  0.2930995645423669 \t 3.457183993275378\n",
            "20     \t [0.03867006 0.69475133 0.7534113 ]. \t  2.7906836037923957 \t 3.457183993275378\n",
            "21     \t [0.02486294 0.57208497 0.57980776]. \t  1.6658299552603169 \t 3.457183993275378\n",
            "22     \t [0.04885327 0.69232283 0.89481865]. \t  3.0944266830502576 \t 3.457183993275378\n",
            "23     \t [0.00853456 0.53126337 0.97882715]. \t  2.411907692500336 \t 3.457183993275378\n",
            "24     \t [0.0286142  0.81411405 0.82827108]. \t  2.131555367069342 \t 3.457183993275378\n",
            "25     \t [1.05734595e-14 1.67322102e-14 2.84724772e-14]. \t  0.06797411659016761 \t 3.457183993275378\n",
            "26     \t [0.23771443 0.50552904 0.94685943]. \t  2.927759953411534 \t 3.457183993275378\n",
            "27     \t [0.98694874 0.41002003 0.01130985]. \t  0.018869161976546287 \t 3.457183993275378\n",
            "28     \t [0.16058444 0.61398367 0.90177234]. \t  \u001b[92m3.5144420137944152\u001b[0m \t 3.5144420137944152\n",
            "29     \t [0.20739069 0.06601235 0.08339898]. \t  0.3266638276141411 \t 3.5144420137944152\n",
            "30     \t [0.09177414 0.25874783 0.46582282]. \t  0.3098176725068744 \t 3.5144420137944152\n",
            "31     \t [0.59223853 0.49019339 0.514645  ]. \t  0.5636584653373499 \t 3.5144420137944152\n",
            "32     \t [0.72667423 0.33162898 0.13623505]. \t  0.25692809763748053 \t 3.5144420137944152\n",
            "33     \t [0.82268004 0.38351431 0.59432374]. \t  0.6940951772220839 \t 3.5144420137944152\n",
            "34     \t [0.68789817 0.31596021 0.63752013]. \t  0.9215222723989691 \t 3.5144420137944152\n",
            "35     \t [0.57752401 0.80350725 0.18556134]. \t  0.0312731366325119 \t 3.5144420137944152\n",
            "36     \t [0.19787002 0.1977767  0.83017448]. \t  1.2277701990258327 \t 3.5144420137944152\n",
            "37     \t [0.16067943 0.02817582 0.75723975]. \t  0.3137567177172803 \t 3.5144420137944152\n",
            "38     \t [0.72541457 0.57787923 0.33583694]. \t  0.16900693522232732 \t 3.5144420137944152\n",
            "39     \t [0.47827067 0.1364753  0.25670492]. \t  0.9585843576107357 \t 3.5144420137944152\n",
            "40     \t [0.23343479 0.59138584 0.30642539]. \t  0.3000119022807604 \t 3.5144420137944152\n",
            "41     \t [0.43976237 0.92250638 0.44201957]. \t  1.447177776885933 \t 3.5144420137944152\n",
            "42     \t [0.01103052 0.19701301 0.73397797]. \t  1.013933980043001 \t 3.5144420137944152\n",
            "43     \t [0.24307251 0.26563776 0.48045205]. \t  0.33360810781241124 \t 3.5144420137944152\n",
            "44     \t [0.26541441 0.20442255 0.21154234]. \t  0.8182007654284428 \t 3.5144420137944152\n",
            "45     \t [0.17598597 0.39637633 0.93595256]. \t  2.4420997621465594 \t 3.5144420137944152\n",
            "46     \t [0.37043523 0.84876641 0.75157091]. \t  1.8070542759678743 \t 3.5144420137944152\n",
            "47     \t [0.65497411 0.21224035 0.76263526]. \t  1.2312315716964708 \t 3.5144420137944152\n",
            "48     \t [0.40726517 0.24250585 0.70979656]. \t  1.1742944616992397 \t 3.5144420137944152\n",
            "49     \t [0.37505632 0.11231425 0.63764472]. \t  0.3404462066127 \t 3.5144420137944152\n",
            "50     \t [0.06155421 0.86108384 0.97645898]. \t  1.069465683646719 \t 3.5144420137944152\n",
            "51     \t [0.64534578 0.18739021 0.25470415]. \t  0.7539909915086813 \t 3.5144420137944152\n",
            "52     \t [0.28313966 0.41353302 0.27735699]. \t  0.4382617694804395 \t 3.5144420137944152\n",
            "53     \t [0.38503878 0.50627476 0.87408068]. \t  \u001b[92m3.727265148396073\u001b[0m \t 3.727265148396073\n",
            "54     \t [0.65515322 0.48343052 0.95854661]. \t  2.6390659543517048 \t 3.727265148396073\n",
            "55     \t [0.35869692 0.89270219 0.01228564]. \t  0.0007101787817227387 \t 3.727265148396073\n",
            "56     \t [0.61181179 0.36417187 0.10662379]. \t  0.20987490090205413 \t 3.727265148396073\n",
            "57     \t [0.93400767 0.1910905  0.00216429]. \t  0.04407628331929119 \t 3.727265148396073\n",
            "58     \t [0.26830483 0.30305693 0.19889753]. \t  0.5988107042239119 \t 3.727265148396073\n",
            "59     \t [0.27043163 0.2608227  0.27860543]. \t  0.7938419092915375 \t 3.727265148396073\n",
            "60     \t [0.34468722 0.65659973 0.86325841]. \t  3.5060239611180912 \t 3.727265148396073\n",
            "61     \t [0.16877957 0.60131644 0.76137919]. \t  3.1874531127871046 \t 3.727265148396073\n",
            "62     \t [0.8598495  0.09687106 0.70544899]. \t  0.45449322998722325 \t 3.727265148396073\n",
            "63     \t [0.16356561 0.91951492 0.89582497]. \t  1.104444384511528 \t 3.727265148396073\n",
            "64     \t [0.09449532 0.3713226  0.38575202]. \t  0.3889635066766884 \t 3.727265148396073\n",
            "65     \t [0.95270096 0.07614477 0.80783348]. \t  0.4895141696876869 \t 3.727265148396073\n",
            "66     \t [0.97256952 0.53652074 0.09495664]. \t  0.023834490049896122 \t 3.727265148396073\n",
            "67     \t [0.04743825 0.43780258 0.59339554]. \t  1.092959659135892 \t 3.727265148396073\n",
            "68     \t [0.37743864 0.46176713 0.30895875]. \t  0.36325840663334485 \t 3.727265148396073\n",
            "69     \t [0.20933675 0.14745675 0.3301066 ]. \t  0.8199469925635752 \t 3.727265148396073\n",
            "70     \t [0.09873953 0.08344149 0.80692709]. \t  0.5296855557748921 \t 3.727265148396073\n",
            "71     \t [0.81629061 0.76645914 0.85429327]. \t  2.4207171980182767 \t 3.727265148396073\n",
            "72     \t [0.50729649 0.54241612 0.82395397]. \t  \u001b[92m3.7483005109548158\u001b[0m \t 3.7483005109548158\n",
            "73     \t [0.84389819 0.06320748 0.9842202 ]. \t  0.20065465243633962 \t 3.7483005109548158\n",
            "74     \t [0.23833763 0.14661407 0.50546705]. \t  0.25480671405943567 \t 3.7483005109548158\n",
            "75     \t [0.42865986 0.34679638 0.33070755]. \t  0.5504560083351115 \t 3.7483005109548158\n",
            "76     \t [0.49552011 0.19991211 0.82820314]. \t  1.2497044559822954 \t 3.7483005109548158\n",
            "77     \t [0.41202769 0.53309334 0.45145642]. \t  0.6403108764028697 \t 3.7483005109548158\n",
            "78     \t [0.96633121 0.2334966  0.14465115]. \t  0.19060165348368643 \t 3.7483005109548158\n",
            "79     \t [0.83820039 0.8749031  0.41691695]. \t  0.3498651032944783 \t 3.7483005109548158\n",
            "80     \t [0.86449676 0.79579375 0.91435755]. \t  1.9827797305941421 \t 3.7483005109548158\n",
            "81     \t [0.93400194 0.03894668 0.94800781]. \t  0.2100098811139738 \t 3.7483005109548158\n",
            "82     \t [0.00313698 0.46054941 0.47015367]. \t  0.5733329196058139 \t 3.7483005109548158\n",
            "83     \t [0.29671487 0.24949484 0.68979245]. \t  1.0839801349581415 \t 3.7483005109548158\n",
            "84     \t [0.76412824 0.85355953 0.26043253]. \t  0.06421503493392128 \t 3.7483005109548158\n",
            "85     \t [0.72595035 0.82629906 0.78520751]. \t  1.643169958970201 \t 3.7483005109548158\n",
            "86     \t [0.21010636 0.94073467 0.28099994]. \t  0.2948632701610779 \t 3.7483005109548158\n",
            "87     \t [0.47684429 0.79057807 0.68083692]. \t  1.937658177452781 \t 3.7483005109548158\n",
            "88     \t [7.93175689e-01 2.53034714e-01 1.34429246e-04]. \t  0.05690454852728574 \t 3.7483005109548158\n",
            "89     \t [0.20677644 0.64188221 0.66110577]. \t  2.3710065060725087 \t 3.7483005109548158\n",
            "90     \t [0.59187883 0.47656673 0.53049912]. \t  0.5999908176225892 \t 3.7483005109548158\n",
            "91     \t [0.01786845 0.04463263 0.66603289]. \t  0.24235692844368947 \t 3.7483005109548158\n",
            "92     \t [0.58293223 0.27863226 0.96653618]. \t  1.2122270629110494 \t 3.7483005109548158\n",
            "93     \t [0.86670758 0.64452272 0.24326527]. \t  0.04633779647468605 \t 3.7483005109548158\n",
            "94     \t [0.23969353 0.81773721 0.53651922]. \t  2.8242970536633676 \t 3.7483005109548158\n",
            "95     \t [0.30600453 0.74269544 0.00722497]. \t  0.002870525365569 \t 3.7483005109548158\n",
            "96     \t [0.84062088 0.07715726 0.05192499]. \t  0.1255534664247606 \t 3.7483005109548158\n",
            "97     \t [0.0562497  0.94139369 0.10719894]. \t  0.007371544767094565 \t 3.7483005109548158\n",
            "98     \t [0.10695383 0.05917109 0.42824318]. \t  0.37253454997322105 \t 3.7483005109548158\n",
            "99     \t [0.16625909 0.72276611 0.68183679]. \t  2.5378207743566707 \t 3.7483005109548158\n",
            "100    \t [0.49060885 0.6520218  0.17341417]. \t  0.05712173825425049 \t 3.7483005109548158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kM4bcwSteJO",
        "outputId": "8f6c23e1-046a-4122-e747-f0f403631450"
      },
      "source": [
        "end_lose = time.time()\r\n",
        "end_lose\r\n",
        "\r\n",
        "time_lose = end_lose - start_lose\r\n",
        "time_lose\r\n",
        "\r\n",
        "start_win = time.time()\r\n",
        "start_win"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1615031431.9247005"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NMzLM7nteQH",
        "outputId": "3b92bc4a-577f-472f-f7df-b4c73165368f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_winner_1 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_1 = dGPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.61217018 0.16906975 0.43605902]. \t  0.37345684462559386 \t 2.3951473341797507\n",
            "init   \t [0.76926247 0.2953253  0.14916296]. \t  0.296162062221701 \t 2.3951473341797507\n",
            "init   \t [0.02247832 0.42022449 0.23868214]. \t  0.2904113287153621 \t 2.3951473341797507\n",
            "init   \t [0.33765619 0.99071246 0.23772645]. \t  0.11013785080555143 \t 2.3951473341797507\n",
            "init   \t [0.08119266 0.66960024 0.62124292]. \t  2.3951473341797507 \t 2.3951473341797507\n",
            "1      \t [0.00485804 0.68286261 0.96018946]. \t  \u001b[92m2.420367869177603\u001b[0m \t 2.420367869177603\n",
            "2      \t [0.02987749 0.90113801 0.80016743]. \t  1.4600402794182648 \t 2.420367869177603\n",
            "3      \t [0.0703517  0.46349377 0.80501238]. \t  \u001b[92m3.3965419857368806\u001b[0m \t 3.3965419857368806\n",
            "4      \t [0.04008199 0.32364229 0.86361405]. \t  2.315277186390045 \t 3.3965419857368806\n",
            "5      \t [0.96143082 0.99972737 0.98139816]. \t  0.3689324168461976 \t 3.3965419857368806\n",
            "6      \t [0.38799203 0.68848673 0.9817648 ]. \t  2.094114721377349 \t 3.3965419857368806\n",
            "7      \t [0.86600723 0.9103292  0.03233   ]. \t  0.0003157758028876371 \t 3.3965419857368806\n",
            "8      \t [0.14551119 0.5802623  0.71326391]. \t  2.693265641355197 \t 3.3965419857368806\n",
            "9      \t [0.81898631 0.2515562  0.97560839]. \t  0.9535945954607079 \t 3.3965419857368806\n",
            "10     \t [0.81269136 0.02513207 0.05336369]. \t  0.12895269805080278 \t 3.3965419857368806\n",
            "11     \t [0.00617101 0.5245956  0.66787481]. \t  2.0755171730286426 \t 3.3965419857368806\n",
            "12     \t [0.06622879 0.92342617 0.14052877]. \t  0.01764013809495383 \t 3.3965419857368806\n",
            "13     \t [0.43197785 0.32276549 0.96859197]. \t  1.50206782402409 \t 3.3965419857368806\n",
            "14     \t [0.97021972 0.09138843 0.44224635]. \t  0.14845787949362158 \t 3.3965419857368806\n",
            "15     \t [0.17101542 0.0660709  0.00298603]. \t  0.10653683352184405 \t 3.3965419857368806\n",
            "16     \t [0.15400911 0.50234951 0.91375522]. \t  3.370092465788365 \t 3.3965419857368806\n",
            "17     \t [0.32437953 0.29485054 0.        ]. \t  0.08495557191246492 \t 3.3965419857368806\n",
            "18     \t [0.15439004 0.64675345 0.89391082]. \t  \u001b[92m3.4264239711136035\u001b[0m \t 3.4264239711136035\n",
            "19     \t [0.04340663 0.52459126 0.90000584]. \t  \u001b[92m3.5616458703828737\u001b[0m \t 3.5616458703828737\n",
            "20     \t [0.08962882 0.57481261 0.8696631 ]. \t  \u001b[92m3.7969711997700597\u001b[0m \t 3.7969711997700597\n",
            "21     \t [0.0519162  0.59360793 0.89970723]. \t  3.5707238379797306 \t 3.7969711997700597\n",
            "22     \t [0.03518789 0.51325407 0.95043018]. \t  2.864714518241448 \t 3.7969711997700597\n",
            "23     \t [0.07699087 0.50775658 0.87446323]. \t  3.7004565592772956 \t 3.7969711997700597\n",
            "24     \t [0.07414375 0.54304308 0.82810303]. \t  3.77544594612987 \t 3.7969711997700597\n",
            "25     \t [0.02938756 0.45830168 0.01424345]. \t  0.03241094855247772 \t 3.7969711997700597\n",
            "26     \t [0.15112422 0.57169865 0.8695877 ]. \t  \u001b[92m3.8131284252695528\u001b[0m \t 3.8131284252695528\n",
            "27     \t [0.15043162 0.60970521 0.85780357]. \t  3.7470332461824105 \t 3.8131284252695528\n",
            "28     \t [0.14473087 0.51249741 0.76906355]. \t  3.2581581107711646 \t 3.8131284252695528\n",
            "29     \t [0.14297099 0.60901975 0.81522079]. \t  3.628358593825721 \t 3.8131284252695528\n",
            "30     \t [0.11181592 0.64508821 0.86932481]. \t  3.552307619911147 \t 3.8131284252695528\n",
            "31     \t [0.11344219 0.51687409 0.82664435]. \t  3.7372328179382146 \t 3.8131284252695528\n",
            "32     \t [0.05124973 0.62995035 0.84292679]. \t  3.631441632928988 \t 3.8131284252695528\n",
            "33     \t [0.06376071 0.61234984 0.90624417]. \t  3.4593974234689147 \t 3.8131284252695528\n",
            "34     \t [0.90386437 0.66454249 0.99932738]. \t  1.8614032726025567 \t 3.8131284252695528\n",
            "35     \t [0.10807051 0.58013227 0.85225737]. \t  \u001b[92m3.820401171130394\u001b[0m \t 3.820401171130394\n",
            "36     \t [0.13983935 0.60982176 0.8363651 ]. \t  3.7212576581323322 \t 3.820401171130394\n",
            "37     \t [0.13551095 0.6147752  0.86099234]. \t  3.7217456557246953 \t 3.820401171130394\n",
            "38     \t [0.03374698 0.59886308 0.8778744 ]. \t  3.700730318087712 \t 3.820401171130394\n",
            "39     \t [0.15754506 0.52463719 0.85708157]. \t  3.8145344406210797 \t 3.820401171130394\n",
            "40     \t [0.05766926 0.62078636 0.81547445]. \t  3.5677144595940287 \t 3.820401171130394\n",
            "41     \t [0.09340409 0.63120493 0.81880677]. \t  3.5500371462617144 \t 3.820401171130394\n",
            "42     \t [0.06911454 0.58625638 0.80149056]. \t  3.577721018787953 \t 3.820401171130394\n",
            "43     \t [0.07606017 0.54771526 0.85297771]. \t  \u001b[92m3.831556940829655\u001b[0m \t 3.831556940829655\n",
            "44     \t [0.09334746 0.55285068 0.80751987]. \t  3.6622062341197514 \t 3.831556940829655\n",
            "45     \t [0.0856121  0.54555675 0.80729877]. \t  3.65681561927263 \t 3.831556940829655\n",
            "46     \t [0.07429079 0.62703882 0.89852588]. \t  3.4744839373764194 \t 3.831556940829655\n",
            "47     \t [0.12669526 0.48199955 0.85021084]. \t  3.662119062527236 \t 3.831556940829655\n",
            "48     \t [0.10740407 0.56352774 0.88559842]. \t  3.7320264760349646 \t 3.831556940829655\n",
            "49     \t [0.04945247 0.58242812 0.84467493]. \t  3.7950268770817575 \t 3.831556940829655\n",
            "50     \t [0.21322008 0.5356371  0.90724896]. \t  3.5426411387974746 \t 3.831556940829655\n",
            "51     \t [0.24219106 0.49246531 0.84003904]. \t  3.7164745928806022 \t 3.831556940829655\n",
            "52     \t [0.15384586 0.58434252 0.88508595]. \t  3.7217618326684545 \t 3.831556940829655\n",
            "53     \t [0.15277728 0.46494925 0.86828361]. \t  3.53839914392516 \t 3.831556940829655\n",
            "54     \t [0.05871781 0.58247926 0.82580641]. \t  3.737769144636796 \t 3.831556940829655\n",
            "55     \t [0.03936718 0.53592166 0.85644784]. \t  3.8082066855287193 \t 3.831556940829655\n",
            "56     \t [0.16685585 0.59337278 0.83909431]. \t  3.7841021865915647 \t 3.831556940829655\n",
            "57     \t [0.18023486 0.50748788 0.89078031]. \t  3.6178859885729238 \t 3.831556940829655\n",
            "58     \t [0.0138844  0.53231397 0.8277841 ]. \t  3.7445261796682767 \t 3.831556940829655\n",
            "59     \t [0.1111081  0.62048332 0.84287953]. \t  3.6892565551300365 \t 3.831556940829655\n",
            "60     \t [0.02885089 0.55207263 0.87813038]. \t  3.7550264615668745 \t 3.831556940829655\n",
            "61     \t [0.23240813 0.56862728 0.86591069]. \t  \u001b[92m3.8366886340122566\u001b[0m \t 3.8366886340122566\n",
            "62     \t [0.39244784 0.58152129 0.85740287]. \t  3.8297001455434136 \t 3.8366886340122566\n",
            "63     \t [0.56064791 0.58202859 0.86492456]. \t  3.791289442464187 \t 3.8366886340122566\n",
            "64     \t [0.31827821 0.53394242 0.86150596]. \t  \u001b[92m3.8379469493458522\u001b[0m \t 3.8379469493458522\n",
            "65     \t [0.57902892 0.5184693  0.86955074]. \t  3.7580640322784475 \t 3.8379469493458522\n",
            "66     \t [0.55318812 0.54323864 0.84986328]. \t  3.825430630151992 \t 3.8379469493458522\n",
            "67     \t [0.45759583 0.56593622 0.85863812]. \t  \u001b[92m3.8418016497977137\u001b[0m \t 3.8418016497977137\n",
            "68     \t [0.57388588 0.53745379 0.86788768]. \t  3.799508973578498 \t 3.8418016497977137\n",
            "69     \t [0.39420189 0.52151271 0.86344924]. \t  3.8072534253427004 \t 3.8418016497977137\n",
            "70     \t [0.41413347 0.55100893 0.85483383]. \t  \u001b[92m3.854200163143677\u001b[0m \t 3.854200163143677\n",
            "71     \t [0.2810529  0.56713554 0.83918976]. \t  3.8373340503830717 \t 3.854200163143677\n",
            "72     \t [0.57754488 0.52251891 0.84083805]. \t  3.777751914742412 \t 3.854200163143677\n",
            "73     \t [0.51904291 0.56046917 0.84054545]. \t  3.814924587958929 \t 3.854200163143677\n",
            "74     \t [0.32843507 0.55776034 0.85564156]. \t  \u001b[92m3.8601181150156396\u001b[0m \t 3.8601181150156396\n",
            "75     \t [0.2907811  0.52827028 0.87908878]. \t  3.763477713825567 \t 3.8601181150156396\n",
            "76     \t [0.48786391 0.54020174 0.85400544]. \t  3.837902241875587 \t 3.8601181150156396\n",
            "77     \t [0.41376098 0.49820659 0.8683911 ]. \t  3.718178253070668 \t 3.8601181150156396\n",
            "78     \t [0.20373922 0.54174141 0.85098626]. \t  3.850281023388409 \t 3.8601181150156396\n",
            "79     \t [0.42910991 0.54554535 0.87320513]. \t  3.8116124862052567 \t 3.8601181150156396\n",
            "80     \t [0.49157402 0.54783684 0.85914493]. \t  3.8397537960517565 \t 3.8601181150156396\n",
            "81     \t [0.27035392 0.55867834 0.83710031]. \t  3.8368806542998137 \t 3.8601181150156396\n",
            "82     \t [0.53750538 0.5421252  0.85315829]. \t  3.83004783092102 \t 3.8601181150156396\n",
            "83     \t [0.35704921 0.56500864 0.86251232]. \t  3.8480792813093014 \t 3.8601181150156396\n",
            "84     \t [0.55500597 0.53157606 0.828363  ]. \t  3.750915429453361 \t 3.8601181150156396\n",
            "85     \t [0.35593435 0.57480541 0.87190279]. \t  3.8134599796364874 \t 3.8601181150156396\n",
            "86     \t [0.28299694 0.53556773 0.86082624]. \t  3.841023413342353 \t 3.8601181150156396\n",
            "87     \t [0.28382102 0.59364834 0.86280712]. \t  3.8016553754319933 \t 3.8601181150156396\n",
            "88     \t [0.30380949 0.55769022 0.86120773]. \t  3.8547433887881137 \t 3.8601181150156396\n",
            "89     \t [0.2319689  0.54942329 0.85765597]. \t  3.8554196688113405 \t 3.8601181150156396\n",
            "90     \t [0.61179451 0.61607218 0.8421558 ]. \t  3.6499133418032965 \t 3.8601181150156396\n",
            "91     \t [0.56784737 0.52974551 0.84134806]. \t  3.7934973311563804 \t 3.8601181150156396\n",
            "92     \t [0.52241762 0.55903243 0.84245926]. \t  3.820130264494512 \t 3.8601181150156396\n",
            "93     \t [0.50054356 0.61749047 0.8483264 ]. \t  3.691250339010449 \t 3.8601181150156396\n",
            "94     \t [0.54255318 0.51820739 0.84955953]. \t  3.7902912131971513 \t 3.8601181150156396\n",
            "95     \t [0.52206587 0.53580877 0.85782882]. \t  3.8255721769700353 \t 3.8601181150156396\n",
            "96     \t [0.30869893 0.56007496 0.83398366]. \t  3.8252668182690543 \t 3.8601181150156396\n",
            "97     \t [0.50482164 0.54484602 0.83217238]. \t  3.790702232639328 \t 3.8601181150156396\n",
            "98     \t [0.49113092 0.52742707 0.86699087]. \t  3.8009204903508484 \t 3.8601181150156396\n",
            "99     \t [0.57164274 0.57348895 0.86770675]. \t  3.797361971696455 \t 3.8601181150156396\n",
            "100    \t [0.53855338 0.53081035 0.84089627]. \t  3.8006419814990315 \t 3.8601181150156396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBLcW6tlteS2",
        "outputId": "55099bd3-a335-4fda-c9eb-5b3da9b6e788"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_winner_2 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_2 = dGPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.85198549 0.0739036  0.89493176]. \t  0.3999083566189884 \t 2.6229838112516717\n",
            "init   \t [0.43649355 0.12767773 0.57585787]. \t  0.24461577848211966 \t 2.6229838112516717\n",
            "init   \t [0.84047092 0.43512055 0.69591056]. \t  1.8897715258798413 \t 2.6229838112516717\n",
            "init   \t [0.6846381  0.70064837 0.77969426]. \t  2.6229838112516717 \t 2.6229838112516717\n",
            "init   \t [0.64274937 0.96102617 0.10846489]. \t  0.003309399496320042 \t 2.6229838112516717\n",
            "1      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.6229838112516717\n",
            "2      \t [0.02182931 0.83935188 0.95475314]. \t  1.381392378991308 \t 2.6229838112516717\n",
            "3      \t [0.56102262 0.63458445 0.82359713]. \t  \u001b[92m3.4911076536027035\u001b[0m \t 3.4911076536027035\n",
            "4      \t [0.50835819 0.38033941 0.95640995]. \t  2.0654454825687214 \t 3.4911076536027035\n",
            "5      \t [0.06744718 0.70192837 0.01433115]. \t  0.003996268705815771 \t 3.4911076536027035\n",
            "6      \t [0.45067926 0.76261712 0.98454777]. \t  1.627202746319736 \t 3.4911076536027035\n",
            "7      \t [0.15193181 0.69229563 0.55457187]. \t  2.3881389683460417 \t 3.4911076536027035\n",
            "8      \t [0.49931903 0.53226856 0.65222961]. \t  1.7398642754916769 \t 3.4911076536027035\n",
            "9      \t [0.00296845 0.9765885  0.19537327]. \t  0.0545303625110021 \t 3.4911076536027035\n",
            "10     \t [3.35374404e-15 3.35377729e-15 3.35371813e-15]. \t  0.06797411659013694 \t 3.4911076536027035\n",
            "11     \t [0.8791368  0.52734748 0.97932208]. \t  2.369507065337188 \t 3.4911076536027035\n",
            "12     \t [0.03342643 0.49520893 0.75978257]. \t  3.0826971071170313 \t 3.4911076536027035\n",
            "13     \t [0.90888605 0.96545518 0.00619077]. \t  8.893896213612553e-05 \t 3.4911076536027035\n",
            "14     \t [0.12838526 0.66047005 0.73527799]. \t  2.816704275992374 \t 3.4911076536027035\n",
            "15     \t [0.68817523 0.36989159 0.86189511]. \t  2.778410504372129 \t 3.4911076536027035\n",
            "16     \t [0.97518227 0.01305882 0.58624255]. \t  0.09772022023383507 \t 3.4911076536027035\n",
            "17     \t [0.09801313 0.03966252 0.67876064]. \t  0.25351173370153407 \t 3.4911076536027035\n",
            "18     \t [0.01071511 0.64792496 0.61855759]. \t  2.2386655409072813 \t 3.4911076536027035\n",
            "19     \t [0.98381083 0.51644723 0.8153559 ]. \t  3.49024330626736 \t 3.4911076536027035\n",
            "20     \t [0.96271584 0.57721565 0.83493793]. \t  \u001b[92m3.6057458286974744\u001b[0m \t 3.6057458286974744\n",
            "21     \t [0.39492038 0.98747658 0.03908025]. \t  0.0008151579228673906 \t 3.6057458286974744\n",
            "22     \t [0.06803357 0.49529474 0.90607817]. \t  3.410387124597217 \t 3.6057458286974744\n",
            "23     \t [0.98948413 0.52364683 0.84230367]. \t  \u001b[92m3.6359273781451815\u001b[0m \t 3.6359273781451815\n",
            "24     \t [0.47763845 0.52532106 0.86371702]. \t  \u001b[92m3.8066927319974138\u001b[0m \t 3.8066927319974138\n",
            "25     \t [0.22796194 0.57346741 0.9297023 ]. \t  3.28244170564132 \t 3.8066927319974138\n",
            "26     \t [0.41773317 0.60592686 0.87758407]. \t  3.7124292986544236 \t 3.8066927319974138\n",
            "27     \t [0.50941232 0.53921388 0.85272482]. \t  \u001b[92m3.8330270717579946\u001b[0m \t 3.8330270717579946\n",
            "28     \t [0.48225058 0.57478998 0.81533606]. \t  3.6784019818936726 \t 3.8330270717579946\n",
            "29     \t [0.52253065 0.55401165 0.85968739]. \t  \u001b[92m3.8348169826420446\u001b[0m \t 3.8348169826420446\n",
            "30     \t [0.48773788 0.49009596 0.84537457]. \t  3.7036870515157783 \t 3.8348169826420446\n",
            "31     \t [0.6352499  0.59514579 0.89203151]. \t  3.6247506078288407 \t 3.8348169826420446\n",
            "32     \t [0.50389264 0.5221982  0.9035887 ]. \t  3.5515419005070368 \t 3.8348169826420446\n",
            "33     \t [0.45449055 0.58273397 0.81921567]. \t  3.697342084337479 \t 3.8348169826420446\n",
            "34     \t [0.41530735 0.49717953 0.89941641]. \t  3.515607917296566 \t 3.8348169826420446\n",
            "35     \t [0.27130473 0.51309451 0.86855135]. \t  3.7706104023921765 \t 3.8348169826420446\n",
            "36     \t [0.61775645 0.52583311 0.85064174]. \t  3.789646222919191 \t 3.8348169826420446\n",
            "37     \t [0.25446017 0.54663699 0.86308418]. \t  \u001b[92m3.8469232469356975\u001b[0m \t 3.8469232469356975\n",
            "38     \t [0.73079904 0.5405699  0.86799547]. \t  3.7614316114739816 \t 3.8469232469356975\n",
            "39     \t [0.86599281 0.5440998  0.85889897]. \t  3.72927679520103 \t 3.8469232469356975\n",
            "40     \t [0.76443502 0.57752464 0.88796689]. \t  3.6517467585796446 \t 3.8469232469356975\n",
            "41     \t [0.77147692 0.55562155 0.85920958]. \t  3.76572989913228 \t 3.8469232469356975\n",
            "42     \t [0.54941459 0.5191193  0.87918466]. \t  3.7240175075747053 \t 3.8469232469356975\n",
            "43     \t [0.66826788 0.56757422 0.86172013]. \t  3.789298977705997 \t 3.8469232469356975\n",
            "44     \t [0.51631282 0.53563639 0.84486937]. \t  3.819972428706672 \t 3.8469232469356975\n",
            "45     \t [0.51691542 0.55552932 0.8316676 ]. \t  3.7854335198656415 \t 3.8469232469356975\n",
            "46     \t [0.2387415  0.48220343 0.82426242]. \t  3.6225792532945467 \t 3.8469232469356975\n",
            "47     \t [0.50349915 0.50414029 0.85558645]. \t  3.7566412787761316 \t 3.8469232469356975\n",
            "48     \t [0.49254674 0.53705031 0.82662167]. \t  3.762743464957274 \t 3.8469232469356975\n",
            "49     \t [0.52538024 0.53680656 0.84573431]. \t  3.8208479014298584 \t 3.8469232469356975\n",
            "50     \t [0.50510254 0.55722956 0.8788006 ]. \t  3.781179012420833 \t 3.8469232469356975\n",
            "51     \t [0.51033144 0.53641523 0.87425439]. \t  3.787630197541957 \t 3.8469232469356975\n",
            "52     \t [0.65748818 0.55581046 0.84986466]. \t  3.7993269672866865 \t 3.8469232469356975\n",
            "53     \t [0.54725283 0.54711312 0.85293168]. \t  3.830650507083113 \t 3.8469232469356975\n",
            "54     \t [0.85268058 0.58066008 0.86369069]. \t  3.702669527494143 \t 3.8469232469356975\n",
            "55     \t [0.5375532  0.54950136 0.82876433]. \t  3.7676665778432517 \t 3.8469232469356975\n",
            "56     \t [0.46653117 0.5744244  0.82252276]. \t  3.7306884554710398 \t 3.8469232469356975\n",
            "57     \t [0.39218286 0.56418359 0.87755416]. \t  3.7976443962653876 \t 3.8469232469356975\n",
            "58     \t [0.71958341 0.55889472 0.86586838]. \t  3.77396131287666 \t 3.8469232469356975\n",
            "59     \t [0.37639746 0.56111091 0.83347963]. \t  3.816951867202446 \t 3.8469232469356975\n",
            "60     \t [0.38787062 0.50888921 0.84065142]. \t  3.7751604634545837 \t 3.8469232469356975\n",
            "61     \t [0.52954714 0.52425704 0.83311033]. \t  3.767306760638038 \t 3.8469232469356975\n",
            "62     \t [0.70553138 0.55927719 0.84790632]. \t  3.779279360907341 \t 3.8469232469356975\n",
            "63     \t [0.74502585 0.586877   0.84639552]. \t  3.717228255274259 \t 3.8469232469356975\n",
            "64     \t [0.60366068 0.55725378 0.84149911]. \t  3.7973641115056314 \t 3.8469232469356975\n",
            "65     \t [0.49918184 0.5465909  0.81879151]. \t  3.7199447281445503 \t 3.8469232469356975\n",
            "66     \t [0.75290972 0.55728133 0.8609116 ]. \t  3.7704764673194413 \t 3.8469232469356975\n",
            "67     \t [0.6542643  0.5502763  0.88737497]. \t  3.701077565221096 \t 3.8469232469356975\n",
            "68     \t [0.51077314 0.52625093 0.86261113]. \t  3.805839274191238 \t 3.8469232469356975\n",
            "69     \t [0.65833145 0.56522854 0.85219252]. \t  3.795263953020572 \t 3.8469232469356975\n",
            "70     \t [0.55026683 0.55481733 0.83456967]. \t  3.7894089904223347 \t 3.8469232469356975\n",
            "71     \t [0.30044625 0.53899622 0.86643847]. \t  3.833287569891059 \t 3.8469232469356975\n",
            "72     \t [0.24013961 0.54310985 0.81720488]. \t  3.742655099078812 \t 3.8469232469356975\n",
            "73     \t [0.71049136 0.56987134 0.86179116]. \t  3.773528944589229 \t 3.8469232469356975\n",
            "74     \t [0.84316877 0.57346399 0.85780339]. \t  3.7221580068919526 \t 3.8469232469356975\n",
            "75     \t [0.6502716  0.5527228  0.87228832]. \t  3.7778899885936865 \t 3.8469232469356975\n",
            "76     \t [0.44703493 0.52995935 0.87234803]. \t  3.7931690481891494 \t 3.8469232469356975\n",
            "77     \t [0.38266962 0.57465056 0.85551857]. \t  3.842860050589706 \t 3.8469232469356975\n",
            "78     \t [0.32333191 0.48936117 0.83150556]. \t  3.683571619727629 \t 3.8469232469356975\n",
            "79     \t [0.65798044 0.55274342 0.87867609]. \t  3.749891629903129 \t 3.8469232469356975\n",
            "80     \t [0.66409453 0.55299935 0.88887562]. \t  3.6896021123264453 \t 3.8469232469356975\n",
            "81     \t [0.50596251 0.53373344 0.86573182]. \t  3.8127137249642695 \t 3.8469232469356975\n",
            "82     \t [0.49368405 0.5251084  0.85653526]. \t  3.8141186068514603 \t 3.8469232469356975\n",
            "83     \t [0.73146298 0.55231222 0.87100127]. \t  3.758612155757983 \t 3.8469232469356975\n",
            "84     \t [0.36308812 0.55493866 0.83289923]. \t  3.8187397913918835 \t 3.8469232469356975\n",
            "85     \t [0.54416017 0.55849686 0.84418742]. \t  3.8193084192860125 \t 3.8469232469356975\n",
            "86     \t [0.48855405 0.53360394 0.82742251]. \t  3.7643252769153 \t 3.8469232469356975\n",
            "87     \t [0.60733923 0.57064515 0.84111707]. \t  3.7821850068340677 \t 3.8469232469356975\n",
            "88     \t [0.5130929  0.55000316 0.84246568]. \t  3.8242897893342294 \t 3.8469232469356975\n",
            "89     \t [0.63478657 0.5284292  0.8667832 ]. \t  3.775153627971675 \t 3.8469232469356975\n",
            "90     \t [0.45764742 0.55130811 0.84998777]. \t  \u001b[92m3.8469272525059894\u001b[0m \t 3.8469272525059894\n",
            "91     \t [0.60199505 0.53568952 0.86675934]. \t  3.794146325686498 \t 3.8469272525059894\n",
            "92     \t [0.48967283 0.54528779 0.84845076]. \t  3.8381165669554846 \t 3.8469272525059894\n",
            "93     \t [0.48664398 0.55514452 0.84297394]. \t  3.8306009052528998 \t 3.8469272525059894\n",
            "94     \t [0.49443786 0.5250426  0.83872044]. \t  3.794721186791997 \t 3.8469272525059894\n",
            "95     \t [0.39509192 0.54855821 0.87267394]. \t  3.8183978176584845 \t 3.8469272525059894\n",
            "96     \t [0.57671506 0.52022276 0.83343385]. \t  3.749831318744243 \t 3.8469272525059894\n",
            "97     \t [0.50471191 0.53841865 0.84351148]. \t  3.822517334114901 \t 3.8469272525059894\n",
            "98     \t [0.63746956 0.53675643 0.8692328 ]. \t  3.780234748420578 \t 3.8469272525059894\n",
            "99     \t [0.48323989 0.5472818  0.85044767]. \t  3.842014829197374 \t 3.8469272525059894\n",
            "100    \t [0.63578048 0.54412107 0.83835459]. \t  3.7791469729820637 \t 3.8469272525059894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emgjXwfeuvRY",
        "outputId": "b50c923a-8692-4d5c-a4ea-003f91e958e9"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_winner_3 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_3 = dGPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.75157561 0.10925128 0.48612128]. \t  0.19395982560567754 \t 0.5647137279144399\n",
            "init   \t [0.49983118 0.65711    0.23588471]. \t  0.10633623603471726 \t 0.5647137279144399\n",
            "init   \t [0.61279489 0.1196524  0.7122023 ]. \t  0.5647137279144399 \t 0.5647137279144399\n",
            "init   \t [0.37256054 0.52476827 0.26328708]. \t  0.24645583811739444 \t 0.5647137279144399\n",
            "init   \t [0.62163122 0.44909976 0.21906361]. \t  0.2633149120115605 \t 0.5647137279144399\n",
            "1      \t [0.00227319 0.27141826 0.95291977]. \t  \u001b[92m1.263004128201619\u001b[0m \t 1.263004128201619\n",
            "2      \t [0.22074887 0.05835947 0.90763313]. \t  0.3319318657983171 \t 1.263004128201619\n",
            "3      \t [0.89438799 0.64500468 1.        ]. \t  \u001b[92m1.9158552255863373\u001b[0m \t 1.9158552255863373\n",
            "4      \t [0.36440183 0.64658727 0.9861336 ]. \t  \u001b[92m2.200649424947822\u001b[0m \t 2.200649424947822\n",
            "5      \t [0.23725292 0.7509311  0.98827581]. \t  1.6510604935868975 \t 2.200649424947822\n",
            "6      \t [0.56104037 0.53283417 0.95940179]. \t  \u001b[92m2.7831797409933086\u001b[0m \t 2.7831797409933086\n",
            "7      \t [0.77869845 0.25063739 0.95779519]. \t  1.0838043786695986 \t 2.7831797409933086\n",
            "8      \t [0.66339985 0.57114634 0.76333622]. \t  \u001b[92m3.0499887463508863\u001b[0m \t 3.0499887463508863\n",
            "9      \t [0.59914079 0.57257811 0.78968156]. \t  \u001b[92m3.403486034261022\u001b[0m \t 3.403486034261022\n",
            "10     \t [0.08388575 0.93644895 0.65690725]. \t  2.3208527414347175 \t 3.403486034261022\n",
            "11     \t [0.39595293 0.74563652 0.74785864]. \t  2.379281281519273 \t 3.403486034261022\n",
            "12     \t [0.32941102 0.54921561 0.71370135]. \t  2.613802958787579 \t 3.403486034261022\n",
            "13     \t [0.59718712 0.47826665 0.78794504]. \t  3.29247459549971 \t 3.403486034261022\n",
            "14     \t [0.0893846  0.92559252 0.0325758 ]. \t  0.001037406181001283 \t 3.403486034261022\n",
            "15     \t [0.60576323 0.51363869 0.80548965]. \t  \u001b[92m3.5537302033616793\u001b[0m \t 3.5537302033616793\n",
            "16     \t [0.60909557 0.52766748 0.8083424 ]. \t  \u001b[92m3.5978710564785397\u001b[0m \t 3.5978710564785397\n",
            "17     \t [0.63877146 0.52845646 0.84313378]. \t  \u001b[92m3.7772723816287392\u001b[0m \t 3.7772723816287392\n",
            "18     \t [0.4593987  0.34987765 0.00418763]. \t  0.07111044622007372 \t 3.7772723816287392\n",
            "19     \t [0.98798183 0.2620475  0.77536636]. \t  1.6097847421269496 \t 3.7772723816287392\n",
            "20     \t [0.6082827  0.53179832 0.87960345]. \t  3.737579819825853 \t 3.7772723816287392\n",
            "21     \t [0.60518453 0.5671988  0.91269616]. \t  3.4848276167172587 \t 3.7772723816287392\n",
            "22     \t [0.80667723 0.47071523 0.89287976]. \t  3.37485907611565 \t 3.7772723816287392\n",
            "23     \t [0.65605721 0.55208557 0.84463285]. \t  \u001b[92m3.7914529204545087\u001b[0m \t 3.7914529204545087\n",
            "24     \t [0.         0.7837036  0.53433076]. \t  2.742142538961756 \t 3.7914529204545087\n",
            "25     \t [0.72461515 0.55008156 0.89588222]. \t  3.620277916822991 \t 3.7914529204545087\n",
            "26     \t [0.66600117 0.53914466 0.8427305 ]. \t  3.7801281837273373 \t 3.7914529204545087\n",
            "27     \t [0.61469273 0.56887748 0.87586474]. \t  3.7665589358796554 \t 3.7914529204545087\n",
            "28     \t [0.69062903 0.51788183 0.87604427]. \t  3.7053698213137323 \t 3.7914529204545087\n",
            "29     \t [0.60958968 0.54835726 0.83375038]. \t  3.770316921201294 \t 3.7914529204545087\n",
            "30     \t [0.60187506 0.57523839 0.85822531]. \t  \u001b[92m3.80015779399758\u001b[0m \t 3.80015779399758\n",
            "31     \t [0.64262508 0.54749744 0.84532948]. \t  3.796919254520793 \t 3.80015779399758\n",
            "32     \t [0.5964265  0.57900626 0.86027467]. \t  3.7940246081256763 \t 3.80015779399758\n",
            "33     \t [0.63538358 0.56559167 0.88223362]. \t  3.7343490412637803 \t 3.80015779399758\n",
            "34     \t [0.62547505 0.53521804 0.8641115 ]. \t  3.7938316167989767 \t 3.80015779399758\n",
            "35     \t [0.01937798 0.85434918 0.76407659]. \t  1.870394467695899 \t 3.80015779399758\n",
            "36     \t [0.6470984  0.56309873 0.8682692 ]. \t  3.7877198044123084 \t 3.80015779399758\n",
            "37     \t [0.62961937 0.57359329 0.85854336]. \t  3.7949886826928614 \t 3.80015779399758\n",
            "38     \t [0.89357855 0.12358528 0.03007646]. \t  0.08090003143637674 \t 3.80015779399758\n",
            "39     \t [0.01439254 0.02165214 0.74423608]. \t  0.2838176261285712 \t 3.80015779399758\n",
            "40     \t [0.6580553  0.53738181 0.83492604]. \t  3.7560087918579663 \t 3.80015779399758\n",
            "41     \t [0.66044382 0.54606357 0.88020052]. \t  3.7389688083187056 \t 3.80015779399758\n",
            "42     \t [0.69562047 0.54494795 0.89508145]. \t  3.6310456921173433 \t 3.80015779399758\n",
            "43     \t [0.59745525 0.58793055 0.86281586]. \t  3.772295168038831 \t 3.80015779399758\n",
            "44     \t [0.58378803 0.57764201 0.85398842]. \t  \u001b[92m3.8005232018450754\u001b[0m \t 3.8005232018450754\n",
            "45     \t [0.61924992 0.56881343 0.91025323]. \t  3.508099153251513 \t 3.8005232018450754\n",
            "46     \t [0.61936678 0.58330533 0.85413795]. \t  3.7794347830326407 \t 3.8005232018450754\n",
            "47     \t [0.63909615 0.55710137 0.85786728]. \t  \u001b[92m3.807761361638626\u001b[0m \t 3.807761361638626\n",
            "48     \t [0.56257796 0.59997953 0.84681793]. \t  3.739671243777024 \t 3.807761361638626\n",
            "49     \t [0.67463059 0.55157972 0.88555012]. \t  3.7079962594539593 \t 3.807761361638626\n",
            "50     \t [0.61423847 0.5686296  0.8462493 ]. \t  3.795888497946321 \t 3.807761361638626\n",
            "51     \t [0.60839594 0.56762812 0.86247466]. \t  3.804979237839305 \t 3.807761361638626\n",
            "52     \t [0.62998385 0.55277618 0.89105859]. \t  3.682240411268699 \t 3.807761361638626\n",
            "53     \t [0.58229961 0.55956825 0.83145573]. \t  3.7642405128880285 \t 3.807761361638626\n",
            "54     \t [0.60794192 0.57917415 0.84644187]. \t  3.781094608091346 \t 3.807761361638626\n",
            "55     \t [0.62572798 0.56098232 0.86501693]. \t  3.801688027316298 \t 3.807761361638626\n",
            "56     \t [0.66717566 0.53103954 0.84862401]. \t  3.7823376919571876 \t 3.807761361638626\n",
            "57     \t [0.00484863 0.96891476 0.42046367]. \t  1.545429105094566 \t 3.807761361638626\n",
            "58     \t [0.65547779 0.50178088 0.01994603]. \t  0.028426850579416747 \t 3.807761361638626\n",
            "59     \t [0.68271658 0.52683673 0.86138502]. \t  3.7702401821845895 \t 3.807761361638626\n",
            "60     \t [0.63237121 0.56386101 0.8548352 ]. \t  3.805750551187906 \t 3.807761361638626\n",
            "61     \t [0.34648982 0.99946119 0.95383159]. \t  0.4696132847036659 \t 3.807761361638626\n",
            "62     \t [0.66405596 0.55521343 0.86957522]. \t  3.7826386725606107 \t 3.807761361638626\n",
            "63     \t [0.68231861 0.5491205  0.83353593]. \t  3.7461076042909225 \t 3.807761361638626\n",
            "64     \t [0.63160799 0.57060867 0.87820213]. \t  3.7506844646022772 \t 3.807761361638626\n",
            "65     \t [0.67856049 0.54640217 0.84959995]. \t  3.792837853245691 \t 3.807761361638626\n",
            "66     \t [0.5893848  0.5869514  0.84219135]. \t  3.7585326206273266 \t 3.807761361638626\n",
            "67     \t [0.6409503  0.550245   0.83735503]. \t  3.7748574059840223 \t 3.807761361638626\n",
            "68     \t [0.66045003 0.5621154  0.88020065]. \t  3.7404988539229254 \t 3.807761361638626\n",
            "69     \t [0.70192597 0.52070082 0.85206495]. \t  3.757006852457075 \t 3.807761361638626\n",
            "70     \t [0.71773918 0.52253097 0.85836493]. \t  3.7549477670741784 \t 3.807761361638626\n",
            "71     \t [0.57522061 0.57696209 0.87539445]. \t  3.767360285333472 \t 3.807761361638626\n",
            "72     \t [0.6698951  0.55641655 0.87300613]. \t  3.7700830649898087 \t 3.807761361638626\n",
            "73     \t [0.56374879 0.59994677 0.8502023 ]. \t  3.7452231161749685 \t 3.807761361638626\n",
            "74     \t [0.65421459 0.55497839 0.88675325]. \t  3.7063546769403173 \t 3.807761361638626\n",
            "75     \t [0.63101608 0.57101279 0.86715579]. \t  3.7871551800237535 \t 3.807761361638626\n",
            "76     \t [0.588303   0.57717849 0.86840009]. \t  3.786294847868546 \t 3.807761361638626\n",
            "77     \t [0.71557555 0.52651052 0.87614994]. \t  3.7175622582095516 \t 3.807761361638626\n",
            "78     \t [0.67463033 0.53020259 0.82643031]. \t  3.703882365927452 \t 3.807761361638626\n",
            "79     \t [0.65478127 0.53241012 0.85148316]. \t  3.7904353654387046 \t 3.807761361638626\n",
            "80     \t [0.67067868 0.53988466 0.84771481]. \t  3.7892992132422587 \t 3.807761361638626\n",
            "81     \t [0.71324497 0.51212364 0.8376741 ]. \t  3.7077303089035243 \t 3.807761361638626\n",
            "82     \t [0.65230892 0.55507289 0.85291005]. \t  3.803984335408729 \t 3.807761361638626\n",
            "83     \t [0.68907103 0.53473742 0.84385523]. \t  3.7717696120641144 \t 3.807761361638626\n",
            "84     \t [0.63159724 0.56079025 0.87717435]. \t  3.7624865430743917 \t 3.807761361638626\n",
            "85     \t [0.62384524 0.56464914 0.86809198]. \t  3.7931862645102936 \t 3.807761361638626\n",
            "86     \t [0.6712912  0.52583054 0.84177843]. \t  3.7604644923890898 \t 3.807761361638626\n",
            "87     \t [0.66890372 0.53069355 0.85819823]. \t  3.7838653442102554 \t 3.807761361638626\n",
            "88     \t [0.66721483 0.54556758 0.85824879]. \t  3.7989366953453696 \t 3.807761361638626\n",
            "89     \t [0.63515967 0.54795922 0.87813692]. \t  3.7564715402397963 \t 3.807761361638626\n",
            "90     \t [0.61821913 0.5565118  0.8383064 ]. \t  3.7833744836027057 \t 3.807761361638626\n",
            "91     \t [0.72265842 0.52198473 0.86175567]. \t  3.7484715868162377 \t 3.807761361638626\n",
            "92     \t [0.63431763 0.55064351 0.86404782]. \t  3.802998295212574 \t 3.807761361638626\n",
            "93     \t [0.64405321 0.56922265 0.88361897]. \t  3.721702496316942 \t 3.807761361638626\n",
            "94     \t [0.65261345 0.56288351 0.86910078]. \t  3.7842385933627596 \t 3.807761361638626\n",
            "95     \t [0.68765119 0.51672349 0.86071312]. \t  3.748067953848733 \t 3.807761361638626\n",
            "96     \t [0.58056228 0.58328083 0.9037609 ]. \t  3.5647589744072143 \t 3.807761361638626\n",
            "97     \t [0.72139602 0.51069474 0.85456624]. \t  3.726957573885084 \t 3.807761361638626\n",
            "98     \t [0.59942865 0.57815655 0.87195379]. \t  3.772063373441007 \t 3.807761361638626\n",
            "99     \t [0.62629199 0.55995198 0.85027931]. \t  3.8070069257569314 \t 3.807761361638626\n",
            "100    \t [0.63778515 0.56482067 0.8817836 ]. \t  3.7366625847861683 \t 3.807761361638626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8riJpBBKuvT4",
        "outputId": "fc53ae57-997e-43a8-91d3-f57744f86c91"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_winner_4 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_4 = dGPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.83936105 0.82129497 0.64220716]. \t  0.8796727111154451 \t 1.9592421489197056\n",
            "init   \t [0.66722262 0.03913991 0.06567239]. \t  0.21283931850025994 \t 1.9592421489197056\n",
            "init   \t [0.27650064 0.3163605  0.19359241]. \t  0.5590312289203825 \t 1.9592421489197056\n",
            "init   \t [0.19709288 0.84920429 0.84235809]. \t  1.818649590001362 \t 1.9592421489197056\n",
            "init   \t [0.34736473 0.7397423  0.49340914]. \t  1.9592421489197056 \t 1.9592421489197056\n",
            "1      \t [0.22412884 0.99744654 0.22062981]. \t  0.08723427566372202 \t 1.9592421489197056\n",
            "2      \t [0.39828366 0.30868605 0.83489368]. \t  \u001b[92m2.2404092798638526\u001b[0m \t 2.2404092798638526\n",
            "3      \t [0.3344556  0.52093208 0.72588575]. \t  \u001b[92m2.725171826988304\u001b[0m \t 2.725171826988304\n",
            "4      \t [0.03960712 0.47812427 0.83831438]. \t  \u001b[92m3.6123693311627187\u001b[0m \t 3.6123693311627187\n",
            "5      \t [0.0271757  0.45756503 0.75278759]. \t  2.8585148930112485 \t 3.6123693311627187\n",
            "6      \t [0.0782292  0.47826417 0.9332263 ]. \t  2.9993686948876612 \t 3.6123693311627187\n",
            "7      \t [0.95284055 0.11936751 0.76080992]. \t  0.6528440709100867 \t 3.6123693311627187\n",
            "8      \t [0.6595103  0.62260661 0.        ]. \t  0.0071238393236356745 \t 3.6123693311627187\n",
            "9      \t [0.00276625 0.7637641  0.72611322]. \t  2.400431939462031 \t 3.6123693311627187\n",
            "10     \t [0.51632308 0.02933911 0.52871941]. \t  0.15751751099719744 \t 3.6123693311627187\n",
            "11     \t [0.35712533 0.29273081 0.00327238]. \t  0.09067775565134946 \t 3.6123693311627187\n",
            "12     \t [0.58831062 0.34302914 0.43846093]. \t  0.3169749532772143 \t 3.6123693311627187\n",
            "13     \t [0.56287075 0.98199608 0.28757135]. \t  0.16947852637838307 \t 3.6123693311627187\n",
            "14     \t [1. 1. 1.]. \t  0.3168836207042166 \t 3.6123693311627187\n",
            "15     \t [0.9602465  0.43184598 0.88078612]. \t  3.153570031250211 \t 3.6123693311627187\n",
            "16     \t [0.99169038 0.0760957  0.96787654]. \t  0.2568359409458297 \t 3.6123693311627187\n",
            "17     \t [1.         0.64563229 0.75775256]. \t  2.4985189349686596 \t 3.6123693311627187\n",
            "18     \t [0.04766799 0.56052349 0.86187567]. \t  \u001b[92m3.816730851560058\u001b[0m \t 3.816730851560058\n",
            "19     \t [0.05363085 0.70068044 0.84425937]. \t  3.166645731837285 \t 3.816730851560058\n",
            "20     \t [5.46842964e-04 5.69445984e-01 8.23395939e-01]. \t  3.725464451538291 \t 3.816730851560058\n",
            "21     \t [0.99981643 0.56443143 0.93367787]. \t  3.1059043263271464 \t 3.816730851560058\n",
            "22     \t [0.02419428 0.50930592 0.8443374 ]. \t  3.742366743780555 \t 3.816730851560058\n",
            "23     \t [0.00361544 0.61760212 0.85461232]. \t  3.6813321324021167 \t 3.816730851560058\n",
            "24     \t [0.04552024 0.58940087 0.85481013]. \t  3.785684783836884 \t 3.816730851560058\n",
            "25     \t [0.10814934 0.58528213 0.84437903]. \t  3.80388839120915 \t 3.816730851560058\n",
            "26     \t [0.0775483  0.56588588 0.80840805]. \t  3.660069438034766 \t 3.816730851560058\n",
            "27     \t [0.07779499 0.5967983  0.7990024 ]. \t  3.5370605747953734 \t 3.816730851560058\n",
            "28     \t [0.0300521  0.63592324 0.80058896]. \t  3.396818516472418 \t 3.816730851560058\n",
            "29     \t [0.02980552 0.58703182 0.82785012]. \t  3.7289677482317556 \t 3.816730851560058\n",
            "30     \t [0.02579377 0.60608966 0.81884265]. \t  3.6304617996081645 \t 3.816730851560058\n",
            "31     \t [0.08338416 0.55633864 0.78803777]. \t  3.4936421793865207 \t 3.816730851560058\n",
            "32     \t [0.12334902 0.58567267 0.78121001]. \t  3.4106685710113442 \t 3.816730851560058\n",
            "33     \t [0.00804203 0.60496554 0.84368585]. \t  3.721285893113495 \t 3.816730851560058\n",
            "34     \t [0.11906002 0.59875517 0.82040123]. \t  3.686334583894952 \t 3.816730851560058\n",
            "35     \t [0.0114913  0.53669787 0.85359683]. \t  3.80235525054694 \t 3.816730851560058\n",
            "36     \t [0.07221495 0.56387775 0.8083205 ]. \t  3.6594399407679985 \t 3.816730851560058\n",
            "37     \t [0.23136974 0.61210155 0.82572952]. \t  3.679649015095695 \t 3.816730851560058\n",
            "38     \t [0.05120548 0.         0.02599642]. \t  0.11230926248915304 \t 3.816730851560058\n",
            "39     \t [0.65066036 0.5140676  0.96043805]. \t  2.712799483876855 \t 3.816730851560058\n",
            "40     \t [0.05970461 0.53394763 0.87106289]. \t  3.7765887954417665 \t 3.816730851560058\n",
            "41     \t [0.07277886 0.6211374  0.82158385]. \t  3.6036255062670017 \t 3.816730851560058\n",
            "42     \t [0.15515773 0.56770454 0.85367133]. \t  \u001b[92m3.845005000567755\u001b[0m \t 3.845005000567755\n",
            "43     \t [0.12961556 0.5619126  0.87976179]. \t  3.7716267098032628 \t 3.845005000567755\n",
            "44     \t [0.09268902 0.60873738 0.83907091]. \t  3.723267486164033 \t 3.845005000567755\n",
            "45     \t [0.23797457 0.53738651 0.88936135]. \t  3.7103076530846297 \t 3.845005000567755\n",
            "46     \t [0.12564942 0.59088431 0.92173948]. \t  3.353429519615728 \t 3.845005000567755\n",
            "47     \t [0.18426921 0.53324078 0.87052841]. \t  3.802541262678103 \t 3.845005000567755\n",
            "48     \t [0.09156323 0.58397407 0.84821738]. \t  3.807553841929566 \t 3.845005000567755\n",
            "49     \t [0.13310552 0.57828838 0.86307157]. \t  3.8185818487958407 \t 3.845005000567755\n",
            "50     \t [0.14155503 0.53426072 0.87921688]. \t  3.7575989242700514 \t 3.845005000567755\n",
            "51     \t [0.24625215 0.53520931 0.88287913]. \t  3.7515563498330744 \t 3.845005000567755\n",
            "52     \t [0.16926385 0.5963513  0.85833408]. \t  3.792319296241407 \t 3.845005000567755\n",
            "53     \t [0.07970411 0.53988095 0.85947084]. \t  3.8205449210868303 \t 3.845005000567755\n",
            "54     \t [0.09937743 0.55373891 0.83399291]. \t  3.8081860594733987 \t 3.845005000567755\n",
            "55     \t [0.21001962 0.48832824 0.85763531]. \t  3.69824551065327 \t 3.845005000567755\n",
            "56     \t [0.12814973 0.5693039  0.8837483 ]. \t  3.744677031867618 \t 3.845005000567755\n",
            "57     \t [0.1772076  0.58823966 0.84471856]. \t  3.8095580045176987 \t 3.845005000567755\n",
            "58     \t [0.18809567 0.57103462 0.86668169]. \t  3.8279240776318857 \t 3.845005000567755\n",
            "59     \t [0.18209719 0.52646063 0.86163618]. \t  3.8149813805056425 \t 3.845005000567755\n",
            "60     \t [0.00358989 0.59954134 0.83744202]. \t  3.722459046982416 \t 3.845005000567755\n",
            "61     \t [0.08115075 0.54764086 0.88420288]. \t  3.7317761798007862 \t 3.845005000567755\n",
            "62     \t [0.04179779 0.53713908 0.82056265]. \t  3.724098160210465 \t 3.845005000567755\n",
            "63     \t [0.07534027 0.53573557 0.84162603]. \t  3.8105007141262837 \t 3.845005000567755\n",
            "64     \t [0.0540208  0.54224953 0.7800608 ]. \t  3.4021634776585854 \t 3.845005000567755\n",
            "65     \t [0.30009557 0.5719694  0.87194827]. \t  3.817652352807336 \t 3.845005000567755\n",
            "66     \t [0.16986001 0.55422611 0.82931885]. \t  3.803289149928497 \t 3.845005000567755\n",
            "67     \t [0.20997366 0.60950137 0.83401484]. \t  3.7223218039574437 \t 3.845005000567755\n",
            "68     \t [0.19412521 0.51131317 0.85084098]. \t  3.7888438763034706 \t 3.845005000567755\n",
            "69     \t [0.26372112 0.62830674 0.78209698]. \t  3.3002340447495775 \t 3.845005000567755\n",
            "70     \t [0.21588236 0.54229993 0.87694307]. \t  3.791091607301559 \t 3.845005000567755\n",
            "71     \t [0.19430843 0.56228003 0.82952532]. \t  3.8044235614567734 \t 3.845005000567755\n",
            "72     \t [0.27032071 0.52843609 0.88760308]. \t  3.7095024920344137 \t 3.845005000567755\n",
            "73     \t [0.157419   0.58024381 0.89528528]. \t  3.6550128628093614 \t 3.845005000567755\n",
            "74     \t [0.23760978 0.57988931 0.89155877]. \t  3.694572006893998 \t 3.845005000567755\n",
            "75     \t [0.29949418 0.6432192  0.84843676]. \t  3.5945691333377687 \t 3.845005000567755\n",
            "76     \t [0.13311365 0.6035278  0.8454124 ]. \t  3.76169235839739 \t 3.845005000567755\n",
            "77     \t [0.1479611  0.61400794 0.88579164]. \t  3.6338495674731623 \t 3.845005000567755\n",
            "78     \t [0.18292148 0.55770377 0.86795889]. \t  3.8307506633513113 \t 3.845005000567755\n",
            "79     \t [0.16733636 0.6241831  0.85227393]. \t  3.691476567931723 \t 3.845005000567755\n",
            "80     \t [0.09741761 0.60484791 0.83591544]. \t  3.729023886088015 \t 3.845005000567755\n",
            "81     \t [0.2205708  0.48186903 0.84188888]. \t  3.6704576478185094 \t 3.845005000567755\n",
            "82     \t [0.17210392 0.59771747 0.85689638]. \t  3.789992640085854 \t 3.845005000567755\n",
            "83     \t [0.22834823 0.51361987 0.89580114]. \t  3.6026874983191832 \t 3.845005000567755\n",
            "84     \t [0.25835387 0.56390659 0.85961348]. \t  \u001b[92m3.8537392187490487\u001b[0m \t 3.8537392187490487\n",
            "85     \t [0.05744471 0.5907606  0.81259486]. \t  3.645293390224859 \t 3.8537392187490487\n",
            "86     \t [0.23530751 0.56794891 0.83570863]. \t  3.8256692086097033 \t 3.8537392187490487\n",
            "87     \t [0.21302609 0.50492664 0.91320826]. \t  3.3939037449054563 \t 3.8537392187490487\n",
            "88     \t [0.25222909 0.55728706 0.85378707]. \t  \u001b[92m3.8602011121720627\u001b[0m \t 3.8602011121720627\n",
            "89     \t [0.06490073 0.61595464 0.82674887]. \t  3.6466718722149576 \t 3.8602011121720627\n",
            "90     \t [0.24530251 0.53918855 0.83384953]. \t  3.8198265438708074 \t 3.8602011121720627\n",
            "91     \t [0.05242026 0.53144571 0.85148659]. \t  3.807386360530833 \t 3.8602011121720627\n",
            "92     \t [0.10132292 0.59064832 0.87319298]. \t  3.758963213453866 \t 3.8602011121720627\n",
            "93     \t [0.13077579 0.59877442 0.82652618]. \t  3.718949868892161 \t 3.8602011121720627\n",
            "94     \t [0.01152437 0.54867672 0.85873981]. \t  3.8091613470228882 \t 3.8602011121720627\n",
            "95     \t [0.10311714 0.60045439 0.82596766]. \t  3.706096614058402 \t 3.8602011121720627\n",
            "96     \t [0.01867701 0.54699914 0.90600092]. \t  3.5328571490781053 \t 3.8602011121720627\n",
            "97     \t [0.16275067 0.50330669 0.89292629]. \t  3.582978659237004 \t 3.8602011121720627\n",
            "98     \t [0.04419369 0.50979699 0.86493886]. \t  3.7338750543478887 \t 3.8602011121720627\n",
            "99     \t [0.06589834 0.53999958 0.82193962]. \t  3.7412769168967053 \t 3.8602011121720627\n",
            "100    \t [0.03331757 0.48916003 0.85636173]. \t  3.6686083601423007 \t 3.8602011121720627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrvkDXP0uvWU",
        "outputId": "9a1c7704-227f-4ca9-d5e2-558951491493"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_winner_5 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_5 = dGPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.55735327 0.54176063 0.56042378]. \t  0.9599610140567721 \t 0.9810564697651996\n",
            "init   \t [0.38204809 0.11909418 0.84599912]. \t  0.6913364116577386 \t 0.9810564697651996\n",
            "init   \t [0.73974161 0.93235756 0.85932801]. \t  0.9810564697651996 \t 0.9810564697651996\n",
            "init   \t [0.70810463 0.68096875 0.4086023 ]. \t  0.4017443293787304 \t 0.9810564697651996\n",
            "init   \t [0.77458462 0.9844762  0.96548486]. \t  0.4769494155906823 \t 0.9810564697651996\n",
            "1      \t [0.0655006  0.90661895 0.81035589]. \t  \u001b[92m1.403888408720423\u001b[0m \t 1.403888408720423\n",
            "2      \t [0.38106609 0.89639669 0.68352492]. \t  \u001b[92m1.863410451334428\u001b[0m \t 1.863410451334428\n",
            "3      \t [0.20017402 0.91862284 0.61948742]. \t  \u001b[92m2.6808698651358958\u001b[0m \t 2.6808698651358958\n",
            "4      \t [0.01071345 0.95865071 0.03545222]. \t  0.0009455180451604354 \t 2.6808698651358958\n",
            "5      \t [0.05775341 0.88475942 0.39193578]. \t  1.3461020355899165 \t 2.6808698651358958\n",
            "6      \t [0.96262286 0.01958409 0.72670421]. \t  0.26072968188104295 \t 2.6808698651358958\n",
            "7      \t [0.04493244 0.98997812 0.49047127]. \t  2.2927483694005155 \t 2.6808698651358958\n",
            "8      \t [0.98395174 0.97095545 0.83102182]. \t  0.6640028915026897 \t 2.6808698651358958\n",
            "9      \t [0.99262637 0.27261155 0.97852239]. \t  1.0360064507708013 \t 2.6808698651358958\n",
            "10     \t [6.91112949e-14 6.39865520e-14 5.60650971e-14]. \t  0.06797411659021398 \t 2.6808698651358958\n",
            "11     \t [0.11664796 0.01630291 0.52200951]. \t  0.14381255132573761 \t 2.6808698651358958\n",
            "12     \t [0.96944106 0.04042503 0.0150799 ]. \t  0.04740468497371845 \t 2.6808698651358958\n",
            "13     \t [0.28223422 0.30846282 0.00335079]. \t  0.08382358240566484 \t 2.6808698651358958\n",
            "14     \t [0.91230781 0.59195167 0.74449319]. \t  2.5846280656463874 \t 2.6808698651358958\n",
            "15     \t [0.96521636 0.51931107 0.76508207]. \t  \u001b[92m2.9573992057507175\u001b[0m \t 2.9573992057507175\n",
            "16     \t [0.12752895 0.95121487 0.66650056]. \t  2.1559070948944514 \t 2.9573992057507175\n",
            "17     \t [0.00564085 0.09530564 0.99766825]. \t  0.2405347814907789 \t 2.9573992057507175\n",
            "18     \t [0.52357669 0.95455415 0.05049259]. \t  0.0010212430821766308 \t 2.9573992057507175\n",
            "19     \t [0.31597363 0.03471488 0.0338958 ]. \t  0.18078734930883514 \t 2.9573992057507175\n",
            "20     \t [0.23377799 0.97613985 0.29901816]. \t  0.3629713080957769 \t 2.9573992057507175\n",
            "21     \t [0.92451892 0.52059053 0.00601311]. \t  0.010034646490271824 \t 2.9573992057507175\n",
            "22     \t [0.10838639 0.57487032 0.77241865]. \t  \u001b[92m3.332754181919075\u001b[0m \t 3.332754181919075\n",
            "23     \t [1.         0.72921182 0.97470112]. \t  1.8855008760520486 \t 3.332754181919075\n",
            "24     \t [0.09852763 0.707424   0.66866832]. \t  2.5413336981900687 \t 3.332754181919075\n",
            "25     \t [0.28834724 0.47355003 0.93353553]. \t  2.996819229796551 \t 3.332754181919075\n",
            "26     \t [0.08042673 0.38293167 0.83776709]. \t  2.9313109715550474 \t 3.332754181919075\n",
            "27     \t [0.18778248 0.46714765 0.87131659]. \t  \u001b[92m3.5447010685084583\u001b[0m \t 3.5447010685084583\n",
            "28     \t [0.18637939 0.61560443 0.80916149]. \t  \u001b[92m3.570159403649143\u001b[0m \t 3.570159403649143\n",
            "29     \t [0.24029262 0.5295675  0.84033935]. \t  \u001b[92m3.8248574561523787\u001b[0m \t 3.8248574561523787\n",
            "30     \t [0.31633496 0.53108389 0.79249924]. \t  3.5324450615895455 \t 3.8248574561523787\n",
            "31     \t [0.90290327 0.92292123 0.1018163 ]. \t  0.0012225119635086705 \t 3.8248574561523787\n",
            "32     \t [0.2171105  0.59129106 0.78794173]. \t  3.469232040638313 \t 3.8248574561523787\n",
            "33     \t [0.2453113  0.57469999 0.78848871]. \t  3.4998720658018523 \t 3.8248574561523787\n",
            "34     \t [0.21057597 0.50443949 0.83143767]. \t  3.7352278923679263 \t 3.8248574561523787\n",
            "35     \t [0.25818559 0.61196898 0.82630578]. \t  3.6823193432904606 \t 3.8248574561523787\n",
            "36     \t [0.24555343 0.49133646 0.80079745]. \t  3.51318527993351 \t 3.8248574561523787\n",
            "37     \t [0.23038541 0.56790712 0.82568528]. \t  3.7849437905683345 \t 3.8248574561523787\n",
            "38     \t [0.29128236 0.55205855 0.86508592]. \t  \u001b[92m3.8464000067304602\u001b[0m \t 3.8464000067304602\n",
            "39     \t [0.20561181 0.44628363 0.88662268]. \t  3.3258415560266763 \t 3.8464000067304602\n",
            "40     \t [0.26689036 0.59046831 0.78735258]. \t  3.4606649131581637 \t 3.8464000067304602\n",
            "41     \t [0.26320628 0.63200333 0.83315807]. \t  3.6221793125590027 \t 3.8464000067304602\n",
            "42     \t [0.18690709 0.54520534 0.85249403]. \t  \u001b[92m3.851248811441325\u001b[0m \t 3.851248811441325\n",
            "43     \t [0.14979133 0.61108946 0.82908358]. \t  3.692475263701602 \t 3.851248811441325\n",
            "44     \t [0.22362357 0.58968912 0.81291027]. \t  3.6744649473973596 \t 3.851248811441325\n",
            "45     \t [0.17495364 0.61852574 0.82045179]. \t  3.626085511502051 \t 3.851248811441325\n",
            "46     \t [0.27767153 0.60335082 0.89629078]. \t  3.6076233760432586 \t 3.851248811441325\n",
            "47     \t [0.14420599 0.6002105  0.86519392]. \t  3.766350722989265 \t 3.851248811441325\n",
            "48     \t [0.2276546  0.62135399 0.84957691]. \t  3.707863736181967 \t 3.851248811441325\n",
            "49     \t [0.24258762 0.58070906 0.84858804]. \t  3.834973465831389 \t 3.851248811441325\n",
            "50     \t [0.19768587 0.62455476 0.86184872]. \t  3.6869219423507227 \t 3.851248811441325\n",
            "51     \t [0.2804274  0.59689843 0.86065489]. \t  3.79593362188764 \t 3.851248811441325\n",
            "52     \t [0.22945121 0.53870288 0.85860758]. \t  3.8453606322483536 \t 3.851248811441325\n",
            "53     \t [0.10760668 0.62187821 0.81377943]. \t  3.5655640164844122 \t 3.851248811441325\n",
            "54     \t [0.34084452 0.61749087 0.8364955 ]. \t  3.693425548578005 \t 3.851248811441325\n",
            "55     \t [0.28620928 0.5268173  0.84499701]. \t  3.829717761676817 \t 3.851248811441325\n",
            "56     \t [0.24838946 0.60944132 0.84662861]. \t  3.753763979357549 \t 3.851248811441325\n",
            "57     \t [0.15793277 0.58441346 0.87110861]. \t  3.79041864588388 \t 3.851248811441325\n",
            "58     \t [0.28342513 0.63491163 0.86891194]. \t  3.6248634705662663 \t 3.851248811441325\n",
            "59     \t [0.32225581 0.64193961 0.81908226]. \t  3.4998972183384582 \t 3.851248811441325\n",
            "60     \t [0.362302   0.59107993 0.77838581]. \t  3.345666225973047 \t 3.851248811441325\n",
            "61     \t [0.15480783 0.60731173 0.83811203]. \t  3.73733535288388 \t 3.851248811441325\n",
            "62     \t [0.3132761  0.63250813 0.85492177]. \t  3.6552390721241177 \t 3.851248811441325\n",
            "63     \t [0.27915207 0.57500215 0.81371056]. \t  3.70564019301675 \t 3.851248811441325\n",
            "64     \t [0.21709033 0.4984351  0.86242171]. \t  3.732751368401696 \t 3.851248811441325\n",
            "65     \t [0.28101445 0.59370474 0.8851871 ]. \t  3.713589915453066 \t 3.851248811441325\n",
            "66     \t [0.31305817 0.59759346 0.83311953]. \t  3.7563793062209125 \t 3.851248811441325\n",
            "67     \t [0.38671822 0.65200243 0.82567982]. \t  3.4612766966253923 \t 3.851248811441325\n",
            "68     \t [0.17930629 0.66868095 0.82119338]. \t  3.3620970042795 \t 3.851248811441325\n",
            "69     \t [0.15563862 0.51864261 0.88994764]. \t  3.655443216523654 \t 3.851248811441325\n",
            "70     \t [0.35423526 0.63903939 0.81934368]. \t  3.510123203381573 \t 3.851248811441325\n",
            "71     \t [0.29627038 0.63535978 0.83642088]. \t  3.61336970584763 \t 3.851248811441325\n",
            "72     \t [0.34560512 0.50715791 0.84703721]. \t  3.781285950836601 \t 3.851248811441325\n",
            "73     \t [0.31464252 0.50047429 0.85187758]. \t  3.759907214120372 \t 3.851248811441325\n",
            "74     \t [0.24069465 0.58568913 0.87841123]. \t  3.7666586067135497 \t 3.851248811441325\n",
            "75     \t [0.32466893 0.59699123 0.85301079]. \t  3.7984895216417454 \t 3.851248811441325\n",
            "76     \t [0.15010152 0.58595341 0.83904269]. \t  3.799175889590776 \t 3.851248811441325\n",
            "77     \t [0.3426165  0.49517918 0.85954722]. \t  3.731473290669352 \t 3.851248811441325\n",
            "78     \t [0.25984649 0.60264448 0.8315051 ]. \t  3.737382659943892 \t 3.851248811441325\n",
            "79     \t [0.80686994 0.42310228 0.82575095]. \t  3.1938871421744013 \t 3.851248811441325\n",
            "80     \t [0.31034284 0.56453549 0.83095678]. \t  3.8105916819892816 \t 3.851248811441325\n",
            "81     \t [0.32285113 0.53726279 0.85003313]. \t  3.8501396583306016 \t 3.851248811441325\n",
            "82     \t [0.19728032 0.5996651  0.82168794]. \t  3.7001254116586453 \t 3.851248811441325\n",
            "83     \t [0.29491405 0.62390359 0.84238321]. \t  3.6853061449609 \t 3.851248811441325\n",
            "84     \t [0.1418136  0.5748949  0.85650265]. \t  3.8337116759562764 \t 3.851248811441325\n",
            "85     \t [0.33366106 0.51912844 0.83876892]. \t  3.8013114820371596 \t 3.851248811441325\n",
            "86     \t [0.31414683 0.60655475 0.82051301]. \t  3.6670122804457943 \t 3.851248811441325\n",
            "87     \t [0.17490746 0.58346378 0.85266647]. \t  3.8259656975897114 \t 3.851248811441325\n",
            "88     \t [0.17488467 0.58959363 0.84950177]. \t  3.8115449764966587 \t 3.851248811441325\n",
            "89     \t [0.35282952 0.58447225 0.86402849]. \t  3.8190253946178965 \t 3.851248811441325\n",
            "90     \t [0.33445933 0.57752347 0.82519606]. \t  3.765417303774832 \t 3.851248811441325\n",
            "91     \t [0.22611958 0.56435795 0.88311233]. \t  3.7654209278938144 \t 3.851248811441325\n",
            "92     \t [0.52352429 0.50660474 0.84690142]. \t  3.7598610356494606 \t 3.851248811441325\n",
            "93     \t [0.40391901 0.56580457 0.81509411]. \t  3.7087289274638775 \t 3.851248811441325\n",
            "94     \t [0.00648924 0.02562773 0.74629113]. \t  0.2955675844706197 \t 3.851248811441325\n",
            "95     \t [0.35029448 0.49942754 0.87722624]. \t  3.6873913334425534 \t 3.851248811441325\n",
            "96     \t [0.17191003 0.49213544 0.85983677]. \t  3.706356570581857 \t 3.851248811441325\n",
            "97     \t [0.33706844 0.53922256 0.88478177]. \t  3.7489953191621996 \t 3.851248811441325\n",
            "98     \t [0.0527791  0.54441325 0.87354106]. \t  3.7780520131649475 \t 3.851248811441325\n",
            "99     \t [0.22610265 0.5327401  0.87949135]. \t  3.7653153689573067 \t 3.851248811441325\n",
            "100    \t [0.25923289 0.49878653 0.8541935 ]. \t  3.7505018829066015 \t 3.851248811441325\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO3I_9cbuvY2",
        "outputId": "7d4ca93b-17d7-4a59-e806-1179b700391a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_winner_6 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_6 = dGPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.89286015 0.33197981 0.82122912]. \t  2.3879127825536575 \t 2.5106636917702634\n",
            "init   \t [0.04169663 0.10765668 0.59505206]. \t  0.2321934630074273 \t 2.5106636917702634\n",
            "init   \t [0.52981736 0.41880743 0.33540785]. \t  0.37989557867087576 \t 2.5106636917702634\n",
            "init   \t [0.62251943 0.43814143 0.73588211]. \t  2.5106636917702634 \t 2.5106636917702634\n",
            "init   \t [0.51803641 0.5788586  0.6453551 ]. \t  1.7289203948034264 \t 2.5106636917702634\n",
            "1      \t [0.62339102 0.39276721 0.97743955]. \t  1.8557550530508597 \t 2.5106636917702634\n",
            "2      \t [0.71020609 0.14342436 0.6823749 ]. \t  0.5592901720613555 \t 2.5106636917702634\n",
            "3      \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.5106636917702634\n",
            "4      \t [0.97849725 0.71027871 0.70548176]. \t  1.5009558083479608 \t 2.5106636917702634\n",
            "5      \t [0.96507948 0.60085005 0.82529255]. \t  \u001b[92m3.479322232477214\u001b[0m \t 3.479322232477214\n",
            "6      \t [0.99744691 0.48911675 0.96461515]. \t  2.4839600952325727 \t 3.479322232477214\n",
            "7      \t [0.93258042 0.03037806 0.07959995]. \t  0.12428519258285813 \t 3.479322232477214\n",
            "8      \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.479322232477214\n",
            "9      \t [0.03212161 0.87446244 0.89971721]. \t  1.432043492767212 \t 3.479322232477214\n",
            "10     \t [0.73896708 0.76851983 1.        ]. \t  1.3844526766304832 \t 3.479322232477214\n",
            "11     \t [0.01394374 0.54668949 0.0710222 ]. \t  0.034945604766983995 \t 3.479322232477214\n",
            "12     \t [0.15252632 0.39910018 0.99481014]. \t  1.6468854056844258 \t 3.479322232477214\n",
            "13     \t [0.92263314 0.48826251 0.73891189]. \t  2.5766109605220704 \t 3.479322232477214\n",
            "14     \t [0.51735215 0.         0.        ]. \t  0.09570527057613612 \t 3.479322232477214\n",
            "15     \t [0.97094455 0.61681975 0.82123681]. \t  3.3806134651071464 \t 3.479322232477214\n",
            "16     \t [0.99202153 0.66586733 0.85397994]. \t  3.233331254185489 \t 3.479322232477214\n",
            "17     \t [0.84623888 0.61597179 0.83098433]. \t  \u001b[92m3.5073107850901164\u001b[0m \t 3.5073107850901164\n",
            "18     \t [0.88513754 0.5807562  0.88391986]. \t  \u001b[92m3.6265071103917013\u001b[0m \t 3.6265071103917013\n",
            "19     \t [0.89586206 0.61686245 0.84955358]. \t  3.5558211277511 \t 3.6265071103917013\n",
            "20     \t [0.87646067 0.5941658  0.88205105]. \t  3.609051564782917 \t 3.6265071103917013\n",
            "21     \t [0.90145838 0.60821772 0.88711009]. \t  3.529598761862402 \t 3.6265071103917013\n",
            "22     \t [0.88693484 0.60107553 0.87606713]. \t  3.608170782312824 \t 3.6265071103917013\n",
            "23     \t [0.86658268 0.60853295 0.86713847]. \t  3.611037863474249 \t 3.6265071103917013\n",
            "24     \t [0.89888321 0.59731005 0.87151303]. \t  \u001b[92m3.628145407765722\u001b[0m \t 3.628145407765722\n",
            "25     \t [0.8275058  0.57229569 0.88914791]. \t  \u001b[92m3.6299126130393122\u001b[0m \t 3.6299126130393122\n",
            "26     \t [0.81863404 0.56807482 0.89117049]. \t  3.6236113464184214 \t 3.6299126130393122\n",
            "27     \t [1.78804385e-04 5.64974133e-01 9.15773708e-01]. \t  3.425229729350826 \t 3.6299126130393122\n",
            "28     \t [0.82253256 0.58132729 0.875213  ]. \t  \u001b[92m3.6870648647356368\u001b[0m \t 3.6870648647356368\n",
            "29     \t [0.82986491 0.54233163 0.86740178]. \t  \u001b[92m3.7304332930652673\u001b[0m \t 3.7304332930652673\n",
            "30     \t [0.91313249 0.5913635  0.87137317]. \t  3.6394755783572332 \t 3.7304332930652673\n",
            "31     \t [0.8489063  0.57460574 0.89053075]. \t  3.6104095561911373 \t 3.7304332930652673\n",
            "32     \t [0.87958174 0.57923679 0.86785493]. \t  3.68851264288416 \t 3.7304332930652673\n",
            "33     \t [0.80488197 0.57674443 0.87794927]. \t  3.6910710578483066 \t 3.7304332930652673\n",
            "34     \t [0.89683969 0.56516159 0.86161877]. \t  3.7096050035413564 \t 3.7304332930652673\n",
            "35     \t [0.85806449 0.5721716  0.86242119]. \t  3.716410414225619 \t 3.7304332930652673\n",
            "36     \t [0.38628025 0.54379701 0.00090316]. \t  0.019307478371997217 \t 3.7304332930652673\n",
            "37     \t [0.8175089  0.57552778 0.84353979]. \t  3.7066515642754987 \t 3.7304332930652673\n",
            "38     \t [0.80399901 0.54996298 0.86238504]. \t  \u001b[92m3.7518810156556777\u001b[0m \t 3.7518810156556777\n",
            "39     \t [0.82900952 0.56301847 0.87194193]. \t  3.719076829211289 \t 3.7518810156556777\n",
            "40     \t [0.76586006 0.55485107 0.85431554]. \t  \u001b[92m3.7679018648531404\u001b[0m \t 3.7679018648531404\n",
            "41     \t [0.77651351 0.48489849 0.88276565]. \t  3.5257052333425296 \t 3.7679018648531404\n",
            "42     \t [0.90995934 0.6125598  0.8971158 ]. \t  3.446491118611175 \t 3.7679018648531404\n",
            "43     \t [0.81017731 0.53567667 0.8457003 ]. \t  3.7346148758866233 \t 3.7679018648531404\n",
            "44     \t [0.90432276 0.54217031 0.90323389]. \t  3.491062082111341 \t 3.7679018648531404\n",
            "45     \t [0.85005041 0.53563284 0.82991268]. \t  3.6618327548049074 \t 3.7679018648531404\n",
            "46     \t [0.90722613 0.57234367 0.84227109]. \t  3.670318125888932 \t 3.7679018648531404\n",
            "47     \t [0.71850661 0.56743638 0.88848537]. \t  3.6738211592102825 \t 3.7679018648531404\n",
            "48     \t [0.91368063 0.55104776 0.88283512]. \t  3.643418240401608 \t 3.7679018648531404\n",
            "49     \t [0.83864209 0.52610165 0.83429252]. \t  3.6771298008394964 \t 3.7679018648531404\n",
            "50     \t [0.81133892 0.52725146 0.83165098]. \t  3.6777307505145167 \t 3.7679018648531404\n",
            "51     \t [0.85917239 0.54239508 0.87517672]. \t  3.695494500524883 \t 3.7679018648531404\n",
            "52     \t [0.84175349 0.56955944 0.84613121]. \t  3.7137737147658454 \t 3.7679018648531404\n",
            "53     \t [0.76722891 0.52715286 0.86583604]. \t  3.736924034738567 \t 3.7679018648531404\n",
            "54     \t [0.02659093 0.78546449 0.98138486]. \t  1.5030674068473044 \t 3.7679018648531404\n",
            "55     \t [0.7540458  0.50352924 0.87977709]. \t  3.6256836323899986 \t 3.7679018648531404\n",
            "56     \t [0.85828605 0.55332022 0.8843243 ]. \t  3.6573322948213027 \t 3.7679018648531404\n",
            "57     \t [0.7501814  0.57284359 0.8928783 ]. \t  3.6291169164195796 \t 3.7679018648531404\n",
            "58     \t [0.80670888 0.52217491 0.86639044]. \t  3.7125633169075107 \t 3.7679018648531404\n",
            "59     \t [0.74938371 0.53767618 0.8032371 ]. \t  3.506843475592679 \t 3.7679018648531404\n",
            "60     \t [0.8465188  0.5360093  0.85457661]. \t  3.7314400166734742 \t 3.7679018648531404\n",
            "61     \t [0.8666964  0.49743407 0.88482756]. \t  3.5375915663427633 \t 3.7679018648531404\n",
            "62     \t [0.81752486 0.54584835 0.90995638]. \t  3.4573870933954245 \t 3.7679018648531404\n",
            "63     \t [0.8617862  0.55149755 0.84578009]. \t  3.7198124677513524 \t 3.7679018648531404\n",
            "64     \t [0.87254559 0.5468038  0.87345319]. \t  3.6993993636257914 \t 3.7679018648531404\n",
            "65     \t [0.79838393 0.61514929 0.80933818]. \t  3.3723268594821185 \t 3.7679018648531404\n",
            "66     \t [0.76516933 0.54045315 0.84460957]. \t  3.752407680289558 \t 3.7679018648531404\n",
            "67     \t [0.83021998 0.52523803 0.86729799]. \t  3.708146929411615 \t 3.7679018648531404\n",
            "68     \t [0.80301637 0.51384278 0.79407349]. \t  3.3735954417712612 \t 3.7679018648531404\n",
            "69     \t [0.72509827 0.56074533 0.86044807]. \t  \u001b[92m3.778413379291127\u001b[0m \t 3.778413379291127\n",
            "70     \t [0.78589757 0.512392   0.83758553]. \t  3.6834929449548595 \t 3.778413379291127\n",
            "71     \t [0.75182451 0.6066733  0.8360198 ]. \t  3.6131997194735956 \t 3.778413379291127\n",
            "72     \t [0.72865808 0.59380841 0.85083191]. \t  3.712736420128867 \t 3.778413379291127\n",
            "73     \t [0.73362939 0.57748397 0.85178233]. \t  3.752294599695716 \t 3.778413379291127\n",
            "74     \t [0.72722346 0.60551864 0.84211875]. \t  3.6515978006190384 \t 3.778413379291127\n",
            "75     \t [0.71502076 0.5774446  0.85639176]. \t  3.762244753112911 \t 3.778413379291127\n",
            "76     \t [0.84198685 0.58773048 0.8324694 ]. \t  3.6219104249568854 \t 3.778413379291127\n",
            "77     \t [0.76282278 0.53112841 0.88362905]. \t  3.6737680057851714 \t 3.778413379291127\n",
            "78     \t [0.70706458 0.60328283 0.83894117]. \t  3.656159806255424 \t 3.778413379291127\n",
            "79     \t [0.77565865 0.52368746 0.85306151]. \t  3.739453353810216 \t 3.778413379291127\n",
            "80     \t [0.75086224 0.5834193  0.82692519]. \t  3.6403459743573734 \t 3.778413379291127\n",
            "81     \t [0.72929897 0.59273365 0.83956184]. \t  3.685501861285374 \t 3.778413379291127\n",
            "82     \t [0.80147698 0.55641023 0.85182886]. \t  3.75226874429868 \t 3.778413379291127\n",
            "83     \t [0.64937874 0.55835934 0.87602226]. \t  3.7636743415944522 \t 3.778413379291127\n",
            "84     \t [0.74504574 0.58034132 0.8173034 ]. \t  3.584075700111198 \t 3.778413379291127\n",
            "85     \t [0.68703075 0.61245434 0.83711399]. \t  3.619885057350351 \t 3.778413379291127\n",
            "86     \t [0.62866458 0.59658921 0.88111575]. \t  3.6868276101972555 \t 3.778413379291127\n",
            "87     \t [0.61409324 0.56561885 0.88285288]. \t  3.7359904977618363 \t 3.778413379291127\n",
            "88     \t [0.65424428 0.57748645 0.88117675]. \t  3.7218314063353457 \t 3.778413379291127\n",
            "89     \t [0.7195221  0.56557973 0.87843673]. \t  3.730023701046264 \t 3.778413379291127\n",
            "90     \t [0.63209985 0.60419964 0.87543579]. \t  3.685774959263176 \t 3.778413379291127\n",
            "91     \t [0.84560899 0.58559108 0.84974077]. \t  3.6882320138538596 \t 3.778413379291127\n",
            "92     \t [0.80471763 0.52285159 0.86682759]. \t  3.71363256025223 \t 3.778413379291127\n",
            "93     \t [0.79143477 0.5378396  0.86295887]. \t  3.7488212095090447 \t 3.778413379291127\n",
            "94     \t [0.78097919 0.55504503 0.8644091 ]. \t  3.7571768502112293 \t 3.778413379291127\n",
            "95     \t [0.00762193 0.54914093 0.82723642]. \t  3.7542285395198967 \t 3.778413379291127\n",
            "96     \t [0.69178424 0.58476683 0.84527525]. \t  3.7389651368395977 \t 3.778413379291127\n",
            "97     \t [0.80415143 0.55512605 0.86184646]. \t  3.7521205207659536 \t 3.778413379291127\n",
            "98     \t [0.7094514  0.55669631 0.86420177]. \t  \u001b[92m3.78084238376605\u001b[0m \t 3.78084238376605\n",
            "99     \t [0.80050327 0.54722835 0.84580438]. \t  3.744378864245617 \t 3.78084238376605\n",
            "100    \t [0.74851383 0.55689392 0.84554985]. \t  3.760966206270442 \t 3.78084238376605\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rQbLZD8uvbI",
        "outputId": "fb00f6a8-baae-443d-ca0d-20b33a384977"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_winner_7 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_7 = dGPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.15266373 0.30235661 0.06203641]. \t  0.1742288077468587 \t 1.6237282255098657\n",
            "init   \t [0.45986034 0.83525338 0.92699705]. \t  1.6237282255098657 \t 1.6237282255098657\n",
            "init   \t [0.72698898 0.76849622 0.26920507]. \t  0.08405715787489784 \t 1.6237282255098657\n",
            "init   \t [0.64402929 0.09337326 0.07968589]. \t  0.2756481743251193 \t 1.6237282255098657\n",
            "init   \t [0.58961375 0.34334054 0.98887615]. \t  1.4020548914621052 \t 1.6237282255098657\n",
            "1      \t [0.1228205  0.69749297 0.96862159]. \t  \u001b[92m2.2321850879875504\u001b[0m \t 2.2321850879875504\n",
            "2      \t [0.0076977  0.39238943 0.9762485 ]. \t  1.8538199616240632 \t 2.2321850879875504\n",
            "3      \t [0.12469335 0.77127172 0.86632032]. \t  \u001b[92m2.5214355469878886\u001b[0m \t 2.5214355469878886\n",
            "4      \t [0.1041318  0.90500514 0.83405015]. \t  1.3678553725685025 \t 2.5214355469878886\n",
            "5      \t [0.25279421 0.44453591 0.62994747]. \t  1.4123554186027867 \t 2.5214355469878886\n",
            "6      \t [0.95618075 0.6884264  0.95773274]. \t  2.3675412650637764 \t 2.5214355469878886\n",
            "7      \t [0.01442092 0.6418408  0.61423899]. \t  2.200040145683039 \t 2.5214355469878886\n",
            "8      \t [0.30586278 0.61766856 0.77639938]. \t  \u001b[92m3.2729413146881026\u001b[0m \t 3.2729413146881026\n",
            "9      \t [0.34792509 0.69063488 0.76390236]. \t  2.8201169112453925 \t 3.2729413146881026\n",
            "10     \t [0.9881109  0.10978808 0.01403095]. \t  0.04618316634775171 \t 3.2729413146881026\n",
            "11     \t [0.94651224 0.95577975 0.04945068]. \t  0.00023928978237340077 \t 3.2729413146881026\n",
            "12     \t [0.75967177 0.34395736 0.00693316]. \t  0.04944950921765359 \t 3.2729413146881026\n",
            "13     \t [0.93188349 0.83576376 0.80982247]. \t  1.5670630573218918 \t 3.2729413146881026\n",
            "14     \t [0.25542042 0.62759994 0.83959488]. \t  \u001b[92m3.6631762631954197\u001b[0m \t 3.6631762631954197\n",
            "15     \t [0.12693224 0.94088802 0.01578644]. \t  0.0006126063891919089 \t 3.6631762631954197\n",
            "16     \t [3.80213085e-15 3.87055813e-15 7.51852114e-15]. \t  0.06797411659014163 \t 3.6631762631954197\n",
            "17     \t [0.19586793 0.45317906 0.95847568]. \t  2.505936313495623 \t 3.6631762631954197\n",
            "18     \t [0.99386303 0.53005931 0.77357096]. \t  3.0593962602155154 \t 3.6631762631954197\n",
            "19     \t [0.10590882 0.56444872 0.79558805]. \t  3.5649077891169636 \t 3.6631762631954197\n",
            "20     \t [0.49062218 0.56260536 0.8773862 ]. \t  \u001b[92m3.7886577646694684\u001b[0m \t 3.7886577646694684\n",
            "21     \t [0.65964162 0.53171858 0.90109329]. \t  3.5698340975181795 \t 3.7886577646694684\n",
            "22     \t [0.05698033 0.53401445 0.83605924]. \t  \u001b[92m3.789995496743377\u001b[0m \t 3.789995496743377\n",
            "23     \t [0.54291187 0.57623806 0.91493272]. \t  3.462557800875688 \t 3.789995496743377\n",
            "24     \t [0.05498385 0.58092463 0.80275549]. \t  3.593033692774356 \t 3.789995496743377\n",
            "25     \t [0.60491521 0.58294891 0.86498087]. \t  3.778502888741545 \t 3.789995496743377\n",
            "26     \t [0.52723276 0.54418996 0.88793047]. \t  3.718497073835522 \t 3.789995496743377\n",
            "27     \t [0.84367082 0.56933244 0.81112246]. \t  3.512914507969666 \t 3.789995496743377\n",
            "28     \t [0.68635254 0.48137246 0.80091734]. \t  3.397752617876571 \t 3.789995496743377\n",
            "29     \t [0.01924337 0.56712374 0.80209471]. \t  3.5928191473013658 \t 3.789995496743377\n",
            "30     \t [0.79934789 0.56748854 0.82622241]. \t  3.6481038986509198 \t 3.789995496743377\n",
            "31     \t [0.037155   0.50365788 0.83558309]. \t  3.71124887338506 \t 3.789995496743377\n",
            "32     \t [0.46586733 0.5912058  0.89598179]. \t  3.634632808984698 \t 3.789995496743377\n",
            "33     \t [0.63750431 0.5684959  0.84727312]. \t  \u001b[92m3.7911097272882177\u001b[0m \t 3.7911097272882177\n",
            "34     \t [0.52211898 0.56977162 0.85805128]. \t  \u001b[92m3.826698651924172\u001b[0m \t 3.826698651924172\n",
            "35     \t [0.51590221 0.58881371 0.82132482]. \t  3.678275384836793 \t 3.826698651924172\n",
            "36     \t [0.60872771 0.55025054 0.83654601]. \t  3.7816428175097396 \t 3.826698651924172\n",
            "37     \t [0.72822591 0.57193554 0.82948938]. \t  3.688534842239563 \t 3.826698651924172\n",
            "38     \t [0.59559431 0.60536316 0.87923851]. \t  3.675957651663423 \t 3.826698651924172\n",
            "39     \t [0.59975552 0.53963737 0.86559749]. \t  3.80193359950598 \t 3.826698651924172\n",
            "40     \t [0.00793405 0.49796891 0.79791881]. \t  3.4700185857117547 \t 3.826698651924172\n",
            "41     \t [0.643099   0.53533723 0.85048082]. \t  3.796478792580488 \t 3.826698651924172\n",
            "42     \t [0.55176877 0.55484955 0.88778918]. \t  3.721159020630474 \t 3.826698651924172\n",
            "43     \t [0.58045324 0.55898217 0.86938196]. \t  3.803260370688478 \t 3.826698651924172\n",
            "44     \t [0.57745283 0.57096269 0.84668465]. \t  3.8040348133536215 \t 3.826698651924172\n",
            "45     \t [0.54993035 0.54133563 0.84520529]. \t  3.8184475869781203 \t 3.826698651924172\n",
            "46     \t [0.55547981 0.62964462 0.84612434]. \t  3.613527348769556 \t 3.826698651924172\n",
            "47     \t [0.6078585  0.54771312 0.82611425]. \t  3.733286195958835 \t 3.826698651924172\n",
            "48     \t [0.01532198 0.59800027 0.83105696]. \t  3.7097300271595506 \t 3.826698651924172\n",
            "49     \t [0.46731008 0.54799948 0.89130677]. \t  3.70498638368235 \t 3.826698651924172\n",
            "50     \t [0.74154987 0.54555055 0.84589161]. \t  3.7657277082951195 \t 3.826698651924172\n",
            "51     \t [0.63678443 0.54648393 0.86819737]. \t  3.79178523637073 \t 3.826698651924172\n",
            "52     \t [0.66283106 0.53927126 0.83944892]. \t  3.771940189373819 \t 3.826698651924172\n",
            "53     \t [0.71136246 0.57234529 0.82358083]. \t  3.6589823209238492 \t 3.826698651924172\n",
            "54     \t [0.56999573 0.53968611 0.82169655]. \t  3.7166509718359344 \t 3.826698651924172\n",
            "55     \t [0.53447735 0.52977849 0.82879455]. \t  3.755910358558721 \t 3.826698651924172\n",
            "56     \t [0.66573513 0.54180712 0.84858005]. \t  3.793334904451174 \t 3.826698651924172\n",
            "57     \t [0.01696386 0.55176904 0.85066532]. \t  3.815766418726344 \t 3.826698651924172\n",
            "58     \t [0.6924728  0.53612105 0.84574831]. \t  3.776010248892571 \t 3.826698651924172\n",
            "59     \t [0.60322953 0.60476444 0.84643603]. \t  3.709941796278266 \t 3.826698651924172\n",
            "60     \t [0.19309683 0.54970667 0.85406449]. \t  \u001b[92m3.8541794828473703\u001b[0m \t 3.8541794828473703\n",
            "61     \t [0.51582329 0.54557572 0.83754311]. \t  3.8087260677499093 \t 3.8541794828473703\n",
            "62     \t [0.03642401 0.5421794  0.83060096]. \t  3.774171704205555 \t 3.8541794828473703\n",
            "63     \t [0.6369959  0.54669297 0.85164956]. \t  3.807256047599469 \t 3.8541794828473703\n",
            "64     \t [0.68976084 0.49189078 0.84042609]. \t  3.658465628169113 \t 3.8541794828473703\n",
            "65     \t [0.58174871 0.55407819 0.84704559]. \t  3.816987001511693 \t 3.8541794828473703\n",
            "66     \t [0.71000529 0.52731274 0.86024119]. \t  3.764550386171388 \t 3.8541794828473703\n",
            "67     \t [0.59326502 0.48654916 0.86038322]. \t  3.665021969701305 \t 3.8541794828473703\n",
            "68     \t [0.53266935 0.51068044 0.86005313]. \t  3.7678527072438848 \t 3.8541794828473703\n",
            "69     \t [0.57135915 0.57075895 0.85499496]. \t  3.8146875459242295 \t 3.8541794828473703\n",
            "70     \t [0.0612485  0.59741803 0.83314225]. \t  3.7336197154934325 \t 3.8541794828473703\n",
            "71     \t [0.15964409 0.54559916 0.8441172 ]. \t  3.8416691651399404 \t 3.8541794828473703\n",
            "72     \t [0.50335979 0.50407685 0.84223116]. \t  3.748555388798298 \t 3.8541794828473703\n",
            "73     \t [0.08371059 0.54714276 0.84712551]. \t  3.830910985634947 \t 3.8541794828473703\n",
            "74     \t [0.50078189 0.52782536 0.86914771]. \t  3.7938156738763444 \t 3.8541794828473703\n",
            "75     \t [0.47799517 0.5431212  0.85710149]. \t  3.8408775787810328 \t 3.8541794828473703\n",
            "76     \t [0.08998209 0.52394217 0.85528971]. \t  3.801174784260364 \t 3.8541794828473703\n",
            "77     \t [0.5283124  0.54800654 0.85501585]. \t  3.8352485665717895 \t 3.8541794828473703\n",
            "78     \t [0.50702494 0.5250258  0.85547592]. \t  3.812371999560926 \t 3.8541794828473703\n",
            "79     \t [0.50345065 0.56763936 0.83772681]. \t  3.8032195935487825 \t 3.8541794828473703\n",
            "80     \t [0.02386371 0.54786089 0.84288699]. \t  3.8085026212839037 \t 3.8541794828473703\n",
            "81     \t [0.40571219 0.56768748 0.84701891]. \t  3.843671713533129 \t 3.8541794828473703\n",
            "82     \t [0.70974999 0.58539053 0.85840418]. \t  3.747567205575607 \t 3.8541794828473703\n",
            "83     \t [0.34585314 0.56790709 0.84444453]. \t  3.8454768801783485 \t 3.8541794828473703\n",
            "84     \t [0.57336963 0.52625156 0.85687252]. \t  3.8011100128648483 \t 3.8541794828473703\n",
            "85     \t [0.7573889  0.54304915 0.85839242]. \t  3.7691685001303 \t 3.8541794828473703\n",
            "86     \t [0.69719331 0.51835258 0.86668769]. \t  3.737674842272293 \t 3.8541794828473703\n",
            "87     \t [0.62894272 0.55778902 0.84056836]. \t  3.7868460979136356 \t 3.8541794828473703\n",
            "88     \t [0.5771285  0.56995275 0.82730899]. \t  3.7332558346138836 \t 3.8541794828473703\n",
            "89     \t [0.47771183 0.56252736 0.83210257]. \t  3.7924100895353736 \t 3.8541794828473703\n",
            "90     \t [0.20440851 0.54177141 0.85388845]. \t  3.8501463901433834 \t 3.8541794828473703\n",
            "91     \t [0.35174784 0.56858545 0.85111608]. \t  3.8521669903171745 \t 3.8541794828473703\n",
            "92     \t [0.05655533 0.54331401 0.85150557]. \t  3.8233473426351607 \t 3.8541794828473703\n",
            "93     \t [0.0084857  0.6650975  0.02722343]. \t  0.006406214077722109 \t 3.8541794828473703\n",
            "94     \t [0.73139698 0.51536559 0.87108394]. \t  3.706983849048971 \t 3.8541794828473703\n",
            "95     \t [0.11532705 0.54804338 0.851647  ]. \t  3.8409480661796644 \t 3.8541794828473703\n",
            "96     \t [0.36542961 0.54703972 0.84855458]. \t  \u001b[92m3.8552134565466973\u001b[0m \t 3.8552134565466973\n",
            "97     \t [0.45837934 0.58134122 0.84421362]. \t  3.8093383877567857 \t 3.8552134565466973\n",
            "98     \t [0.46784003 0.58999501 0.87648203]. \t  3.7569980122066453 \t 3.8552134565466973\n",
            "99     \t [0.6459592  0.56279662 0.83911369]. \t  3.772995215470467 \t 3.8552134565466973\n",
            "100    \t [0.32429938 0.54060259 0.84236897]. \t  3.8442000958025035 \t 3.8552134565466973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YRio_skuvd2",
        "outputId": "98e34efc-27aa-46fe-8f5f-0cdecc3c9fed"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_winner_8 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_8 = dGPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.10949987 0.04221501 0.59969258]. \t  0.15732379460832624 \t 0.8830091449513892\n",
            "init   \t [0.29987071 0.79661178 0.36619613]. \t  0.8830091449513892 \t 0.8830091449513892\n",
            "init   \t [0.88060834 0.29784163 0.32910326]. \t  0.2992873424905509 \t 0.8830091449513892\n",
            "init   \t [0.56871692 0.74392742 0.05385289]. \t  0.0051668955191694075 \t 0.8830091449513892\n",
            "init   \t [0.60207437 0.42931858 0.20547361]. \t  0.2908249205255991 \t 0.8830091449513892\n",
            "1      \t [0.19816249 0.92572195 0.98825083]. \t  0.653164991364588 \t 0.8830091449513892\n",
            "2      \t [0.03762084 0.88738442 0.32955494]. \t  0.6464733859302134 \t 0.8830091449513892\n",
            "3      \t [0.97939743 0.96542716 0.91333339]. \t  0.6808959887841568 \t 0.8830091449513892\n",
            "4      \t [0.49865894 0.65316463 0.74953382]. \t  \u001b[92m2.7456535957813655\u001b[0m \t 2.7456535957813655\n",
            "5      \t [0.7756353  0.24692951 0.94219957]. \t  1.1705476258119498 \t 2.7456535957813655\n",
            "6      \t [0.16255619 0.40729302 0.91314355]. \t  \u001b[92m2.7883202474653848\u001b[0m \t 2.7883202474653848\n",
            "7      \t [0.4098729  0.5607599  0.84633383]. \t  \u001b[92m3.8475130629519976\u001b[0m \t 3.8475130629519976\n",
            "8      \t [0.57307906 0.43942751 0.97176975]. \t  2.220614062116511 \t 3.8475130629519976\n",
            "9      \t [0.19537846 0.4782499  0.67928698]. \t  2.069610131464016 \t 3.8475130629519976\n",
            "10     \t [0.54227563 0.00114018 0.54400968]. \t  0.12513676670448495 \t 3.8475130629519976\n",
            "11     \t [0.08083624 0.74312768 0.00925588]. \t  0.0024337729082821256 \t 3.8475130629519976\n",
            "12     \t [0.31487353 0.51781206 0.87757733]. \t  3.7470764388674986 \t 3.8475130629519976\n",
            "13     \t [0.2026131  0.90973368 0.02599246]. \t  0.0009568874318973229 \t 3.8475130629519976\n",
            "14     \t [0.8613736  0.14774695 0.00951219]. \t  0.06517527043637528 \t 3.8475130629519976\n",
            "15     \t [0.26322611 0.036377   0.91784865]. \t  0.25480779018613087 \t 3.8475130629519976\n",
            "16     \t [0.38845734 0.64455623 0.99897677]. \t  1.987956246610217 \t 3.8475130629519976\n",
            "17     \t [5.49019184e-15 1.29792888e-14 3.89288192e-15]. \t  0.06797411659013942 \t 3.8475130629519976\n",
            "18     \t [0.37858625 0.48037213 0.81211052]. \t  3.546220367057 \t 3.8475130629519976\n",
            "19     \t [0.98902971 0.54205717 0.91942945]. \t  3.28457789764589 \t 3.8475130629519976\n",
            "20     \t [0.2122119  0.72194485 0.80873911]. \t  2.9188700339334934 \t 3.8475130629519976\n",
            "21     \t [0.98719545 0.56938158 0.77799039]. \t  3.0782574578002206 \t 3.8475130629519976\n",
            "22     \t [0.39250378 0.48667236 0.82032287]. \t  3.619749230730842 \t 3.8475130629519976\n",
            "23     \t [0.98088512 0.40005444 0.81964994]. \t  2.943206808295248 \t 3.8475130629519976\n",
            "24     \t [0.3553483  0.47688403 0.86078594]. \t  3.6431633940025625 \t 3.8475130629519976\n",
            "25     \t [0.3448575  0.58388103 0.79723317]. \t  3.5507142687129045 \t 3.8475130629519976\n",
            "26     \t [0.32733396 0.47852482 0.84158767]. \t  3.6584427328834703 \t 3.8475130629519976\n",
            "27     \t [0.34662502 0.53173329 0.80435048]. \t  3.6343276150120007 \t 3.8475130629519976\n",
            "28     \t [0.3937131  0.49304056 0.82225622]. \t  3.654612607251607 \t 3.8475130629519976\n",
            "29     \t [0.29206334 0.52581095 0.83811972]. \t  3.8145029161418647 \t 3.8475130629519976\n",
            "30     \t [0.35428888 0.52564898 0.82331688]. \t  3.7531087583951313 \t 3.8475130629519976\n",
            "31     \t [0.40799104 0.45207011 0.84024056]. \t  3.5034615324955194 \t 3.8475130629519976\n",
            "32     \t [0.31864398 0.50112316 0.85412911]. \t  3.761291670956137 \t 3.8475130629519976\n",
            "33     \t [0.33746942 0.54684666 0.80699548]. \t  3.6674277828844266 \t 3.8475130629519976\n",
            "34     \t [0.29101632 0.52363381 0.84969161]. \t  3.8273325580104 \t 3.8475130629519976\n",
            "35     \t [0.37318809 0.54726857 0.81816116]. \t  3.7418744815514593 \t 3.8475130629519976\n",
            "36     \t [0.33670759 0.53928548 0.8320637 ]. \t  3.8122748603799232 \t 3.8475130629519976\n",
            "37     \t [0.31881108 0.56910635 0.78562488]. \t  3.4671599520128096 \t 3.8475130629519976\n",
            "38     \t [0.42529493 0.45915079 0.81965217]. \t  3.479035903369593 \t 3.8475130629519976\n",
            "39     \t [0.3865261  0.57123201 0.89047821]. \t  3.7150155424359044 \t 3.8475130629519976\n",
            "40     \t [0.35474914 0.48793582 0.8946866 ]. \t  3.518024571783889 \t 3.8475130629519976\n",
            "41     \t [0.43761059 0.45324756 0.838759  ]. \t  3.506404342518156 \t 3.8475130629519976\n",
            "42     \t [0.45882332 0.5536904  0.83350667]. \t  3.806703702546363 \t 3.8475130629519976\n",
            "43     \t [0.34611748 0.59271859 0.856113  ]. \t  3.8089400498621995 \t 3.8475130629519976\n",
            "44     \t [0.35868254 0.57253443 0.85473902]. \t  \u001b[92m3.847888354937641\u001b[0m \t 3.847888354937641\n",
            "45     \t [0.27701239 0.5769035  0.87920273]. \t  3.7792716657282455 \t 3.847888354937641\n",
            "46     \t [0.43330538 0.4950265  0.84911726]. \t  3.7331307992439227 \t 3.847888354937641\n",
            "47     \t [0.37151578 0.56671362 0.88442346]. \t  3.759843343762049 \t 3.847888354937641\n",
            "48     \t [0.40986933 0.5487377  0.83747394]. \t  3.828856812716581 \t 3.847888354937641\n",
            "49     \t [0.3914415  0.56188128 0.75259128]. \t  3.071268041199648 \t 3.847888354937641\n",
            "50     \t [0.32452031 0.47448466 0.86006348]. \t  3.631944256869967 \t 3.847888354937641\n",
            "51     \t [0.46469803 0.51621603 0.84260437]. \t  3.7900899627502014 \t 3.847888354937641\n",
            "52     \t [0.40806913 0.54259978 0.87326799]. \t  3.8106297770269166 \t 3.847888354937641\n",
            "53     \t [0.35036361 0.48852123 0.86492611]. \t  3.6900886813844846 \t 3.847888354937641\n",
            "54     \t [0.46165302 0.52948436 0.82498047]. \t  3.7523023812807237 \t 3.847888354937641\n",
            "55     \t [0.47004564 0.51708793 0.84375601]. \t  3.793482102478536 \t 3.847888354937641\n",
            "56     \t [0.48931329 0.52505576 0.83604451]. \t  3.787562159865762 \t 3.847888354937641\n",
            "57     \t [0.42950108 0.56095241 0.88118394]. \t  3.77743240551864 \t 3.847888354937641\n",
            "58     \t [0.37574745 0.46920575 0.84778767]. \t  3.6134945085094357 \t 3.847888354937641\n",
            "59     \t [0.69759708 0.52185211 0.84185681]. \t  3.7456057832682346 \t 3.847888354937641\n",
            "60     \t [0.37461117 0.51635475 0.84390307]. \t  3.8025935024767072 \t 3.847888354937641\n",
            "61     \t [0.71754688 0.52630037 0.8464476 ]. \t  3.7566091504002563 \t 3.847888354937641\n",
            "62     \t [0.53704263 0.52568985 0.84865579]. \t  3.8063099410764187 \t 3.847888354937641\n",
            "63     \t [0.71151697 0.51964138 0.82978276]. \t  3.693205868777259 \t 3.847888354937641\n",
            "64     \t [0.47822785 0.49893758 0.85723141]. \t  3.7400260755599493 \t 3.847888354937641\n",
            "65     \t [0.54513824 0.53924162 0.86353931]. \t  3.8174309974621043 \t 3.847888354937641\n",
            "66     \t [0.33015855 0.54819143 0.88859966]. \t  3.7320985759817047 \t 3.847888354937641\n",
            "67     \t [0.78320802 0.52769301 0.85410704]. \t  3.7440450830029177 \t 3.847888354937641\n",
            "68     \t [0.69368906 0.54688044 0.85423381]. \t  3.7919382468126828 \t 3.847888354937641\n",
            "69     \t [0.76918819 0.52755317 0.87305318]. \t  3.715724624340162 \t 3.847888354937641\n",
            "70     \t [0.51399204 0.53867847 0.84922691]. \t  3.8295369209709667 \t 3.847888354937641\n",
            "71     \t [0.27116133 0.58443783 0.85087348]. \t  3.830187031607081 \t 3.847888354937641\n",
            "72     \t [0.85075398 0.50882263 0.85832592]. \t  3.675893336549908 \t 3.847888354937641\n",
            "73     \t [0.49568962 0.50483888 0.85059   ]. \t  3.761290420677044 \t 3.847888354937641\n",
            "74     \t [0.46617707 0.51665492 0.87171231]. \t  3.763248086297602 \t 3.847888354937641\n",
            "75     \t [0.49034124 0.54853857 0.82635005]. \t  3.7670664751440692 \t 3.847888354937641\n",
            "76     \t [0.56714124 0.5218797  0.84489875]. \t  3.7874799524813176 \t 3.847888354937641\n",
            "77     \t [0.49181944 0.52488204 0.87562283]. \t  3.7638589474271704 \t 3.847888354937641\n",
            "78     \t [0.51853138 0.53601469 0.86531557]. \t  3.814789619294645 \t 3.847888354937641\n",
            "79     \t [0.71283006 0.50368902 0.85524346]. \t  3.7080249534352623 \t 3.847888354937641\n",
            "80     \t [0.71570076 0.52588535 0.83783094]. \t  3.734921653075729 \t 3.847888354937641\n",
            "81     \t [0.5350007  0.52153953 0.86772019]. \t  3.779380440838126 \t 3.847888354937641\n",
            "82     \t [0.54093974 0.54871168 0.83876674]. \t  3.807504869502849 \t 3.847888354937641\n",
            "83     \t [0.4975631  0.50597472 0.86132197]. \t  3.7563884342308915 \t 3.847888354937641\n",
            "84     \t [0.67908263 0.53839086 0.84241239]. \t  3.774693825115529 \t 3.847888354937641\n",
            "85     \t [0.3230666  0.584199   0.84912687]. \t  3.8282693651663466 \t 3.847888354937641\n",
            "86     \t [0.55878784 0.52413988 0.87139845]. \t  3.768889537682844 \t 3.847888354937641\n",
            "87     \t [0.51086454 0.51793367 0.86143071]. \t  3.789282515172729 \t 3.847888354937641\n",
            "88     \t [0.53317656 0.5344833  0.86533266]. \t  3.810100838330986 \t 3.847888354937641\n",
            "89     \t [0.49923721 0.5292724  0.85187851]. \t  3.82167936248842 \t 3.847888354937641\n",
            "90     \t [0.63607166 0.51435076 0.86957248]. \t  3.734645755087433 \t 3.847888354937641\n",
            "91     \t [0.43378682 0.52957014 0.87085847]. \t  3.7993662510318904 \t 3.847888354937641\n",
            "92     \t [0.70585762 0.54233143 0.87887131]. \t  3.7304226977314205 \t 3.847888354937641\n",
            "93     \t [0.72227715 0.52798498 0.86607445]. \t  3.7520532742918675 \t 3.847888354937641\n",
            "94     \t [0.63523528 0.52601821 0.86900775]. \t  3.76432460037428 \t 3.847888354937641\n",
            "95     \t [0.28194215 0.5665518  0.84499582]. \t  \u001b[92m3.850126086745435\u001b[0m \t 3.850126086745435\n",
            "96     \t [0.51978988 0.54573616 0.85187482]. \t  3.8353718029482815 \t 3.850126086745435\n",
            "97     \t [0.52052174 0.52531853 0.84479105]. \t  3.803827523144731 \t 3.850126086745435\n",
            "98     \t [0.42405192 0.53293305 0.87198379]. \t  3.8018803000783836 \t 3.850126086745435\n",
            "99     \t [0.35176576 0.55320977 0.84581302]. \t  \u001b[92m3.8544618525636816\u001b[0m \t 3.8544618525636816\n",
            "100    \t [0.3261195  0.59460952 0.87068424]. \t  3.7792723500945637 \t 3.8544618525636816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejw6v-Ihuvf_",
        "outputId": "ed753e66-0b55-40bd-c469-0b25000aae91"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_winner_9 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_9 = dGPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.80342804 0.5275223  0.11911147]. \t  0.05516938924925502 \t 1.6536488994056173\n",
            "init   \t [0.63968144 0.09092526 0.33222568]. \t  0.7039339223433662 \t 1.6536488994056173\n",
            "init   \t [0.42738095 0.55438581 0.62812652]. \t  1.6536488994056173 \t 1.6536488994056173\n",
            "init   \t [0.69739294 0.78994969 0.13189035]. \t  0.009151170523361311 \t 1.6536488994056173\n",
            "init   \t [0.34277045 0.20155961 0.70732423]. \t  0.9342632521680911 \t 1.6536488994056173\n",
            "1      \t [0.21003505 0.96880326 0.94623317]. \t  0.629144655843092 \t 1.6536488994056173\n",
            "2      \t [0.97663773 0.72624962 0.94166983]. \t  \u001b[92m2.310279719248254\u001b[0m \t 2.310279719248254\n",
            "3      \t [0.95502736 0.49357293 0.8380044 ]. \t  \u001b[92m3.562786096844574\u001b[0m \t 3.562786096844574\n",
            "4      \t [0.96253837 0.13205862 0.89598849]. \t  0.6453064181412691 \t 3.562786096844574\n",
            "5      \t [0.98929292 0.54327091 0.51972551]. \t  0.3048618569682297 \t 3.562786096844574\n",
            "6      \t [0.87865108 0.39505523 0.93637248]. \t  2.3782308383660524 \t 3.562786096844574\n",
            "7      \t [1.05693706e-15 9.78823086e-16 1.47282273e-15]. \t  0.06797411659013423 \t 3.562786096844574\n",
            "8      \t [0.19799559 0.72334668 0.0088902 ]. \t  0.0034352452446391618 \t 3.562786096844574\n",
            "9      \t [0.92699926 0.54272774 0.97578823]. \t  2.4416617726638217 \t 3.562786096844574\n",
            "10     \t [0.47237764 0.37726986 0.00090342]. \t  0.05852840412483257 \t 3.562786096844574\n",
            "11     \t [0.85458777 0.47682542 0.77165873]. \t  3.0161631530097432 \t 3.562786096844574\n",
            "12     \t [0.00174632 0.96976502 0.39285099]. \t  1.2044471858729089 \t 3.562786096844574\n",
            "13     \t [0.99512145 0.46215091 0.9880184 ]. \t  1.996631155348129 \t 3.562786096844574\n",
            "14     \t [0.88185659 0.01602548 0.07218953]. \t  0.1308933598386536 \t 3.562786096844574\n",
            "15     \t [0.86532091 0.81133557 0.01052207]. \t  0.0006042451131377279 \t 3.562786096844574\n",
            "16     \t [0.98440241 0.50063311 0.90623641]. \t  3.3318207066665915 \t 3.562786096844574\n",
            "17     \t [0.98079765 0.50332483 0.83441237]. \t  \u001b[92m3.569955722330226\u001b[0m \t 3.569955722330226\n",
            "18     \t [0.99840809 0.4980752  0.82250009]. \t  3.48987120502156 \t 3.569955722330226\n",
            "19     \t [0.02230049 0.78878948 0.69538689]. \t  2.40615137094168 \t 3.569955722330226\n",
            "20     \t [0.90625852 0.57799485 0.85002825]. \t  \u001b[92m3.6800979452468483\u001b[0m \t 3.6800979452468483\n",
            "21     \t [0.91450377 0.62177305 0.81567113]. \t  3.340157733650757 \t 3.6800979452468483\n",
            "22     \t [0.96405472 0.52921439 0.85756299]. \t  3.673906562747093 \t 3.6800979452468483\n",
            "23     \t [0.89245303 0.56524807 0.89621556]. \t  3.5615612706856017 \t 3.6800979452468483\n",
            "24     \t [0.31660777 0.00272069 0.18974426]. \t  0.7267236860946709 \t 3.6800979452468483\n",
            "25     \t [0.72521017 0.563609   0.85312906]. \t  \u001b[92m3.7759982345691006\u001b[0m \t 3.7759982345691006\n",
            "26     \t [0.33544457 0.61405639 0.89692927]. \t  3.5674131246250145 \t 3.7759982345691006\n",
            "27     \t [0.56749593 0.67941128 0.87757855]. \t  3.2704820310333704 \t 3.7759982345691006\n",
            "28     \t [0.5868697  0.53498989 0.86929282]. \t  \u001b[92m3.78951230805684\u001b[0m \t 3.78951230805684\n",
            "29     \t [0.52895217 0.51304346 0.88396256]. \t  3.6826484275363223 \t 3.78951230805684\n",
            "30     \t [0.00360322 0.87497679 0.87950053]. \t  1.4941089604039657 \t 3.78951230805684\n",
            "31     \t [0.53890692 0.55302277 0.87479165]. \t  \u001b[92m3.7933532507716565\u001b[0m \t 3.7933532507716565\n",
            "32     \t [0.47891463 0.53801544 0.87405082]. \t  \u001b[92m3.795024143523237\u001b[0m \t 3.795024143523237\n",
            "33     \t [0.54849151 0.54715655 0.88326547]. \t  3.747012124896676 \t 3.795024143523237\n",
            "34     \t [0.58080062 0.53518791 0.86722564]. \t  \u001b[92m3.796968828772906\u001b[0m \t 3.796968828772906\n",
            "35     \t [0.53828792 0.58305747 0.85965976]. \t  \u001b[92m3.800643179844891\u001b[0m \t 3.800643179844891\n",
            "36     \t [0.53847917 0.5591507  0.87260288]. \t  \u001b[92m3.8014891898989225\u001b[0m \t 3.8014891898989225\n",
            "37     \t [0.52229924 0.57607981 0.8481682 ]. \t  \u001b[92m3.8126272827759236\u001b[0m \t 3.8126272827759236\n",
            "38     \t [0.50021313 0.54381991 0.88378563]. \t  3.748816061530942 \t 3.8126272827759236\n",
            "39     \t [0.42216637 0.5503078  0.87516355]. \t  3.806884711744975 \t 3.8126272827759236\n",
            "40     \t [0.49993109 0.54339429 0.87190347]. \t  3.806097757759968 \t 3.8126272827759236\n",
            "41     \t [0.99054663 0.58509811 0.83411021]. \t  3.5694451915660994 \t 3.8126272827759236\n",
            "42     \t [0.57432791 0.55168237 0.84769777]. \t  \u001b[92m3.8201937335285647\u001b[0m \t 3.8201937335285647\n",
            "43     \t [0.52397395 0.57390575 0.8483368 ]. \t  3.8159858782379734 \t 3.8201937335285647\n",
            "44     \t [0.52867636 0.53345258 0.86054988]. \t  3.8182796460532438 \t 3.8201937335285647\n",
            "45     \t [0.51041684 0.53643266 0.85765721]. \t  \u001b[92m3.828573884401387\u001b[0m \t 3.828573884401387\n",
            "46     \t [0.56831037 0.55539317 0.85137136]. \t  3.825408971709046 \t 3.828573884401387\n",
            "47     \t [0.41146811 0.53185298 0.88240521]. \t  3.7498894399885367 \t 3.828573884401387\n",
            "48     \t [0.56144845 0.54729136 0.86815406]. \t  3.809700395641072 \t 3.828573884401387\n",
            "49     \t [0.55310908 0.57739809 0.84572599]. \t  3.798305527287201 \t 3.828573884401387\n",
            "50     \t [0.41599948 0.60431802 0.84616464]. \t  3.7575571246073767 \t 3.828573884401387\n",
            "51     \t [0.68084817 0.51685944 0.85072957]. \t  3.7539421164115847 \t 3.828573884401387\n",
            "52     \t [0.56020923 0.5387869  0.85244701]. \t  3.8221201580840165 \t 3.828573884401387\n",
            "53     \t [0.55210376 0.54789322 0.87878696]. \t  3.771005242509979 \t 3.828573884401387\n",
            "54     \t [0.56054084 0.5777664  0.84941568]. \t  3.802069836135269 \t 3.828573884401387\n",
            "55     \t [0.57740721 0.53999708 0.86094814]. \t  3.815565984683566 \t 3.828573884401387\n",
            "56     \t [0.52311346 0.55893567 0.84219825]. \t  3.8193641237971816 \t 3.828573884401387\n",
            "57     \t [0.54516265 0.55015831 0.85499527]. \t  \u001b[92m3.8322811201773055\u001b[0m \t 3.8322811201773055\n",
            "58     \t [0.5536379  0.58988437 0.84326571]. \t  3.7642661240977047 \t 3.8322811201773055\n",
            "59     \t [0.69410905 0.52646516 0.8401428 ]. \t  3.750003065248798 \t 3.8322811201773055\n",
            "60     \t [0.62634606 0.53503126 0.83865899]. \t  3.7768357495386233 \t 3.8322811201773055\n",
            "61     \t [0.55549854 0.59615788 0.86012694]. \t  3.7630341167610237 \t 3.8322811201773055\n",
            "62     \t [0.53812356 0.56299371 0.85349687]. \t  3.8299776945738087 \t 3.8322811201773055\n",
            "63     \t [0.5284888  0.57617263 0.87986619]. \t  3.758006801081118 \t 3.8322811201773055\n",
            "64     \t [0.61531646 0.52419708 0.88094555]. \t  3.714119243887829 \t 3.8322811201773055\n",
            "65     \t [0.40488274 0.55120512 0.86324453]. \t  \u001b[92m3.84613862422853\u001b[0m \t 3.84613862422853\n",
            "66     \t [0.55833545 0.57074231 0.84454476]. \t  3.804981856467598 \t 3.84613862422853\n",
            "67     \t [0.42179664 0.57267732 0.85479751]. \t  3.84117187246492 \t 3.84613862422853\n",
            "68     \t [0.57674642 0.5304621  0.82434293]. \t  3.722579258377568 \t 3.84613862422853\n",
            "69     \t [0.52550019 0.54647139 0.84759471]. \t  3.8304705919317503 \t 3.84613862422853\n",
            "70     \t [0.56750951 0.55429095 0.85007625]. \t  3.824646904389228 \t 3.84613862422853\n",
            "71     \t [0.53676698 0.54156587 0.85794884]. \t  3.828586747623888 \t 3.84613862422853\n",
            "72     \t [0.5315721  0.52283461 0.88159012]. \t  3.722823280787966 \t 3.84613862422853\n",
            "73     \t [0.43854169 0.54540606 0.87886591]. \t  3.784857142405505 \t 3.84613862422853\n",
            "74     \t [0.58467653 0.53026554 0.83906948]. \t  3.7838350564184737 \t 3.84613862422853\n",
            "75     \t [0.60760988 0.529809   0.8798032 ]. \t  3.7332047618971087 \t 3.84613862422853\n",
            "76     \t [0.4954054  0.55679605 0.83392307]. \t  3.799612359931312 \t 3.84613862422853\n",
            "77     \t [0.54936162 0.54301595 0.85588866]. \t  3.828133524311112 \t 3.84613862422853\n",
            "78     \t [0.58368772 0.5279796  0.85251337]. \t  3.8025914152077123 \t 3.84613862422853\n",
            "79     \t [0.55657582 0.55292199 0.86083292]. \t  3.8265798604481764 \t 3.84613862422853\n",
            "80     \t [0.52812166 0.54474856 0.86371997]. \t  3.8250173092069173 \t 3.84613862422853\n",
            "81     \t [0.47873816 0.63342499 0.87106567]. \t  3.6071390962136416 \t 3.84613862422853\n",
            "82     \t [0.52659499 0.56316683 0.84469706]. \t  3.821484721842521 \t 3.84613862422853\n",
            "83     \t [0.60066548 0.5411751  0.8343678 ]. \t  3.7734398839867067 \t 3.84613862422853\n",
            "84     \t [0.4894156  0.51303611 0.82506174]. \t  3.7161342403827735 \t 3.84613862422853\n",
            "85     \t [0.51078818 0.57125309 0.83505508]. \t  3.7869180180714856 \t 3.84613862422853\n",
            "86     \t [0.54233968 0.54523417 0.8401848 ]. \t  3.8105504614661263 \t 3.84613862422853\n",
            "87     \t [0.52593456 0.52618846 0.84852109]. \t  3.8093026640248566 \t 3.84613862422853\n",
            "88     \t [0.5255028  0.53018559 0.85852543]. \t  3.8162362565624237 \t 3.84613862422853\n",
            "89     \t [0.52500302 0.56162597 0.84412065]. \t  3.821795278958335 \t 3.84613862422853\n",
            "90     \t [0.51896332 0.54446154 0.84385675]. \t  3.824534287568365 \t 3.84613862422853\n",
            "91     \t [0.54613439 0.55740734 0.87069056]. \t  3.806893800464668 \t 3.84613862422853\n",
            "92     \t [0.59688809 0.54445401 0.82307504]. \t  3.718689292093058 \t 3.84613862422853\n",
            "93     \t [0.50972861 0.54137566 0.893224  ]. \t  3.678950829868816 \t 3.84613862422853\n",
            "94     \t [0.62232972 0.5212781  0.86946828]. \t  3.7558053648154246 \t 3.84613862422853\n",
            "95     \t [0.53702028 0.53766233 0.84764328]. \t  3.822056454843745 \t 3.84613862422853\n",
            "96     \t [0.62850654 0.53849017 0.83965571]. \t  3.7823302517303503 \t 3.84613862422853\n",
            "97     \t [0.49721286 0.58974897 0.84399115]. \t  3.7809707982710568 \t 3.84613862422853\n",
            "98     \t [0.57377582 0.56940119 0.84910637]. \t  3.811052072948529 \t 3.84613862422853\n",
            "99     \t [0.56234459 0.54190239 0.84750817]. \t  3.8196842166664027 \t 3.84613862422853\n",
            "100    \t [0.50896087 0.54520251 0.88384439]. \t  3.7484321012140938 \t 3.84613862422853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4xG5dbuuvig",
        "outputId": "85a63e2e-5780-4750-c373-4016656146f0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_winner_10 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_10 = dGPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.65358959 0.11500694 0.95028286]. \t  0.42730315147591735 \t 1.1029187088185965\n",
            "init   \t [0.4821914  0.87247454 0.21233268]. \t  0.06161964400032635 \t 1.1029187088185965\n",
            "init   \t [0.04070962 0.39719446 0.2331322 ]. \t  0.33269334660262956 \t 1.1029187088185965\n",
            "init   \t [0.84174072 0.20708234 0.74246953]. \t  1.1029187088185965 \t 1.1029187088185965\n",
            "init   \t [0.39215413 0.18225652 0.74353941]. \t  0.9779763535009853 \t 1.1029187088185965\n",
            "1      \t [0.65784615 0.04582828 0.06191159]. \t  0.20874301192282538 \t 1.1029187088185965\n",
            "2      \t [1.         0.68758819 0.68752697]. \t  \u001b[92m1.3837919723751193\u001b[0m \t 1.3837919723751193\n",
            "3      \t [0.9573045  0.39959208 0.60939099]. \t  0.7998894601032833 \t 1.3837919723751193\n",
            "4      \t [1. 1. 1.]. \t  0.31688362070415665 \t 1.3837919723751193\n",
            "5      \t [0.18963557 0.01593838 0.24701576]. \t  0.8100092766197674 \t 1.3837919723751193\n",
            "6      \t [0.98237517 0.96235042 0.61466973]. \t  0.34854339682296 \t 1.3837919723751193\n",
            "7      \t [0.62602067 0.59540478 0.72457474]. \t  \u001b[92m2.500435953484395\u001b[0m \t 2.500435953484395\n",
            "8      \t [0.60810924 0.63862715 0.75460631]. \t  \u001b[92m2.767181170230893\u001b[0m \t 2.767181170230893\n",
            "9      \t [0.10835486 0.93694113 0.85714394]. \t  1.0831989505772224 \t 2.767181170230893\n",
            "10     \t [0.53195364 0.76275613 0.87477019]. \t  2.542133890467548 \t 2.767181170230893\n",
            "11     \t [0.2341185  0.96338065 0.43061003]. \t  1.6658752515514381 \t 2.767181170230893\n",
            "12     \t [0.04404714 0.50272876 0.73887553]. \t  \u001b[92m2.855915612120908\u001b[0m \t 2.855915612120908\n",
            "13     \t [0.42455823 0.79914463 0.66805293]. \t  2.080336043364428 \t 2.855915612120908\n",
            "14     \t [0.73097684 0.60671869 0.93810377]. \t  \u001b[92m3.0663884760639535\u001b[0m \t 3.0663884760639535\n",
            "15     \t [1.41004216e-14 6.30726895e-15 1.56250169e-14]. \t  0.0679741165901524 \t 3.0663884760639535\n",
            "16     \t [0.68848169 0.5933104  0.88714662]. \t  \u001b[92m3.6474143939785204\u001b[0m \t 3.6474143939785204\n",
            "17     \t [0.35034888 0.96159609 0.01056276]. \t  0.00043372534468517654 \t 3.6474143939785204\n",
            "18     \t [0.43007517 0.50255795 0.9589602 ]. \t  2.7236138961947183 \t 3.6474143939785204\n",
            "19     \t [0.69645582 0.50596684 0.90338553]. \t  3.4712584413623317 \t 3.6474143939785204\n",
            "20     \t [0.64591987 0.52502489 0.8707785 ]. \t  \u001b[92m3.7540345131455313\u001b[0m \t 3.7540345131455313\n",
            "21     \t [0.91211735 0.86389024 0.03440292]. \t  0.0004350049964589211 \t 3.7540345131455313\n",
            "22     \t [0.         0.75972467 0.55309409]. \t  2.707864241821032 \t 3.7540345131455313\n",
            "23     \t [0.60240137 0.46652653 0.85485618]. \t  3.5683661525652433 \t 3.7540345131455313\n",
            "24     \t [0.61924998 0.46237306 0.85377099]. \t  3.5419207348266664 \t 3.7540345131455313\n",
            "25     \t [0.60882614 0.49249794 0.88250821]. \t  3.602496304726599 \t 3.7540345131455313\n",
            "26     \t [0.63708322 0.52047906 0.89362418]. \t  3.6150142786981916 \t 3.7540345131455313\n",
            "27     \t [0.59020959 0.50031328 0.88980944]. \t  3.588986119962619 \t 3.7540345131455313\n",
            "28     \t [0.58178465 0.49981338 0.85970866]. \t  3.722886361773047 \t 3.7540345131455313\n",
            "29     \t [0.55821479 0.51274611 0.87779762]. \t  3.711884801519515 \t 3.7540345131455313\n",
            "30     \t [0.56548377 0.50958378 0.85212999]. \t  \u001b[92m3.7638024174618363\u001b[0m \t 3.7638024174618363\n",
            "31     \t [0.95245323 0.01762913 0.04078752]. \t  0.06997727821502839 \t 3.7638024174618363\n",
            "32     \t [0.43829723 0.57211151 0.82461331]. \t  3.75310375181346 \t 3.7638024174618363\n",
            "33     \t [0.18730635 0.57959702 0.72918805]. \t  2.8599829174838463 \t 3.7638024174618363\n",
            "34     \t [0.43976425 0.58822442 0.83976152]. \t  \u001b[92m3.7860729423645916\u001b[0m \t 3.7860729423645916\n",
            "35     \t [0.46569054 0.59190296 0.87511932]. \t  3.758156195420672 \t 3.7860729423645916\n",
            "36     \t [0.52443718 0.54372326 0.79887095]. \t  3.5512242427520704 \t 3.7860729423645916\n",
            "37     \t [0.4469376  0.51504459 0.83938271]. \t  3.7826980010822258 \t 3.7860729423645916\n",
            "38     \t [0.52212218 0.55121483 0.84680292]. \t  \u001b[92m3.8310199425327123\u001b[0m \t 3.8310199425327123\n",
            "39     \t [0.43388588 0.58677229 0.83876688]. \t  3.787712476558343 \t 3.8310199425327123\n",
            "40     \t [0.99373795 0.4260166  0.89188817]. \t  3.026946690389706 \t 3.8310199425327123\n",
            "41     \t [0.47869382 0.57061514 0.84114542]. \t  3.8155708048627184 \t 3.8310199425327123\n",
            "42     \t [0.45090608 0.59637031 0.86138721]. \t  3.78264384961189 \t 3.8310199425327123\n",
            "43     \t [0.47144723 0.55829614 0.88968995]. \t  3.719821970205832 \t 3.8310199425327123\n",
            "44     \t [0.45183842 0.57779289 0.85579978]. \t  3.828952338380292 \t 3.8310199425327123\n",
            "45     \t [0.44825888 0.51071242 0.8521125 ]. \t  3.7863265922663736 \t 3.8310199425327123\n",
            "46     \t [0.48501351 0.56806103 0.84597923]. \t  3.828334473944397 \t 3.8310199425327123\n",
            "47     \t [0.48265804 0.57002665 0.83924022]. \t  3.8099411057315304 \t 3.8310199425327123\n",
            "48     \t [0.45356915 0.57416272 0.84509534]. \t  3.824730699216726 \t 3.8310199425327123\n",
            "49     \t [0.50564435 0.54712501 0.85975046]. \t  \u001b[92m3.836383888785377\u001b[0m \t 3.836383888785377\n",
            "50     \t [0.59120599 0.52070206 0.88101538]. \t  3.710504214770599 \t 3.836383888785377\n",
            "51     \t [0.58319761 0.51084275 0.88319015]. \t  3.671709574106022 \t 3.836383888785377\n",
            "52     \t [0.44390227 0.60263228 0.85509375]. \t  3.7671320342200567 \t 3.836383888785377\n",
            "53     \t [0.49820495 0.57877172 0.89839648]. \t  3.6331334731425367 \t 3.836383888785377\n",
            "54     \t [0.45666076 0.58533958 0.84029268]. \t  3.7911053421289935 \t 3.836383888785377\n",
            "55     \t [0.45579206 0.59854368 0.82802441]. \t  3.705753405228344 \t 3.836383888785377\n",
            "56     \t [0.4492787  0.59781316 0.83417871]. \t  3.737620641363449 \t 3.836383888785377\n",
            "57     \t [0.48022486 0.55580929 0.85589597]. \t  \u001b[92m3.8447311249948894\u001b[0m \t 3.8447311249948894\n",
            "58     \t [0.43136575 0.61461762 0.84081878]. \t  3.7029123087274245 \t 3.8447311249948894\n",
            "59     \t [0.43383106 0.54212599 0.79845747]. \t  3.573964094297194 \t 3.8447311249948894\n",
            "60     \t [0.50544093 0.50743737 0.88441373]. \t  3.6649179013256257 \t 3.8447311249948894\n",
            "61     \t [0.49162103 0.55045302 0.85686807]. \t  3.8422769786972695 \t 3.8447311249948894\n",
            "62     \t [0.6223264  0.52759716 0.89118122]. \t  3.6533388169926337 \t 3.8447311249948894\n",
            "63     \t [0.45520739 0.58622738 0.851286  ]. \t  3.809944252257178 \t 3.8447311249948894\n",
            "64     \t [0.47208933 0.51128225 0.86308569]. \t  3.773127674889911 \t 3.8447311249948894\n",
            "65     \t [0.49865571 0.53082409 0.87119194]. \t  3.7926361219772886 \t 3.8447311249948894\n",
            "66     \t [0.42149762 0.53863703 0.82640554]. \t  3.7767720151880186 \t 3.8447311249948894\n",
            "67     \t [0.4568869  0.56887131 0.84690784]. \t  3.8343343354724793 \t 3.8447311249948894\n",
            "68     \t [0.43166079 0.56265655 0.84506897]. \t  3.8411262024768327 \t 3.8447311249948894\n",
            "69     \t [0.49842763 0.51341322 0.85938579]. \t  3.782297134224307 \t 3.8447311249948894\n",
            "70     \t [0.43924584 0.55751581 0.87461884]. \t  3.8087167344619495 \t 3.8447311249948894\n",
            "71     \t [0.50770724 0.58703098 0.82228661]. \t  3.6914525766994304 \t 3.8447311249948894\n",
            "72     \t [0.57324923 0.56007518 0.89812091]. \t  3.638016145531582 \t 3.8447311249948894\n",
            "73     \t [0.46480287 0.5517991  0.83645009]. \t  3.8166749219531493 \t 3.8447311249948894\n",
            "74     \t [0.62270356 0.52609978 0.8929804 ]. \t  3.6363094330479124 \t 3.8447311249948894\n",
            "75     \t [0.58191999 0.53782069 0.85330095]. \t  3.8164549600070354 \t 3.8447311249948894\n",
            "76     \t [0.41924181 0.60778006 0.8543392 ]. \t  3.7528623949570754 \t 3.8447311249948894\n",
            "77     \t [0.62015638 0.51963581 0.889025  ]. \t  3.6511603297721664 \t 3.8447311249948894\n",
            "78     \t [0.46803152 0.54745316 0.87138265]. \t  3.8153534967704794 \t 3.8447311249948894\n",
            "79     \t [0.57305878 0.5390344  0.8765224 ]. \t  3.7700620462523378 \t 3.8447311249948894\n",
            "80     \t [0.41149835 0.60117987 0.82888383]. \t  3.711990583464119 \t 3.8447311249948894\n",
            "81     \t [0.44224162 0.62759267 0.82897054]. \t  3.596356160691016 \t 3.8447311249948894\n",
            "82     \t [0.52860854 0.6061396  0.84144815]. \t  3.7140491026018556 \t 3.8447311249948894\n",
            "83     \t [0.47865153 0.56360753 0.83691166]. \t  3.81008670086583 \t 3.8447311249948894\n",
            "84     \t [0.44385297 0.58866175 0.83686323]. \t  3.7747575352844476 \t 3.8447311249948894\n",
            "85     \t [0.4173896  0.54305589 0.82004539]. \t  3.7449039414355543 \t 3.8447311249948894\n",
            "86     \t [0.50376605 0.48608346 0.87035797]. \t  3.6499013512699765 \t 3.8447311249948894\n",
            "87     \t [0.50080348 0.50130726 0.88830714]. \t  3.616842323640304 \t 3.8447311249948894\n",
            "88     \t [0.42715711 0.5984038  0.82583858]. \t  3.7019504094468987 \t 3.8447311249948894\n",
            "89     \t [0.50221749 0.56746246 0.86243163]. \t  3.8286902325320393 \t 3.8447311249948894\n",
            "90     \t [0.47619753 0.58307804 0.86628119]. \t  3.8032505972155226 \t 3.8447311249948894\n",
            "91     \t [0.51760554 0.5445624  0.86604339]. \t  3.8217967460383737 \t 3.8447311249948894\n",
            "92     \t [0.40963499 0.5695629  0.79714027]. \t  3.5561740395660726 \t 3.8447311249948894\n",
            "93     \t [0.49848622 0.52951493 0.84170752]. \t  3.8089643728610403 \t 3.8447311249948894\n",
            "94     \t [0.46726152 0.56038617 0.88285779]. \t  3.764168558501989 \t 3.8447311249948894\n",
            "95     \t [0.43345625 0.54035281 0.83437299]. \t  3.8107564817562896 \t 3.8447311249948894\n",
            "96     \t [0.61918901 0.52354015 0.9155573 ]. \t  3.4069801847270216 \t 3.8447311249948894\n",
            "97     \t [0.46214471 0.50056412 0.8859685 ]. \t  3.6342404923830998 \t 3.8447311249948894\n",
            "98     \t [0.43753506 0.612036   0.87215774]. \t  3.708150837666326 \t 3.8447311249948894\n",
            "99     \t [0.47367392 0.54356706 0.8320324 ]. \t  3.7962837896315875 \t 3.8447311249948894\n",
            "100    \t [0.47529022 0.5107491  0.83726942]. \t  3.7614659304043077 \t 3.8447311249948894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSj_CQIAuvk3",
        "outputId": "a9a429de-b43e-4f5c-a096-7ce91309fb86"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_winner_11 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_11 = dGPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.98103566 0.82124785 0.64589361]. \t  0.6570240879762975 \t 0.687459437576373\n",
            "init   \t [0.42368801 0.20231098 0.49190677]. \t  0.29901296656621285 \t 0.687459437576373\n",
            "init   \t [0.13855833 0.45252104 0.11373463]. \t  0.1378633538075966 \t 0.687459437576373\n",
            "init   \t [0.00292449 0.52342617 0.18997116]. \t  0.12299763462541902 \t 0.687459437576373\n",
            "init   \t [0.14171979 0.33586637 0.58369219]. \t  0.687459437576373 \t 0.687459437576373\n",
            "1      \t [0.387927   0.9056551  0.98354533]. \t  \u001b[92m0.7767843770377878\u001b[0m \t 0.7767843770377878\n",
            "2      \t [0.10922994 0.17736885 0.99587964]. \t  0.4885993482312173 \t 0.7767843770377878\n",
            "3      \t [0.66146056 0.99353117 0.56328544]. \t  \u001b[92m1.0692505807903523\u001b[0m \t 1.0692505807903523\n",
            "4      \t [0.91846318 0.91585767 0.18029432]. \t  0.006703439567235947 \t 1.0692505807903523\n",
            "5      \t [0.91420764 0.99481403 0.91425773]. \t  0.5403243270021761 \t 1.0692505807903523\n",
            "6      \t [0.46888158 0.7241203  0.68792912]. \t  \u001b[92m2.103145964580509\u001b[0m \t 2.103145964580509\n",
            "7      \t [0.61916634 0.568086   0.83220008]. \t  \u001b[92m3.7473628826806085\u001b[0m \t 3.7473628826806085\n",
            "8      \t [0.96983274 0.29376425 0.9862999 ]. \t  1.0949811496745363 \t 3.7473628826806085\n",
            "9      \t [0.70576372 0.57438375 0.94510273]. \t  3.019899905044683 \t 3.7473628826806085\n",
            "10     \t [0.56924027 0.39153309 0.83303375]. \t  3.0137777261764436 \t 3.7473628826806085\n",
            "11     \t [0.91268663 0.27816666 0.62972237]. \t  0.7162383495685753 \t 3.7473628826806085\n",
            "12     \t [0.         0.16257131 0.        ]. \t  0.07634567219104671 \t 3.7473628826806085\n",
            "13     \t [0.68551176 0.50739986 0.72123636]. \t  2.455891205685287 \t 3.7473628826806085\n",
            "14     \t [0.9497692  0.07484901 0.73953193]. \t  0.4367754777684912 \t 3.7473628826806085\n",
            "15     \t [0.41180688 0.50140167 0.90805992]. \t  3.4458210700348997 \t 3.7473628826806085\n",
            "16     \t [0.50175601 0.61412519 0.90249635]. \t  3.5066780690366177 \t 3.7473628826806085\n",
            "17     \t [0.0085435  0.98359006 0.0875289 ]. \t  0.0038347077884172717 \t 3.7473628826806085\n",
            "18     \t [0.98112754 0.95185224 0.01484128]. \t  9.106430043826732e-05 \t 3.7473628826806085\n",
            "19     \t [0.99263914 0.15298368 0.10210187]. \t  0.13549979390863812 \t 3.7473628826806085\n",
            "20     \t [0.54370836 0.59517212 0.85285815]. \t  \u001b[92m3.7683728710828417\u001b[0m \t 3.7683728710828417\n",
            "21     \t [0.50297032 0.60182275 0.85007745]. \t  3.7542905519922836 \t 3.7683728710828417\n",
            "22     \t [0.53122107 0.65573306 0.88427624]. \t  3.417073591572586 \t 3.7683728710828417\n",
            "23     \t [0.49698662 0.50329242 0.84586446]. \t  3.7525631043151226 \t 3.7683728710828417\n",
            "24     \t [0.57510872 0.5107381  0.81837467]. \t  3.6521825894768427 \t 3.7683728710828417\n",
            "25     \t [0.98193173 0.01440496 0.29423294]. \t  0.28540838056210327 \t 3.7683728710828417\n",
            "26     \t [0.49967723 0.53577576 0.87561774]. \t  \u001b[92m3.7824381012145096\u001b[0m \t 3.7824381012145096\n",
            "27     \t [0.46530289 0.56618438 0.82141748]. \t  3.7356284054236553 \t 3.7824381012145096\n",
            "28     \t [0.48262862 0.5823131  0.89142261]. \t  3.6850633646565942 \t 3.7824381012145096\n",
            "29     \t [0.50859546 0.64723732 0.84347156]. \t  3.52182630813789 \t 3.7824381012145096\n",
            "30     \t [0.50277591 0.46357503 0.83444561]. \t  3.550599924088779 \t 3.7824381012145096\n",
            "31     \t [0.42696709 0.45960949 0.80147483]. \t  3.366559271566909 \t 3.7824381012145096\n",
            "32     \t [0.44004829 0.57021048 0.83858703]. \t  \u001b[92m3.8162505296339466\u001b[0m \t 3.8162505296339466\n",
            "33     \t [0.55608902 0.592519   0.85144408]. \t  3.771546393555294 \t 3.8162505296339466\n",
            "34     \t [0.57749984 0.52398783 0.89858598]. \t  3.5928785613873018 \t 3.8162505296339466\n",
            "35     \t [0.51930532 0.54220617 0.84393173]. \t  \u001b[92m3.823340472573653\u001b[0m \t 3.823340472573653\n",
            "36     \t [0.50940053 0.57562184 0.85740423]. \t  3.821596768146967 \t 3.823340472573653\n",
            "37     \t [0.41503592 0.54752483 0.84909805]. \t  \u001b[92m3.8511165079641128\u001b[0m \t 3.8511165079641128\n",
            "38     \t [0.37715115 0.01358625 0.03693524]. \t  0.18283277673824266 \t 3.8511165079641128\n",
            "39     \t [0.39426488 0.56012499 0.86381096]. \t  3.8456017883525604 \t 3.8511165079641128\n",
            "40     \t [0.00538918 0.71211452 0.91174786]. \t  2.7979540581903963 \t 3.8511165079641128\n",
            "41     \t [0.50624964 0.5299612  0.81830715]. \t  3.7039200729171697 \t 3.8511165079641128\n",
            "42     \t [0.52366481 0.55263577 0.83960945]. \t  3.814189387295296 \t 3.8511165079641128\n",
            "43     \t [0.49335945 0.57386869 0.84591551]. \t  3.818823006153655 \t 3.8511165079641128\n",
            "44     \t [0.53501837 0.56195286 0.82298284]. \t  3.7305304641252475 \t 3.8511165079641128\n",
            "45     \t [0.51908844 0.55629807 0.82279569]. \t  3.7379308193542053 \t 3.8511165079641128\n",
            "46     \t [0.60059134 0.57304807 0.82515093]. \t  3.708296014518343 \t 3.8511165079641128\n",
            "47     \t [0.38763583 0.54929606 0.86997649]. \t  3.8290115335012085 \t 3.8511165079641128\n",
            "48     \t [0.39907664 0.54513725 0.83494006]. \t  3.820679031601678 \t 3.8511165079641128\n",
            "49     \t [0.43218909 0.54259079 0.87147339]. \t  3.815431590493205 \t 3.8511165079641128\n",
            "50     \t [0.44279235 0.5287268  0.86927644]. \t  3.8023879886397784 \t 3.8511165079641128\n",
            "51     \t [0.43044827 0.51950746 0.86643541]. \t  3.7919909918231274 \t 3.8511165079641128\n",
            "52     \t [0.41222986 0.53539135 0.81534244]. \t  3.710155752346536 \t 3.8511165079641128\n",
            "53     \t [0.4836289  0.56703853 0.840715  ]. \t  3.817621983522513 \t 3.8511165079641128\n",
            "54     \t [0.42684998 0.59491894 0.87427067]. \t  3.7585874357238342 \t 3.8511165079641128\n",
            "55     \t [0.47938311 0.54189288 0.85833442]. \t  3.8387337357457185 \t 3.8511165079641128\n",
            "56     \t [0.40051521 0.50627995 0.85588257]. \t  3.7752226908311983 \t 3.8511165079641128\n",
            "57     \t [0.50448061 0.51374624 0.84011462]. \t  3.7718716047372265 \t 3.8511165079641128\n",
            "58     \t [0.46575503 0.55130759 0.89115449]. \t  3.708219833513949 \t 3.8511165079641128\n",
            "59     \t [0.46528333 0.55143171 0.88015414]. \t  3.778802061462404 \t 3.8511165079641128\n",
            "60     \t [0.51985906 0.53024038 0.79905187]. \t  3.547091927889166 \t 3.8511165079641128\n",
            "61     \t [0.52588261 0.50794017 0.85579422]. \t  3.7652650314793603 \t 3.8511165079641128\n",
            "62     \t [0.45662526 0.54503265 0.86130641]. \t  3.840791352436408 \t 3.8511165079641128\n",
            "63     \t [0.51396601 0.54073087 0.82079664]. \t  3.7270511667897734 \t 3.8511165079641128\n",
            "64     \t [0.4669806  0.54176722 0.85599378]. \t  3.8420961548784645 \t 3.8511165079641128\n",
            "65     \t [0.49712643 0.5229197  0.85025835]. \t  3.8095394841129258 \t 3.8511165079641128\n",
            "66     \t [0.43812388 0.50799807 0.86986784]. \t  3.7467665336018774 \t 3.8511165079641128\n",
            "67     \t [0.47492823 0.540338   0.82226983]. \t  3.745115621282457 \t 3.8511165079641128\n",
            "68     \t [0.53465737 0.53699197 0.82432412]. \t  3.7401712367852227 \t 3.8511165079641128\n",
            "69     \t [0.49561494 0.56234961 0.84189883]. \t  3.8225408692758163 \t 3.8511165079641128\n",
            "70     \t [0.52424294 0.55886207 0.83820897]. \t  3.807466311836629 \t 3.8511165079641128\n",
            "71     \t [0.44849307 0.52811279 0.86912626]. \t  3.8011132301318584 \t 3.8511165079641128\n",
            "72     \t [0.34894696 0.48082405 0.82663853]. \t  3.6272598694521165 \t 3.8511165079641128\n",
            "73     \t [0.57257093 0.50714167 0.89185178]. \t  3.6014124860121566 \t 3.8511165079641128\n",
            "74     \t [0.44667444 0.55076332 0.85985505]. \t  3.846674677846021 \t 3.8511165079641128\n",
            "75     \t [0.52466575 0.52578409 0.8728663 ]. \t  3.772724676115651 \t 3.8511165079641128\n",
            "76     \t [0.46945665 0.57138488 0.85597001]. \t  3.83544836928594 \t 3.8511165079641128\n",
            "77     \t [0.46880187 0.54875664 0.87606328]. \t  3.797196566315152 \t 3.8511165079641128\n",
            "78     \t [0.55695509 0.57683405 0.84396703]. \t  3.7943475000749705 \t 3.8511165079641128\n",
            "79     \t [0.52585049 0.52994796 0.8585277 ]. \t  3.8157794992214225 \t 3.8511165079641128\n",
            "80     \t [0.41701531 0.51532823 0.84339364]. \t  3.7954156051783676 \t 3.8511165079641128\n",
            "81     \t [0.4001369  0.50705294 0.86249134]. \t  3.7677603455303044 \t 3.8511165079641128\n",
            "82     \t [0.44361542 0.54553758 0.84735637]. \t  3.8445347273683805 \t 3.8511165079641128\n",
            "83     \t [0.48119376 0.56135438 0.85115515]. \t  3.8413685457085207 \t 3.8511165079641128\n",
            "84     \t [0.33492399 0.58863229 0.83549964]. \t  3.787151544081179 \t 3.8511165079641128\n",
            "85     \t [0.44013863 0.52513795 0.86226619]. \t  3.813824118025571 \t 3.8511165079641128\n",
            "86     \t [0.43742194 0.53851097 0.85661386]. \t  3.842598992394255 \t 3.8511165079641128\n",
            "87     \t [0.4424313  0.60022146 0.84841972]. \t  3.770198990656392 \t 3.8511165079641128\n",
            "88     \t [0.45496473 0.62961805 0.82639358]. \t  3.56984888150037 \t 3.8511165079641128\n",
            "89     \t [0.51454242 0.50890118 0.89248568]. \t  3.610829953611593 \t 3.8511165079641128\n",
            "90     \t [0.44974982 0.53134594 0.84067112]. \t  3.817571051686908 \t 3.8511165079641128\n",
            "91     \t [0.20444802 0.56990552 0.86078753]. \t  3.843399206862092 \t 3.8511165079641128\n",
            "92     \t [0.00532643 0.52964613 0.91283593]. \t  3.4286704686013247 \t 3.8511165079641128\n",
            "93     \t [0.48801245 0.55398644 0.87039934]. \t  3.818062993968076 \t 3.8511165079641128\n",
            "94     \t [0.55956501 0.59003213 0.80375244]. \t  3.522710488804784 \t 3.8511165079641128\n",
            "95     \t [0.29621203 0.5904916  0.85652731]. \t  3.8165726603289176 \t 3.8511165079641128\n",
            "96     \t [0.40452035 0.57742158 0.88102454]. \t  3.7663081433125862 \t 3.8511165079641128\n",
            "97     \t [0.38228653 0.5237909  0.88250682]. \t  3.734081401210668 \t 3.8511165079641128\n",
            "98     \t [0.35901789 0.53820626 0.84549034]. \t  3.8453496252606296 \t 3.8511165079641128\n",
            "99     \t [0.52988098 0.51763833 0.85937476]. \t  3.788433648496153 \t 3.8511165079641128\n",
            "100    \t [0.27540994 0.51735984 0.86356069]. \t  3.7976011138527794 \t 3.8511165079641128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l98Nt7Tguvna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27675eb-45fc-43be-a679-f31a2180e70a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_winner_12 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_12 = dGPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.19151945 0.62210877 0.43772774]. \t  1.1006281843679786 \t 1.6482992955272024\n",
            "init   \t [0.78535858 0.77997581 0.27259261]. \t  0.07153771680480671 \t 1.6482992955272024\n",
            "init   \t [0.27646426 0.80187218 0.95813935]. \t  1.6482992955272024 \t 1.6482992955272024\n",
            "init   \t [0.87593263 0.35781727 0.50099513]. \t  0.2282556248207173 \t 1.6482992955272024\n",
            "init   \t [0.68346294 0.71270203 0.37025075]. \t  0.33032494760932407 \t 1.6482992955272024\n",
            "1      \t [0.01398415 0.60897714 0.98856638]. \t  \u001b[92m2.228026380269567\u001b[0m \t 2.228026380269567\n",
            "2      \t [0.03494356 0.11089546 0.89536938]. \t  0.5535687532914756 \t 2.228026380269567\n",
            "3      \t [0.00761147 0.89633798 0.99755295]. \t  0.7296905183544732 \t 2.228026380269567\n",
            "4      \t [0.87278659 0.50804111 0.98303484]. \t  \u001b[92m2.2637740820310066\u001b[0m \t 2.2637740820310066\n",
            "5      \t [0.62997572 0.44202894 0.99618939]. \t  1.8391155649095643 \t 2.2637740820310066\n",
            "6      \t [0.97781767 0.75579939 0.98488856]. \t  1.6024844551418271 \t 2.2637740820310066\n",
            "7      \t [0.96778278 0.42886318 0.94780496]. \t  \u001b[92m2.443857842268981\u001b[0m \t 2.443857842268981\n",
            "8      \t [0.91655431 0.4196556  0.96277451]. \t  2.187205040291904 \t 2.443857842268981\n",
            "9      \t [0.1854075  0.54433027 0.97608866]. \t  \u001b[92m2.5041901554687356\u001b[0m \t 2.5041901554687356\n",
            "10     \t [0.99934251 0.53058197 0.91832449]. \t  \u001b[92m3.2765886697802613\u001b[0m \t 3.2765886697802613\n",
            "11     \t [0.96510751 0.67128096 0.51747961]. \t  0.34913310922522633 \t 3.2765886697802613\n",
            "12     \t [0.41076589 0.97497551 0.0714349 ]. \t  0.0020652750441005094 \t 3.2765886697802613\n",
            "13     \t [5.33352291e-01 5.18373367e-01 6.61135873e-15]. \t  0.021636629394565565 \t 3.2765886697802613\n",
            "14     \t [0.37849967 0.44714907 0.79310138]. \t  3.2379115281755224 \t 3.2765886697802613\n",
            "15     \t [0.84100824 0.91885913 0.02827365]. \t  0.00029325254021220286 \t 3.2765886697802613\n",
            "16     \t [0.00531112 0.62922539 0.77789272]. \t  3.2321599928834326 \t 3.2765886697802613\n",
            "17     \t [0.4093186  0.01062064 0.30465147]. \t  0.852622114750648 \t 3.2765886697802613\n",
            "18     \t [0.23937865 0.53276523 0.7623061 ]. \t  3.2177057946422476 \t 3.2765886697802613\n",
            "19     \t [0.21462763 0.50538318 0.83593812]. \t  \u001b[92m3.7525007482209\u001b[0m \t 3.7525007482209\n",
            "20     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.7525007482209\n",
            "21     \t [0.05396871 0.30838085 0.63882911]. \t  0.9623051656020811 \t 3.7525007482209\n",
            "22     \t [0.71341914 0.0219494  0.14989326]. \t  0.42318642348550495 \t 3.7525007482209\n",
            "23     \t [0.16527867 0.44126236 0.88290897]. \t  3.3094514274812092 \t 3.7525007482209\n",
            "24     \t [0.08076927 0.67645062 0.80490026]. \t  3.21831324150564 \t 3.7525007482209\n",
            "25     \t [0.17661658 0.51045693 0.81284231]. \t  3.656242050080457 \t 3.7525007482209\n",
            "26     \t [0.26952006 0.51349352 0.82124816]. \t  3.719751994164808 \t 3.7525007482209\n",
            "27     \t [0.03715396 0.59737614 0.80623555]. \t  3.5787328174639765 \t 3.7525007482209\n",
            "28     \t [0.29096313 0.57093526 0.77616299]. \t  3.3723659334098253 \t 3.7525007482209\n",
            "29     \t [0.13236804 0.60735909 0.88660395]. \t  3.6496344162352563 \t 3.7525007482209\n",
            "30     \t [0.04937873 0.55580749 0.8573546 ]. \t  \u001b[92m3.824119293155888\u001b[0m \t 3.824119293155888\n",
            "31     \t [0.08467141 0.5189958  0.85224417]. \t  3.7898169767010548 \t 3.824119293155888\n",
            "32     \t [0.22443182 0.60714943 0.00763059]. \t  0.011423049421869325 \t 3.824119293155888\n",
            "33     \t [0.07020664 0.5121215  0.83373381]. \t  3.7410712981712697 \t 3.824119293155888\n",
            "34     \t [0.10567857 0.58458178 0.82616629]. \t  3.7472184575823064 \t 3.824119293155888\n",
            "35     \t [0.24335431 0.50241091 0.86071048]. \t  3.754111511435558 \t 3.824119293155888\n",
            "36     \t [0.10013206 0.56094464 0.88280983]. \t  3.7485081558768245 \t 3.824119293155888\n",
            "37     \t [0.109921   0.48816829 0.80619509]. \t  3.5264918349105505 \t 3.824119293155888\n",
            "38     \t [0.04269709 0.56068197 0.86176729]. \t  3.8154438153584627 \t 3.824119293155888\n",
            "39     \t [0.12726301 0.52402271 0.88224719]. \t  3.716830777144037 \t 3.824119293155888\n",
            "40     \t [0.20264556 0.51447597 0.78979413]. \t  3.48265558491033 \t 3.824119293155888\n",
            "41     \t [0.06466806 0.60271798 0.83095383]. \t  3.7106840567949497 \t 3.824119293155888\n",
            "42     \t [0.18393507 0.59972227 0.79777121]. \t  3.5354440635498343 \t 3.824119293155888\n",
            "43     \t [0.06163776 0.58421537 0.85504871]. \t  3.801231712345441 \t 3.824119293155888\n",
            "44     \t [0.18489281 0.58160652 0.84202762]. \t  3.8192545281078107 \t 3.824119293155888\n",
            "45     \t [0.13548259 0.54085016 0.80907721]. \t  3.677090062969726 \t 3.824119293155888\n",
            "46     \t [0.04258092 0.59840652 0.82147368]. \t  3.6737410521063922 \t 3.824119293155888\n",
            "47     \t [0.10296887 0.63694323 0.85927259]. \t  3.6148720505859515 \t 3.824119293155888\n",
            "48     \t [0.12395748 0.61255715 0.84246119]. \t  3.7231342155920126 \t 3.824119293155888\n",
            "49     \t [0.12414939 0.5467394  0.86914611]. \t  3.813475410884502 \t 3.824119293155888\n",
            "50     \t [0.14377665 0.5859914  0.85798784]. \t  3.8139341239618814 \t 3.824119293155888\n",
            "51     \t [0.20083099 0.57580999 0.8005578 ]. \t  3.607132204969482 \t 3.824119293155888\n",
            "52     \t [0.14529722 0.52635983 0.82819711]. \t  3.769965126980696 \t 3.824119293155888\n",
            "53     \t [0.11409552 0.55246507 0.82642159]. \t  3.781131911556768 \t 3.824119293155888\n",
            "54     \t [0.25520531 0.51069993 0.83296985]. \t  3.762624892614338 \t 3.824119293155888\n",
            "55     \t [0.2518437  0.55910633 0.80683965]. \t  3.67216423363251 \t 3.824119293155888\n",
            "56     \t [0.09275399 0.63176312 0.80071486]. \t  3.434380912122272 \t 3.824119293155888\n",
            "57     \t [0.12475098 0.57095723 0.84072382]. \t  3.823053607330215 \t 3.824119293155888\n",
            "58     \t [0.1261705  0.61612448 0.84088521]. \t  3.706321638650646 \t 3.824119293155888\n",
            "59     \t [0.13881636 0.55583137 0.87096196]. \t  3.8136248668213173 \t 3.824119293155888\n",
            "60     \t [0.24697096 0.57686851 0.89111672]. \t  3.702689389785432 \t 3.824119293155888\n",
            "61     \t [0.18280746 0.55017059 0.86073578]. \t  \u001b[92m3.8463790119399603\u001b[0m \t 3.8463790119399603\n",
            "62     \t [0.11854629 0.55906574 0.8535707 ]. \t  3.843011996032586 \t 3.8463790119399603\n",
            "63     \t [0.06147875 0.60445566 0.88509285]. \t  3.652975430521429 \t 3.8463790119399603\n",
            "64     \t [0.15305314 0.52373561 0.83830875]. \t  3.7995549397429995 \t 3.8463790119399603\n",
            "65     \t [0.35676433 0.54163919 0.83743008]. \t  3.831029958452278 \t 3.8463790119399603\n",
            "66     \t [0.16505982 0.62233863 0.858789  ]. \t  3.6970632629204276 \t 3.8463790119399603\n",
            "67     \t [0.34320554 0.50697969 0.81202007]. \t  3.6435654306820853 \t 3.8463790119399603\n",
            "68     \t [0.40177016 0.51711822 0.87599622]. \t  3.751418177733317 \t 3.8463790119399603\n",
            "69     \t [0.19724798 0.53742258 0.86771657]. \t  3.820208671210885 \t 3.8463790119399603\n",
            "70     \t [0.37889939 0.54277234 0.86303057]. \t  3.8436706056171266 \t 3.8463790119399603\n",
            "71     \t [0.25190548 0.57792465 0.83436091]. \t  3.8086107018692603 \t 3.8463790119399603\n",
            "72     \t [0.43442614 0.51681426 0.8675189 ]. \t  3.7814295300546736 \t 3.8463790119399603\n",
            "73     \t [0.20729507 0.60339401 0.86396045]. \t  3.767197055025085 \t 3.8463790119399603\n",
            "74     \t [0.56919132 0.51291958 0.86468093]. \t  3.758764770526699 \t 3.8463790119399603\n",
            "75     \t [0.50244557 0.52040663 0.8550101 ]. \t  3.803478547281337 \t 3.8463790119399603\n",
            "76     \t [0.28436252 0.5752692  0.8616867 ]. \t  3.8403982392905887 \t 3.8463790119399603\n",
            "77     \t [0.06261951 0.60256061 0.8358513 ]. \t  3.727465427650894 \t 3.8463790119399603\n",
            "78     \t [0.7300574  0.56324935 0.85188007]. \t  3.7735068980953956 \t 3.8463790119399603\n",
            "79     \t [0.6788844  0.53854429 0.8581561 ]. \t  3.7906619436313003 \t 3.8463790119399603\n",
            "80     \t [0.82162099 0.52213415 0.84386681]. \t  3.7079996683239735 \t 3.8463790119399603\n",
            "81     \t [0.51760807 0.57372328 0.84214578]. \t  3.804976554356325 \t 3.8463790119399603\n",
            "82     \t [0.41360203 0.55990436 0.84555976]. \t  3.846227101305223 \t 3.8463790119399603\n",
            "83     \t [0.58025223 0.5446182  0.8656443 ]. \t  3.81036386122735 \t 3.8463790119399603\n",
            "84     \t [0.21641639 0.60066442 0.84546915]. \t  3.780763090388098 \t 3.8463790119399603\n",
            "85     \t [0.41324229 0.54149394 0.82495295]. \t  3.772683548618565 \t 3.8463790119399603\n",
            "86     \t [0.44514996 0.54227771 0.8972296 ]. \t  3.6534589987656143 \t 3.8463790119399603\n",
            "87     \t [0.45997742 0.52950439 0.86133431]. \t  3.821534660802568 \t 3.8463790119399603\n",
            "88     \t [0.4173802  0.54242067 0.87145398]. \t  3.8166966823180974 \t 3.8463790119399603\n",
            "89     \t [0.77452459 0.53532708 0.85575706]. \t  3.757472645538325 \t 3.8463790119399603\n",
            "90     \t [0.42483021 0.55662337 0.8782009 ]. \t  3.7939442299529405 \t 3.8463790119399603\n",
            "91     \t [0.18045341 0.57032866 0.85993272]. \t  3.841392193416735 \t 3.8463790119399603\n",
            "92     \t [0.55457681 0.55510604 0.85265954]. \t  3.829567106394622 \t 3.8463790119399603\n",
            "93     \t [0.59122447 0.527701   0.88050319]. \t  3.7288567081652584 \t 3.8463790119399603\n",
            "94     \t [0.20620003 0.57935171 0.85102826]. \t  3.8364698971969484 \t 3.8463790119399603\n",
            "95     \t [0.27482804 0.58308131 0.8558579 ]. \t  3.8331990371228226 \t 3.8463790119399603\n",
            "96     \t [0.09447598 0.56880132 0.85275296]. \t  3.83218403763175 \t 3.8463790119399603\n",
            "97     \t [0.53418259 0.56194007 0.85902405]. \t  3.830505377082498 \t 3.8463790119399603\n",
            "98     \t [0.58728285 0.56922253 0.85409843]. \t  3.8122648558285746 \t 3.8463790119399603\n",
            "99     \t [0.22933872 0.54316989 0.84466907]. \t  \u001b[92m3.8486415021271196\u001b[0m \t 3.8486415021271196\n",
            "100    \t [0.15404359 0.56739163 0.84851361]. \t  3.843514741081507 \t 3.8486415021271196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bpn-kmNuvqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8611127-080c-457a-f79d-eeee6e8a50a7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_winner_13 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_13 = dGPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.66064431 0.41360065 0.66810256]. \t  1.5457680899590245 \t 2.6697919207500047\n",
            "init   \t [0.22994342 0.80767834 0.63681846]. \t  2.6697919207500047 \t 2.6697919207500047\n",
            "init   \t [0.17219385 0.26038587 0.91531999]. \t  1.4685599870430508 \t 2.6697919207500047\n",
            "init   \t [0.46281551 0.12019095 0.88088551]. \t  0.639297028201682 \t 2.6697919207500047\n",
            "init   \t [0.22621895 0.81144033 0.44587892]. \t  1.960293029067079 \t 2.6697919207500047\n",
            "1      \t [0.81649528 0.95104598 1.        ]. \t  0.48034960454018577 \t 2.6697919207500047\n",
            "2      \t [0.02210984 0.92408402 0.84182838]. \t  1.1986876793966825 \t 2.6697919207500047\n",
            "3      \t [0.42913166 0.73987547 0.69732915]. \t  2.177319696230969 \t 2.6697919207500047\n",
            "4      \t [0.05049986 0.32901475 0.50425656]. \t  0.37412455804557115 \t 2.6697919207500047\n",
            "5      \t [0.96079806 0.29216074 0.95264917]. \t  1.3919250763683912 \t 2.6697919207500047\n",
            "6      \t [0.08474767 0.64903648 0.60027095]. \t  2.245170528835066 \t 2.6697919207500047\n",
            "7      \t [0.01353532 0.99061694 0.51626832]. \t  2.4465715320203687 \t 2.6697919207500047\n",
            "8      \t [0.98329464 0.08978514 0.36154986]. \t  0.2471825916301199 \t 2.6697919207500047\n",
            "9      \t [0.20657715 0.91250326 0.51497011]. \t  \u001b[92m2.7665786135207076\u001b[0m \t 2.7665786135207076\n",
            "10     \t [0.99255224 0.90613196 0.70521787]. \t  0.6081202005593442 \t 2.7665786135207076\n",
            "11     \t [0.31580615 0.97591959 0.53260238]. \t  2.362431816307285 \t 2.7665786135207076\n",
            "12     \t [0.97483059 0.46741727 0.81933428]. \t  \u001b[92m3.36681359952184\u001b[0m \t 3.36681359952184\n",
            "13     \t [0.94741454 0.56584692 0.76867165]. \t  2.980474921251275 \t 3.36681359952184\n",
            "14     \t [0.99006416 0.0522045  0.64210473]. \t  0.20972983522317368 \t 3.36681359952184\n",
            "15     \t [0.93003853 0.69014292 0.84275947]. \t  3.0433923522037745 \t 3.36681359952184\n",
            "16     \t [0.99136547 0.70194395 0.86153216]. \t  2.960164271308369 \t 3.36681359952184\n",
            "17     \t [0.70197448 0.47353959 0.87694414]. \t  \u001b[92m3.5203781983472977\u001b[0m \t 3.5203781983472977\n",
            "18     \t [0.6782866  0.56572361 0.94430072]. \t  3.0414771424820675 \t 3.5203781983472977\n",
            "19     \t [0.00510558 0.94421222 0.0106993 ]. \t  0.0004844833631589182 \t 3.5203781983472977\n",
            "20     \t [0.84345513 0.48591914 0.86601453]. \t  \u001b[92m3.580669618434115\u001b[0m \t 3.580669618434115\n",
            "21     \t [0.82818031 0.485131   0.88446088]. \t  3.50047684729066 \t 3.580669618434115\n",
            "22     \t [0.93608316 0.52595637 0.83979907]. \t  \u001b[92m3.6563152788065674\u001b[0m \t 3.6563152788065674\n",
            "23     \t [0.97960633 0.54009052 0.85194921]. \t  \u001b[92m3.6761763723029124\u001b[0m \t 3.6761763723029124\n",
            "24     \t [0.94522381 0.51505806 0.86395096]. \t  3.6473611983522995 \t 3.6761763723029124\n",
            "25     \t [0.99084051 0.50412891 0.90889989]. \t  3.3146925564700793 \t 3.6761763723029124\n",
            "26     \t [0.90717481 0.52753354 0.84233009]. \t  \u001b[92m3.6783923316884133\u001b[0m \t 3.6783923316884133\n",
            "27     \t [0.74413859 0.50011464 0.86989513]. \t  3.6589177774387114 \t 3.6783923316884133\n",
            "28     \t [0.86660131 0.51435459 0.84253153]. \t  3.6716111992210383 \t 3.6783923316884133\n",
            "29     \t [0.75470552 0.57341082 0.84398362]. \t  \u001b[92m3.73620072137577\u001b[0m \t 3.73620072137577\n",
            "30     \t [0.8297128  0.02887558 0.03393805]. \t  0.09551836203937242 \t 3.73620072137577\n",
            "31     \t [0.88831623 0.48868538 0.84400931]. \t  3.5859053354596298 \t 3.73620072137577\n",
            "32     \t [0.84999272 0.51988311 0.87332658]. \t  3.6708543262424618 \t 3.73620072137577\n",
            "33     \t [0.88943315 0.50341704 0.8773319 ]. \t  3.5917973963852505 \t 3.73620072137577\n",
            "34     \t [0.94025302 0.46405402 0.8484468 ]. \t  3.4541869297128818 \t 3.73620072137577\n",
            "35     \t [0.75250149 0.4744481  0.88581993]. \t  3.461056000877347 \t 3.73620072137577\n",
            "36     \t [0.83707872 0.54399345 0.8888279 ]. \t  3.6329000849748074 \t 3.73620072137577\n",
            "37     \t [0.83161539 0.50061943 0.87530664]. \t  3.6119525098576917 \t 3.73620072137577\n",
            "38     \t [0.8342847  0.5186291  0.87510043]. \t  3.6665670728849067 \t 3.73620072137577\n",
            "39     \t [0.82280882 0.4947945  0.85958879]. \t  3.637377371326799 \t 3.73620072137577\n",
            "40     \t [0.86815085 0.51796978 0.8844884 ]. \t  3.6051863671416253 \t 3.73620072137577\n",
            "41     \t [0.93138131 0.53915936 0.86111522]. \t  3.697322905050939 \t 3.73620072137577\n",
            "42     \t [0.82923277 0.51671845 0.88589292]. \t  3.6070853356726014 \t 3.73620072137577\n",
            "43     \t [0.87998151 0.52611981 0.85883061]. \t  3.704063427086355 \t 3.73620072137577\n",
            "44     \t [0.80585134 0.59305275 0.83733996]. \t  3.6449196886607287 \t 3.73620072137577\n",
            "45     \t [0.88391235 0.51146295 0.89037018]. \t  3.542712673055436 \t 3.73620072137577\n",
            "46     \t [0.90332222 0.52666669 0.8721835 ]. \t  3.6684689549229077 \t 3.73620072137577\n",
            "47     \t [0.858897   0.55033794 0.85230661]. \t  3.7318154059551736 \t 3.73620072137577\n",
            "48     \t [0.78100835 0.50916    0.87816801]. \t  3.6443154867467364 \t 3.73620072137577\n",
            "49     \t [0.89609116 0.52884885 0.87405216]. \t  3.6685338559175675 \t 3.73620072137577\n",
            "50     \t [0.88835505 0.52899932 0.87009911]. \t  3.685119260659447 \t 3.73620072137577\n",
            "51     \t [0.84245438 0.54789575 0.86971258]. \t  3.7229133803798917 \t 3.73620072137577\n",
            "52     \t [0.9158436  0.52481448 0.87619886]. \t  3.644590460557219 \t 3.73620072137577\n",
            "53     \t [9.50425810e-01 4.58765646e-04 6.54217112e-02]. \t  0.09320265235290406 \t 3.73620072137577\n",
            "54     \t [4.12045041e-15 4.06238631e-15 4.05799271e-15]. \t  0.06797411659013797 \t 3.73620072137577\n",
            "55     \t [0.88686152 0.49806048 0.89334005]. \t  3.4734656018592136 \t 3.73620072137577\n",
            "56     \t [0.86461202 0.52782253 0.89087501]. \t  3.5865612914344442 \t 3.73620072137577\n",
            "57     \t [0.89037735 0.51845151 0.86867207]. \t  3.667295168983726 \t 3.73620072137577\n",
            "58     \t [0.84440548 0.52042942 0.8871863 ]. \t  3.6029410393812222 \t 3.73620072137577\n",
            "59     \t [0.87750236 0.49137104 0.8429348 ]. \t  3.5985260419243414 \t 3.73620072137577\n",
            "60     \t [0.85907714 0.5225175  0.90272255]. \t  3.4787309093912073 \t 3.73620072137577\n",
            "61     \t [0.84179622 0.50878357 0.86469805]. \t  3.6696796509221556 \t 3.73620072137577\n",
            "62     \t [0.94062609 0.47782695 0.86664141]. \t  3.504651426667109 \t 3.73620072137577\n",
            "63     \t [0.80566094 0.58671379 0.85542143]. \t  3.7087682310338224 \t 3.73620072137577\n",
            "64     \t [0.88968091 0.57316886 0.85323896]. \t  3.70070453753467 \t 3.73620072137577\n",
            "65     \t [0.75045456 0.48873149 0.88231172]. \t  3.553452608760152 \t 3.73620072137577\n",
            "66     \t [0.8794918  0.49942264 0.85860881]. \t  3.634692964893697 \t 3.73620072137577\n",
            "67     \t [0.96489823 0.47418491 0.89145975]. \t  3.3497162697659784 \t 3.73620072137577\n",
            "68     \t [0.90396592 0.49753678 0.87472349]. \t  3.576574320615805 \t 3.73620072137577\n",
            "69     \t [0.83755943 0.5657734  0.86898253]. \t  3.7215304853720825 \t 3.73620072137577\n",
            "70     \t [0.85027355 0.53950942 0.88431683]. \t  3.651938440131709 \t 3.73620072137577\n",
            "71     \t [0.89469649 0.53462404 0.86137057]. \t  3.7080484996792724 \t 3.73620072137577\n",
            "72     \t [0.8655349  0.51403322 0.86758329]. \t  3.6687909043856908 \t 3.73620072137577\n",
            "73     \t [0.84147363 0.53070831 0.88510442]. \t  3.638182537624542 \t 3.73620072137577\n",
            "74     \t [0.86520269 0.51579153 0.86032152]. \t  3.68704512061527 \t 3.73620072137577\n",
            "75     \t [0.86206983 0.55072163 0.84847665]. \t  3.725293620910472 \t 3.73620072137577\n",
            "76     \t [0.90267259 0.48749323 0.84482546]. \t  3.576812615197019 \t 3.73620072137577\n",
            "77     \t [0.76444922 0.67368325 0.85027365]. \t  3.264797042155669 \t 3.73620072137577\n",
            "78     \t [0.85302403 0.52762037 0.89710785]. \t  3.5418598200998517 \t 3.73620072137577\n",
            "79     \t [0.85072558 0.52760788 0.87857919]. \t  3.664453524973024 \t 3.73620072137577\n",
            "80     \t [0.86409632 0.54161351 0.88931715]. \t  3.6177261804530794 \t 3.73620072137577\n",
            "81     \t [0.85034324 0.52957767 0.88384373]. \t  3.640484850885848 \t 3.73620072137577\n",
            "82     \t [0.83776022 0.59727994 0.8530879 ]. \t  3.6631775193513967 \t 3.73620072137577\n",
            "83     \t [0.85761822 0.53465539 0.86197092]. \t  3.722197784706503 \t 3.73620072137577\n",
            "84     \t [0.89420755 0.5127114  0.87436366]. \t  3.631757935837367 \t 3.73620072137577\n",
            "85     \t [0.89709527 0.48289758 0.84752567]. \t  3.563041062117332 \t 3.73620072137577\n",
            "86     \t [0.76978081 0.54514281 0.85993448]. \t  \u001b[92m3.7650360166740175\u001b[0m \t 3.7650360166740175\n",
            "87     \t [0.89804669 0.53317176 0.8669315 ]. \t  3.695647760722932 \t 3.7650360166740175\n",
            "88     \t [0.89688556 0.5315559  0.86711447]. \t  3.693478043797434 \t 3.7650360166740175\n",
            "89     \t [0.80917356 0.57070542 0.86600638]. \t  3.732749453682472 \t 3.7650360166740175\n",
            "90     \t [0.89943293 0.50389474 0.86859485]. \t  3.6226821399947613 \t 3.7650360166740175\n",
            "91     \t [0.88950971 0.54121111 0.86628856]. \t  3.708839145344991 \t 3.7650360166740175\n",
            "92     \t [0.97825274 0.51785568 0.85606169]. \t  3.6472427376897496 \t 3.7650360166740175\n",
            "93     \t [0.83211469 0.56565312 0.86527008]. \t  3.7309515811929383 \t 3.7650360166740175\n",
            "94     \t [0.90378124 0.53577262 0.84372891]. \t  3.6924631500473626 \t 3.7650360166740175\n",
            "95     \t [0.96006898 0.4777325  0.83799856]. \t  3.497655288204357 \t 3.7650360166740175\n",
            "96     \t [0.90718486 0.49238932 0.85217017]. \t  3.6005934408950746 \t 3.7650360166740175\n",
            "97     \t [0.95083567 0.479838   0.82365717]. \t  3.454114932313084 \t 3.7650360166740175\n",
            "98     \t [0.94268457 0.52024139 0.86803451]. \t  3.651615536516142 \t 3.7650360166740175\n",
            "99     \t [0.8586582  0.53204264 0.85304921]. \t  3.721611532889103 \t 3.7650360166740175\n",
            "100    \t [0.84921082 0.56579304 0.87156511]. \t  3.7103410046047838 \t 3.7650360166740175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NdFRXtPuvsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26f13556-9758-48dc-a557-5d47e95c0514"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_winner_14 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_14 = dGPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.64755105 0.50714969 0.52834138]. \t  0.5963212059988954 \t 2.610000357863649\n",
            "init   \t [0.8962852  0.69999119 0.7142971 ]. \t  1.7197848290620104 \t 2.610000357863649\n",
            "init   \t [0.71733838 0.22281946 0.17515452]. \t  0.48166052848103497 \t 2.610000357863649\n",
            "init   \t [0.45684149 0.92873843 0.00988589]. \t  0.0004588015757462679 \t 2.610000357863649\n",
            "init   \t [0.08992219 0.85020027 0.48562106]. \t  2.610000357863649 \t 2.610000357863649\n",
            "1      \t [0.07426471 0.99752623 0.88021278]. \t  0.6630933245090485 \t 2.610000357863649\n",
            "2      \t [0.         0.34337934 0.03146401]. \t  0.07512025465566176 \t 2.610000357863649\n",
            "3      \t [0.04945322 0.96746298 0.34336512]. \t  0.71192198606886 \t 2.610000357863649\n",
            "4      \t [1.         1.         0.62991162]. \t  0.2838463321657515 \t 2.610000357863649\n",
            "5      \t [0.01293246 0.57982797 0.77642605]. \t  \u001b[92m3.3404984056224087\u001b[0m \t 3.3404984056224087\n",
            "6      \t [0.95837713 0.62910703 0.12515201]. \t  0.014715862521621976 \t 3.3404984056224087\n",
            "7      \t [0.99766685 0.8194797  0.97508462]. \t  1.299153211667077 \t 3.3404984056224087\n",
            "8      \t [0.01347814 0.71295034 0.61184469]. \t  2.539390549512252 \t 3.3404984056224087\n",
            "9      \t [0.72212939 0.         0.        ]. \t  0.07032126552835634 \t 3.3404984056224087\n",
            "10     \t [0.05553423 0.38501019 0.96609074]. \t  1.951059198948275 \t 3.3404984056224087\n",
            "11     \t [0.34776367 0.93269768 0.51754496]. \t  2.366830099397843 \t 3.3404984056224087\n",
            "12     \t [0.26589898 0.58625495 0.01740287]. \t  0.016664306033616124 \t 3.3404984056224087\n",
            "13     \t [0.96820511 0.07331263 0.34993783]. \t  0.27353878045859903 \t 3.3404984056224087\n",
            "14     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.3404984056224087\n",
            "15     \t [0.11914463 0.61485194 0.97952108]. \t  2.391214324756784 \t 3.3404984056224087\n",
            "16     \t [0.97703009 0.65919945 0.59735747]. \t  0.6825797629991116 \t 3.3404984056224087\n",
            "17     \t [0.19103923 0.68353569 0.70358434]. \t  2.5869368556125374 \t 3.3404984056224087\n",
            "18     \t [0.03234376 0.7072331  0.76063109]. \t  2.76703640378025 \t 3.3404984056224087\n",
            "19     \t [0.         0.13882695 0.38696462]. \t  0.441444279963961 \t 3.3404984056224087\n",
            "20     \t [0.03418832 0.63548203 0.86620695]. \t  \u001b[92m3.5937382871341335\u001b[0m \t 3.5937382871341335\n",
            "21     \t [0.17512309 0.6745487  0.39361451]. \t  0.9501897642027308 \t 3.5937382871341335\n",
            "22     \t [0.00382304 0.49261766 0.97992377]. \t  2.295132164439291 \t 3.5937382871341335\n",
            "23     \t [0.96938281 0.12359161 0.84613015]. \t  0.6943150727229959 \t 3.5937382871341335\n",
            "24     \t [0.79702263 0.29099513 0.75905533]. \t  1.781680039549627 \t 3.5937382871341335\n",
            "25     \t [0.72601411 0.56580313 0.01358361]. \t  0.013251002725244408 \t 3.5937382871341335\n",
            "26     \t [0.77091664 0.03326031 0.65182031]. \t  0.2007625929401024 \t 3.5937382871341335\n",
            "27     \t [0.02414142 0.38468649 0.79474672]. \t  2.7841574326496676 \t 3.5937382871341335\n",
            "28     \t [0.88257393 0.42361072 1.        ]. \t  1.6629532812947698 \t 3.5937382871341335\n",
            "29     \t [0.02023259 0.47003044 0.85221165]. \t  3.5729249697244017 \t 3.5937382871341335\n",
            "30     \t [0.02983626 0.70946742 0.73216469]. \t  2.6264345323280076 \t 3.5937382871341335\n",
            "31     \t [0.02506469 0.5101154  0.91027239]. \t  3.410623041616264 \t 3.5937382871341335\n",
            "32     \t [0.23337699 0.59025077 0.87929311]. \t  \u001b[92m3.7520896285296397\u001b[0m \t 3.7520896285296397\n",
            "33     \t [0.33007346 0.54705772 0.92983898]. \t  3.2825130716359934 \t 3.7520896285296397\n",
            "34     \t [0.17555762 0.5856851  0.88981037]. \t  3.6909597446126736 \t 3.7520896285296397\n",
            "35     \t [0.2491679  0.56829007 0.86205731]. \t  \u001b[92m3.8463719539915955\u001b[0m \t 3.8463719539915955\n",
            "36     \t [0.41764769 0.00093327 0.20422039]. \t  0.7701493143963235 \t 3.8463719539915955\n",
            "37     \t [0.20444362 0.46862802 0.89402288]. \t  3.4126015623246815 \t 3.8463719539915955\n",
            "38     \t [0.03355875 0.57856473 0.83279675]. \t  3.7660810924926134 \t 3.8463719539915955\n",
            "39     \t [0.08205605 0.6020283  0.8612276 ]. \t  3.7554324268930177 \t 3.8463719539915955\n",
            "40     \t [0.1224224  0.57673615 0.88353713]. \t  3.7372829362749447 \t 3.8463719539915955\n",
            "41     \t [0.16026763 0.6209057  0.84685796]. \t  3.7014849685723252 \t 3.8463719539915955\n",
            "42     \t [0.07522621 0.61892902 0.83978507]. \t  3.6812469023292635 \t 3.8463719539915955\n",
            "43     \t [0.11234588 0.54554574 0.81486437]. \t  3.7150332575705236 \t 3.8463719539915955\n",
            "44     \t [0.10286778 0.58826587 0.86356787]. \t  3.7928709931534494 \t 3.8463719539915955\n",
            "45     \t [0.22393499 0.49915234 0.89718987]. \t  3.5383180289076215 \t 3.8463719539915955\n",
            "46     \t [0.11181219 0.57071731 0.83557069]. \t  3.806908946278864 \t 3.8463719539915955\n",
            "47     \t [0.11882362 0.53786658 0.89775284]. \t  3.6251810195629637 \t 3.8463719539915955\n",
            "48     \t [0.19189332 0.52528031 0.89228649]. \t  3.6595064758373264 \t 3.8463719539915955\n",
            "49     \t [0.28746381 0.62454033 0.84792192]. \t  3.691803704172396 \t 3.8463719539915955\n",
            "50     \t [0.36412248 0.58655781 0.88076643]. \t  3.754295295742643 \t 3.8463719539915955\n",
            "51     \t [0.45148821 0.55970643 0.87038826]. \t  3.822529309757142 \t 3.8463719539915955\n",
            "52     \t [0.61744116 0.55972687 0.88786522]. \t  3.70749374230577 \t 3.8463719539915955\n",
            "53     \t [0.05778708 0.54827091 0.87348379]. \t  3.7826366510285823 \t 3.8463719539915955\n",
            "54     \t [0.12716641 0.59117849 0.87422275]. \t  3.758941817549336 \t 3.8463719539915955\n",
            "55     \t [0.49615721 0.57365868 0.86726358]. \t  3.8133332751933025 \t 3.8463719539915955\n",
            "56     \t [0.10841409 0.53317105 0.85768172]. \t  3.820505084117861 \t 3.8463719539915955\n",
            "57     \t [0.55275005 0.56991463 0.86287447]. \t  3.815280824324352 \t 3.8463719539915955\n",
            "58     \t [0.62478137 0.54211545 0.90716436]. \t  3.532681033385625 \t 3.8463719539915955\n",
            "59     \t [0.21283294 0.55661224 0.86920426]. \t  3.830403357259814 \t 3.8463719539915955\n",
            "60     \t [0.19025423 0.50899544 0.89819826]. \t  3.561803613091537 \t 3.8463719539915955\n",
            "61     \t [0.32614171 0.51883578 0.88324259]. \t  3.7176854301312785 \t 3.8463719539915955\n",
            "62     \t [0.33154261 0.60801583 0.84168846]. \t  3.746553984166949 \t 3.8463719539915955\n",
            "63     \t [0.15654859 0.51810975 0.86930923]. \t  3.769637099844297 \t 3.8463719539915955\n",
            "64     \t [0.36523716 0.56158162 0.8758228 ]. \t  3.8082150869628553 \t 3.8463719539915955\n",
            "65     \t [0.43571763 0.58667083 0.84825303]. \t  3.8090191381059917 \t 3.8463719539915955\n",
            "66     \t [0.57119959 0.57024747 0.89199974]. \t  3.6823450782018035 \t 3.8463719539915955\n",
            "67     \t [0.23505273 0.55939065 0.79847195]. \t  3.6040858634247788 \t 3.8463719539915955\n",
            "68     \t [0.6190678  0.55284051 0.87706161]. \t  3.766900271482471 \t 3.8463719539915955\n",
            "69     \t [0.32214666 0.58598291 0.86306706]. \t  3.8190054287911326 \t 3.8463719539915955\n",
            "70     \t [0.2074817  0.54656596 0.90330736]. \t  3.596960071231127 \t 3.8463719539915955\n",
            "71     \t [0.42646516 0.61580913 0.82515533]. \t  3.6348109652736134 \t 3.8463719539915955\n",
            "72     \t [0.25856899 0.5884484  0.88523862]. \t  3.7242631639434105 \t 3.8463719539915955\n",
            "73     \t [0.15985115 0.57141307 0.84566513]. \t  3.8373768809891855 \t 3.8463719539915955\n",
            "74     \t [0.33904859 0.55045122 0.86381677]. \t  \u001b[92m3.848620201639168\u001b[0m \t 3.848620201639168\n",
            "75     \t [0.20202451 0.58357268 0.83931886]. \t  3.8109200343602336 \t 3.848620201639168\n",
            "76     \t [0.22469779 0.5415253  0.87382573]. \t  3.805557397446913 \t 3.848620201639168\n",
            "77     \t [0.25344529 0.57881091 0.85729742]. \t  3.8393920948142832 \t 3.848620201639168\n",
            "78     \t [0.18878348 0.53436054 0.86835263]. \t  3.812589709167426 \t 3.848620201639168\n",
            "79     \t [0.25404222 0.52615251 0.85797783]. \t  3.8275494356722164 \t 3.848620201639168\n",
            "80     \t [0.00608601 0.59653385 0.82538927]. \t  3.68605105348532 \t 3.848620201639168\n",
            "81     \t [0.349019   0.61730327 0.85532605]. \t  3.7239171247256824 \t 3.848620201639168\n",
            "82     \t [0.16364155 0.58757611 0.88188214]. \t  3.7360687952757266 \t 3.848620201639168\n",
            "83     \t [0.1760721  0.55059022 0.84630986]. \t  \u001b[92m3.8492561193985564\u001b[0m \t 3.8492561193985564\n",
            "84     \t [0.57603438 0.52969147 0.89804333]. \t  3.6102249523451144 \t 3.8492561193985564\n",
            "85     \t [0.14238164 0.58544772 0.86357205]. \t  3.8065820202687943 \t 3.8492561193985564\n",
            "86     \t [0.36132183 0.52962909 0.84634668]. \t  3.8346213934699915 \t 3.8492561193985564\n",
            "87     \t [0.46183475 0.54694014 0.8802435 ]. \t  3.776349209069666 \t 3.8492561193985564\n",
            "88     \t [0.07435789 0.53858346 0.86482323]. \t  3.8068695706892015 \t 3.8492561193985564\n",
            "89     \t [0.30560266 0.59054029 0.84830133]. \t  3.8137175875507485 \t 3.8492561193985564\n",
            "90     \t [0.13197083 0.57824907 0.84550879]. \t  3.823182027458723 \t 3.8492561193985564\n",
            "91     \t [0.05875553 0.54237427 0.86623275]. \t  3.803329056958783 \t 3.8492561193985564\n",
            "92     \t [0.31385584 0.5342574  0.85365275]. \t  3.8466818156427607 \t 3.8492561193985564\n",
            "93     \t [0.50417807 0.56360098 0.90126656]. \t  3.619557151026483 \t 3.8492561193985564\n",
            "94     \t [0.4706744  0.55899679 0.84951596]. \t  3.843065444126601 \t 3.8492561193985564\n",
            "95     \t [0.43935937 0.5691479  0.88929172]. \t  3.7214832408842717 \t 3.8492561193985564\n",
            "96     \t [0.28566764 0.48600014 0.83610048]. \t  3.6824521380778323 \t 3.8492561193985564\n",
            "97     \t [0.42145475 0.55632792 0.84415073]. \t  3.8439437746388947 \t 3.8492561193985564\n",
            "98     \t [0.11763404 0.52302132 0.8335751 ]. \t  3.7782235337353196 \t 3.8492561193985564\n",
            "99     \t [0.45152687 0.54376763 0.81954642]. \t  3.73537348684052 \t 3.8492561193985564\n",
            "100    \t [0.08259153 0.5382192  0.81479099]. \t  3.702404496787487 \t 3.8492561193985564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d86panpOuvum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6490ae1-de7e-4a97-9cbe-15909580a0fa"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_winner_15 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_15 = dGPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.57788186 0.25550133 0.25252687]. \t  0.7217338409961529 \t 1.540625560354162\n",
            "init   \t [0.70990435 0.44755236 0.22694296]. \t  0.23199690080875204 \t 1.540625560354162\n",
            "init   \t [0.40170957 0.88225774 0.43721347]. \t  1.540625560354162 \t 1.540625560354162\n",
            "init   \t [0.87842518 0.78052775 0.53421429]. \t  0.5653855252190516 \t 1.540625560354162\n",
            "init   \t [0.01173301 0.26575648 0.3311941 ]. \t  0.5019682819961726 \t 1.540625560354162\n",
            "1      \t [0.45870705 0.97826428 0.97993811]. \t  0.46749494069699443 \t 1.540625560354162\n",
            "2      \t [0.58908487 0.06356837 0.72214978]. \t  0.3847977331981157 \t 1.540625560354162\n",
            "3      \t [0.29062705 0.98880101 0.29539427]. \t  0.31682912945841557 \t 1.540625560354162\n",
            "4      \t [0.35830916 0.57348131 0.57569195]. \t  1.483660993659281 \t 1.540625560354162\n",
            "5      \t [0.57655701 0.96655517 0.45190655]. \t  1.044454263539026 \t 1.540625560354162\n",
            "6      \t [0.29839982 0.94779509 0.61815286]. \t  \u001b[92m2.376883919188496\u001b[0m \t 2.376883919188496\n",
            "7      \t [0.20225734 0.91762416 0.5993738 ]. \t  \u001b[92m2.8143502335642547\u001b[0m \t 2.8143502335642547\n",
            "8      \t [0.05726953 0.95949547 0.73838485]. \t  1.4281001613955855 \t 2.8143502335642547\n",
            "9      \t [0.96497544 0.07697573 0.01139424]. \t  0.04752173213497603 \t 2.8143502335642547\n",
            "10     \t [0.03638618 0.7831191  0.41109762]. \t  1.478957331069793 \t 2.8143502335642547\n",
            "11     \t [0.2846894  0.01441969 0.3254019 ]. \t  0.7970516712663931 \t 2.8143502335642547\n",
            "12     \t [1. 1. 1.]. \t  0.31688362070415665 \t 2.8143502335642547\n",
            "13     \t [0.98123017 0.02689864 0.99370805]. \t  0.12575069423354387 \t 2.8143502335642547\n",
            "14     \t [0.99248475 0.89393169 0.25379208]. \t  0.019773995845805473 \t 2.8143502335642547\n",
            "15     \t [0.90548947 0.152541   0.36635517]. \t  0.3142882149685416 \t 2.8143502335642547\n",
            "16     \t [0.18147782 0.8508714  0.52985713]. \t  \u001b[92m2.9458934218402075\u001b[0m \t 2.9458934218402075\n",
            "17     \t [0.04125693 0.24808795 0.89096805]. \t  1.4874659879997851 \t 2.9458934218402075\n",
            "18     \t [0.10138781 0.76829497 0.60548495]. \t  2.83309150230065 \t 2.9458934218402075\n",
            "19     \t [0.11811667 0.67171276 0.6296264 ]. \t  2.421333271654245 \t 2.9458934218402075\n",
            "20     \t [0.18886148 0.03614456 0.99106523]. \t  0.14613141770516785 \t 2.9458934218402075\n",
            "21     \t [0.06833449 0.98065482 0.51704313]. \t  2.564324377325894 \t 2.9458934218402075\n",
            "22     \t [0.00783768 0.01174457 0.57247569]. \t  0.10869176195230904 \t 2.9458934218402075\n",
            "23     \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.9458934218402075\n",
            "24     \t [0.75812964 0.4744     0.99163431]. \t  2.031412034788625 \t 2.9458934218402075\n",
            "25     \t [0.1496267  0.83896971 0.66374867]. \t  2.547574930306317 \t 2.9458934218402075\n",
            "26     \t [0.08282586 0.77020464 0.55400076]. \t  2.8494528624919098 \t 2.9458934218402075\n",
            "27     \t [0.10454511 0.8355504  0.51140913]. \t  2.844654602391224 \t 2.9458934218402075\n",
            "28     \t [0.18374988 0.8686762  0.55497609]. \t  \u001b[92m3.032624131081467\u001b[0m \t 3.032624131081467\n",
            "29     \t [0.02701892 0.73247455 0.54921471]. \t  2.5873896378178856 \t 3.032624131081467\n",
            "30     \t [0.16202041 0.86821179 0.56604579]. \t  \u001b[92m3.062704393865933\u001b[0m \t 3.062704393865933\n",
            "31     \t [0.14038174 0.83069911 0.48708205]. \t  2.593228763484177 \t 3.062704393865933\n",
            "32     \t [0.09185293 0.74550382 0.61485494]. \t  2.7270945552334847 \t 3.062704393865933\n",
            "33     \t [0.06539025 0.78252986 0.62610497]. \t  2.7795746555317145 \t 3.062704393865933\n",
            "34     \t [0.17808068 0.92514539 0.54283964]. \t  2.9090056191652525 \t 3.062704393865933\n",
            "35     \t [0.14895755 0.93780778 0.57337376]. \t  2.8979361529672074 \t 3.062704393865933\n",
            "36     \t [0.23189651 0.85539511 0.52585779]. \t  2.8430554961358805 \t 3.062704393865933\n",
            "37     \t [0.08144986 0.80104367 0.63184958]. \t  2.7859881045311914 \t 3.062704393865933\n",
            "38     \t [0.24927779 0.91669342 0.50320654]. \t  2.584919746851072 \t 3.062704393865933\n",
            "39     \t [0.03266288 0.71939505 0.58698326]. \t  2.594404701710182 \t 3.062704393865933\n",
            "40     \t [0.15096656 0.9115402  0.54335761]. \t  2.9773900533411437 \t 3.062704393865933\n",
            "41     \t [0.15500095 0.90036262 0.59550597]. \t  2.939399983670485 \t 3.062704393865933\n",
            "42     \t [0.1953386  0.96887906 0.62355341]. \t  2.428237927384099 \t 3.062704393865933\n",
            "43     \t [0.19753684 0.85523527 0.54027051]. \t  2.974151597650664 \t 3.062704393865933\n",
            "44     \t [0.06575517 0.84477006 0.57208495]. \t  3.06239233261359 \t 3.062704393865933\n",
            "45     \t [0.13147574 0.72413448 0.57702335]. \t  2.6516438079228646 \t 3.062704393865933\n",
            "46     \t [0.1520434  0.75722537 0.63578239]. \t  2.6899477584152724 \t 3.062704393865933\n",
            "47     \t [0.08007929 0.88723749 0.51174664]. \t  2.858471117493557 \t 3.062704393865933\n",
            "48     \t [0.23458523 0.93983436 0.54123253]. \t  2.7587066797970032 \t 3.062704393865933\n",
            "49     \t [0.2164811  0.89700415 0.55450097]. \t  2.947601708842175 \t 3.062704393865933\n",
            "50     \t [0.10403048 0.84267713 0.60246733]. \t  2.985652898826652 \t 3.062704393865933\n",
            "51     \t [0.13504279 0.85312682 0.51137572]. \t  2.861475943518015 \t 3.062704393865933\n",
            "52     \t [0.10218161 0.81225071 0.60019915]. \t  2.964466137945088 \t 3.062704393865933\n",
            "53     \t [0.11923729 0.8476935  0.58282976]. \t  3.0611014503601273 \t 3.062704393865933\n",
            "54     \t [0.1617036  0.8661409  0.54916154]. \t  3.0474928236807948 \t 3.062704393865933\n",
            "55     \t [0.17794261 0.95298018 0.52194154]. \t  2.7097768498027466 \t 3.062704393865933\n",
            "56     \t [0.19068419 0.82912167 0.61826871]. \t  2.8440006982450496 \t 3.062704393865933\n",
            "57     \t [0.10081822 0.80345718 0.61011396]. \t  2.907310227144233 \t 3.062704393865933\n",
            "58     \t [0.04950271 0.7876851  0.53774037]. \t  2.84253136891548 \t 3.062704393865933\n",
            "59     \t [0.09953867 0.83453912 0.59474633]. \t  3.0142018449110908 \t 3.062704393865933\n",
            "60     \t [0.16401696 0.90507668 0.51788764]. \t  2.857545395049901 \t 3.062704393865933\n",
            "61     \t [0.1181284  0.90435265 0.56016706]. \t  3.0356949982969175 \t 3.062704393865933\n",
            "62     \t [0.06504579 0.89740139 0.52310216]. \t  2.9148579692127288 \t 3.062704393865933\n",
            "63     \t [0.11704427 0.81664263 0.60127064]. \t  2.9675476613048315 \t 3.062704393865933\n",
            "64     \t [0.15609207 0.88898729 0.52216942]. \t  2.9193405232721905 \t 3.062704393865933\n",
            "65     \t [0.17902847 0.77243239 0.63400399]. \t  2.7083195286953714 \t 3.062704393865933\n",
            "66     \t [0.07026848 0.7689341  0.63031971]. \t  2.7362193388858773 \t 3.062704393865933\n",
            "67     \t [0.18945757 0.81554412 0.53081544]. \t  2.878169309176224 \t 3.062704393865933\n",
            "68     \t [0.18106814 0.87366119 0.56278242]. \t  3.03898257761039 \t 3.062704393865933\n",
            "69     \t [0.14476101 0.93882028 0.54240413]. \t  2.886863404113033 \t 3.062704393865933\n",
            "70     \t [0.18077205 0.81330769 0.59603637]. \t  2.940864411995432 \t 3.062704393865933\n",
            "71     \t [0.10763143 0.81637044 0.60952114]. \t  2.930690528745108 \t 3.062704393865933\n",
            "72     \t [0.11753727 0.66117172 0.55322273]. \t  2.1768091627173574 \t 3.062704393865933\n",
            "73     \t [0.16665162 0.81834212 0.63901726]. \t  2.7352054917383706 \t 3.062704393865933\n",
            "74     \t [0.09047285 0.83631964 0.51887265]. \t  2.900254661673674 \t 3.062704393865933\n",
            "75     \t [0.08319889 0.83957255 0.60646434]. \t  2.9589006716708934 \t 3.062704393865933\n",
            "76     \t [0.09427488 0.85887236 0.53676733]. \t  3.027990954105804 \t 3.062704393865933\n",
            "77     \t [0.0936423  0.88750066 0.61396516]. \t  2.8723879982038962 \t 3.062704393865933\n",
            "78     \t [0.01632739 0.91861653 0.574902  ]. \t  2.9056807662841933 \t 3.062704393865933\n",
            "79     \t [0.02093918 0.79903889 0.56974125]. \t  2.92428890913382 \t 3.062704393865933\n",
            "80     \t [0.14495833 0.85628565 0.60713506]. \t  2.949326825989386 \t 3.062704393865933\n",
            "81     \t [0.13304719 0.83036934 0.58165189]. \t  3.0446192792379203 \t 3.062704393865933\n",
            "82     \t [0.05340489 0.80230089 0.56865223]. \t  2.9732765282872586 \t 3.062704393865933\n",
            "83     \t [0.03922756 0.8985075  0.50656641]. \t  2.767157136659806 \t 3.062704393865933\n",
            "84     \t [0.02417255 0.94260149 0.57935839]. \t  2.814352396700717 \t 3.062704393865933\n",
            "85     \t [0.05753637 0.88350565 0.58655115]. \t  3.006650630310926 \t 3.062704393865933\n",
            "86     \t [0.11891041 0.73742396 0.6163636 ]. \t  2.6960298816205834 \t 3.062704393865933\n",
            "87     \t [0.00538866 0.84756316 0.62696963]. \t  2.7518750264658776 \t 3.062704393865933\n",
            "88     \t [0.16626769 0.79480356 0.51195142]. \t  2.710380653771164 \t 3.062704393865933\n",
            "89     \t [0.1408972  0.75972673 0.58062218]. \t  2.830885150633012 \t 3.062704393865933\n",
            "90     \t [0.09187336 0.77991507 0.62406471]. \t  2.7957073346322088 \t 3.062704393865933\n",
            "91     \t [0.06347953 0.89932595 0.54683088]. \t  3.0116993912637584 \t 3.062704393865933\n",
            "92     \t [0.08529083 0.78763086 0.4948962 ]. \t  2.5503376837053504 \t 3.062704393865933\n",
            "93     \t [0.15823063 0.82949897 0.57085316]. \t  3.041763829103173 \t 3.062704393865933\n",
            "94     \t [0.03930613 0.80489181 0.57816572]. \t  2.9616420251779876 \t 3.062704393865933\n",
            "95     \t [0.15719402 0.85746632 0.63929715]. \t  2.714069847507931 \t 3.062704393865933\n",
            "96     \t [0.0234645  0.80764124 0.61851087]. \t  2.8163574562511835 \t 3.062704393865933\n",
            "97     \t [0.2251457  0.84129189 0.58698514]. \t  2.9339035312898574 \t 3.062704393865933\n",
            "98     \t [0.06138694 0.81205223 0.61460788]. \t  2.8811332325174837 \t 3.062704393865933\n",
            "99     \t [0.0575322  0.91789929 0.59318835]. \t  2.894035776410521 \t 3.062704393865933\n",
            "100    \t [0.08162042 0.90354869 0.53547054]. \t  2.9774155575313963 \t 3.062704393865933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "any0xrgYuvxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27654d9c-96df-48c8-b243-d6c5461cd0f2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_winner_16 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_16 = dGPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.86394833 0.316366   0.67215078]. \t  1.1947633822631252 \t 3.8084053754826726\n",
            "init   \t [0.50791167 0.78166043 0.28368168]. \t  0.20052765717741963 \t 3.8084053754826726\n",
            "init   \t [0.23370878 0.56379969 0.87502436]. \t  3.8084053754826726 \t 3.8084053754826726\n",
            "init   \t [0.71894292 0.18213174 0.24380041]. \t  0.6532826728302027 \t 3.8084053754826726\n",
            "init   \t [0.08605673 0.53424539 0.83969965]. \t  3.8072565344663847 \t 3.8084053754826726\n",
            "1      \t [0.12378065 0.3168353  0.9881258 ]. \t  1.2451880833929392 \t 3.8084053754826726\n",
            "2      \t [0.27777137 0.94321124 0.96390235]. \t  0.6864412627036871 \t 3.8084053754826726\n",
            "3      \t [0.00916054 0.49924288 0.41582338]. \t  0.5101682193417341 \t 3.8084053754826726\n",
            "4      \t [0.15821774 0.63269303 0.7891081 ]. \t  3.35125938979847 \t 3.8084053754826726\n",
            "5      \t [0.84624088 0.50131507 0.00568586]. \t  0.01480746504362111 \t 3.8084053754826726\n",
            "6      \t [0.15025688 0.60895724 0.92813325]. \t  3.231297753073477 \t 3.8084053754826726\n",
            "7      \t [0.92341363 0.99382665 0.91671332]. \t  0.540118167817335 \t 3.8084053754826726\n",
            "8      \t [0.4704549  0.48013331 0.6189838 ]. \t  1.30919929752299 \t 3.8084053754826726\n",
            "9      \t [0.94595544 0.08472824 0.05944089]. \t  0.09970384471485459 \t 3.8084053754826726\n",
            "10     \t [0.80321937 0.9517176  0.02576245]. \t  0.00024341109661934083 \t 3.8084053754826726\n",
            "11     \t [0.96803825 0.02593838 0.96540619]. \t  0.1605011116410514 \t 3.8084053754826726\n",
            "12     \t [0.3619492  0.96441886 0.03879689]. \t  0.0009350407548918815 \t 3.8084053754826726\n",
            "13     \t [0.78185016 0.68968437 0.99132301]. \t  1.9054533574370216 \t 3.8084053754826726\n",
            "14     \t [0.00617671 0.36546392 0.71982754]. \t  1.9881325841759696 \t 3.8084053754826726\n",
            "15     \t [0.1359275  0.50758838 0.79512294]. \t  3.5071627458018875 \t 3.8084053754826726\n",
            "16     \t [4.40028665e-04 5.57242651e-01 8.00265835e-01]. \t  3.5760738218167125 \t 3.8084053754826726\n",
            "17     \t [0.53632409 1.         0.80189776]. \t  0.6970172536478709 \t 3.8084053754826726\n",
            "18     \t [0.11111849 0.54131346 0.7862915 ]. \t  3.4779468834968217 \t 3.8084053754826726\n",
            "19     \t [0.16985304 0.4993737  0.8764749 ]. \t  3.6779589803808053 \t 3.8084053754826726\n",
            "20     \t [0.2411347  0.58546032 0.89480099]. \t  3.6604796223264513 \t 3.8084053754826726\n",
            "21     \t [0.22898502 0.53965435 0.87806158]. \t  3.7836660984221737 \t 3.8084053754826726\n",
            "22     \t [0.30358047 0.5751254  0.95824384]. \t  2.837115309936554 \t 3.8084053754826726\n",
            "23     \t [0.04245604 0.17940171 0.00395283]. \t  0.08723427904764045 \t 3.8084053754826726\n",
            "24     \t [0.2839663  0.55254076 0.00135744]. \t  0.01769144198825377 \t 3.8084053754826726\n",
            "25     \t [0.11667043 0.53585389 0.80916659]. \t  3.669628975170707 \t 3.8084053754826726\n",
            "26     \t [0.08396723 0.47948766 0.8988177 ]. \t  3.410459032783401 \t 3.8084053754826726\n",
            "27     \t [0.0796643  0.536108   0.88064967]. \t  3.7395446042988794 \t 3.8084053754826726\n",
            "28     \t [0.07857228 0.56712785 0.83669024]. \t  3.806044039477019 \t 3.8084053754826726\n",
            "29     \t [0.25025865 0.49817968 0.80930379]. \t  3.6007228933938826 \t 3.8084053754826726\n",
            "30     \t [0.08934153 0.48178943 0.84929768]. \t  3.6533056075788517 \t 3.8084053754826726\n",
            "31     \t [0.99558738 0.53129676 0.89995433]. \t  3.4688217834392607 \t 3.8084053754826726\n",
            "32     \t [0.13687621 0.47143308 0.81992687]. \t  3.5387665080059554 \t 3.8084053754826726\n",
            "33     \t [0.21459135 0.60490896 0.8109247 ]. \t  3.619473242766057 \t 3.8084053754826726\n",
            "34     \t [0.114807   0.53212495 0.80689509]. \t  3.648218378393098 \t 3.8084053754826726\n",
            "35     \t [0.21857828 0.54948051 0.81863965]. \t  3.7532070118739425 \t 3.8084053754826726\n",
            "36     \t [0.13044132 0.58809795 0.87017856]. \t  3.781360571917484 \t 3.8084053754826726\n",
            "37     \t [0.23221419 0.60339509 0.8170558 ]. \t  3.6632753478173674 \t 3.8084053754826726\n",
            "38     \t [0.17948477 0.51744393 0.8494003 ]. \t  3.803914879489386 \t 3.8084053754826726\n",
            "39     \t [0.08484384 0.5591754  0.88227241]. \t  3.748419270456142 \t 3.8084053754826726\n",
            "40     \t [0.21354019 0.59635179 0.86288733]. \t  3.7909788095095998 \t 3.8084053754826726\n",
            "41     \t [0.25951379 0.53828901 0.86606878]. \t  \u001b[92m3.8318753200219033\u001b[0m \t 3.8318753200219033\n",
            "42     \t [0.07492047 0.6482167  0.8192523 ]. \t  3.4611983133399056 \t 3.8318753200219033\n",
            "43     \t [0.23780197 0.54368366 0.83686875]. \t  \u001b[92m3.8326842041604\u001b[0m \t 3.8326842041604\n",
            "44     \t [0.256176   0.52040045 0.83138407]. \t  3.7816462769653105 \t 3.8326842041604\n",
            "45     \t [0.25331844 0.56301604 0.87170652]. \t  3.8236316496517073 \t 3.8326842041604\n",
            "46     \t [0.2336326  0.52605065 0.85861873]. \t  3.8250042311398733 \t 3.8326842041604\n",
            "47     \t [0.1837558  0.53621011 0.83102889]. \t  3.8015455754378564 \t 3.8326842041604\n",
            "48     \t [0.33000641 0.56594289 0.83774366]. \t  3.832394318806762 \t 3.8326842041604\n",
            "49     \t [0.26560399 0.60727553 0.83291138]. \t  3.726895584109559 \t 3.8326842041604\n",
            "50     \t [0.68133095 0.03735436 0.04534701]. \t  0.1597468732369745 \t 3.8326842041604\n",
            "51     \t [0.24189021 0.50140136 0.85614268]. \t  3.7573454851393633 \t 3.8326842041604\n",
            "52     \t [0.19166521 0.53257158 0.87023412]. \t  3.8034897601789885 \t 3.8326842041604\n",
            "53     \t [0.23037453 0.53675083 0.82496254]. \t  3.779772463337153 \t 3.8326842041604\n",
            "54     \t [0.01484925 0.47915862 0.00262479]. \t  0.02267903369305669 \t 3.8326842041604\n",
            "55     \t [0.02245595 0.5457974  0.80062959]. \t  3.585991590284652 \t 3.8326842041604\n",
            "56     \t [0.24897079 0.51686375 0.82526942]. \t  3.7476715256726854 \t 3.8326842041604\n",
            "57     \t [0.25780147 0.57445949 0.88128491]. \t  3.7703320037585506 \t 3.8326842041604\n",
            "58     \t [0.0911822  0.53898996 0.85493981]. \t  3.8270592052714383 \t 3.8326842041604\n",
            "59     \t [0.12642183 0.58101714 0.86051078]. \t  3.8172372735912905 \t 3.8326842041604\n",
            "60     \t [0.23662976 0.60535261 0.81679117]. \t  3.6553080784688454 \t 3.8326842041604\n",
            "61     \t [0.17097808 0.64174818 0.83023391]. \t  3.559510560248089 \t 3.8326842041604\n",
            "62     \t [0.13418125 0.55624636 0.84054145]. \t  \u001b[92m3.833463233321761\u001b[0m \t 3.833463233321761\n",
            "63     \t [0.23457252 0.56798149 0.86478689]. \t  \u001b[92m3.840122567246137\u001b[0m \t 3.840122567246137\n",
            "64     \t [0.14645762 0.55993544 0.87819459]. \t  3.783297683428135 \t 3.840122567246137\n",
            "65     \t [0.03304343 0.58501639 0.85062718]. \t  3.7910505990452155 \t 3.840122567246137\n",
            "66     \t [0.19442083 0.56395044 0.83327883]. \t  3.8179285092961006 \t 3.840122567246137\n",
            "67     \t [0.08423246 0.51881348 0.83993929]. \t  3.778023868707068 \t 3.840122567246137\n",
            "68     \t [0.31564494 0.50916969 0.88112839]. \t  3.7010842184731167 \t 3.840122567246137\n",
            "69     \t [0.26914284 0.51432452 0.80221619]. \t  3.592771112285854 \t 3.840122567246137\n",
            "70     \t [0.18947325 0.57543829 0.78666456]. \t  3.4831276311205266 \t 3.840122567246137\n",
            "71     \t [0.17177871 0.49687268 0.82650467]. \t  3.685632281645761 \t 3.840122567246137\n",
            "72     \t [0.2425648  0.53177735 0.89049082]. \t  3.6927870099330407 \t 3.840122567246137\n",
            "73     \t [0.1476394  0.54103778 0.84949813]. \t  \u001b[92m3.8412922219954617\u001b[0m \t 3.8412922219954617\n",
            "74     \t [0.01471892 0.52767853 0.83725391]. \t  3.770355627665735 \t 3.8412922219954617\n",
            "75     \t [0.26856466 0.62927896 0.86594192]. \t  3.660527396003911 \t 3.8412922219954617\n",
            "76     \t [0.25329007 0.62752177 0.82925488]. \t  3.6295401710651083 \t 3.8412922219954617\n",
            "77     \t [0.17139661 0.51587318 0.86655527]. \t  3.774945440172103 \t 3.8412922219954617\n",
            "78     \t [0.05444869 0.51201356 0.81179206]. \t  3.628944805100691 \t 3.8412922219954617\n",
            "79     \t [0.17610609 0.49873531 0.86743776]. \t  3.7144179398125745 \t 3.8412922219954617\n",
            "80     \t [0.16899306 0.53967973 0.8557073 ]. \t  \u001b[92m3.8423923921080028\u001b[0m \t 3.8423923921080028\n",
            "81     \t [0.33911393 0.58080266 0.84848968]. \t  3.8333360831381174 \t 3.8423923921080028\n",
            "82     \t [0.39382366 0.48830547 0.89435714]. \t  3.5220225757369774 \t 3.8423923921080028\n",
            "83     \t [0.22389738 0.48825282 0.84854293]. \t  3.7054201623970107 \t 3.8423923921080028\n",
            "84     \t [0.12307519 0.54867747 0.85989416]. \t  3.8369983817073705 \t 3.8423923921080028\n",
            "85     \t [0.16307611 0.61688606 0.86893902]. \t  3.700607189046127 \t 3.8423923921080028\n",
            "86     \t [0.25879636 0.49741    0.83259501]. \t  3.7179892064166564 \t 3.8423923921080028\n",
            "87     \t [0.09148698 0.52696593 0.88756349]. \t  3.681090779068207 \t 3.8423923921080028\n",
            "88     \t [0.08502535 0.48905319 0.84115752]. \t  3.679681136887694 \t 3.8423923921080028\n",
            "89     \t [0.07557307 0.57273055 0.8386859 ]. \t  3.804908951099899 \t 3.8423923921080028\n",
            "90     \t [0.18896694 0.56774293 0.88163866]. \t  3.7680075312780996 \t 3.8423923921080028\n",
            "91     \t [0.0117149  0.57140342 0.84146846]. \t  3.793686772574496 \t 3.8423923921080028\n",
            "92     \t [0.30335047 0.47661848 0.87179619]. \t  3.6058707139084754 \t 3.8423923921080028\n",
            "93     \t [0.3181172  0.6146084  0.86354697]. \t  3.7298770428904646 \t 3.8423923921080028\n",
            "94     \t [0.15517418 0.4893462  0.82974761]. \t  3.666080783497935 \t 3.8423923921080028\n",
            "95     \t [0.07279852 0.53715072 0.82083715]. \t  3.734459812965917 \t 3.8423923921080028\n",
            "96     \t [0.18344197 0.58506406 0.83805259]. \t  3.8027721225456217 \t 3.8423923921080028\n",
            "97     \t [0.0163306  0.5628218  0.79097059]. \t  3.4984361368519123 \t 3.8423923921080028\n",
            "98     \t [0.31306379 0.5041394  0.83992811]. \t  3.7619845713917837 \t 3.8423923921080028\n",
            "99     \t [0.32716261 0.4702691  0.87567366]. \t  3.5533599815904826 \t 3.8423923921080028\n",
            "100    \t [0.21871199 0.49256593 0.85108122]. \t  3.7236028523188285 \t 3.8423923921080028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reLyKt6Quvzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b15d94-c08c-4dbd-dbf1-0731bc62165c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_winner_17 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_17 = dGPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.65182403 0.73051962 0.19336736]. \t  0.03562358530655929 \t 3.1179188940604616\n",
            "init   \t [0.51967724 0.67638327 0.80444487]. \t  3.1179188940604616 \t 3.1179188940604616\n",
            "init   \t [0.53351286 0.90803418 0.44348113]. \t  1.1984486017622078 \t 3.1179188940604616\n",
            "init   \t [0.25597221 0.27866502 0.04332186]. \t  0.1645724004847893 \t 3.1179188940604616\n",
            "init   \t [0.46897499 0.9416138  0.60040624]. \t  1.9192522196289552 \t 3.1179188940604616\n",
            "1      \t [0.24286651 0.38597556 0.96454773]. \t  1.996297956656514 \t 3.1179188940604616\n",
            "2      \t [1.         0.95222433 1.        ]. \t  0.46605714699763057 \t 3.1179188940604616\n",
            "3      \t [0.75571534 0.33466529 0.78006847]. \t  2.266554598856901 \t 3.1179188940604616\n",
            "4      \t [0.45426531 0.7171866  0.9593009 ]. \t  2.250872678699735 \t 3.1179188940604616\n",
            "5      \t [0.37244605 0.51025462 0.61539851]. \t  1.462684674049977 \t 3.1179188940604616\n",
            "6      \t [0.77167209 0.89977482 0.69423069]. \t  0.8885381911174197 \t 3.1179188940604616\n",
            "7      \t [0.85290458 0.45298432 0.94988391]. \t  2.5878930875677453 \t 3.1179188940604616\n",
            "8      \t [0.45502735 0.17986228 0.9399878 ]. \t  0.768851014033499 \t 3.1179188940604616\n",
            "9      \t [0.09367814 0.94508404 0.06566792]. \t  0.0024276834403190208 \t 3.1179188940604616\n",
            "10     \t [0.739934   0.10683805 0.00457855]. \t  0.08335017310102226 \t 3.1179188940604616\n",
            "11     \t [0.59261554 0.63270299 0.79465898]. \t  \u001b[92m3.2537060455044644\u001b[0m \t 3.2537060455044644\n",
            "12     \t [0.66644654 0.57433108 0.80782745]. \t  \u001b[92m3.5516603830410283\u001b[0m \t 3.5516603830410283\n",
            "13     \t [0.76411665 0.56984749 0.7999946 ]. \t  3.442253910069545 \t 3.5516603830410283\n",
            "14     \t [0.76092022 0.54245962 0.7960535 ]. \t  3.430452623032548 \t 3.5516603830410283\n",
            "15     \t [0.61705213 0.55203039 0.82772646]. \t  \u001b[92m3.738570275616318\u001b[0m \t 3.738570275616318\n",
            "16     \t [0.67944152 0.53063432 0.85475357]. \t  \u001b[92m3.7819791717917846\u001b[0m \t 3.7819791717917846\n",
            "17     \t [0.59421704 0.55003465 0.87796316]. \t  3.76759003718539 \t 3.7819791717917846\n",
            "18     \t [0.65060798 0.52623423 0.90917133]. \t  3.4795280967650193 \t 3.7819791717917846\n",
            "19     \t [0.79163388 0.98135759 0.012932  ]. \t  0.00014671683587279732 \t 3.7819791717917846\n",
            "20     \t [0.64736767 0.44837342 0.84314153]. \t  3.4480685359892904 \t 3.7819791717917846\n",
            "21     \t [0.55556779 0.50670129 0.85022322]. \t  3.7565222225664896 \t 3.7819791717917846\n",
            "22     \t [0.65371517 0.47123587 0.83944785]. \t  3.573299629190301 \t 3.7819791717917846\n",
            "23     \t [0.64181454 0.4995408  0.84813467]. \t  3.711734480628256 \t 3.7819791717917846\n",
            "24     \t [0.54735686 0.5581502  0.84067865]. \t  \u001b[92m3.809775366675879\u001b[0m \t 3.809775366675879\n",
            "25     \t [2.44409700e-01 9.89411038e-01 4.04951227e-04]. \t  0.0003021334801372294 \t 3.809775366675879\n",
            "26     \t [0.62354518 0.49594559 0.89250429]. \t  3.5448238154930167 \t 3.809775366675879\n",
            "27     \t [0.99078863 0.64356925 0.84396562]. \t  3.3539538714250927 \t 3.809775366675879\n",
            "28     \t [0.53176288 0.45715129 0.88059833]. \t  3.4357650538049365 \t 3.809775366675879\n",
            "29     \t [0.6983435  0.52809783 0.85303514]. \t  3.772167093246405 \t 3.809775366675879\n",
            "30     \t [0.66207655 0.54404133 0.84799434]. \t  3.794771770256933 \t 3.809775366675879\n",
            "31     \t [0.64929952 0.52671019 0.86341748]. \t  3.775766139940171 \t 3.809775366675879\n",
            "32     \t [0.68409215 0.55552162 0.83875384]. \t  3.764366140688522 \t 3.809775366675879\n",
            "33     \t [0.68302544 0.51968441 0.84270966]. \t  3.7478508616123216 \t 3.809775366675879\n",
            "34     \t [0.68041411 0.55085911 0.84508392]. \t  3.784944776893893 \t 3.809775366675879\n",
            "35     \t [0.66654993 0.55489542 0.87672833]. \t  3.7566125294832835 \t 3.809775366675879\n",
            "36     \t [0.70264892 0.54455309 0.83148273]. \t  3.729466228159111 \t 3.809775366675879\n",
            "37     \t [0.62286553 0.51232416 0.83071445]. \t  3.7090981155778033 \t 3.809775366675879\n",
            "38     \t [0.74209842 0.58830021 0.90287394]. \t  3.5256253545172496 \t 3.809775366675879\n",
            "39     \t [0.558343   0.53574007 0.87713592]. \t  3.7658093208540757 \t 3.809775366675879\n",
            "40     \t [0.60872253 0.53819006 0.85464961]. \t  \u001b[92m3.810441960294527\u001b[0m \t 3.810441960294527\n",
            "41     \t [0.55508521 0.52958601 0.856703  ]. \t  \u001b[92m3.8109232633338648\u001b[0m \t 3.8109232633338648\n",
            "42     \t [0.55116915 0.51516832 0.87299002]. \t  3.741885924296383 \t 3.8109232633338648\n",
            "43     \t [0.55850153 0.5109378  0.83695254]. \t  3.744834851067032 \t 3.8109232633338648\n",
            "44     \t [0.60787227 0.53308924 0.86458656]. \t  3.7942165428017214 \t 3.8109232633338648\n",
            "45     \t [0.60079009 0.53784152 0.86135823]. \t  3.8074664555706113 \t 3.8109232633338648\n",
            "46     \t [0.62777458 0.56350957 0.85455377]. \t  3.8072553737900754 \t 3.8109232633338648\n",
            "47     \t [0.62627647 0.52978879 0.88497566]. \t  3.699433376503814 \t 3.8109232633338648\n",
            "48     \t [0.57243427 0.52244131 0.84966788]. \t  3.7933269025666267 \t 3.8109232633338648\n",
            "49     \t [0.5979789  0.48916154 0.84989695]. \t  3.682931822178702 \t 3.8109232633338648\n",
            "50     \t [0.59636419 0.54783362 0.87691732]. \t  3.770914172592485 \t 3.8109232633338648\n",
            "51     \t [0.60847378 0.51927606 0.81892132]. \t  3.664013414464779 \t 3.8109232633338648\n",
            "52     \t [0.58425847 0.50892073 0.86576127]. \t  3.741096262609275 \t 3.8109232633338648\n",
            "53     \t [0.58054575 0.54123877 0.84751105]. \t  \u001b[92m3.81483188947558\u001b[0m \t 3.81483188947558\n",
            "54     \t [0.61918985 0.54519629 0.82303454]. \t  3.7114752028962164 \t 3.81483188947558\n",
            "55     \t [0.58547812 0.50628857 0.84877742]. \t  3.7480456679220264 \t 3.81483188947558\n",
            "56     \t [0.66008093 0.53156187 0.85414603]. \t  3.7888166384850885 \t 3.81483188947558\n",
            "57     \t [0.63007488 0.54492093 0.86706949]. \t  3.795460484016629 \t 3.81483188947558\n",
            "58     \t [0.58758267 0.49631014 0.8581185 ]. \t  3.710592830116659 \t 3.81483188947558\n",
            "59     \t [0.61198946 0.54906399 0.8762825 ]. \t  3.7708216877094256 \t 3.81483188947558\n",
            "60     \t [0.57733655 0.5117252  0.87996721]. \t  3.694059870230058 \t 3.81483188947558\n",
            "61     \t [0.61197883 0.54332829 0.83829748]. \t  3.7856475991652894 \t 3.81483188947558\n",
            "62     \t [0.58297424 0.53222155 0.86213454]. \t  3.803131380962757 \t 3.81483188947558\n",
            "63     \t [0.67216136 0.5329614  0.88103495]. \t  3.7169003246313825 \t 3.81483188947558\n",
            "64     \t [0.61558261 0.55137254 0.8453242 ]. \t  3.804975026274839 \t 3.81483188947558\n",
            "65     \t [0.59565804 0.55008768 0.90059725]. \t  3.6095457988501556 \t 3.81483188947558\n",
            "66     \t [0.59702661 0.5272772  0.81192152]. \t  3.6298548329766143 \t 3.81483188947558\n",
            "67     \t [0.59493429 0.57052471 0.8387672 ]. \t  3.7784436387615394 \t 3.81483188947558\n",
            "68     \t [0.61409979 0.51560174 0.83926959]. \t  3.749679747993351 \t 3.81483188947558\n",
            "69     \t [0.65073382 0.54660883 0.86836644]. \t  3.787772017406466 \t 3.81483188947558\n",
            "70     \t [0.6976596  0.52307289 0.86578167]. \t  3.7504786875375107 \t 3.81483188947558\n",
            "71     \t [0.67029387 0.52612055 0.87174068]. \t  3.746699965257604 \t 3.81483188947558\n",
            "72     \t [0.58977963 0.54320895 0.8610513 ]. \t  \u001b[92m3.815253350207353\u001b[0m \t 3.815253350207353\n",
            "73     \t [0.62127394 0.55318263 0.82543676]. \t  3.724003715486454 \t 3.815253350207353\n",
            "74     \t [0.05062916 0.00239021 0.80484225]. \t  0.25380715234130435 \t 3.815253350207353\n",
            "75     \t [0.62511163 0.53716409 0.89700374]. \t  3.622640151613865 \t 3.815253350207353\n",
            "76     \t [0.56730141 0.5381959  0.85179321]. \t  \u001b[92m3.8196501262347304\u001b[0m \t 3.8196501262347304\n",
            "77     \t [0.73452193 0.52203858 0.86652107]. \t  3.735621576078322 \t 3.8196501262347304\n",
            "78     \t [0.58071753 0.53079077 0.85335114]. \t  3.8079363337396916 \t 3.8196501262347304\n",
            "79     \t [0.59096901 0.51441065 0.81443882]. \t  3.6294216629844 \t 3.8196501262347304\n",
            "80     \t [0.72314477 0.53686341 0.88152923]. \t  3.7059515162902987 \t 3.8196501262347304\n",
            "81     \t [0.66382569 0.54828313 0.82499473]. \t  3.7083171262962225 \t 3.8196501262347304\n",
            "82     \t [0.57489787 0.51967927 0.85046247]. \t  3.787406401643103 \t 3.8196501262347304\n",
            "83     \t [0.60652648 0.52983666 0.88630589]. \t  3.695200357760867 \t 3.8196501262347304\n",
            "84     \t [0.64649618 0.58025492 0.86101013]. \t  3.77728306502684 \t 3.8196501262347304\n",
            "85     \t [0.63422126 0.52959655 0.85341136]. \t  3.792867181097813 \t 3.8196501262347304\n",
            "86     \t [0.68310297 0.54918576 0.86553445]. \t  3.786399513903068 \t 3.8196501262347304\n",
            "87     \t [0.62862293 0.54946895 0.82258305]. \t  3.705415147083148 \t 3.8196501262347304\n",
            "88     \t [0.68014498 0.51866751 0.86103235]. \t  3.754431266691809 \t 3.8196501262347304\n",
            "89     \t [0.63193462 0.52610816 0.85698195]. \t  3.7869145579583776 \t 3.8196501262347304\n",
            "90     \t [0.62136464 0.55208211 0.84588928]. \t  3.8044052007986413 \t 3.8196501262347304\n",
            "91     \t [0.54967485 0.51730815 0.85480168]. \t  3.787641495874505 \t 3.8196501262347304\n",
            "92     \t [0.63298341 0.54745437 0.85016875]. \t  3.8072475529989345 \t 3.8196501262347304\n",
            "93     \t [0.59975708 0.53993085 0.84249121]. \t  3.7991869092608033 \t 3.8196501262347304\n",
            "94     \t [0.66705872 0.54054641 0.85601073]. \t  3.796630833698272 \t 3.8196501262347304\n",
            "95     \t [0.68887593 0.505077   0.85705043]. \t  3.7183239599881794 \t 3.8196501262347304\n",
            "96     \t [0.71674464 0.53519416 0.8499544 ]. \t  3.7736325432653803 \t 3.8196501262347304\n",
            "97     \t [0.64352607 0.53963041 0.85298014]. \t  3.802141746464336 \t 3.8196501262347304\n",
            "98     \t [0.49793737 0.53963558 0.85039098]. \t  \u001b[92m3.83441365402527\u001b[0m \t 3.83441365402527\n",
            "99     \t [0.68432133 0.5549217  0.84908405]. \t  3.7902241723646832 \t 3.83441365402527\n",
            "100    \t [0.04587395 0.95571109 0.5809046 ]. \t  2.7741201145734795 \t 3.83441365402527\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI3FtfYSuv2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b154bb3d-a7a5-4f6c-e286-2b275d61c2d8"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_winner_18 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_18 = dGPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.8734294  0.96854066 0.86919454]. \t  0.722531189286755 \t 1.1210522139432408\n",
            "init   \t [0.53085569 0.23272833 0.0113988 ]. \t  0.1133646794700679 \t 1.1210522139432408\n",
            "init   \t [0.43046882 0.40235136 0.52267467]. \t  0.5232646985053151 \t 1.1210522139432408\n",
            "init   \t [0.4783918  0.55535647 0.54338602]. \t  1.0390748854353227 \t 1.1210522139432408\n",
            "init   \t [0.76089558 0.71237457 0.6196821 ]. \t  1.1210522139432408 \t 1.1210522139432408\n",
            "1      \t [0.28982083 0.95201409 0.18404427]. \t  0.04220734680078784 \t 1.1210522139432408\n",
            "2      \t [0.12883542 0.98258307 0.90180597]. \t  0.6910738837771888 \t 1.1210522139432408\n",
            "3      \t [0.54286833 0.73041576 0.81867901]. \t  \u001b[92m2.765888636449169\u001b[0m \t 2.765888636449169\n",
            "4      \t [0.64204955 0.40622708 0.95949622]. \t  2.1950874293706226 \t 2.765888636449169\n",
            "5      \t [0.40510596 0.760361   0.96167652]. \t  1.9196355889659544 \t 2.765888636449169\n",
            "6      \t [0.55732039 0.88146889 0.83249741]. \t  1.4292539801170538 \t 2.765888636449169\n",
            "7      \t [7.17993872e-15 8.61443954e-15 9.01730990e-15]. \t  0.06797411659014456 \t 2.765888636449169\n",
            "8      \t [0.03591706 0.95476209 0.07665491]. \t  0.0031251970395711403 \t 2.765888636449169\n",
            "9      \t [0.85504982 0.66874916 0.93976319]. \t  2.748684397547618 \t 2.765888636449169\n",
            "10     \t [0.18694771 0.58805442 0.8135521 ]. \t  \u001b[92m3.681009367444163\u001b[0m \t 3.681009367444163\n",
            "11     \t [0.0447135  0.54010732 0.85782954]. \t  \u001b[92m3.8135455746851665\u001b[0m \t 3.8135455746851665\n",
            "12     \t [0.00596352 0.72682047 0.77483297]. \t  2.709188816010623 \t 3.8135455746851665\n",
            "13     \t [0.08888028 0.35374352 0.94404956]. \t  1.9954683010064809 \t 3.8135455746851665\n",
            "14     \t [0.18989049 0.53361531 0.89159021]. \t  3.6818055981702527 \t 3.8135455746851665\n",
            "15     \t [0.19487023 0.60753049 0.83802195]. \t  3.7406875224332836 \t 3.8135455746851665\n",
            "16     \t [0.00242278 0.6514562  0.89701126]. \t  3.343616677379247 \t 3.8135455746851665\n",
            "17     \t [0.25012248 0.54918925 0.82079865]. \t  3.7666018079523047 \t 3.8135455746851665\n",
            "18     \t [0.15916754 0.52186345 0.82216695]. \t  3.7352871896022464 \t 3.8135455746851665\n",
            "19     \t [0.34575246 0.56265704 0.83329946]. \t  \u001b[92m3.8186006218466795\u001b[0m \t 3.8186006218466795\n",
            "20     \t [0.23082112 0.51981109 0.85882598]. \t  3.810184033282763 \t 3.8186006218466795\n",
            "21     \t [0.42915581 0.53058228 0.82110434]. \t  3.7382094072969525 \t 3.8186006218466795\n",
            "22     \t [0.23167983 0.57022001 0.83927308]. \t  \u001b[92m3.833431829223387\u001b[0m \t 3.833431829223387\n",
            "23     \t [0.28436178 0.53660734 0.83885419]. \t  3.8328285254927414 \t 3.833431829223387\n",
            "24     \t [0.20903543 0.53527477 0.86068952]. \t  \u001b[92m3.835675928940222\u001b[0m \t 3.835675928940222\n",
            "25     \t [0.12583939 0.53821094 0.85233906]. \t  3.8346421445733085 \t 3.835675928940222\n",
            "26     \t [0.24894912 0.47790573 0.86213164]. \t  3.642022766292432 \t 3.835675928940222\n",
            "27     \t [0.07086571 0.56023637 0.86266978]. \t  3.8216307810438535 \t 3.835675928940222\n",
            "28     \t [0.02132868 0.54595264 0.83637062]. \t  3.7914749281780824 \t 3.835675928940222\n",
            "29     \t [0.00331252 0.53487091 0.86747601]. \t  3.7737342888434715 \t 3.835675928940222\n",
            "30     \t [0.40522698 0.528548   0.83621893]. \t  3.8070325760135795 \t 3.835675928940222\n",
            "31     \t [0.44587369 0.5152545  0.83745294]. \t  3.77825884085659 \t 3.835675928940222\n",
            "32     \t [0.2555393  0.58405216 0.85276607]. \t  3.831288652879492 \t 3.835675928940222\n",
            "33     \t [0.24625681 0.58182187 0.85524631]. \t  3.8349769243952587 \t 3.835675928940222\n",
            "34     \t [0.12921315 0.53718732 0.84178851]. \t  3.8247641763365223 \t 3.835675928940222\n",
            "35     \t [0.1872391  0.58069704 0.85793729]. \t  3.830370859964452 \t 3.835675928940222\n",
            "36     \t [0.30152842 0.55281078 0.82864814]. \t  3.805872527480709 \t 3.835675928940222\n",
            "37     \t [0.30867282 0.53599935 0.83617995]. \t  3.8244030699044798 \t 3.835675928940222\n",
            "38     \t [0.51086117 0.50382122 0.84389691]. \t  3.7492985314573293 \t 3.835675928940222\n",
            "39     \t [0.614035   0.52157287 0.8324904 ]. \t  3.7387286124534307 \t 3.835675928940222\n",
            "40     \t [0.23913699 0.53010755 0.83224022]. \t  3.802341092036368 \t 3.835675928940222\n",
            "41     \t [0.17015443 0.59163767 0.84518252]. \t  3.801265339653119 \t 3.835675928940222\n",
            "42     \t [0.45714147 0.53038755 0.84909553]. \t  3.8286160024779514 \t 3.835675928940222\n",
            "43     \t [0.26655567 0.5419577  0.85334683]. \t  \u001b[92m3.8550489178935106\u001b[0m \t 3.8550489178935106\n",
            "44     \t [0.42910364 0.5163162  0.83708916]. \t  3.7820934377442765 \t 3.8550489178935106\n",
            "45     \t [0.10772309 0.54997123 0.85360419]. \t  3.8400010149605883 \t 3.8550489178935106\n",
            "46     \t [0.36250441 0.53151619 0.87473643]. \t  3.7915800797356334 \t 3.8550489178935106\n",
            "47     \t [0.20207097 0.54405791 0.87996598]. \t  3.775511067997826 \t 3.8550489178935106\n",
            "48     \t [0.29656481 0.55129041 0.83874328]. \t  3.8420931161896132 \t 3.8550489178935106\n",
            "49     \t [0.38746516 0.99872253 0.00499081]. \t  0.00028809199138130423 \t 3.8550489178935106\n",
            "50     \t [0.10870324 0.52629642 0.85912941]. \t  3.8060219726085043 \t 3.8550489178935106\n",
            "51     \t [0.33281268 0.5186048  0.85140888]. \t  3.8159882919334756 \t 3.8550489178935106\n",
            "52     \t [0.38174601 0.55120253 0.83690162]. \t  3.830989962824293 \t 3.8550489178935106\n",
            "53     \t [0.22847531 0.55990436 0.84515887]. \t  3.852720838376992 \t 3.8550489178935106\n",
            "54     \t [0.48846988 0.53048631 0.83130525]. \t  3.778352145978607 \t 3.8550489178935106\n",
            "55     \t [0.08819913 0.53473537 0.83699355]. \t  3.8017671738945986 \t 3.8550489178935106\n",
            "56     \t [0.88776006 0.02609699 0.0235836 ]. \t  0.06909933345804102 \t 3.8550489178935106\n",
            "57     \t [0.11495527 0.53963073 0.87486264]. \t  3.781982319063708 \t 3.8550489178935106\n",
            "58     \t [0.29717787 0.57300869 0.8447506 ]. \t  3.8425169512387827 \t 3.8550489178935106\n",
            "59     \t [0.0049983  0.5723884  0.84151796]. \t  3.7902964536281334 \t 3.8550489178935106\n",
            "60     \t [0.11290714 0.55516229 0.85302255]. \t  3.842340365296862 \t 3.8550489178935106\n",
            "61     \t [0.22666967 0.54644785 0.84039898]. \t  3.8427382067865827 \t 3.8550489178935106\n",
            "62     \t [0.50281973 0.54796341 0.84088715]. \t  3.8221796719388674 \t 3.8550489178935106\n",
            "63     \t [0.22561763 0.53120662 0.83477415]. \t  3.811764861557007 \t 3.8550489178935106\n",
            "64     \t [0.26700731 0.54886362 0.84085902]. \t  3.8466390128986183 \t 3.8550489178935106\n",
            "65     \t [0.21207953 0.54116504 0.85639421]. \t  3.8488357436712732 \t 3.8550489178935106\n",
            "66     \t [0.46557642 0.5400986  0.82171332]. \t  3.7437444824038657 \t 3.8550489178935106\n",
            "67     \t [0.25850233 0.57032689 0.82332421]. \t  3.7706735710434067 \t 3.8550489178935106\n",
            "68     \t [0.42217819 0.57596829 0.82326953]. \t  3.7429714867718706 \t 3.8550489178935106\n",
            "69     \t [0.4032964  0.55231343 0.83089933]. \t  3.805729384987856 \t 3.8550489178935106\n",
            "70     \t [0.06338911 0.55122028 0.84827929]. \t  3.828266808397376 \t 3.8550489178935106\n",
            "71     \t [0.00886577 0.53218103 0.86041436]. \t  3.7879565017613976 \t 3.8550489178935106\n",
            "72     \t [0.34654771 0.55473757 0.8076602 ]. \t  3.671320630261447 \t 3.8550489178935106\n",
            "73     \t [0.51812212 0.55557452 0.8302126 ]. \t  3.7784241654597692 \t 3.8550489178935106\n",
            "74     \t [0.36919956 0.54779968 0.84292088]. \t  3.8470531214101116 \t 3.8550489178935106\n",
            "75     \t [0.42165128 0.560889   0.85548007]. \t  3.851776726046721 \t 3.8550489178935106\n",
            "76     \t [0.31911455 0.5791383  0.84154603]. \t  3.82576785701343 \t 3.8550489178935106\n",
            "77     \t [0.39963324 0.54716019 0.82546921]. \t  3.779997944388147 \t 3.8550489178935106\n",
            "78     \t [0.04030326 0.52397973 0.86287257]. \t  3.7766900904333496 \t 3.8550489178935106\n",
            "79     \t [0.22859371 0.50472719 0.85164257]. \t  3.771132290777171 \t 3.8550489178935106\n",
            "80     \t [0.4143877  0.56511915 0.8480556 ]. \t  3.8462774224445333 \t 3.8550489178935106\n",
            "81     \t [0.28632899 0.5876058  0.8222311 ]. \t  3.7339411537903313 \t 3.8550489178935106\n",
            "82     \t [0.26192823 0.58523474 0.85969135]. \t  3.8255133698337525 \t 3.8550489178935106\n",
            "83     \t [0.30505916 0.51369216 0.83927652]. \t  3.7894649200944897 \t 3.8550489178935106\n",
            "84     \t [0.39074702 0.54229937 0.86096354]. \t  3.8461157972613416 \t 3.8550489178935106\n",
            "85     \t [0.18096075 0.53416297 0.87894521]. \t  3.7652703562790837 \t 3.8550489178935106\n",
            "86     \t [0.40659379 0.53382129 0.84797275]. \t  3.83893859483548 \t 3.8550489178935106\n",
            "87     \t [0.15528353 0.55656726 0.84261031]. \t  3.8412107590210267 \t 3.8550489178935106\n",
            "88     \t [0.226149   0.52419861 0.86114069]. \t  3.8162020808345076 \t 3.8550489178935106\n",
            "89     \t [0.25706772 0.57083037 0.83810976]. \t  3.8304533002978713 \t 3.8550489178935106\n",
            "90     \t [0.23084881 0.5549318  0.87730495]. \t  3.798747521629894 \t 3.8550489178935106\n",
            "91     \t [0.06716183 0.55098289 0.88314188]. \t  3.7370494059116544 \t 3.8550489178935106\n",
            "92     \t [0.17675091 0.59096359 0.83753805]. \t  3.787218306528965 \t 3.8550489178935106\n",
            "93     \t [0.04394746 0.56975231 0.87720643]. \t  3.760094543377745 \t 3.8550489178935106\n",
            "94     \t [0.23043249 0.51249124 0.85407828]. \t  3.795146878605015 \t 3.8550489178935106\n",
            "95     \t [0.22531628 0.54753524 0.86383117]. \t  3.843690927788908 \t 3.8550489178935106\n",
            "96     \t [0.34214814 0.56143431 0.83882939]. \t  3.837959687973556 \t 3.8550489178935106\n",
            "97     \t [0.18085215 0.58185102 0.83593636]. \t  3.8026178426436834 \t 3.8550489178935106\n",
            "98     \t [0.14760407 0.54705317 0.85704563]. \t  3.844197555986243 \t 3.8550489178935106\n",
            "99     \t [0.08006852 0.56918456 0.84343988]. \t  3.8202531144850767 \t 3.8550489178935106\n",
            "100    \t [0.24932198 0.54514921 0.87424064]. \t  3.809385358893562 \t 3.8550489178935106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YT-CgKvuv4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87956f72-25ef-48fd-bccf-90d88b10b746"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_winner_19 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_19 = dGPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.55944558 0.76149199 0.24453155]. \t  0.09433080659504385 \t 2.524990008735946\n",
            "init   \t [0.77168947 0.35447208 0.7966974 ]. \t  2.524990008735946 \t 2.524990008735946\n",
            "init   \t [0.34600002 0.44387444 0.30330359]. \t  0.3914194506273641 \t 2.524990008735946\n",
            "init   \t [0.44211392 0.57320936 0.06566402]. \t  0.036928944981920925 \t 2.524990008735946\n",
            "init   \t [0.02408889 0.82062263 0.36513765]. \t  0.9772047942100802 \t 2.524990008735946\n",
            "1      \t [0.96499061 0.19185662 0.98563058]. \t  0.5815385407774518 \t 2.524990008735946\n",
            "2      \t [0.35021227 0.89949578 0.95419986]. \t  0.9824452595236306 \t 2.524990008735946\n",
            "3      \t [0.86557359 0.48719963 0.73405917]. \t  \u001b[92m2.529321047962304\u001b[0m \t 2.529321047962304\n",
            "4      \t [0.67777399 0.46624492 0.84022585]. \t  \u001b[92m3.5425091977240846\u001b[0m \t 3.5425091977240846\n",
            "5      \t [0.5420832  0.44256445 0.98376186]. \t  2.0477303896288763 \t 3.5425091977240846\n",
            "6      \t [0.85628369 0.74998348 0.90253047]. \t  2.471819292444532 \t 3.5425091977240846\n",
            "7      \t [0.69016707 0.5585057  0.86524787]. \t  \u001b[92m3.784335493156992\u001b[0m \t 3.784335493156992\n",
            "8      \t [0.50681928 0.62915445 0.74708692]. \t  2.801014079471593 \t 3.784335493156992\n",
            "9      \t [0.85832412 0.51966317 0.99041339]. \t  2.1639417190659627 \t 3.784335493156992\n",
            "10     \t [0.01815437 0.38083575 0.00101819]. \t  0.04110188115182861 \t 3.784335493156992\n",
            "11     \t [0.75149645 0.60559607 0.81382304]. \t  3.473578803551014 \t 3.784335493156992\n",
            "12     \t [0.05592965 0.71493738 0.06335925]. \t  0.007651529371869512 \t 3.784335493156992\n",
            "13     \t [0.98986012 0.68494222 0.00835794]. \t  0.0016982651496915456 \t 3.784335493156992\n",
            "14     \t [0.70705298 0.60738886 0.83471011]. \t  3.6222073673961646 \t 3.784335493156992\n",
            "15     \t [0.63553833 0.66238037 0.86739057]. \t  3.3991381127617157 \t 3.784335493156992\n",
            "16     \t [0.59271082 0.59342639 0.82230856]. \t  3.6451371850189216 \t 3.784335493156992\n",
            "17     \t [0.96675485 0.90630054 0.1134832 ]. \t  0.0012813855195354267 \t 3.784335493156992\n",
            "18     \t [0.61429976 0.55980825 0.78928857]. \t  3.4102698317490967 \t 3.784335493156992\n",
            "19     \t [0.72803622 0.60701502 0.83022901]. \t  3.5922579487147983 \t 3.784335493156992\n",
            "20     \t [0.65068547 0.56298381 0.80672754]. \t  3.566337726316838 \t 3.784335493156992\n",
            "21     \t [0.70944355 0.52706367 0.86641217]. \t  3.7534339621996375 \t 3.784335493156992\n",
            "22     \t [0.64656126 0.58533531 0.85669418]. \t  3.7677417803146644 \t 3.784335493156992\n",
            "23     \t [0.6634897  0.56044041 0.85531313]. \t  \u001b[92m3.7993012109231934\u001b[0m \t 3.7993012109231934\n",
            "24     \t [0.64813546 0.57916233 0.86400736]. \t  3.7755920389835076 \t 3.7993012109231934\n",
            "25     \t [0.68265673 0.57802356 0.81506806]. \t  3.5980235136032395 \t 3.7993012109231934\n",
            "26     \t [0.69965244 0.55218355 0.86045515]. \t  3.7891748157425735 \t 3.7993012109231934\n",
            "27     \t [0.69748996 0.5287149  0.8436245 ]. \t  3.76118323790693 \t 3.7993012109231934\n",
            "28     \t [0.71028837 0.50238245 0.83671911]. \t  3.67860690957045 \t 3.7993012109231934\n",
            "29     \t [0.01300604 0.4880764  0.36422522]. \t  0.3601955591631558 \t 3.7993012109231934\n",
            "30     \t [0.66764952 0.56492334 0.83467052]. \t  3.746261206199707 \t 3.7993012109231934\n",
            "31     \t [0.70083371 0.51246534 0.8685604 ]. \t  3.7158637776145906 \t 3.7993012109231934\n",
            "32     \t [0.7076945  0.55551048 0.83049328]. \t  3.7205360934649 \t 3.7993012109231934\n",
            "33     \t [0.71738641 0.53037847 0.8248302 ]. \t  3.680402026662153 \t 3.7993012109231934\n",
            "34     \t [0.65906202 0.52960367 0.83542641]. \t  3.74969033841937 \t 3.7993012109231934\n",
            "35     \t [0.6969861  0.55112827 0.85468914]. \t  3.791707709032377 \t 3.7993012109231934\n",
            "36     \t [0.67853592 0.59711628 0.82182333]. \t  3.5967251921773977 \t 3.7993012109231934\n",
            "37     \t [0.7243727  0.56747249 0.81785269]. \t  3.6218758693707835 \t 3.7993012109231934\n",
            "38     \t [0.76705603 0.47689431 0.8894142 ]. \t  3.4452099581967177 \t 3.7993012109231934\n",
            "39     \t [0.72482027 0.54348644 0.86823103]. \t  3.7649888184974802 \t 3.7993012109231934\n",
            "40     \t [0.76383983 0.59697617 0.80579429]. \t  3.4281125527432676 \t 3.7993012109231934\n",
            "41     \t [0.70767798 0.58584516 0.86503148]. \t  3.7421207884302286 \t 3.7993012109231934\n",
            "42     \t [0.6487773  0.62776917 0.84276894]. \t  3.5829739247957026 \t 3.7993012109231934\n",
            "43     \t [0.62013906 0.54899063 0.87203273]. \t  3.78539064416137 \t 3.7993012109231934\n",
            "44     \t [0.71496841 0.60924279 0.9067699 ]. \t  3.4408900487110885 \t 3.7993012109231934\n",
            "45     \t [0.65453249 0.57777615 0.84270789]. \t  3.759932410397071 \t 3.7993012109231934\n",
            "46     \t [0.67199992 0.59505258 0.79861672]. \t  3.407738902971558 \t 3.7993012109231934\n",
            "47     \t [0. 0. 0.]. \t  0.06797411659013229 \t 3.7993012109231934\n",
            "48     \t [0.56427463 0.55216963 0.86277665]. \t  \u001b[92m3.8221804370408234\u001b[0m \t 3.8221804370408234\n",
            "49     \t [0.64286673 0.58665394 0.90500177]. \t  3.5343362258603106 \t 3.8221804370408234\n",
            "50     \t [0.71055695 0.57177418 0.86635969]. \t  3.7646186807958135 \t 3.8221804370408234\n",
            "51     \t [0.60829302 0.5809273  0.81066048]. \t  3.5859202068124065 \t 3.8221804370408234\n",
            "52     \t [0.04667617 0.99571649 0.01551155]. \t  0.00046442370310982025 \t 3.8221804370408234\n",
            "53     \t [0.69871387 0.51928066 0.83097401]. \t  3.7022737979189584 \t 3.8221804370408234\n",
            "54     \t [0.71166296 0.55240373 0.82230046]. \t  3.6730146316936345 \t 3.8221804370408234\n",
            "55     \t [0.76237869 0.45380575 0.85970752]. \t  3.4469712512099235 \t 3.8221804370408234\n",
            "56     \t [0.66177371 0.46988187 0.84320287]. \t  3.570732562623527 \t 3.8221804370408234\n",
            "57     \t [0.67537958 0.5805822  0.89210617]. \t  3.6451355734876127 \t 3.8221804370408234\n",
            "58     \t [0.67549165 0.57683914 0.88171446]. \t  3.7144223095973032 \t 3.8221804370408234\n",
            "59     \t [0.72354824 0.48856965 0.86554863]. \t  3.631397860292344 \t 3.8221804370408234\n",
            "60     \t [0.70154091 0.52221157 0.83675095]. \t  3.7297920880847006 \t 3.8221804370408234\n",
            "61     \t [0.7176124  0.51439531 0.85981475]. \t  3.734594171173123 \t 3.8221804370408234\n",
            "62     \t [0.7364998  0.54861474 0.82378105]. \t  3.6743966827647867 \t 3.8221804370408234\n",
            "63     \t [0.68685817 0.61696091 0.83014656]. \t  3.5662587458875064 \t 3.8221804370408234\n",
            "64     \t [0.58725436 0.57189871 0.88467204]. \t  3.726018810600891 \t 3.8221804370408234\n",
            "65     \t [0.73235239 0.52208521 0.85068275]. \t  3.7494714875863564 \t 3.8221804370408234\n",
            "66     \t [0.69291246 0.58764118 0.84461013]. \t  3.729578071863271 \t 3.8221804370408234\n",
            "67     \t [0.69828274 0.50251761 0.88046117]. \t  3.6338209255331004 \t 3.8221804370408234\n",
            "68     \t [0.61008552 0.55264991 0.90414026]. \t  3.5744285766442516 \t 3.8221804370408234\n",
            "69     \t [0.69198461 0.55497328 0.81303567]. \t  3.6114204198307815 \t 3.8221804370408234\n",
            "70     \t [0.6873927 0.5659863 0.8378493]. \t  3.751114719943719 \t 3.8221804370408234\n",
            "71     \t [0.74679688 0.49429476 0.82368836]. \t  3.5846212718215407 \t 3.8221804370408234\n",
            "72     \t [0.7126786  0.56220312 0.84641829]. \t  3.77172541908642 \t 3.8221804370408234\n",
            "73     \t [0.79712385 0.56259683 0.83782797]. \t  3.71397034104081 \t 3.8221804370408234\n",
            "74     \t [0.81209002 0.54359238 0.85922037]. \t  3.7496050719889626 \t 3.8221804370408234\n",
            "75     \t [0.58303257 0.57569302 0.82317786]. \t  3.6974812088065185 \t 3.8221804370408234\n",
            "76     \t [0.64610137 0.55901758 0.88366145]. \t  3.7267751122153485 \t 3.8221804370408234\n",
            "77     \t [0.6163959  0.54787736 0.8565755 ]. \t  3.8144688911684255 \t 3.8221804370408234\n",
            "78     \t [0.64151007 0.57195287 0.85381111]. \t  3.7933944351160767 \t 3.8221804370408234\n",
            "79     \t [0.81896748 0.56599191 0.84257249]. \t  3.718012739830349 \t 3.8221804370408234\n",
            "80     \t [0.9630822  0.51501552 0.87962545]. \t  3.586414680171297 \t 3.8221804370408234\n",
            "81     \t [0.77957498 0.53710347 0.86056285]. \t  3.755157799575473 \t 3.8221804370408234\n",
            "82     \t [0.7557866  0.54442595 0.85825863]. \t  3.7705186305804386 \t 3.8221804370408234\n",
            "83     \t [0.57544552 0.5196964  0.86841575]. \t  3.7654102321918392 \t 3.8221804370408234\n",
            "84     \t [0.74453636 0.56433216 0.84111684]. \t  3.744049718673875 \t 3.8221804370408234\n",
            "85     \t [0.78257334 0.54892414 0.83681193]. \t  3.7236061400145526 \t 3.8221804370408234\n",
            "86     \t [0.79123365 0.52559984 0.82875226]. \t  3.669113804519274 \t 3.8221804370408234\n",
            "87     \t [0.75725417 0.54587903 0.83555404]. \t  3.728102200929457 \t 3.8221804370408234\n",
            "88     \t [0.72875174 0.53472784 0.84756278]. \t  3.7660260590804735 \t 3.8221804370408234\n",
            "89     \t [0.73938036 0.54311779 0.87102112]. \t  3.752228655336591 \t 3.8221804370408234\n",
            "90     \t [0.63352853 0.53107824 0.85148653]. \t  3.7944695590671333 \t 3.8221804370408234\n",
            "91     \t [0.68456445 0.54423895 0.85342511]. \t  3.7933934766083532 \t 3.8221804370408234\n",
            "92     \t [0.60505544 0.53589539 0.85602292]. \t  3.808715255084141 \t 3.8221804370408234\n",
            "93     \t [0.70817892 0.56315502 0.88432046]. \t  3.7049782654019414 \t 3.8221804370408234\n",
            "94     \t [0.83386531 0.57800352 0.87476896]. \t  3.6903712842551255 \t 3.8221804370408234\n",
            "95     \t [0.66632944 0.54175039 0.85409484]. \t  3.7976238835704015 \t 3.8221804370408234\n",
            "96     \t [0.68675172 0.55496261 0.86301733]. \t  3.7899366951104865 \t 3.8221804370408234\n",
            "97     \t [0.02588759 0.89968274 0.99737802]. \t  0.7161611664889634 \t 3.8221804370408234\n",
            "98     \t [0.87241843 0.54585338 0.85944011]. \t  3.7271391707406742 \t 3.8221804370408234\n",
            "99     \t [0.73354764 0.52565922 0.8675283 ]. \t  3.740776610777362 \t 3.8221804370408234\n",
            "100    \t [0.75682143 0.55949937 0.84086963]. \t  3.743237146290065 \t 3.8221804370408234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHz_Jg2_uv7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de29880f-2477-4b26-fcc0-871b3c28642c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_winner_20 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_20 = dGPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.57051729 0.56452876 0.48844183]. \t  0.675391399411646 \t 0.675391399411646\n",
            "init   \t [0.33647775 0.37586818 0.53203587]. \t  0.5331349538596052 \t 0.675391399411646\n",
            "init   \t [0.06810629 0.58452906 0.23789776]. \t  0.14747335095307315 \t 0.675391399411646\n",
            "init   \t [0.16075658 0.15211915 0.12706922]. \t  0.48089725804912437 \t 0.675391399411646\n",
            "init   \t [0.32744117 0.69415387 0.35896647]. \t  0.6289408047543804 \t 0.675391399411646\n",
            "1      \t [0.837611   0.97862966 0.10314233]. \t  0.0013418779384681767 \t 0.675391399411646\n",
            "2      \t [0.45636436 0.96313765 0.9652847 ]. \t  0.5824465268628222 \t 0.675391399411646\n",
            "3      \t [0.67634823 0.00287263 0.01121946]. \t  0.09244511485253058 \t 0.675391399411646\n",
            "4      \t [0.56803068 0.99098437 0.50767036]. \t  \u001b[92m1.310904866486966\u001b[0m \t 1.310904866486966\n",
            "5      \t [0.70676205 0.94012438 0.69570272]. \t  0.8730461226539943 \t 1.310904866486966\n",
            "6      \t [0.35053001 0.97725529 0.52543404]. \t  \u001b[92m2.2220332954136595\u001b[0m \t 2.2220332954136595\n",
            "7      \t [0.03402977 0.98853375 0.5817744 ]. \t  \u001b[92m2.5720392312337808\u001b[0m \t 2.5720392312337808\n",
            "8      \t [0.99370161 0.27096475 0.95603809]. \t  1.208901618613421 \t 2.5720392312337808\n",
            "9      \t [0.11964012 0.95307846 0.57102   ]. \t  \u001b[92m2.8449336013454536\u001b[0m \t 2.8449336013454536\n",
            "10     \t [0. 0. 0.]. \t  0.06797411659013229 \t 2.8449336013454536\n",
            "11     \t [1.         0.54177811 0.29818221]. \t  0.062192615518285047 \t 2.8449336013454536\n",
            "12     \t [0.0989103  0.98431367 0.66221182]. \t  2.0285277629074137 \t 2.8449336013454536\n",
            "13     \t [0.11491845 0.92415466 0.53339618]. \t  \u001b[92m2.9191944053853316\u001b[0m \t 2.9191944053853316\n",
            "14     \t [0.03601295 0.83167567 0.56057103]. \t  \u001b[92m3.016700690003538\u001b[0m \t 3.016700690003538\n",
            "15     \t [0.08983609 0.81797578 0.51035207]. \t  2.794279687754132 \t 3.016700690003538\n",
            "16     \t [0.04530916 0.96196939 0.42665803]. \t  1.6777695283429288 \t 3.016700690003538\n",
            "17     \t [0.9840872  0.8157324  0.02030657]. \t  0.00044614405060649316 \t 3.016700690003538\n",
            "18     \t [0.92571885 0.78937808 0.99986933]. \t  1.2483590303791356 \t 3.016700690003538\n",
            "19     \t [0.96908539 0.12871887 0.3029565 ]. \t  0.3268652021316285 \t 3.016700690003538\n",
            "20     \t [0.07082467 0.73699452 0.50631414]. \t  2.396671476586378 \t 3.016700690003538\n",
            "21     \t [0.41188991 0.00410914 0.93329557]. \t  0.16721483691335176 \t 3.016700690003538\n",
            "22     \t [0.79656334 0.39283349 0.99992005]. \t  1.524101880081512 \t 3.016700690003538\n",
            "23     \t [0.65643882 0.00458179 0.6752822 ]. \t  0.18418782093946884 \t 3.016700690003538\n",
            "24     \t [0.04025223 0.94665343 0.57761347]. \t  2.8215927413587356 \t 3.016700690003538\n",
            "25     \t [0.28634431 0.86976507 0.56834181]. \t  2.8178420926861314 \t 3.016700690003538\n",
            "26     \t [0.20565907 0.84867382 0.62753628]. \t  2.761468570345886 \t 3.016700690003538\n",
            "27     \t [0.00560039 0.83621531 0.5670785 ]. \t  2.979184208707876 \t 3.016700690003538\n",
            "28     \t [0.26022755 0.94512016 0.60619099]. \t  2.55818934338015 \t 3.016700690003538\n",
            "29     \t [0.11961685 0.88369196 0.58788334]. \t  \u001b[92m3.0240556551083606\u001b[0m \t 3.0240556551083606\n",
            "30     \t [0.15002604 0.84524066 0.55101876]. \t  \u001b[92m3.0526304276491967\u001b[0m \t 3.0526304276491967\n",
            "31     \t [0.01625753 0.84421325 0.56160998]. \t  3.0044427666019136 \t 3.0526304276491967\n",
            "32     \t [0.08899621 0.93110733 0.58908234]. \t  2.8837468580240753 \t 3.0526304276491967\n",
            "33     \t [0.06476248 0.86915754 0.52440657]. \t  2.9487548700327113 \t 3.0526304276491967\n",
            "34     \t [0.15525218 0.89752008 0.60991497]. \t  2.8620167005503134 \t 3.0526304276491967\n",
            "35     \t [0.04752808 0.86455745 0.58283419]. \t  3.0277326056679015 \t 3.0526304276491967\n",
            "36     \t [0.25618774 0.89944606 0.55771059]. \t  2.8598493292744194 \t 3.0526304276491967\n",
            "37     \t [0.10260885 0.83026617 0.58106232]. \t  3.0499367335768675 \t 3.0526304276491967\n",
            "38     \t [0.20076283 0.92022831 0.592371  ]. \t  2.8432474674145265 \t 3.0526304276491967\n",
            "39     \t [0.13817756 0.81138472 0.62125798]. \t  2.857569458485345 \t 3.0526304276491967\n",
            "40     \t [0.03370541 0.88740203 0.58465359]. \t  2.9806040196427768 \t 3.0526304276491967\n",
            "41     \t [0.25180827 0.87165148 0.64428695]. \t  2.5251442369560366 \t 3.0526304276491967\n",
            "42     \t [0.05721426 0.74202101 0.52541702]. \t  2.555494028136048 \t 3.0526304276491967\n",
            "43     \t [0.01635725 0.22255791 0.00775594]. \t  0.08166647645591277 \t 3.0526304276491967\n",
            "44     \t [0.12810903 0.80756399 0.58674521]. \t  2.993333958487421 \t 3.0526304276491967\n",
            "45     \t [0.20872583 0.9015854  0.6226724 ]. \t  2.6985660080715066 \t 3.0526304276491967\n",
            "46     \t [0.00245857 0.88727939 0.53122361]. \t  2.8928228205605495 \t 3.0526304276491967\n",
            "47     \t [0.05309948 0.85593272 0.58176581]. \t  3.0384098284119223 \t 3.0526304276491967\n",
            "48     \t [0.05503549 0.76023318 0.54133674]. \t  2.738089006645524 \t 3.0526304276491967\n",
            "49     \t [0.02470671 0.86273666 0.50322971]. \t  2.7429903638004762 \t 3.0526304276491967\n",
            "50     \t [0.11656346 0.92248508 0.61113877]. \t  2.7919763221031024 \t 3.0526304276491967\n",
            "51     \t [0.08972856 0.90748074 0.54502423]. \t  3.0042496620137893 \t 3.0526304276491967\n",
            "52     \t [0.09126893 0.85627198 0.57362721]. \t  \u001b[92m3.0795556847092755\u001b[0m \t 3.0795556847092755\n",
            "53     \t [0.03213989 0.86579997 0.55874047]. \t  3.0335013492903213 \t 3.0795556847092755\n",
            "54     \t [0.14318656 0.7951634  0.53078135]. \t  2.8565655406890427 \t 3.0795556847092755\n",
            "55     \t [0.02964672 0.81601455 0.63603221]. \t  2.7296308013686392 \t 3.0795556847092755\n",
            "56     \t [0.12041283 0.94269123 0.55158042]. \t  2.898870916124141 \t 3.0795556847092755\n",
            "57     \t [1.         1.         0.15244024]. \t  0.0019245729408650023 \t 3.0795556847092755\n",
            "58     \t [0.18013213 0.79845486 0.5856727 ]. \t  2.9344308903218317 \t 3.0795556847092755\n",
            "59     \t [0.13289139 0.8127801  0.56489065]. \t  3.023136064341117 \t 3.0795556847092755\n",
            "60     \t [0.08837236 0.79893603 0.57712332]. \t  2.9837794407138287 \t 3.0795556847092755\n",
            "61     \t [0.10568601 0.90000091 0.52474549]. \t  2.9369599573995506 \t 3.0795556847092755\n",
            "62     \t [0.10289429 0.83610942 0.56307164]. \t  3.072162012727591 \t 3.0795556847092755\n",
            "63     \t [0.26153946 0.88397673 0.54497229]. \t  2.849064535702583 \t 3.0795556847092755\n",
            "64     \t [0.01665586 0.79178508 0.57283838]. \t  2.895149336575522 \t 3.0795556847092755\n",
            "65     \t [0.08748043 0.79993283 0.54509611]. \t  2.9443279623877734 \t 3.0795556847092755\n",
            "66     \t [0.17876634 0.92219648 0.55058298]. \t  2.934889847869157 \t 3.0795556847092755\n",
            "67     \t [0.08401156 0.9191324  0.56276691]. \t  2.9888378221995704 \t 3.0795556847092755\n",
            "68     \t [0.10664126 0.87808033 0.49010619]. \t  2.6696921771929887 \t 3.0795556847092755\n",
            "69     \t [0.03832642 0.90438127 0.52913695]. \t  2.9089643967501257 \t 3.0795556847092755\n",
            "70     \t [0.23390535 0.78656131 0.60222983]. \t  2.7826084578603667 \t 3.0795556847092755\n",
            "71     \t [0.16149975 0.93162881 0.56706604]. \t  2.9232640308640874 \t 3.0795556847092755\n",
            "72     \t [0.17205665 0.86117926 0.57179856]. \t  3.050176936390939 \t 3.0795556847092755\n",
            "73     \t [0.10342226 0.75641489 0.55055739]. \t  2.7767699913876296 \t 3.0795556847092755\n",
            "74     \t [0.02339994 0.88233641 0.59957187]. \t  2.9109982004823864 \t 3.0795556847092755\n",
            "75     \t [0.23556698 0.82860431 0.59314515]. \t  2.8845042817751136 \t 3.0795556847092755\n",
            "76     \t [0.25324931 0.86674756 0.5521361 ]. \t  2.8974649482061445 \t 3.0795556847092755\n",
            "77     \t [0.13712076 0.77864584 0.59670231]. \t  2.885185579849039 \t 3.0795556847092755\n",
            "78     \t [0.64590738 0.80915796 0.98625708]. \t  1.3039552297178867 \t 3.0795556847092755\n",
            "79     \t [0.1672906  0.86576073 0.5887243 ]. \t  3.012761329619867 \t 3.0795556847092755\n",
            "80     \t [0.15139254 0.88610508 0.52223476]. \t  2.926697970654429 \t 3.0795556847092755\n",
            "81     \t [0.05029448 0.86327821 0.55453949]. \t  3.051123911003168 \t 3.0795556847092755\n",
            "82     \t [0.19765774 0.84917295 0.57254732]. \t  3.013645637125004 \t 3.0795556847092755\n",
            "83     \t [0.06865483 0.81492946 0.52137448]. \t  2.8585607593811635 \t 3.0795556847092755\n",
            "84     \t [0.01945563 0.89060591 0.52845086]. \t  2.9031787981463406 \t 3.0795556847092755\n",
            "85     \t [0.21183769 0.87915207 0.53607926]. \t  2.930568961058633 \t 3.0795556847092755\n",
            "86     \t [0.12348541 0.87467047 0.61419008]. \t  2.8949445727880057 \t 3.0795556847092755\n",
            "87     \t [0.07116666 0.84739824 0.6085107 ]. \t  2.942090089978891 \t 3.0795556847092755\n",
            "88     \t [0.22808648 0.90157054 0.54166901]. \t  2.8885523441430068 \t 3.0795556847092755\n",
            "89     \t [0.09092935 0.86760648 0.59166713]. \t  3.0257442513563886 \t 3.0795556847092755\n",
            "90     \t [0.20105515 0.75205953 0.59408717]. \t  2.734062502860064 \t 3.0795556847092755\n",
            "91     \t [0.20176141 0.85762051 0.52755783]. \t  2.908813555615129 \t 3.0795556847092755\n",
            "92     \t [0.24823419 0.78208001 0.63846099]. \t  2.610763097363748 \t 3.0795556847092755\n",
            "93     \t [0.05784182 0.85844821 0.6649175 ]. \t  2.491112534809163 \t 3.0795556847092755\n",
            "94     \t [0.05252533 0.81183737 0.5272291 ]. \t  2.873862121172054 \t 3.0795556847092755\n",
            "95     \t [0.15415997 0.93497728 0.55776565]. \t  2.9197880170428867 \t 3.0795556847092755\n",
            "96     \t [0.16285478 0.81034431 0.57349213]. \t  2.9974456328830406 \t 3.0795556847092755\n",
            "97     \t [0.06405024 0.79338052 0.50741756]. \t  2.676775238976352 \t 3.0795556847092755\n",
            "98     \t [0.12030438 0.91052613 0.57802532]. \t  2.996434674630926 \t 3.0795556847092755\n",
            "99     \t [0.10318104 0.85583356 0.61536053]. \t  2.9090738170584958 \t 3.0795556847092755\n",
            "100    \t [0.10439075 0.89441543 0.60544802]. \t  2.915136011842499 \t 3.0795556847092755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUnhsKpCuv9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f85cd984-f90b-4eb3-b051-c2c4ab700047"
      },
      "source": [
        "end_win = time.time()\r\n",
        "end_win\r\n",
        "\r\n",
        "time_win = end_win - start_win\r\n",
        "time_win"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22324.984003543854"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJG0SLpwuwAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "416754c5-884a-4854-cb49-872ebd2a54b5"
      },
      "source": [
        "### Training regret minimization: run number = 1\r\n",
        "\r\n",
        "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_1 = np.log(y_global_orig - loser_output_1)\r\n",
        "regret_winner_1 = np.log(y_global_orig - winner_output_1)\r\n",
        "\r\n",
        "train_regret_loser_1 = min_max_array(regret_loser_1)\r\n",
        "train_regret_winner_1 = min_max_array(regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1 = min(train_regret_loser_1)\r\n",
        "min_train_regret_winner_1 = min(train_regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1, min_train_regret_winner_1"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.282774727971788, -5.928720766418879)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lA9eZf0uwCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95766bbf-49d6-453d-91c6-145a86d13950"
      },
      "source": [
        "### Training regret minimization: run number = 2\r\n",
        "\r\n",
        "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_2 = np.log(y_global_orig - loser_output_2)\r\n",
        "regret_winner_2 = np.log(y_global_orig - winner_output_2)\r\n",
        "\r\n",
        "train_regret_loser_2 = min_max_array(regret_loser_2)\r\n",
        "train_regret_winner_2 = min_max_array(regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2 = min(train_regret_loser_2)\r\n",
        "min_train_regret_winner_2 = min(train_regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2, min_train_regret_winner_2"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.8749236392611095, -4.1444124502104005)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glrTGcpAuwFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2665d60-5713-40fa-8cdb-f8acb6879298"
      },
      "source": [
        "### Training regret minimization: run number = 3\r\n",
        "\r\n",
        "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_3 = np.log(y_global_orig - loser_output_3)\r\n",
        "regret_winner_3 = np.log(y_global_orig - winner_output_3)\r\n",
        "\r\n",
        "train_regret_loser_3 = min_max_array(regret_loser_3)\r\n",
        "train_regret_winner_3 = min_max_array(regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3 = min(train_regret_loser_3)\r\n",
        "min_train_regret_winner_3 = min(train_regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3, min_train_regret_winner_3"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.6160962144747533, -2.9000832718585774)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaRwfbxeuwIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1d13acf-7178-4673-91f7-c958c6ef2bad"
      },
      "source": [
        "### Training regret minimization: run number = 4\r\n",
        "\r\n",
        "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_4 = np.log(y_global_orig - loser_output_4)\r\n",
        "regret_winner_4 = np.log(y_global_orig - winner_output_4)\r\n",
        "\r\n",
        "train_regret_loser_4 = min_max_array(regret_loser_4)\r\n",
        "train_regret_winner_4 = min_max_array(regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4 = min(train_regret_loser_4)\r\n",
        "min_train_regret_winner_4 = min(train_regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4, min_train_regret_winner_4"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.854269340274048, -5.960397047430879)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nb5NkfyuwKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46bfa7c4-5be7-4e01-816d-474da53377a7"
      },
      "source": [
        "### Training regret minimization: run number = 5\r\n",
        "\r\n",
        "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_5 = np.log(y_global_orig - loser_output_5)\r\n",
        "regret_winner_5 = np.log(y_global_orig - winner_output_5)\r\n",
        "\r\n",
        "train_regret_loser_5 = min_max_array(regret_loser_5)\r\n",
        "train_regret_winner_5 = min_max_array(regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5 = min(train_regret_loser_5)\r\n",
        "min_train_regret_winner_5 = min(train_regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5, min_train_regret_winner_5"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.9864402258065152, -4.462699866000511)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0Q-WfXbuwNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7409255-45a5-450b-a1d7-24e4e20440b9"
      },
      "source": [
        "### Training regret minimization: run number = 6\r\n",
        "\r\n",
        "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_6 = np.log(y_global_orig - loser_output_6)\r\n",
        "regret_winner_6 = np.log(y_global_orig - winner_output_6)\r\n",
        "\r\n",
        "train_regret_loser_6 = min_max_array(regret_loser_6)\r\n",
        "train_regret_winner_6 = min_max_array(regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6 = min(train_regret_loser_6)\r\n",
        "min_train_regret_winner_6 = min(train_regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6, min_train_regret_winner_6"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.652528982265508, -2.5017970988908034)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqqS7VLcuwPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a53196c-de16-456c-9c4e-4f7fe3f2ce5b"
      },
      "source": [
        "### Training regret minimization: run number = 7\r\n",
        "\r\n",
        "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_7 = np.log(y_global_orig - loser_output_7)\r\n",
        "regret_winner_7 = np.log(y_global_orig - winner_output_7)\r\n",
        "\r\n",
        "train_regret_loser_7 = min_max_array(regret_loser_7)\r\n",
        "train_regret_winner_7 = min_max_array(regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7 = min(train_regret_loser_7)\r\n",
        "min_train_regret_winner_7 = min(train_regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7, min_train_regret_winner_7"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.427527665591056, -4.88401892699998)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOi5iX8guwSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d652bfa0-e71a-476a-f2e2-d5975dd3903d"
      },
      "source": [
        "### Training regret minimization: run number = 8\r\n",
        "\r\n",
        "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_8 = np.log(y_global_orig - loser_output_8)\r\n",
        "regret_winner_8 = np.log(y_global_orig - winner_output_8)\r\n",
        "\r\n",
        "train_regret_loser_8 = min_max_array(regret_loser_8)\r\n",
        "train_regret_winner_8 = min_max_array(regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8 = min(train_regret_loser_8)\r\n",
        "min_train_regret_winner_8 = min(train_regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8, min_train_regret_winner_8"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.405697629363659, -4.789315712846356)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFVApeazuwU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d691fa-b44f-4f33-ac49-d718df6bef2a"
      },
      "source": [
        "### Training regret minimization: run number = 9\r\n",
        "\r\n",
        "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_9 = np.log(y_global_orig - loser_output_9)\r\n",
        "regret_winner_9 = np.log(y_global_orig - winner_output_9)\r\n",
        "\r\n",
        "train_regret_loser_9 = min_max_array(regret_loser_9)\r\n",
        "train_regret_winner_9 = min_max_array(regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9 = min(train_regret_loser_9)\r\n",
        "min_train_regret_winner_9 = min(train_regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9, min_train_regret_winner_9"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.9276396500365369, -4.09586316843285)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g92jk9WJuwXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c0bbe6-14b6-4766-ec2e-06eb2ddf2021"
      },
      "source": [
        "### Training regret minimization: run number = 10\r\n",
        "\r\n",
        "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_10 = np.log(y_global_orig - loser_output_10)\r\n",
        "regret_winner_10 = np.log(y_global_orig - winner_output_10)\r\n",
        "\r\n",
        "train_regret_loser_10 = min_max_array(regret_loser_10)\r\n",
        "train_regret_winner_10 = min_max_array(regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10 = min(train_regret_loser_10)\r\n",
        "min_train_regret_winner_10 = min(train_regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10, min_train_regret_winner_10"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.301444590661275, -4.014671922732309)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmcF1x-NuwZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19dc150c-834d-4675-939c-0f91459fb9b2"
      },
      "source": [
        "### Training regret minimization: run number = 11\r\n",
        "\r\n",
        "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_11 = np.log(y_global_orig - loser_output_11)\r\n",
        "regret_winner_11 = np.log(y_global_orig - winner_output_11)\r\n",
        "\r\n",
        "train_regret_loser_11 = min_max_array(regret_loser_11)\r\n",
        "train_regret_winner_11 = min_max_array(regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11 = min(train_regret_loser_11)\r\n",
        "min_train_regret_winner_11 = min(train_regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11, min_train_regret_winner_11"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.451291654399481, -4.451291654399481)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8axVhb6uwcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a08b26-436b-4339-f980-2f7aac1eb20c"
      },
      "source": [
        "### Training regret minimization: run number = 12\r\n",
        "\r\n",
        "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_12 = np.log(y_global_orig - loser_output_12)\r\n",
        "regret_winner_12 = np.log(y_global_orig - winner_output_12)\r\n",
        "\r\n",
        "train_regret_loser_12 = min_max_array(regret_loser_12)\r\n",
        "train_regret_winner_12 = min_max_array(regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12 = min(train_regret_loser_12)\r\n",
        "min_train_regret_winner_12 = min(train_regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12, min_train_regret_winner_12"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.1007531149503116, -4.258853856626351)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzlrL8XFuwfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75f8dbf6-342d-471c-8ce5-256dde3fbabf"
      },
      "source": [
        "### Training regret minimization: run number = 13\r\n",
        "\r\n",
        "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_13 = np.log(y_global_orig - loser_output_13)\r\n",
        "regret_winner_13 = np.log(y_global_orig - winner_output_13)\r\n",
        "\r\n",
        "train_regret_loser_13 = min_max_array(regret_loser_13)\r\n",
        "train_regret_winner_13 = min_max_array(regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13 = min(train_regret_loser_13)\r\n",
        "min_train_regret_winner_13 = min(train_regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13, min_train_regret_winner_13"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.9578012996071412, -2.3254036336633974)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uZlLJ1quwh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa92a487-4e52-4497-c4a8-8a6c71a46c89"
      },
      "source": [
        "### Training regret minimization: run number = 14\r\n",
        "\r\n",
        "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_14 = np.log(y_global_orig - loser_output_14)\r\n",
        "regret_winner_14 = np.log(y_global_orig - winner_output_14)\r\n",
        "\r\n",
        "train_regret_loser_14 = min_max_array(regret_loser_14)\r\n",
        "train_regret_winner_14 = min_max_array(regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14 = min(train_regret_loser_14)\r\n",
        "min_train_regret_winner_14 = min(train_regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14, min_train_regret_winner_14"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.4423529446398398, -4.3032982228176255)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVBcHRiMuwjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68f18564-4931-4152-b2b4-7b0756080cfc"
      },
      "source": [
        "### Training regret minimization: run number = 15\r\n",
        "\r\n",
        "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_15 = np.log(y_global_orig - loser_output_15)\r\n",
        "regret_winner_15 = np.log(y_global_orig - winner_output_15)\r\n",
        "\r\n",
        "train_regret_loser_15 = min_max_array(regret_loser_15)\r\n",
        "train_regret_winner_15 = min_max_array(regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15 = min(train_regret_loser_15)\r\n",
        "min_train_regret_winner_15 = min(train_regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15, min_train_regret_winner_15"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.431852117724113, -0.22304904811219423)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8nZ_DrKuwnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44fa093c-f8d5-4aeb-a0e2-c1c53f446db8"
      },
      "source": [
        "### Training regret minimization: run number = 16\r\n",
        "\r\n",
        "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_16 = np.log(y_global_orig - loser_output_16)\r\n",
        "regret_winner_16 = np.log(y_global_orig - winner_output_16)\r\n",
        "\r\n",
        "train_regret_loser_16 = min_max_array(regret_loser_16)\r\n",
        "train_regret_winner_16 = min_max_array(regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16 = min(train_regret_loser_16)\r\n",
        "min_train_regret_winner_16 = min(train_regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16, min_train_regret_winner_16"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.2310411317387597, -3.8928280189829323)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg6qzzQFuwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f600b0b8-7638-4f15-a1a3-fdded759a37a"
      },
      "source": [
        "### Training regret minimization: run number = 17\r\n",
        "\r\n",
        "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_17 = np.log(y_global_orig - loser_output_17)\r\n",
        "regret_winner_17 = np.log(y_global_orig - winner_output_17)\r\n",
        "\r\n",
        "train_regret_loser_17 = min_max_array(regret_loser_17)\r\n",
        "train_regret_winner_17 = min_max_array(regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17 = min(train_regret_loser_17)\r\n",
        "min_train_regret_winner_17 = min(train_regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17, min_train_regret_winner_17"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.02432998517116, -3.562551837373488)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrRtDLMFuwsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b8e10af-993f-4674-9f9c-a573441a84d5"
      },
      "source": [
        "### Training regret minimization: run number = 18\r\n",
        "\r\n",
        "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_18 = np.log(y_global_orig - loser_output_18)\r\n",
        "regret_winner_18 = np.log(y_global_orig - winner_output_18)\r\n",
        "\r\n",
        "train_regret_loser_18 = min_max_array(regret_loser_18)\r\n",
        "train_regret_winner_18 = min_max_array(regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18 = min(train_regret_loser_18)\r\n",
        "min_train_regret_winner_18 = min(train_regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18, min_train_regret_winner_18"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.3352725279655404, -4.862506438277311)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkmSu4CUuwuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a07803e6-f689-4b24-a448-f2496ac45728"
      },
      "source": [
        "### Training regret minimization: run number = 19\r\n",
        "\r\n",
        "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_19 = np.log(y_global_orig - loser_output_19)\r\n",
        "regret_winner_19 = np.log(y_global_orig - winner_output_19)\r\n",
        "\r\n",
        "train_regret_loser_19 = min_max_array(regret_loser_19)\r\n",
        "train_regret_winner_19 = min_max_array(regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19 = min(train_regret_loser_19)\r\n",
        "min_train_regret_winner_19 = min(train_regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19, min_train_regret_winner_19"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.2665005804060472, -3.203997976984693)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eymecjwkuwxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bff8cfb-9e07-4fda-c25b-b8e1a389a4a2"
      },
      "source": [
        "### Training regret minimization: run number = 20\r\n",
        "\r\n",
        "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_20 = np.log(y_global_orig - loser_output_20)\r\n",
        "regret_winner_20 = np.log(y_global_orig - winner_output_20)\r\n",
        "\r\n",
        "train_regret_loser_20 = min_max_array(regret_loser_20)\r\n",
        "train_regret_winner_20 = min_max_array(regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20 = min(train_regret_loser_20)\r\n",
        "min_train_regret_winner_20 = min(train_regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20, min_train_regret_winner_20"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.167359607010987, -0.2443361421666473)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5P6kq2Kuw0d"
      },
      "source": [
        "# Iteration1 :\r\n",
        "\r\n",
        "slice1 = 0\r\n",
        "\r\n",
        "loser1 = [train_regret_loser_1[slice1],\r\n",
        "       train_regret_loser_2[slice1],\r\n",
        "       train_regret_loser_3[slice1],\r\n",
        "       train_regret_loser_4[slice1],\r\n",
        "       train_regret_loser_5[slice1],\r\n",
        "       train_regret_loser_6[slice1],\r\n",
        "       train_regret_loser_7[slice1],\r\n",
        "       train_regret_loser_8[slice1],\r\n",
        "       train_regret_loser_9[slice1],\r\n",
        "       train_regret_loser_10[slice1],\r\n",
        "       train_regret_loser_11[slice1],\r\n",
        "       train_regret_loser_12[slice1],\r\n",
        "       train_regret_loser_13[slice1],\r\n",
        "       train_regret_loser_14[slice1],\r\n",
        "       train_regret_loser_15[slice1],\r\n",
        "       train_regret_loser_16[slice1],\r\n",
        "       train_regret_loser_17[slice1],\r\n",
        "       train_regret_loser_18[slice1],\r\n",
        "       train_regret_loser_19[slice1],\r\n",
        "       train_regret_loser_20[slice1]]\r\n",
        "\r\n",
        "winner1 = [train_regret_winner_1[slice1],\r\n",
        "       train_regret_winner_2[slice1],\r\n",
        "       train_regret_winner_3[slice1],\r\n",
        "       train_regret_winner_4[slice1],\r\n",
        "       train_regret_winner_5[slice1],\r\n",
        "       train_regret_winner_6[slice1],\r\n",
        "       train_regret_winner_7[slice1],\r\n",
        "       train_regret_winner_8[slice1],\r\n",
        "       train_regret_winner_9[slice1],\r\n",
        "       train_regret_winner_10[slice1],\r\n",
        "       train_regret_winner_11[slice1],\r\n",
        "       train_regret_winner_12[slice1],\r\n",
        "       train_regret_winner_13[slice1],\r\n",
        "       train_regret_winner_14[slice1],\r\n",
        "       train_regret_winner_15[slice1],\r\n",
        "       train_regret_winner_16[slice1],\r\n",
        "       train_regret_winner_17[slice1],\r\n",
        "       train_regret_winner_18[slice1],\r\n",
        "       train_regret_winner_19[slice1],\r\n",
        "       train_regret_winner_20[slice1]]\r\n",
        "\r\n",
        "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\r\n",
        "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\r\n",
        "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\r\n",
        "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\r\n",
        "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\r\n",
        "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXcnj2pNuw3X"
      },
      "source": [
        "# Iteration11 :\r\n",
        "\r\n",
        "slice11 = 10\r\n",
        "\r\n",
        "loser11 = [train_regret_loser_1[slice11],\r\n",
        "       train_regret_loser_2[slice11],\r\n",
        "       train_regret_loser_3[slice11],\r\n",
        "       train_regret_loser_4[slice11],\r\n",
        "       train_regret_loser_5[slice11],\r\n",
        "       train_regret_loser_6[slice11],\r\n",
        "       train_regret_loser_7[slice11],\r\n",
        "       train_regret_loser_8[slice11],\r\n",
        "       train_regret_loser_9[slice11],\r\n",
        "       train_regret_loser_10[slice11],\r\n",
        "       train_regret_loser_11[slice11],\r\n",
        "       train_regret_loser_12[slice11],\r\n",
        "       train_regret_loser_13[slice11],\r\n",
        "       train_regret_loser_14[slice11],\r\n",
        "       train_regret_loser_15[slice11],\r\n",
        "       train_regret_loser_16[slice11],\r\n",
        "       train_regret_loser_17[slice11],\r\n",
        "       train_regret_loser_18[slice11],\r\n",
        "       train_regret_loser_19[slice11],\r\n",
        "       train_regret_loser_20[slice11]]\r\n",
        "\r\n",
        "winner11 = [train_regret_winner_1[slice11],\r\n",
        "       train_regret_winner_2[slice11],\r\n",
        "       train_regret_winner_3[slice11],\r\n",
        "       train_regret_winner_4[slice11],\r\n",
        "       train_regret_winner_5[slice11],\r\n",
        "       train_regret_winner_6[slice11],\r\n",
        "       train_regret_winner_7[slice11],\r\n",
        "       train_regret_winner_8[slice11],\r\n",
        "       train_regret_winner_9[slice11],\r\n",
        "       train_regret_winner_10[slice11],\r\n",
        "       train_regret_winner_11[slice11],\r\n",
        "       train_regret_winner_12[slice11],\r\n",
        "       train_regret_winner_13[slice11],\r\n",
        "       train_regret_winner_14[slice11],\r\n",
        "       train_regret_winner_15[slice11],\r\n",
        "       train_regret_winner_16[slice11],\r\n",
        "       train_regret_winner_17[slice11],\r\n",
        "       train_regret_winner_18[slice11],\r\n",
        "       train_regret_winner_19[slice11],\r\n",
        "       train_regret_winner_20[slice11]]\r\n",
        "\r\n",
        "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\r\n",
        "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\r\n",
        "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\r\n",
        "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\r\n",
        "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\r\n",
        "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxQa6BAVuw6L"
      },
      "source": [
        "# Iteration21 :\r\n",
        "\r\n",
        "slice21 = 20\r\n",
        "\r\n",
        "loser21 = [train_regret_loser_1[slice21],\r\n",
        "       train_regret_loser_2[slice21],\r\n",
        "       train_regret_loser_3[slice21],\r\n",
        "       train_regret_loser_4[slice21],\r\n",
        "       train_regret_loser_5[slice21],\r\n",
        "       train_regret_loser_6[slice21],\r\n",
        "       train_regret_loser_7[slice21],\r\n",
        "       train_regret_loser_8[slice21],\r\n",
        "       train_regret_loser_9[slice21],\r\n",
        "       train_regret_loser_10[slice21],\r\n",
        "       train_regret_loser_11[slice21],\r\n",
        "       train_regret_loser_12[slice21],\r\n",
        "       train_regret_loser_13[slice21],\r\n",
        "       train_regret_loser_14[slice21],\r\n",
        "       train_regret_loser_15[slice21],\r\n",
        "       train_regret_loser_16[slice21],\r\n",
        "       train_regret_loser_17[slice21],\r\n",
        "       train_regret_loser_18[slice21],\r\n",
        "       train_regret_loser_19[slice21],\r\n",
        "       train_regret_loser_20[slice21]]\r\n",
        "\r\n",
        "winner21 = [train_regret_winner_1[slice21],\r\n",
        "       train_regret_winner_2[slice21],\r\n",
        "       train_regret_winner_3[slice21],\r\n",
        "       train_regret_winner_4[slice21],\r\n",
        "       train_regret_winner_5[slice21],\r\n",
        "       train_regret_winner_6[slice21],\r\n",
        "       train_regret_winner_7[slice21],\r\n",
        "       train_regret_winner_8[slice21],\r\n",
        "       train_regret_winner_9[slice21],\r\n",
        "       train_regret_winner_10[slice21],\r\n",
        "       train_regret_winner_11[slice21],\r\n",
        "       train_regret_winner_12[slice21],\r\n",
        "       train_regret_winner_13[slice21],\r\n",
        "       train_regret_winner_14[slice21],\r\n",
        "       train_regret_winner_15[slice21],\r\n",
        "       train_regret_winner_16[slice21],\r\n",
        "       train_regret_winner_17[slice21],\r\n",
        "       train_regret_winner_18[slice21],\r\n",
        "       train_regret_winner_19[slice21],\r\n",
        "       train_regret_winner_20[slice21]]\r\n",
        "\r\n",
        "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\r\n",
        "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\r\n",
        "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\r\n",
        "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\r\n",
        "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\r\n",
        "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wdJdIsWuw8r"
      },
      "source": [
        "# Iteration31 :\r\n",
        "\r\n",
        "slice31 = 30\r\n",
        "\r\n",
        "loser31 = [train_regret_loser_1[slice31],\r\n",
        "       train_regret_loser_2[slice31],\r\n",
        "       train_regret_loser_3[slice31],\r\n",
        "       train_regret_loser_4[slice31],\r\n",
        "       train_regret_loser_5[slice31],\r\n",
        "       train_regret_loser_6[slice31],\r\n",
        "       train_regret_loser_7[slice31],\r\n",
        "       train_regret_loser_8[slice31],\r\n",
        "       train_regret_loser_9[slice31],\r\n",
        "       train_regret_loser_10[slice31],\r\n",
        "       train_regret_loser_11[slice31],\r\n",
        "       train_regret_loser_12[slice31],\r\n",
        "       train_regret_loser_13[slice31],\r\n",
        "       train_regret_loser_14[slice31],\r\n",
        "       train_regret_loser_15[slice31],\r\n",
        "       train_regret_loser_16[slice31],\r\n",
        "       train_regret_loser_17[slice31],\r\n",
        "       train_regret_loser_18[slice31],\r\n",
        "       train_regret_loser_19[slice31],\r\n",
        "       train_regret_loser_20[slice31]]\r\n",
        "\r\n",
        "winner31 = [train_regret_winner_1[slice31],\r\n",
        "       train_regret_winner_2[slice31],\r\n",
        "       train_regret_winner_3[slice31],\r\n",
        "       train_regret_winner_4[slice31],\r\n",
        "       train_regret_winner_5[slice31],\r\n",
        "       train_regret_winner_6[slice31],\r\n",
        "       train_regret_winner_7[slice31],\r\n",
        "       train_regret_winner_8[slice31],\r\n",
        "       train_regret_winner_9[slice31],\r\n",
        "       train_regret_winner_10[slice31],\r\n",
        "       train_regret_winner_11[slice31],\r\n",
        "       train_regret_winner_12[slice31],\r\n",
        "       train_regret_winner_13[slice31],\r\n",
        "       train_regret_winner_14[slice31],\r\n",
        "       train_regret_winner_15[slice31],\r\n",
        "       train_regret_winner_16[slice31],\r\n",
        "       train_regret_winner_17[slice31],\r\n",
        "       train_regret_winner_18[slice31],\r\n",
        "       train_regret_winner_19[slice31],\r\n",
        "       train_regret_winner_20[slice31]]\r\n",
        "\r\n",
        "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\r\n",
        "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\r\n",
        "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\r\n",
        "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\r\n",
        "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\r\n",
        "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWZBDufuw_Z"
      },
      "source": [
        "# Iteration41 :\r\n",
        "\r\n",
        "slice41 = 40\r\n",
        "\r\n",
        "loser41 = [train_regret_loser_1[slice41],\r\n",
        "       train_regret_loser_2[slice41],\r\n",
        "       train_regret_loser_3[slice41],\r\n",
        "       train_regret_loser_4[slice41],\r\n",
        "       train_regret_loser_5[slice41],\r\n",
        "       train_regret_loser_6[slice41],\r\n",
        "       train_regret_loser_7[slice41],\r\n",
        "       train_regret_loser_8[slice41],\r\n",
        "       train_regret_loser_9[slice41],\r\n",
        "       train_regret_loser_10[slice41],\r\n",
        "       train_regret_loser_11[slice41],\r\n",
        "       train_regret_loser_12[slice41],\r\n",
        "       train_regret_loser_13[slice41],\r\n",
        "       train_regret_loser_14[slice41],\r\n",
        "       train_regret_loser_15[slice41],\r\n",
        "       train_regret_loser_16[slice41],\r\n",
        "       train_regret_loser_17[slice41],\r\n",
        "       train_regret_loser_18[slice41],\r\n",
        "       train_regret_loser_19[slice41],\r\n",
        "       train_regret_loser_20[slice41]]\r\n",
        "\r\n",
        "winner41 = [train_regret_winner_1[slice41],\r\n",
        "       train_regret_winner_2[slice41],\r\n",
        "       train_regret_winner_3[slice41],\r\n",
        "       train_regret_winner_4[slice41],\r\n",
        "       train_regret_winner_5[slice41],\r\n",
        "       train_regret_winner_6[slice41],\r\n",
        "       train_regret_winner_7[slice41],\r\n",
        "       train_regret_winner_8[slice41],\r\n",
        "       train_regret_winner_9[slice41],\r\n",
        "       train_regret_winner_10[slice41],\r\n",
        "       train_regret_winner_11[slice41],\r\n",
        "       train_regret_winner_12[slice41],\r\n",
        "       train_regret_winner_13[slice41],\r\n",
        "       train_regret_winner_14[slice41],\r\n",
        "       train_regret_winner_15[slice41],\r\n",
        "       train_regret_winner_16[slice41],\r\n",
        "       train_regret_winner_17[slice41],\r\n",
        "       train_regret_winner_18[slice41],\r\n",
        "       train_regret_winner_19[slice41],\r\n",
        "       train_regret_winner_20[slice41]]\r\n",
        "\r\n",
        "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\r\n",
        "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\r\n",
        "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\r\n",
        "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\r\n",
        "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\r\n",
        "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwi_GzjiuxB6"
      },
      "source": [
        "# Iteration51 :\r\n",
        "\r\n",
        "slice51 = 50\r\n",
        "\r\n",
        "loser51 = [train_regret_loser_1[slice51],\r\n",
        "       train_regret_loser_2[slice51],\r\n",
        "       train_regret_loser_3[slice51],\r\n",
        "       train_regret_loser_4[slice51],\r\n",
        "       train_regret_loser_5[slice51],\r\n",
        "       train_regret_loser_6[slice51],\r\n",
        "       train_regret_loser_7[slice51],\r\n",
        "       train_regret_loser_8[slice51],\r\n",
        "       train_regret_loser_9[slice51],\r\n",
        "       train_regret_loser_10[slice51],\r\n",
        "       train_regret_loser_11[slice51],\r\n",
        "       train_regret_loser_12[slice51],\r\n",
        "       train_regret_loser_13[slice51],\r\n",
        "       train_regret_loser_14[slice51],\r\n",
        "       train_regret_loser_15[slice51],\r\n",
        "       train_regret_loser_16[slice51],\r\n",
        "       train_regret_loser_17[slice51],\r\n",
        "       train_regret_loser_18[slice51],\r\n",
        "       train_regret_loser_19[slice51],\r\n",
        "       train_regret_loser_20[slice51]]\r\n",
        "\r\n",
        "winner51 = [train_regret_winner_1[slice51],\r\n",
        "       train_regret_winner_2[slice51],\r\n",
        "       train_regret_winner_3[slice51],\r\n",
        "       train_regret_winner_4[slice51],\r\n",
        "       train_regret_winner_5[slice51],\r\n",
        "       train_regret_winner_6[slice51],\r\n",
        "       train_regret_winner_7[slice51],\r\n",
        "       train_regret_winner_8[slice51],\r\n",
        "       train_regret_winner_9[slice51],\r\n",
        "       train_regret_winner_10[slice51],\r\n",
        "       train_regret_winner_11[slice51],\r\n",
        "       train_regret_winner_12[slice51],\r\n",
        "       train_regret_winner_13[slice51],\r\n",
        "       train_regret_winner_14[slice51],\r\n",
        "       train_regret_winner_15[slice51],\r\n",
        "       train_regret_winner_16[slice51],\r\n",
        "       train_regret_winner_17[slice51],\r\n",
        "       train_regret_winner_18[slice51],\r\n",
        "       train_regret_winner_19[slice51],\r\n",
        "       train_regret_winner_20[slice51]]\r\n",
        "\r\n",
        "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\r\n",
        "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\r\n",
        "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\r\n",
        "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\r\n",
        "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\r\n",
        "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk9nlS6puxFZ"
      },
      "source": [
        "# Iteration61 :\r\n",
        "\r\n",
        "slice61 = 60\r\n",
        "\r\n",
        "loser61 = [train_regret_loser_1[slice61],\r\n",
        "       train_regret_loser_2[slice61],\r\n",
        "       train_regret_loser_3[slice61],\r\n",
        "       train_regret_loser_4[slice61],\r\n",
        "       train_regret_loser_5[slice61],\r\n",
        "       train_regret_loser_6[slice61],\r\n",
        "       train_regret_loser_7[slice61],\r\n",
        "       train_regret_loser_8[slice61],\r\n",
        "       train_regret_loser_9[slice61],\r\n",
        "       train_regret_loser_10[slice61],\r\n",
        "       train_regret_loser_11[slice61],\r\n",
        "       train_regret_loser_12[slice61],\r\n",
        "       train_regret_loser_13[slice61],\r\n",
        "       train_regret_loser_14[slice61],\r\n",
        "       train_regret_loser_15[slice61],\r\n",
        "       train_regret_loser_16[slice61],\r\n",
        "       train_regret_loser_17[slice61],\r\n",
        "       train_regret_loser_18[slice61],\r\n",
        "       train_regret_loser_19[slice61],\r\n",
        "       train_regret_loser_20[slice61]]\r\n",
        "\r\n",
        "winner61 = [train_regret_winner_1[slice61],\r\n",
        "       train_regret_winner_2[slice61],\r\n",
        "       train_regret_winner_3[slice61],\r\n",
        "       train_regret_winner_4[slice61],\r\n",
        "       train_regret_winner_5[slice61],\r\n",
        "       train_regret_winner_6[slice61],\r\n",
        "       train_regret_winner_7[slice61],\r\n",
        "       train_regret_winner_8[slice61],\r\n",
        "       train_regret_winner_9[slice61],\r\n",
        "       train_regret_winner_10[slice61],\r\n",
        "       train_regret_winner_11[slice61],\r\n",
        "       train_regret_winner_12[slice61],\r\n",
        "       train_regret_winner_13[slice61],\r\n",
        "       train_regret_winner_14[slice61],\r\n",
        "       train_regret_winner_15[slice61],\r\n",
        "       train_regret_winner_16[slice61],\r\n",
        "       train_regret_winner_17[slice61],\r\n",
        "       train_regret_winner_18[slice61],\r\n",
        "       train_regret_winner_19[slice61],\r\n",
        "       train_regret_winner_20[slice61]]\r\n",
        "\r\n",
        "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\r\n",
        "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\r\n",
        "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\r\n",
        "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\r\n",
        "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\r\n",
        "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMus9vDpuxHN"
      },
      "source": [
        "# Iteration71 :\r\n",
        "\r\n",
        "slice71 = 70\r\n",
        "\r\n",
        "loser71 = [train_regret_loser_1[slice71],\r\n",
        "       train_regret_loser_2[slice71],\r\n",
        "       train_regret_loser_3[slice71],\r\n",
        "       train_regret_loser_4[slice71],\r\n",
        "       train_regret_loser_5[slice71],\r\n",
        "       train_regret_loser_6[slice71],\r\n",
        "       train_regret_loser_7[slice71],\r\n",
        "       train_regret_loser_8[slice71],\r\n",
        "       train_regret_loser_9[slice71],\r\n",
        "       train_regret_loser_10[slice71],\r\n",
        "       train_regret_loser_11[slice71],\r\n",
        "       train_regret_loser_12[slice71],\r\n",
        "       train_regret_loser_13[slice71],\r\n",
        "       train_regret_loser_14[slice71],\r\n",
        "       train_regret_loser_15[slice71],\r\n",
        "       train_regret_loser_16[slice71],\r\n",
        "       train_regret_loser_17[slice71],\r\n",
        "       train_regret_loser_18[slice71],\r\n",
        "       train_regret_loser_19[slice71],\r\n",
        "       train_regret_loser_20[slice71]]\r\n",
        "\r\n",
        "winner71 = [train_regret_winner_1[slice71],\r\n",
        "       train_regret_winner_2[slice71],\r\n",
        "       train_regret_winner_3[slice71],\r\n",
        "       train_regret_winner_4[slice71],\r\n",
        "       train_regret_winner_5[slice71],\r\n",
        "       train_regret_winner_6[slice71],\r\n",
        "       train_regret_winner_7[slice71],\r\n",
        "       train_regret_winner_8[slice71],\r\n",
        "       train_regret_winner_9[slice71],\r\n",
        "       train_regret_winner_10[slice71],\r\n",
        "       train_regret_winner_11[slice71],\r\n",
        "       train_regret_winner_12[slice71],\r\n",
        "       train_regret_winner_13[slice71],\r\n",
        "       train_regret_winner_14[slice71],\r\n",
        "       train_regret_winner_15[slice71],\r\n",
        "       train_regret_winner_16[slice71],\r\n",
        "       train_regret_winner_17[slice71],\r\n",
        "       train_regret_winner_18[slice71],\r\n",
        "       train_regret_winner_19[slice71],\r\n",
        "       train_regret_winner_20[slice71]]\r\n",
        "\r\n",
        "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\r\n",
        "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\r\n",
        "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\r\n",
        "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\r\n",
        "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\r\n",
        "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Mg0x9vuxL4"
      },
      "source": [
        "# Iteration81 :\r\n",
        "\r\n",
        "slice81 = 80\r\n",
        "\r\n",
        "loser81 = [train_regret_loser_1[slice81],\r\n",
        "       train_regret_loser_2[slice81],\r\n",
        "       train_regret_loser_3[slice81],\r\n",
        "       train_regret_loser_4[slice81],\r\n",
        "       train_regret_loser_5[slice81],\r\n",
        "       train_regret_loser_6[slice81],\r\n",
        "       train_regret_loser_7[slice81],\r\n",
        "       train_regret_loser_8[slice81],\r\n",
        "       train_regret_loser_9[slice81],\r\n",
        "       train_regret_loser_10[slice81],\r\n",
        "       train_regret_loser_11[slice81],\r\n",
        "       train_regret_loser_12[slice81],\r\n",
        "       train_regret_loser_13[slice81],\r\n",
        "       train_regret_loser_14[slice81],\r\n",
        "       train_regret_loser_15[slice81],\r\n",
        "       train_regret_loser_16[slice81],\r\n",
        "       train_regret_loser_17[slice81],\r\n",
        "       train_regret_loser_18[slice81],\r\n",
        "       train_regret_loser_19[slice81],\r\n",
        "       train_regret_loser_20[slice81]]\r\n",
        "\r\n",
        "winner81 = [train_regret_winner_1[slice81],\r\n",
        "       train_regret_winner_2[slice81],\r\n",
        "       train_regret_winner_3[slice81],\r\n",
        "       train_regret_winner_4[slice81],\r\n",
        "       train_regret_winner_5[slice81],\r\n",
        "       train_regret_winner_6[slice81],\r\n",
        "       train_regret_winner_7[slice81],\r\n",
        "       train_regret_winner_8[slice81],\r\n",
        "       train_regret_winner_9[slice81],\r\n",
        "       train_regret_winner_10[slice81],\r\n",
        "       train_regret_winner_11[slice81],\r\n",
        "       train_regret_winner_12[slice81],\r\n",
        "       train_regret_winner_13[slice81],\r\n",
        "       train_regret_winner_14[slice81],\r\n",
        "       train_regret_winner_15[slice81],\r\n",
        "       train_regret_winner_16[slice81],\r\n",
        "       train_regret_winner_17[slice81],\r\n",
        "       train_regret_winner_18[slice81],\r\n",
        "       train_regret_winner_19[slice81],\r\n",
        "       train_regret_winner_20[slice81]]\r\n",
        "\r\n",
        "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\r\n",
        "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\r\n",
        "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\r\n",
        "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\r\n",
        "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\r\n",
        "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gu9K8XlteV4"
      },
      "source": [
        "# Iteration91 :\r\n",
        "\r\n",
        "slice91 = 90\r\n",
        "\r\n",
        "loser91 = [train_regret_loser_1[slice91],\r\n",
        "       train_regret_loser_2[slice91],\r\n",
        "       train_regret_loser_3[slice91],\r\n",
        "       train_regret_loser_4[slice91],\r\n",
        "       train_regret_loser_5[slice91],\r\n",
        "       train_regret_loser_6[slice91],\r\n",
        "       train_regret_loser_7[slice91],\r\n",
        "       train_regret_loser_8[slice91],\r\n",
        "       train_regret_loser_9[slice91],\r\n",
        "       train_regret_loser_10[slice91],\r\n",
        "       train_regret_loser_11[slice91],\r\n",
        "       train_regret_loser_12[slice91],\r\n",
        "       train_regret_loser_13[slice91],\r\n",
        "       train_regret_loser_14[slice91],\r\n",
        "       train_regret_loser_15[slice91],\r\n",
        "       train_regret_loser_16[slice91],\r\n",
        "       train_regret_loser_17[slice91],\r\n",
        "       train_regret_loser_18[slice91],\r\n",
        "       train_regret_loser_19[slice91],\r\n",
        "       train_regret_loser_20[slice91]]\r\n",
        "\r\n",
        "winner91 = [train_regret_winner_1[slice91],\r\n",
        "       train_regret_winner_2[slice91],\r\n",
        "       train_regret_winner_3[slice91],\r\n",
        "       train_regret_winner_4[slice91],\r\n",
        "       train_regret_winner_5[slice91],\r\n",
        "       train_regret_winner_6[slice91],\r\n",
        "       train_regret_winner_7[slice91],\r\n",
        "       train_regret_winner_8[slice91],\r\n",
        "       train_regret_winner_9[slice91],\r\n",
        "       train_regret_winner_10[slice91],\r\n",
        "       train_regret_winner_11[slice91],\r\n",
        "       train_regret_winner_12[slice91],\r\n",
        "       train_regret_winner_13[slice91],\r\n",
        "       train_regret_winner_14[slice91],\r\n",
        "       train_regret_winner_15[slice91],\r\n",
        "       train_regret_winner_16[slice91],\r\n",
        "       train_regret_winner_17[slice91],\r\n",
        "       train_regret_winner_18[slice91],\r\n",
        "       train_regret_winner_19[slice91],\r\n",
        "       train_regret_winner_20[slice91]]\r\n",
        "\r\n",
        "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\r\n",
        "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\r\n",
        "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\r\n",
        "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\r\n",
        "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\r\n",
        "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVVeft6steXv"
      },
      "source": [
        "# Iteration101 :\r\n",
        "\r\n",
        "slice101 = 100\r\n",
        "\r\n",
        "loser101 = [train_regret_loser_1[slice101],\r\n",
        "       train_regret_loser_2[slice101],\r\n",
        "       train_regret_loser_3[slice101],\r\n",
        "       train_regret_loser_4[slice101],\r\n",
        "       train_regret_loser_5[slice101],\r\n",
        "       train_regret_loser_6[slice101],\r\n",
        "       train_regret_loser_7[slice101],\r\n",
        "       train_regret_loser_8[slice101],\r\n",
        "       train_regret_loser_9[slice101],\r\n",
        "       train_regret_loser_10[slice101],\r\n",
        "       train_regret_loser_11[slice101],\r\n",
        "       train_regret_loser_12[slice101],\r\n",
        "       train_regret_loser_13[slice101],\r\n",
        "       train_regret_loser_14[slice101],\r\n",
        "       train_regret_loser_15[slice101],\r\n",
        "       train_regret_loser_16[slice101],\r\n",
        "       train_regret_loser_17[slice101],\r\n",
        "       train_regret_loser_18[slice101],\r\n",
        "       train_regret_loser_19[slice101],\r\n",
        "       train_regret_loser_20[slice101]]\r\n",
        "\r\n",
        "winner101 = [train_regret_winner_1[slice101],\r\n",
        "       train_regret_winner_2[slice101],\r\n",
        "       train_regret_winner_3[slice101],\r\n",
        "       train_regret_winner_4[slice101],\r\n",
        "       train_regret_winner_5[slice101],\r\n",
        "       train_regret_winner_6[slice101],\r\n",
        "       train_regret_winner_7[slice101],\r\n",
        "       train_regret_winner_8[slice101],\r\n",
        "       train_regret_winner_9[slice101],\r\n",
        "       train_regret_winner_10[slice101],\r\n",
        "       train_regret_winner_11[slice101],\r\n",
        "       train_regret_winner_12[slice101],\r\n",
        "       train_regret_winner_13[slice101],\r\n",
        "       train_regret_winner_14[slice101],\r\n",
        "       train_regret_winner_15[slice101],\r\n",
        "       train_regret_winner_16[slice101],\r\n",
        "       train_regret_winner_17[slice101],\r\n",
        "       train_regret_winner_18[slice101],\r\n",
        "       train_regret_winner_19[slice101],\r\n",
        "       train_regret_winner_20[slice101]]\r\n",
        "\r\n",
        "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\r\n",
        "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\r\n",
        "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\r\n",
        "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\r\n",
        "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\r\n",
        "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maqbtTXXtebT"
      },
      "source": [
        "# Iteration2 :\r\n",
        "\r\n",
        "slice2 = 1\r\n",
        "\r\n",
        "loser2 = [train_regret_loser_1[slice2],\r\n",
        "       train_regret_loser_2[slice2],\r\n",
        "       train_regret_loser_3[slice2],\r\n",
        "       train_regret_loser_4[slice2],\r\n",
        "       train_regret_loser_5[slice2],\r\n",
        "       train_regret_loser_6[slice2],\r\n",
        "       train_regret_loser_7[slice2],\r\n",
        "       train_regret_loser_8[slice2],\r\n",
        "       train_regret_loser_9[slice2],\r\n",
        "       train_regret_loser_10[slice2],\r\n",
        "       train_regret_loser_11[slice2],\r\n",
        "       train_regret_loser_12[slice2],\r\n",
        "       train_regret_loser_13[slice2],\r\n",
        "       train_regret_loser_14[slice2],\r\n",
        "       train_regret_loser_15[slice2],\r\n",
        "       train_regret_loser_16[slice2],\r\n",
        "       train_regret_loser_17[slice2],\r\n",
        "       train_regret_loser_18[slice2],\r\n",
        "       train_regret_loser_19[slice2],\r\n",
        "       train_regret_loser_20[slice2]]\r\n",
        "\r\n",
        "winner2 = [train_regret_winner_1[slice2],\r\n",
        "       train_regret_winner_2[slice2],\r\n",
        "       train_regret_winner_3[slice2],\r\n",
        "       train_regret_winner_4[slice2],\r\n",
        "       train_regret_winner_5[slice2],\r\n",
        "       train_regret_winner_6[slice2],\r\n",
        "       train_regret_winner_7[slice2],\r\n",
        "       train_regret_winner_8[slice2],\r\n",
        "       train_regret_winner_9[slice2],\r\n",
        "       train_regret_winner_10[slice2],\r\n",
        "       train_regret_winner_11[slice2],\r\n",
        "       train_regret_winner_12[slice2],\r\n",
        "       train_regret_winner_13[slice2],\r\n",
        "       train_regret_winner_14[slice2],\r\n",
        "       train_regret_winner_15[slice2],\r\n",
        "       train_regret_winner_16[slice2],\r\n",
        "       train_regret_winner_17[slice2],\r\n",
        "       train_regret_winner_18[slice2],\r\n",
        "       train_regret_winner_19[slice2],\r\n",
        "       train_regret_winner_20[slice2]]\r\n",
        "\r\n",
        "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\r\n",
        "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\r\n",
        "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\r\n",
        "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\r\n",
        "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\r\n",
        "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIBh0dQw-ZM"
      },
      "source": [
        "# Iteration12 :\r\n",
        "\r\n",
        "slice12 = 11\r\n",
        "\r\n",
        "loser12 = [train_regret_loser_1[slice12],\r\n",
        "       train_regret_loser_2[slice12],\r\n",
        "       train_regret_loser_3[slice12],\r\n",
        "       train_regret_loser_4[slice12],\r\n",
        "       train_regret_loser_5[slice12],\r\n",
        "       train_regret_loser_6[slice12],\r\n",
        "       train_regret_loser_7[slice12],\r\n",
        "       train_regret_loser_8[slice12],\r\n",
        "       train_regret_loser_9[slice12],\r\n",
        "       train_regret_loser_10[slice12],\r\n",
        "       train_regret_loser_11[slice12],\r\n",
        "       train_regret_loser_12[slice12],\r\n",
        "       train_regret_loser_13[slice12],\r\n",
        "       train_regret_loser_14[slice12],\r\n",
        "       train_regret_loser_15[slice12],\r\n",
        "       train_regret_loser_16[slice12],\r\n",
        "       train_regret_loser_17[slice12],\r\n",
        "       train_regret_loser_18[slice12],\r\n",
        "       train_regret_loser_19[slice12],\r\n",
        "       train_regret_loser_20[slice12]]\r\n",
        "\r\n",
        "winner12 = [train_regret_winner_1[slice12],\r\n",
        "       train_regret_winner_2[slice12],\r\n",
        "       train_regret_winner_3[slice12],\r\n",
        "       train_regret_winner_4[slice12],\r\n",
        "       train_regret_winner_5[slice12],\r\n",
        "       train_regret_winner_6[slice12],\r\n",
        "       train_regret_winner_7[slice12],\r\n",
        "       train_regret_winner_8[slice12],\r\n",
        "       train_regret_winner_9[slice12],\r\n",
        "       train_regret_winner_10[slice12],\r\n",
        "       train_regret_winner_11[slice12],\r\n",
        "       train_regret_winner_12[slice12],\r\n",
        "       train_regret_winner_13[slice12],\r\n",
        "       train_regret_winner_14[slice12],\r\n",
        "       train_regret_winner_15[slice12],\r\n",
        "       train_regret_winner_16[slice12],\r\n",
        "       train_regret_winner_17[slice12],\r\n",
        "       train_regret_winner_18[slice12],\r\n",
        "       train_regret_winner_19[slice12],\r\n",
        "       train_regret_winner_20[slice12]]\r\n",
        "\r\n",
        "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\r\n",
        "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\r\n",
        "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\r\n",
        "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\r\n",
        "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjIhsei3w-cV"
      },
      "source": [
        "# Iteration22 :\r\n",
        "\r\n",
        "slice22 = 21\r\n",
        "\r\n",
        "loser22 = [train_regret_loser_1[slice22],\r\n",
        "       train_regret_loser_2[slice22],\r\n",
        "       train_regret_loser_3[slice22],\r\n",
        "       train_regret_loser_4[slice22],\r\n",
        "       train_regret_loser_5[slice22],\r\n",
        "       train_regret_loser_6[slice22],\r\n",
        "       train_regret_loser_7[slice22],\r\n",
        "       train_regret_loser_8[slice22],\r\n",
        "       train_regret_loser_9[slice22],\r\n",
        "       train_regret_loser_10[slice22],\r\n",
        "       train_regret_loser_11[slice22],\r\n",
        "       train_regret_loser_12[slice22],\r\n",
        "       train_regret_loser_13[slice22],\r\n",
        "       train_regret_loser_14[slice22],\r\n",
        "       train_regret_loser_15[slice22],\r\n",
        "       train_regret_loser_16[slice22],\r\n",
        "       train_regret_loser_17[slice22],\r\n",
        "       train_regret_loser_18[slice22],\r\n",
        "       train_regret_loser_19[slice22],\r\n",
        "       train_regret_loser_20[slice22]]\r\n",
        "\r\n",
        "winner22 = [train_regret_winner_1[slice22],\r\n",
        "       train_regret_winner_2[slice22],\r\n",
        "       train_regret_winner_3[slice22],\r\n",
        "       train_regret_winner_4[slice22],\r\n",
        "       train_regret_winner_5[slice22],\r\n",
        "       train_regret_winner_6[slice22],\r\n",
        "       train_regret_winner_7[slice22],\r\n",
        "       train_regret_winner_8[slice22],\r\n",
        "       train_regret_winner_9[slice22],\r\n",
        "       train_regret_winner_10[slice22],\r\n",
        "       train_regret_winner_11[slice22],\r\n",
        "       train_regret_winner_12[slice22],\r\n",
        "       train_regret_winner_13[slice22],\r\n",
        "       train_regret_winner_14[slice22],\r\n",
        "       train_regret_winner_15[slice22],\r\n",
        "       train_regret_winner_16[slice22],\r\n",
        "       train_regret_winner_17[slice22],\r\n",
        "       train_regret_winner_18[slice22],\r\n",
        "       train_regret_winner_19[slice22],\r\n",
        "       train_regret_winner_20[slice22]]\r\n",
        "\r\n",
        "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\r\n",
        "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\r\n",
        "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\r\n",
        "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\r\n",
        "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\r\n",
        "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur9p9GkHw-fq"
      },
      "source": [
        "# Iteration32 :\r\n",
        "\r\n",
        "slice32 = 31\r\n",
        "\r\n",
        "loser32 = [train_regret_loser_1[slice32],\r\n",
        "       train_regret_loser_2[slice32],\r\n",
        "       train_regret_loser_3[slice32],\r\n",
        "       train_regret_loser_4[slice32],\r\n",
        "       train_regret_loser_5[slice32],\r\n",
        "       train_regret_loser_6[slice32],\r\n",
        "       train_regret_loser_7[slice32],\r\n",
        "       train_regret_loser_8[slice32],\r\n",
        "       train_regret_loser_9[slice32],\r\n",
        "       train_regret_loser_10[slice32],\r\n",
        "       train_regret_loser_11[slice32],\r\n",
        "       train_regret_loser_12[slice32],\r\n",
        "       train_regret_loser_13[slice32],\r\n",
        "       train_regret_loser_14[slice32],\r\n",
        "       train_regret_loser_15[slice32],\r\n",
        "       train_regret_loser_16[slice32],\r\n",
        "       train_regret_loser_17[slice32],\r\n",
        "       train_regret_loser_18[slice32],\r\n",
        "       train_regret_loser_19[slice32],\r\n",
        "       train_regret_loser_20[slice32]]\r\n",
        "\r\n",
        "winner32 = [train_regret_winner_1[slice32],\r\n",
        "       train_regret_winner_2[slice32],\r\n",
        "       train_regret_winner_3[slice32],\r\n",
        "       train_regret_winner_4[slice32],\r\n",
        "       train_regret_winner_5[slice32],\r\n",
        "       train_regret_winner_6[slice32],\r\n",
        "       train_regret_winner_7[slice32],\r\n",
        "       train_regret_winner_8[slice32],\r\n",
        "       train_regret_winner_9[slice32],\r\n",
        "       train_regret_winner_10[slice32],\r\n",
        "       train_regret_winner_11[slice32],\r\n",
        "       train_regret_winner_12[slice32],\r\n",
        "       train_regret_winner_13[slice32],\r\n",
        "       train_regret_winner_14[slice32],\r\n",
        "       train_regret_winner_15[slice32],\r\n",
        "       train_regret_winner_16[slice32],\r\n",
        "       train_regret_winner_17[slice32],\r\n",
        "       train_regret_winner_18[slice32],\r\n",
        "       train_regret_winner_19[slice32],\r\n",
        "       train_regret_winner_20[slice32]]\r\n",
        "\r\n",
        "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\r\n",
        "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\r\n",
        "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\r\n",
        "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\r\n",
        "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\r\n",
        "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_rzHB8w-hy"
      },
      "source": [
        "# Iteration42 :\r\n",
        "\r\n",
        "slice42 = 41\r\n",
        "\r\n",
        "loser42 = [train_regret_loser_1[slice42],\r\n",
        "       train_regret_loser_2[slice42],\r\n",
        "       train_regret_loser_3[slice42],\r\n",
        "       train_regret_loser_4[slice42],\r\n",
        "       train_regret_loser_5[slice42],\r\n",
        "       train_regret_loser_6[slice42],\r\n",
        "       train_regret_loser_7[slice42],\r\n",
        "       train_regret_loser_8[slice42],\r\n",
        "       train_regret_loser_9[slice42],\r\n",
        "       train_regret_loser_10[slice42],\r\n",
        "       train_regret_loser_11[slice42],\r\n",
        "       train_regret_loser_12[slice42],\r\n",
        "       train_regret_loser_13[slice42],\r\n",
        "       train_regret_loser_14[slice42],\r\n",
        "       train_regret_loser_15[slice42],\r\n",
        "       train_regret_loser_16[slice42],\r\n",
        "       train_regret_loser_17[slice42],\r\n",
        "       train_regret_loser_18[slice42],\r\n",
        "       train_regret_loser_19[slice42],\r\n",
        "       train_regret_loser_20[slice42]]\r\n",
        "\r\n",
        "winner42 = [train_regret_winner_1[slice42],\r\n",
        "       train_regret_winner_2[slice42],\r\n",
        "       train_regret_winner_3[slice42],\r\n",
        "       train_regret_winner_4[slice42],\r\n",
        "       train_regret_winner_5[slice42],\r\n",
        "       train_regret_winner_6[slice42],\r\n",
        "       train_regret_winner_7[slice42],\r\n",
        "       train_regret_winner_8[slice42],\r\n",
        "       train_regret_winner_9[slice42],\r\n",
        "       train_regret_winner_10[slice42],\r\n",
        "       train_regret_winner_11[slice42],\r\n",
        "       train_regret_winner_12[slice42],\r\n",
        "       train_regret_winner_13[slice42],\r\n",
        "       train_regret_winner_14[slice42],\r\n",
        "       train_regret_winner_15[slice42],\r\n",
        "       train_regret_winner_16[slice42],\r\n",
        "       train_regret_winner_17[slice42],\r\n",
        "       train_regret_winner_18[slice42],\r\n",
        "       train_regret_winner_19[slice42],\r\n",
        "       train_regret_winner_20[slice42]]\r\n",
        "\r\n",
        "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\r\n",
        "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\r\n",
        "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\r\n",
        "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\r\n",
        "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\r\n",
        "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKUUG6vAw-kz"
      },
      "source": [
        "# Iteration52 :\r\n",
        "\r\n",
        "slice52 = 51\r\n",
        "\r\n",
        "loser52 = [train_regret_loser_1[slice52],\r\n",
        "       train_regret_loser_2[slice52],\r\n",
        "       train_regret_loser_3[slice52],\r\n",
        "       train_regret_loser_4[slice52],\r\n",
        "       train_regret_loser_5[slice52],\r\n",
        "       train_regret_loser_6[slice52],\r\n",
        "       train_regret_loser_7[slice52],\r\n",
        "       train_regret_loser_8[slice52],\r\n",
        "       train_regret_loser_9[slice52],\r\n",
        "       train_regret_loser_10[slice52],\r\n",
        "       train_regret_loser_11[slice52],\r\n",
        "       train_regret_loser_12[slice52],\r\n",
        "       train_regret_loser_13[slice52],\r\n",
        "       train_regret_loser_14[slice52],\r\n",
        "       train_regret_loser_15[slice52],\r\n",
        "       train_regret_loser_16[slice52],\r\n",
        "       train_regret_loser_17[slice52],\r\n",
        "       train_regret_loser_18[slice52],\r\n",
        "       train_regret_loser_19[slice52],\r\n",
        "       train_regret_loser_20[slice52]]\r\n",
        "\r\n",
        "winner52 = [train_regret_winner_1[slice52],\r\n",
        "       train_regret_winner_2[slice52],\r\n",
        "       train_regret_winner_3[slice52],\r\n",
        "       train_regret_winner_4[slice52],\r\n",
        "       train_regret_winner_5[slice52],\r\n",
        "       train_regret_winner_6[slice52],\r\n",
        "       train_regret_winner_7[slice52],\r\n",
        "       train_regret_winner_8[slice52],\r\n",
        "       train_regret_winner_9[slice52],\r\n",
        "       train_regret_winner_10[slice52],\r\n",
        "       train_regret_winner_11[slice52],\r\n",
        "       train_regret_winner_12[slice52],\r\n",
        "       train_regret_winner_13[slice52],\r\n",
        "       train_regret_winner_14[slice52],\r\n",
        "       train_regret_winner_15[slice52],\r\n",
        "       train_regret_winner_16[slice52],\r\n",
        "       train_regret_winner_17[slice52],\r\n",
        "       train_regret_winner_18[slice52],\r\n",
        "       train_regret_winner_19[slice52],\r\n",
        "       train_regret_winner_20[slice52]]\r\n",
        "\r\n",
        "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\r\n",
        "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\r\n",
        "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\r\n",
        "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\r\n",
        "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\r\n",
        "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9cJNkew-nY"
      },
      "source": [
        "# Iteration62 :\r\n",
        "\r\n",
        "slice62 = 61\r\n",
        "\r\n",
        "loser62 = [train_regret_loser_1[slice62],\r\n",
        "       train_regret_loser_2[slice62],\r\n",
        "       train_regret_loser_3[slice62],\r\n",
        "       train_regret_loser_4[slice62],\r\n",
        "       train_regret_loser_5[slice62],\r\n",
        "       train_regret_loser_6[slice62],\r\n",
        "       train_regret_loser_7[slice62],\r\n",
        "       train_regret_loser_8[slice62],\r\n",
        "       train_regret_loser_9[slice62],\r\n",
        "       train_regret_loser_10[slice62],\r\n",
        "       train_regret_loser_11[slice62],\r\n",
        "       train_regret_loser_12[slice62],\r\n",
        "       train_regret_loser_13[slice62],\r\n",
        "       train_regret_loser_14[slice62],\r\n",
        "       train_regret_loser_15[slice62],\r\n",
        "       train_regret_loser_16[slice62],\r\n",
        "       train_regret_loser_17[slice62],\r\n",
        "       train_regret_loser_18[slice62],\r\n",
        "       train_regret_loser_19[slice62],\r\n",
        "       train_regret_loser_20[slice62]]\r\n",
        "\r\n",
        "winner62 = [train_regret_winner_1[slice62],\r\n",
        "       train_regret_winner_2[slice62],\r\n",
        "       train_regret_winner_3[slice62],\r\n",
        "       train_regret_winner_4[slice62],\r\n",
        "       train_regret_winner_5[slice62],\r\n",
        "       train_regret_winner_6[slice62],\r\n",
        "       train_regret_winner_7[slice62],\r\n",
        "       train_regret_winner_8[slice62],\r\n",
        "       train_regret_winner_9[slice62],\r\n",
        "       train_regret_winner_10[slice62],\r\n",
        "       train_regret_winner_11[slice62],\r\n",
        "       train_regret_winner_12[slice62],\r\n",
        "       train_regret_winner_13[slice62],\r\n",
        "       train_regret_winner_14[slice62],\r\n",
        "       train_regret_winner_15[slice62],\r\n",
        "       train_regret_winner_16[slice62],\r\n",
        "       train_regret_winner_17[slice62],\r\n",
        "       train_regret_winner_18[slice62],\r\n",
        "       train_regret_winner_19[slice62],\r\n",
        "       train_regret_winner_20[slice62]]\r\n",
        "\r\n",
        "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\r\n",
        "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\r\n",
        "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\r\n",
        "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\r\n",
        "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\r\n",
        "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DZ8kHHmw-qK"
      },
      "source": [
        "# Iteration72 :\r\n",
        "\r\n",
        "slice72 = 71\r\n",
        "\r\n",
        "loser72 = [train_regret_loser_1[slice72],\r\n",
        "       train_regret_loser_2[slice72],\r\n",
        "       train_regret_loser_3[slice72],\r\n",
        "       train_regret_loser_4[slice72],\r\n",
        "       train_regret_loser_5[slice72],\r\n",
        "       train_regret_loser_6[slice72],\r\n",
        "       train_regret_loser_7[slice72],\r\n",
        "       train_regret_loser_8[slice72],\r\n",
        "       train_regret_loser_9[slice72],\r\n",
        "       train_regret_loser_10[slice72],\r\n",
        "       train_regret_loser_11[slice72],\r\n",
        "       train_regret_loser_12[slice72],\r\n",
        "       train_regret_loser_13[slice72],\r\n",
        "       train_regret_loser_14[slice72],\r\n",
        "       train_regret_loser_15[slice72],\r\n",
        "       train_regret_loser_16[slice72],\r\n",
        "       train_regret_loser_17[slice72],\r\n",
        "       train_regret_loser_18[slice72],\r\n",
        "       train_regret_loser_19[slice72],\r\n",
        "       train_regret_loser_20[slice72]]\r\n",
        "\r\n",
        "winner72 = [train_regret_winner_1[slice72],\r\n",
        "       train_regret_winner_2[slice72],\r\n",
        "       train_regret_winner_3[slice72],\r\n",
        "       train_regret_winner_4[slice72],\r\n",
        "       train_regret_winner_5[slice72],\r\n",
        "       train_regret_winner_6[slice72],\r\n",
        "       train_regret_winner_7[slice72],\r\n",
        "       train_regret_winner_8[slice72],\r\n",
        "       train_regret_winner_9[slice72],\r\n",
        "       train_regret_winner_10[slice72],\r\n",
        "       train_regret_winner_11[slice72],\r\n",
        "       train_regret_winner_12[slice72],\r\n",
        "       train_regret_winner_13[slice72],\r\n",
        "       train_regret_winner_14[slice72],\r\n",
        "       train_regret_winner_15[slice72],\r\n",
        "       train_regret_winner_16[slice72],\r\n",
        "       train_regret_winner_17[slice72],\r\n",
        "       train_regret_winner_18[slice72],\r\n",
        "       train_regret_winner_19[slice72],\r\n",
        "       train_regret_winner_20[slice72]]\r\n",
        "\r\n",
        "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\r\n",
        "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\r\n",
        "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\r\n",
        "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\r\n",
        "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\r\n",
        "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBzBvnVyw-sz"
      },
      "source": [
        "# Iteration82 :\r\n",
        "\r\n",
        "slice82 = 81\r\n",
        "\r\n",
        "loser82 = [train_regret_loser_1[slice82],\r\n",
        "       train_regret_loser_2[slice82],\r\n",
        "       train_regret_loser_3[slice82],\r\n",
        "       train_regret_loser_4[slice82],\r\n",
        "       train_regret_loser_5[slice82],\r\n",
        "       train_regret_loser_6[slice82],\r\n",
        "       train_regret_loser_7[slice82],\r\n",
        "       train_regret_loser_8[slice82],\r\n",
        "       train_regret_loser_9[slice82],\r\n",
        "       train_regret_loser_10[slice82],\r\n",
        "       train_regret_loser_11[slice82],\r\n",
        "       train_regret_loser_12[slice82],\r\n",
        "       train_regret_loser_13[slice82],\r\n",
        "       train_regret_loser_14[slice82],\r\n",
        "       train_regret_loser_15[slice82],\r\n",
        "       train_regret_loser_16[slice82],\r\n",
        "       train_regret_loser_17[slice82],\r\n",
        "       train_regret_loser_18[slice82],\r\n",
        "       train_regret_loser_19[slice82],\r\n",
        "       train_regret_loser_20[slice82]]\r\n",
        "\r\n",
        "winner82 = [train_regret_winner_1[slice82],\r\n",
        "       train_regret_winner_2[slice82],\r\n",
        "       train_regret_winner_3[slice82],\r\n",
        "       train_regret_winner_4[slice82],\r\n",
        "       train_regret_winner_5[slice82],\r\n",
        "       train_regret_winner_6[slice82],\r\n",
        "       train_regret_winner_7[slice82],\r\n",
        "       train_regret_winner_8[slice82],\r\n",
        "       train_regret_winner_9[slice82],\r\n",
        "       train_regret_winner_10[slice82],\r\n",
        "       train_regret_winner_11[slice82],\r\n",
        "       train_regret_winner_12[slice82],\r\n",
        "       train_regret_winner_13[slice82],\r\n",
        "       train_regret_winner_14[slice82],\r\n",
        "       train_regret_winner_15[slice82],\r\n",
        "       train_regret_winner_16[slice82],\r\n",
        "       train_regret_winner_17[slice82],\r\n",
        "       train_regret_winner_18[slice82],\r\n",
        "       train_regret_winner_19[slice82],\r\n",
        "       train_regret_winner_20[slice82]]\r\n",
        "\r\n",
        "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\r\n",
        "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\r\n",
        "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\r\n",
        "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\r\n",
        "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\r\n",
        "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qj--n6Bw-y2"
      },
      "source": [
        "# Iteration92 :\r\n",
        "\r\n",
        "slice92 = 91\r\n",
        "\r\n",
        "loser92 = [train_regret_loser_1[slice92],\r\n",
        "       train_regret_loser_2[slice92],\r\n",
        "       train_regret_loser_3[slice92],\r\n",
        "       train_regret_loser_4[slice92],\r\n",
        "       train_regret_loser_5[slice92],\r\n",
        "       train_regret_loser_6[slice92],\r\n",
        "       train_regret_loser_7[slice92],\r\n",
        "       train_regret_loser_8[slice92],\r\n",
        "       train_regret_loser_9[slice92],\r\n",
        "       train_regret_loser_10[slice92],\r\n",
        "       train_regret_loser_11[slice92],\r\n",
        "       train_regret_loser_12[slice92],\r\n",
        "       train_regret_loser_13[slice92],\r\n",
        "       train_regret_loser_14[slice92],\r\n",
        "       train_regret_loser_15[slice92],\r\n",
        "       train_regret_loser_16[slice92],\r\n",
        "       train_regret_loser_17[slice92],\r\n",
        "       train_regret_loser_18[slice92],\r\n",
        "       train_regret_loser_19[slice92],\r\n",
        "       train_regret_loser_20[slice92]]\r\n",
        "\r\n",
        "winner92 = [train_regret_winner_1[slice92],\r\n",
        "       train_regret_winner_2[slice92],\r\n",
        "       train_regret_winner_3[slice92],\r\n",
        "       train_regret_winner_4[slice92],\r\n",
        "       train_regret_winner_5[slice92],\r\n",
        "       train_regret_winner_6[slice92],\r\n",
        "       train_regret_winner_7[slice92],\r\n",
        "       train_regret_winner_8[slice92],\r\n",
        "       train_regret_winner_9[slice92],\r\n",
        "       train_regret_winner_10[slice92],\r\n",
        "       train_regret_winner_11[slice92],\r\n",
        "       train_regret_winner_12[slice92],\r\n",
        "       train_regret_winner_13[slice92],\r\n",
        "       train_regret_winner_14[slice92],\r\n",
        "       train_regret_winner_15[slice92],\r\n",
        "       train_regret_winner_16[slice92],\r\n",
        "       train_regret_winner_17[slice92],\r\n",
        "       train_regret_winner_18[slice92],\r\n",
        "       train_regret_winner_19[slice92],\r\n",
        "       train_regret_winner_20[slice92]]\r\n",
        "\r\n",
        "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\r\n",
        "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\r\n",
        "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\r\n",
        "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\r\n",
        "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\r\n",
        "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJ5KqLdw-0L"
      },
      "source": [
        "# Iteration3 :\r\n",
        "\r\n",
        "slice3 = 2\r\n",
        "\r\n",
        "loser3 = [train_regret_loser_1[slice3],\r\n",
        "       train_regret_loser_2[slice3],\r\n",
        "       train_regret_loser_3[slice3],\r\n",
        "       train_regret_loser_4[slice3],\r\n",
        "       train_regret_loser_5[slice3],\r\n",
        "       train_regret_loser_6[slice3],\r\n",
        "       train_regret_loser_7[slice3],\r\n",
        "       train_regret_loser_8[slice3],\r\n",
        "       train_regret_loser_9[slice3],\r\n",
        "       train_regret_loser_10[slice3],\r\n",
        "       train_regret_loser_11[slice3],\r\n",
        "       train_regret_loser_12[slice3],\r\n",
        "       train_regret_loser_13[slice3],\r\n",
        "       train_regret_loser_14[slice3],\r\n",
        "       train_regret_loser_15[slice3],\r\n",
        "       train_regret_loser_16[slice3],\r\n",
        "       train_regret_loser_17[slice3],\r\n",
        "       train_regret_loser_18[slice3],\r\n",
        "       train_regret_loser_19[slice3],\r\n",
        "       train_regret_loser_20[slice3]]\r\n",
        "\r\n",
        "winner3 = [train_regret_winner_1[slice3],\r\n",
        "       train_regret_winner_2[slice3],\r\n",
        "       train_regret_winner_3[slice3],\r\n",
        "       train_regret_winner_4[slice3],\r\n",
        "       train_regret_winner_5[slice3],\r\n",
        "       train_regret_winner_6[slice3],\r\n",
        "       train_regret_winner_7[slice3],\r\n",
        "       train_regret_winner_8[slice3],\r\n",
        "       train_regret_winner_9[slice3],\r\n",
        "       train_regret_winner_10[slice3],\r\n",
        "       train_regret_winner_11[slice3],\r\n",
        "       train_regret_winner_12[slice3],\r\n",
        "       train_regret_winner_13[slice3],\r\n",
        "       train_regret_winner_14[slice3],\r\n",
        "       train_regret_winner_15[slice3],\r\n",
        "       train_regret_winner_16[slice3],\r\n",
        "       train_regret_winner_17[slice3],\r\n",
        "       train_regret_winner_18[slice3],\r\n",
        "       train_regret_winner_19[slice3],\r\n",
        "       train_regret_winner_20[slice3]]\r\n",
        "\r\n",
        "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\r\n",
        "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\r\n",
        "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\r\n",
        "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\r\n",
        "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\r\n",
        "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkaoT1dyw-16"
      },
      "source": [
        "# Iteration13 :\r\n",
        "\r\n",
        "slice13 = 12\r\n",
        "\r\n",
        "loser13 = [train_regret_loser_1[slice13],\r\n",
        "       train_regret_loser_2[slice13],\r\n",
        "       train_regret_loser_3[slice13],\r\n",
        "       train_regret_loser_4[slice13],\r\n",
        "       train_regret_loser_5[slice13],\r\n",
        "       train_regret_loser_6[slice13],\r\n",
        "       train_regret_loser_7[slice13],\r\n",
        "       train_regret_loser_8[slice13],\r\n",
        "       train_regret_loser_9[slice13],\r\n",
        "       train_regret_loser_10[slice13],\r\n",
        "       train_regret_loser_11[slice13],\r\n",
        "       train_regret_loser_12[slice13],\r\n",
        "       train_regret_loser_13[slice13],\r\n",
        "       train_regret_loser_14[slice13],\r\n",
        "       train_regret_loser_15[slice13],\r\n",
        "       train_regret_loser_16[slice13],\r\n",
        "       train_regret_loser_17[slice13],\r\n",
        "       train_regret_loser_18[slice13],\r\n",
        "       train_regret_loser_19[slice13],\r\n",
        "       train_regret_loser_20[slice13]]\r\n",
        "\r\n",
        "winner13 = [train_regret_winner_1[slice13],\r\n",
        "       train_regret_winner_2[slice13],\r\n",
        "       train_regret_winner_3[slice13],\r\n",
        "       train_regret_winner_4[slice13],\r\n",
        "       train_regret_winner_5[slice13],\r\n",
        "       train_regret_winner_6[slice13],\r\n",
        "       train_regret_winner_7[slice13],\r\n",
        "       train_regret_winner_8[slice13],\r\n",
        "       train_regret_winner_9[slice13],\r\n",
        "       train_regret_winner_10[slice13],\r\n",
        "       train_regret_winner_11[slice13],\r\n",
        "       train_regret_winner_12[slice13],\r\n",
        "       train_regret_winner_13[slice13],\r\n",
        "       train_regret_winner_14[slice13],\r\n",
        "       train_regret_winner_15[slice13],\r\n",
        "       train_regret_winner_16[slice13],\r\n",
        "       train_regret_winner_17[slice13],\r\n",
        "       train_regret_winner_18[slice13],\r\n",
        "       train_regret_winner_19[slice13],\r\n",
        "       train_regret_winner_20[slice13]]\r\n",
        "\r\n",
        "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\r\n",
        "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\r\n",
        "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\r\n",
        "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\r\n",
        "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZbVMrBaw-4B"
      },
      "source": [
        "# Iteration23 :\r\n",
        "\r\n",
        "slice23 = 22\r\n",
        "\r\n",
        "loser23 = [train_regret_loser_1[slice23],\r\n",
        "       train_regret_loser_2[slice23],\r\n",
        "       train_regret_loser_3[slice23],\r\n",
        "       train_regret_loser_4[slice23],\r\n",
        "       train_regret_loser_5[slice23],\r\n",
        "       train_regret_loser_6[slice23],\r\n",
        "       train_regret_loser_7[slice23],\r\n",
        "       train_regret_loser_8[slice23],\r\n",
        "       train_regret_loser_9[slice23],\r\n",
        "       train_regret_loser_10[slice23],\r\n",
        "       train_regret_loser_11[slice23],\r\n",
        "       train_regret_loser_12[slice23],\r\n",
        "       train_regret_loser_13[slice23],\r\n",
        "       train_regret_loser_14[slice23],\r\n",
        "       train_regret_loser_15[slice23],\r\n",
        "       train_regret_loser_16[slice23],\r\n",
        "       train_regret_loser_17[slice23],\r\n",
        "       train_regret_loser_18[slice23],\r\n",
        "       train_regret_loser_19[slice23],\r\n",
        "       train_regret_loser_20[slice23]]\r\n",
        "\r\n",
        "winner23 = [train_regret_winner_1[slice23],\r\n",
        "       train_regret_winner_2[slice23],\r\n",
        "       train_regret_winner_3[slice23],\r\n",
        "       train_regret_winner_4[slice23],\r\n",
        "       train_regret_winner_5[slice23],\r\n",
        "       train_regret_winner_6[slice23],\r\n",
        "       train_regret_winner_7[slice23],\r\n",
        "       train_regret_winner_8[slice23],\r\n",
        "       train_regret_winner_9[slice23],\r\n",
        "       train_regret_winner_10[slice23],\r\n",
        "       train_regret_winner_11[slice23],\r\n",
        "       train_regret_winner_12[slice23],\r\n",
        "       train_regret_winner_13[slice23],\r\n",
        "       train_regret_winner_14[slice23],\r\n",
        "       train_regret_winner_15[slice23],\r\n",
        "       train_regret_winner_16[slice23],\r\n",
        "       train_regret_winner_17[slice23],\r\n",
        "       train_regret_winner_18[slice23],\r\n",
        "       train_regret_winner_19[slice23],\r\n",
        "       train_regret_winner_20[slice23]]\r\n",
        "\r\n",
        "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\r\n",
        "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\r\n",
        "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\r\n",
        "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\r\n",
        "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\r\n",
        "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUa6HRCUw-7G"
      },
      "source": [
        "# Iteration33 :\r\n",
        "\r\n",
        "slice33 = 32\r\n",
        "\r\n",
        "loser33 = [train_regret_loser_1[slice33],\r\n",
        "       train_regret_loser_2[slice33],\r\n",
        "       train_regret_loser_3[slice33],\r\n",
        "       train_regret_loser_4[slice33],\r\n",
        "       train_regret_loser_5[slice33],\r\n",
        "       train_regret_loser_6[slice33],\r\n",
        "       train_regret_loser_7[slice33],\r\n",
        "       train_regret_loser_8[slice33],\r\n",
        "       train_regret_loser_9[slice33],\r\n",
        "       train_regret_loser_10[slice33],\r\n",
        "       train_regret_loser_11[slice33],\r\n",
        "       train_regret_loser_12[slice33],\r\n",
        "       train_regret_loser_13[slice33],\r\n",
        "       train_regret_loser_14[slice33],\r\n",
        "       train_regret_loser_15[slice33],\r\n",
        "       train_regret_loser_16[slice33],\r\n",
        "       train_regret_loser_17[slice33],\r\n",
        "       train_regret_loser_18[slice33],\r\n",
        "       train_regret_loser_19[slice33],\r\n",
        "       train_regret_loser_20[slice33]]\r\n",
        "\r\n",
        "winner33 = [train_regret_winner_1[slice33],\r\n",
        "       train_regret_winner_2[slice33],\r\n",
        "       train_regret_winner_3[slice33],\r\n",
        "       train_regret_winner_4[slice33],\r\n",
        "       train_regret_winner_5[slice33],\r\n",
        "       train_regret_winner_6[slice33],\r\n",
        "       train_regret_winner_7[slice33],\r\n",
        "       train_regret_winner_8[slice33],\r\n",
        "       train_regret_winner_9[slice33],\r\n",
        "       train_regret_winner_10[slice33],\r\n",
        "       train_regret_winner_11[slice33],\r\n",
        "       train_regret_winner_12[slice33],\r\n",
        "       train_regret_winner_13[slice33],\r\n",
        "       train_regret_winner_14[slice33],\r\n",
        "       train_regret_winner_15[slice33],\r\n",
        "       train_regret_winner_16[slice33],\r\n",
        "       train_regret_winner_17[slice33],\r\n",
        "       train_regret_winner_18[slice33],\r\n",
        "       train_regret_winner_19[slice33],\r\n",
        "       train_regret_winner_20[slice33]]\r\n",
        "\r\n",
        "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\r\n",
        "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\r\n",
        "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\r\n",
        "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\r\n",
        "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\r\n",
        "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EVxQwsxw-9n"
      },
      "source": [
        "# Iteration43 :\r\n",
        "\r\n",
        "slice43 = 42\r\n",
        "\r\n",
        "loser43 = [train_regret_loser_1[slice43],\r\n",
        "       train_regret_loser_2[slice43],\r\n",
        "       train_regret_loser_3[slice43],\r\n",
        "       train_regret_loser_4[slice43],\r\n",
        "       train_regret_loser_5[slice43],\r\n",
        "       train_regret_loser_6[slice43],\r\n",
        "       train_regret_loser_7[slice43],\r\n",
        "       train_regret_loser_8[slice43],\r\n",
        "       train_regret_loser_9[slice43],\r\n",
        "       train_regret_loser_10[slice43],\r\n",
        "       train_regret_loser_11[slice43],\r\n",
        "       train_regret_loser_12[slice43],\r\n",
        "       train_regret_loser_13[slice43],\r\n",
        "       train_regret_loser_14[slice43],\r\n",
        "       train_regret_loser_15[slice43],\r\n",
        "       train_regret_loser_16[slice43],\r\n",
        "       train_regret_loser_17[slice43],\r\n",
        "       train_regret_loser_18[slice43],\r\n",
        "       train_regret_loser_19[slice43],\r\n",
        "       train_regret_loser_20[slice43]]\r\n",
        "\r\n",
        "winner43 = [train_regret_winner_1[slice43],\r\n",
        "       train_regret_winner_2[slice43],\r\n",
        "       train_regret_winner_3[slice43],\r\n",
        "       train_regret_winner_4[slice43],\r\n",
        "       train_regret_winner_5[slice43],\r\n",
        "       train_regret_winner_6[slice43],\r\n",
        "       train_regret_winner_7[slice43],\r\n",
        "       train_regret_winner_8[slice43],\r\n",
        "       train_regret_winner_9[slice43],\r\n",
        "       train_regret_winner_10[slice43],\r\n",
        "       train_regret_winner_11[slice43],\r\n",
        "       train_regret_winner_12[slice43],\r\n",
        "       train_regret_winner_13[slice43],\r\n",
        "       train_regret_winner_14[slice43],\r\n",
        "       train_regret_winner_15[slice43],\r\n",
        "       train_regret_winner_16[slice43],\r\n",
        "       train_regret_winner_17[slice43],\r\n",
        "       train_regret_winner_18[slice43],\r\n",
        "       train_regret_winner_19[slice43],\r\n",
        "       train_regret_winner_20[slice43]]\r\n",
        "\r\n",
        "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\r\n",
        "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\r\n",
        "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\r\n",
        "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\r\n",
        "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\r\n",
        "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQtRyMv-w_AZ"
      },
      "source": [
        "# Iteration53 :\r\n",
        "\r\n",
        "slice53 = 52\r\n",
        "\r\n",
        "loser53 = [train_regret_loser_1[slice53],\r\n",
        "       train_regret_loser_2[slice53],\r\n",
        "       train_regret_loser_3[slice53],\r\n",
        "       train_regret_loser_4[slice53],\r\n",
        "       train_regret_loser_5[slice53],\r\n",
        "       train_regret_loser_6[slice53],\r\n",
        "       train_regret_loser_7[slice53],\r\n",
        "       train_regret_loser_8[slice53],\r\n",
        "       train_regret_loser_9[slice53],\r\n",
        "       train_regret_loser_10[slice53],\r\n",
        "       train_regret_loser_11[slice53],\r\n",
        "       train_regret_loser_12[slice53],\r\n",
        "       train_regret_loser_13[slice53],\r\n",
        "       train_regret_loser_14[slice53],\r\n",
        "       train_regret_loser_15[slice53],\r\n",
        "       train_regret_loser_16[slice53],\r\n",
        "       train_regret_loser_17[slice53],\r\n",
        "       train_regret_loser_18[slice53],\r\n",
        "       train_regret_loser_19[slice53],\r\n",
        "       train_regret_loser_20[slice53]]\r\n",
        "\r\n",
        "winner53 = [train_regret_winner_1[slice53],\r\n",
        "       train_regret_winner_2[slice53],\r\n",
        "       train_regret_winner_3[slice53],\r\n",
        "       train_regret_winner_4[slice53],\r\n",
        "       train_regret_winner_5[slice53],\r\n",
        "       train_regret_winner_6[slice53],\r\n",
        "       train_regret_winner_7[slice53],\r\n",
        "       train_regret_winner_8[slice53],\r\n",
        "       train_regret_winner_9[slice53],\r\n",
        "       train_regret_winner_10[slice53],\r\n",
        "       train_regret_winner_11[slice53],\r\n",
        "       train_regret_winner_12[slice53],\r\n",
        "       train_regret_winner_13[slice53],\r\n",
        "       train_regret_winner_14[slice53],\r\n",
        "       train_regret_winner_15[slice53],\r\n",
        "       train_regret_winner_16[slice53],\r\n",
        "       train_regret_winner_17[slice53],\r\n",
        "       train_regret_winner_18[slice53],\r\n",
        "       train_regret_winner_19[slice53],\r\n",
        "       train_regret_winner_20[slice53]]\r\n",
        "\r\n",
        "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\r\n",
        "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\r\n",
        "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\r\n",
        "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\r\n",
        "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\r\n",
        "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CIMEXXYw_DR"
      },
      "source": [
        "# Iteration63 :\r\n",
        "\r\n",
        "slice63 = 62\r\n",
        "\r\n",
        "loser63 = [train_regret_loser_1[slice63],\r\n",
        "       train_regret_loser_2[slice63],\r\n",
        "       train_regret_loser_3[slice63],\r\n",
        "       train_regret_loser_4[slice63],\r\n",
        "       train_regret_loser_5[slice63],\r\n",
        "       train_regret_loser_6[slice63],\r\n",
        "       train_regret_loser_7[slice63],\r\n",
        "       train_regret_loser_8[slice63],\r\n",
        "       train_regret_loser_9[slice63],\r\n",
        "       train_regret_loser_10[slice63],\r\n",
        "       train_regret_loser_11[slice63],\r\n",
        "       train_regret_loser_12[slice63],\r\n",
        "       train_regret_loser_13[slice63],\r\n",
        "       train_regret_loser_14[slice63],\r\n",
        "       train_regret_loser_15[slice63],\r\n",
        "       train_regret_loser_16[slice63],\r\n",
        "       train_regret_loser_17[slice63],\r\n",
        "       train_regret_loser_18[slice63],\r\n",
        "       train_regret_loser_19[slice63],\r\n",
        "       train_regret_loser_20[slice63]]\r\n",
        "\r\n",
        "winner63 = [train_regret_winner_1[slice63],\r\n",
        "       train_regret_winner_2[slice63],\r\n",
        "       train_regret_winner_3[slice63],\r\n",
        "       train_regret_winner_4[slice63],\r\n",
        "       train_regret_winner_5[slice63],\r\n",
        "       train_regret_winner_6[slice63],\r\n",
        "       train_regret_winner_7[slice63],\r\n",
        "       train_regret_winner_8[slice63],\r\n",
        "       train_regret_winner_9[slice63],\r\n",
        "       train_regret_winner_10[slice63],\r\n",
        "       train_regret_winner_11[slice63],\r\n",
        "       train_regret_winner_12[slice63],\r\n",
        "       train_regret_winner_13[slice63],\r\n",
        "       train_regret_winner_14[slice63],\r\n",
        "       train_regret_winner_15[slice63],\r\n",
        "       train_regret_winner_16[slice63],\r\n",
        "       train_regret_winner_17[slice63],\r\n",
        "       train_regret_winner_18[slice63],\r\n",
        "       train_regret_winner_19[slice63],\r\n",
        "       train_regret_winner_20[slice63]]\r\n",
        "\r\n",
        "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\r\n",
        "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\r\n",
        "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\r\n",
        "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\r\n",
        "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\r\n",
        "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg4GfwU_w_GF"
      },
      "source": [
        "# Iteration73 :\r\n",
        "\r\n",
        "slice73 = 72\r\n",
        "\r\n",
        "loser73 = [train_regret_loser_1[slice73],\r\n",
        "       train_regret_loser_2[slice73],\r\n",
        "       train_regret_loser_3[slice73],\r\n",
        "       train_regret_loser_4[slice73],\r\n",
        "       train_regret_loser_5[slice73],\r\n",
        "       train_regret_loser_6[slice73],\r\n",
        "       train_regret_loser_7[slice73],\r\n",
        "       train_regret_loser_8[slice73],\r\n",
        "       train_regret_loser_9[slice73],\r\n",
        "       train_regret_loser_10[slice73],\r\n",
        "       train_regret_loser_11[slice73],\r\n",
        "       train_regret_loser_12[slice73],\r\n",
        "       train_regret_loser_13[slice73],\r\n",
        "       train_regret_loser_14[slice73],\r\n",
        "       train_regret_loser_15[slice73],\r\n",
        "       train_regret_loser_16[slice73],\r\n",
        "       train_regret_loser_17[slice73],\r\n",
        "       train_regret_loser_18[slice73],\r\n",
        "       train_regret_loser_19[slice73],\r\n",
        "       train_regret_loser_20[slice73]]\r\n",
        "\r\n",
        "winner73 = [train_regret_winner_1[slice73],\r\n",
        "       train_regret_winner_2[slice73],\r\n",
        "       train_regret_winner_3[slice73],\r\n",
        "       train_regret_winner_4[slice73],\r\n",
        "       train_regret_winner_5[slice73],\r\n",
        "       train_regret_winner_6[slice73],\r\n",
        "       train_regret_winner_7[slice73],\r\n",
        "       train_regret_winner_8[slice73],\r\n",
        "       train_regret_winner_9[slice73],\r\n",
        "       train_regret_winner_10[slice73],\r\n",
        "       train_regret_winner_11[slice73],\r\n",
        "       train_regret_winner_12[slice73],\r\n",
        "       train_regret_winner_13[slice73],\r\n",
        "       train_regret_winner_14[slice73],\r\n",
        "       train_regret_winner_15[slice73],\r\n",
        "       train_regret_winner_16[slice73],\r\n",
        "       train_regret_winner_17[slice73],\r\n",
        "       train_regret_winner_18[slice73],\r\n",
        "       train_regret_winner_19[slice73],\r\n",
        "       train_regret_winner_20[slice73]]\r\n",
        "\r\n",
        "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\r\n",
        "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\r\n",
        "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\r\n",
        "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\r\n",
        "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\r\n",
        "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WmpcEV4w_I9"
      },
      "source": [
        "# Iteration83 :\r\n",
        "\r\n",
        "slice83 = 82\r\n",
        "\r\n",
        "loser83 = [train_regret_loser_1[slice83],\r\n",
        "       train_regret_loser_2[slice83],\r\n",
        "       train_regret_loser_3[slice83],\r\n",
        "       train_regret_loser_4[slice83],\r\n",
        "       train_regret_loser_5[slice83],\r\n",
        "       train_regret_loser_6[slice83],\r\n",
        "       train_regret_loser_7[slice83],\r\n",
        "       train_regret_loser_8[slice83],\r\n",
        "       train_regret_loser_9[slice83],\r\n",
        "       train_regret_loser_10[slice83],\r\n",
        "       train_regret_loser_11[slice83],\r\n",
        "       train_regret_loser_12[slice83],\r\n",
        "       train_regret_loser_13[slice83],\r\n",
        "       train_regret_loser_14[slice83],\r\n",
        "       train_regret_loser_15[slice83],\r\n",
        "       train_regret_loser_16[slice83],\r\n",
        "       train_regret_loser_17[slice83],\r\n",
        "       train_regret_loser_18[slice83],\r\n",
        "       train_regret_loser_19[slice83],\r\n",
        "       train_regret_loser_20[slice83]]\r\n",
        "\r\n",
        "winner83 = [train_regret_winner_1[slice83],\r\n",
        "       train_regret_winner_2[slice83],\r\n",
        "       train_regret_winner_3[slice83],\r\n",
        "       train_regret_winner_4[slice83],\r\n",
        "       train_regret_winner_5[slice83],\r\n",
        "       train_regret_winner_6[slice83],\r\n",
        "       train_regret_winner_7[slice83],\r\n",
        "       train_regret_winner_8[slice83],\r\n",
        "       train_regret_winner_9[slice83],\r\n",
        "       train_regret_winner_10[slice83],\r\n",
        "       train_regret_winner_11[slice83],\r\n",
        "       train_regret_winner_12[slice83],\r\n",
        "       train_regret_winner_13[slice83],\r\n",
        "       train_regret_winner_14[slice83],\r\n",
        "       train_regret_winner_15[slice83],\r\n",
        "       train_regret_winner_16[slice83],\r\n",
        "       train_regret_winner_17[slice83],\r\n",
        "       train_regret_winner_18[slice83],\r\n",
        "       train_regret_winner_19[slice83],\r\n",
        "       train_regret_winner_20[slice83]]\r\n",
        "\r\n",
        "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\r\n",
        "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\r\n",
        "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\r\n",
        "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\r\n",
        "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\r\n",
        "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGPwoBZyw_MF"
      },
      "source": [
        "# Iteration93 :\r\n",
        "\r\n",
        "slice93 = 92\r\n",
        "\r\n",
        "loser93 = [train_regret_loser_1[slice93],\r\n",
        "       train_regret_loser_2[slice93],\r\n",
        "       train_regret_loser_3[slice93],\r\n",
        "       train_regret_loser_4[slice93],\r\n",
        "       train_regret_loser_5[slice93],\r\n",
        "       train_regret_loser_6[slice93],\r\n",
        "       train_regret_loser_7[slice93],\r\n",
        "       train_regret_loser_8[slice93],\r\n",
        "       train_regret_loser_9[slice93],\r\n",
        "       train_regret_loser_10[slice93],\r\n",
        "       train_regret_loser_11[slice93],\r\n",
        "       train_regret_loser_12[slice93],\r\n",
        "       train_regret_loser_13[slice93],\r\n",
        "       train_regret_loser_14[slice93],\r\n",
        "       train_regret_loser_15[slice93],\r\n",
        "       train_regret_loser_16[slice93],\r\n",
        "       train_regret_loser_17[slice93],\r\n",
        "       train_regret_loser_18[slice93],\r\n",
        "       train_regret_loser_19[slice93],\r\n",
        "       train_regret_loser_20[slice93]]\r\n",
        "\r\n",
        "winner93 = [train_regret_winner_1[slice93],\r\n",
        "       train_regret_winner_2[slice93],\r\n",
        "       train_regret_winner_3[slice93],\r\n",
        "       train_regret_winner_4[slice93],\r\n",
        "       train_regret_winner_5[slice93],\r\n",
        "       train_regret_winner_6[slice93],\r\n",
        "       train_regret_winner_7[slice93],\r\n",
        "       train_regret_winner_8[slice93],\r\n",
        "       train_regret_winner_9[slice93],\r\n",
        "       train_regret_winner_10[slice93],\r\n",
        "       train_regret_winner_11[slice93],\r\n",
        "       train_regret_winner_12[slice93],\r\n",
        "       train_regret_winner_13[slice93],\r\n",
        "       train_regret_winner_14[slice93],\r\n",
        "       train_regret_winner_15[slice93],\r\n",
        "       train_regret_winner_16[slice93],\r\n",
        "       train_regret_winner_17[slice93],\r\n",
        "       train_regret_winner_18[slice93],\r\n",
        "       train_regret_winner_19[slice93],\r\n",
        "       train_regret_winner_20[slice93]]\r\n",
        "\r\n",
        "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\r\n",
        "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\r\n",
        "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\r\n",
        "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\r\n",
        "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\r\n",
        "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ghdGzPw_Oq"
      },
      "source": [
        "# Iteration4 :\r\n",
        "\r\n",
        "slice4 = 3\r\n",
        "\r\n",
        "loser4 = [train_regret_loser_1[slice4],\r\n",
        "       train_regret_loser_2[slice4],\r\n",
        "       train_regret_loser_3[slice4],\r\n",
        "       train_regret_loser_4[slice4],\r\n",
        "       train_regret_loser_5[slice4],\r\n",
        "       train_regret_loser_6[slice4],\r\n",
        "       train_regret_loser_7[slice4],\r\n",
        "       train_regret_loser_8[slice4],\r\n",
        "       train_regret_loser_9[slice4],\r\n",
        "       train_regret_loser_10[slice4],\r\n",
        "       train_regret_loser_11[slice4],\r\n",
        "       train_regret_loser_12[slice4],\r\n",
        "       train_regret_loser_13[slice4],\r\n",
        "       train_regret_loser_14[slice4],\r\n",
        "       train_regret_loser_15[slice4],\r\n",
        "       train_regret_loser_16[slice4],\r\n",
        "       train_regret_loser_17[slice4],\r\n",
        "       train_regret_loser_18[slice4],\r\n",
        "       train_regret_loser_19[slice4],\r\n",
        "       train_regret_loser_20[slice4]]\r\n",
        "\r\n",
        "winner4 = [train_regret_winner_1[slice4],\r\n",
        "       train_regret_winner_2[slice4],\r\n",
        "       train_regret_winner_3[slice4],\r\n",
        "       train_regret_winner_4[slice4],\r\n",
        "       train_regret_winner_5[slice4],\r\n",
        "       train_regret_winner_6[slice4],\r\n",
        "       train_regret_winner_7[slice4],\r\n",
        "       train_regret_winner_8[slice4],\r\n",
        "       train_regret_winner_9[slice4],\r\n",
        "       train_regret_winner_10[slice4],\r\n",
        "       train_regret_winner_11[slice4],\r\n",
        "       train_regret_winner_12[slice4],\r\n",
        "       train_regret_winner_13[slice4],\r\n",
        "       train_regret_winner_14[slice4],\r\n",
        "       train_regret_winner_15[slice4],\r\n",
        "       train_regret_winner_16[slice4],\r\n",
        "       train_regret_winner_17[slice4],\r\n",
        "       train_regret_winner_18[slice4],\r\n",
        "       train_regret_winner_19[slice4],\r\n",
        "       train_regret_winner_20[slice4]]\r\n",
        "\r\n",
        "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\r\n",
        "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\r\n",
        "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\r\n",
        "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\r\n",
        "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\r\n",
        "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNvDt4QMw_Rz"
      },
      "source": [
        "# Iteration14 :\r\n",
        "\r\n",
        "slice14 = 13\r\n",
        "\r\n",
        "loser14 = [train_regret_loser_1[slice14],\r\n",
        "       train_regret_loser_2[slice14],\r\n",
        "       train_regret_loser_3[slice14],\r\n",
        "       train_regret_loser_4[slice14],\r\n",
        "       train_regret_loser_5[slice14],\r\n",
        "       train_regret_loser_6[slice14],\r\n",
        "       train_regret_loser_7[slice14],\r\n",
        "       train_regret_loser_8[slice14],\r\n",
        "       train_regret_loser_9[slice14],\r\n",
        "       train_regret_loser_10[slice14],\r\n",
        "       train_regret_loser_11[slice14],\r\n",
        "       train_regret_loser_12[slice14],\r\n",
        "       train_regret_loser_13[slice14],\r\n",
        "       train_regret_loser_14[slice14],\r\n",
        "       train_regret_loser_15[slice14],\r\n",
        "       train_regret_loser_16[slice14],\r\n",
        "       train_regret_loser_17[slice14],\r\n",
        "       train_regret_loser_18[slice14],\r\n",
        "       train_regret_loser_19[slice14],\r\n",
        "       train_regret_loser_20[slice14]]\r\n",
        "\r\n",
        "winner14 = [train_regret_winner_1[slice14],\r\n",
        "       train_regret_winner_2[slice14],\r\n",
        "       train_regret_winner_3[slice14],\r\n",
        "       train_regret_winner_4[slice14],\r\n",
        "       train_regret_winner_5[slice14],\r\n",
        "       train_regret_winner_6[slice14],\r\n",
        "       train_regret_winner_7[slice14],\r\n",
        "       train_regret_winner_8[slice14],\r\n",
        "       train_regret_winner_9[slice14],\r\n",
        "       train_regret_winner_10[slice14],\r\n",
        "       train_regret_winner_11[slice14],\r\n",
        "       train_regret_winner_12[slice14],\r\n",
        "       train_regret_winner_13[slice14],\r\n",
        "       train_regret_winner_14[slice14],\r\n",
        "       train_regret_winner_15[slice14],\r\n",
        "       train_regret_winner_16[slice14],\r\n",
        "       train_regret_winner_17[slice14],\r\n",
        "       train_regret_winner_18[slice14],\r\n",
        "       train_regret_winner_19[slice14],\r\n",
        "       train_regret_winner_20[slice14]]\r\n",
        "\r\n",
        "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\r\n",
        "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\r\n",
        "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\r\n",
        "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\r\n",
        "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\r\n",
        "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3kMQV0w_Ua"
      },
      "source": [
        "# Iteration24 :\r\n",
        "\r\n",
        "slice24 = 23\r\n",
        "\r\n",
        "loser24 = [train_regret_loser_1[slice24],\r\n",
        "       train_regret_loser_2[slice24],\r\n",
        "       train_regret_loser_3[slice24],\r\n",
        "       train_regret_loser_4[slice24],\r\n",
        "       train_regret_loser_5[slice24],\r\n",
        "       train_regret_loser_6[slice24],\r\n",
        "       train_regret_loser_7[slice24],\r\n",
        "       train_regret_loser_8[slice24],\r\n",
        "       train_regret_loser_9[slice24],\r\n",
        "       train_regret_loser_10[slice24],\r\n",
        "       train_regret_loser_11[slice24],\r\n",
        "       train_regret_loser_12[slice24],\r\n",
        "       train_regret_loser_13[slice24],\r\n",
        "       train_regret_loser_14[slice24],\r\n",
        "       train_regret_loser_15[slice24],\r\n",
        "       train_regret_loser_16[slice24],\r\n",
        "       train_regret_loser_17[slice24],\r\n",
        "       train_regret_loser_18[slice24],\r\n",
        "       train_regret_loser_19[slice24],\r\n",
        "       train_regret_loser_20[slice24]]\r\n",
        "\r\n",
        "winner24 = [train_regret_winner_1[slice24],\r\n",
        "       train_regret_winner_2[slice24],\r\n",
        "       train_regret_winner_3[slice24],\r\n",
        "       train_regret_winner_4[slice24],\r\n",
        "       train_regret_winner_5[slice24],\r\n",
        "       train_regret_winner_6[slice24],\r\n",
        "       train_regret_winner_7[slice24],\r\n",
        "       train_regret_winner_8[slice24],\r\n",
        "       train_regret_winner_9[slice24],\r\n",
        "       train_regret_winner_10[slice24],\r\n",
        "       train_regret_winner_11[slice24],\r\n",
        "       train_regret_winner_12[slice24],\r\n",
        "       train_regret_winner_13[slice24],\r\n",
        "       train_regret_winner_14[slice24],\r\n",
        "       train_regret_winner_15[slice24],\r\n",
        "       train_regret_winner_16[slice24],\r\n",
        "       train_regret_winner_17[slice24],\r\n",
        "       train_regret_winner_18[slice24],\r\n",
        "       train_regret_winner_19[slice24],\r\n",
        "       train_regret_winner_20[slice24]]\r\n",
        "\r\n",
        "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\r\n",
        "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\r\n",
        "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\r\n",
        "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\r\n",
        "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\r\n",
        "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ljkyMfw_Xj"
      },
      "source": [
        "# Iteration34 :\r\n",
        "\r\n",
        "slice34 = 33\r\n",
        "\r\n",
        "loser34 = [train_regret_loser_1[slice34],\r\n",
        "       train_regret_loser_2[slice34],\r\n",
        "       train_regret_loser_3[slice34],\r\n",
        "       train_regret_loser_4[slice34],\r\n",
        "       train_regret_loser_5[slice34],\r\n",
        "       train_regret_loser_6[slice34],\r\n",
        "       train_regret_loser_7[slice34],\r\n",
        "       train_regret_loser_8[slice34],\r\n",
        "       train_regret_loser_9[slice34],\r\n",
        "       train_regret_loser_10[slice34],\r\n",
        "       train_regret_loser_11[slice34],\r\n",
        "       train_regret_loser_12[slice34],\r\n",
        "       train_regret_loser_13[slice34],\r\n",
        "       train_regret_loser_14[slice34],\r\n",
        "       train_regret_loser_15[slice34],\r\n",
        "       train_regret_loser_16[slice34],\r\n",
        "       train_regret_loser_17[slice34],\r\n",
        "       train_regret_loser_18[slice34],\r\n",
        "       train_regret_loser_19[slice34],\r\n",
        "       train_regret_loser_20[slice34]]\r\n",
        "\r\n",
        "winner34 = [train_regret_winner_1[slice34],\r\n",
        "       train_regret_winner_2[slice34],\r\n",
        "       train_regret_winner_3[slice34],\r\n",
        "       train_regret_winner_4[slice34],\r\n",
        "       train_regret_winner_5[slice34],\r\n",
        "       train_regret_winner_6[slice34],\r\n",
        "       train_regret_winner_7[slice34],\r\n",
        "       train_regret_winner_8[slice34],\r\n",
        "       train_regret_winner_9[slice34],\r\n",
        "       train_regret_winner_10[slice34],\r\n",
        "       train_regret_winner_11[slice34],\r\n",
        "       train_regret_winner_12[slice34],\r\n",
        "       train_regret_winner_13[slice34],\r\n",
        "       train_regret_winner_14[slice34],\r\n",
        "       train_regret_winner_15[slice34],\r\n",
        "       train_regret_winner_16[slice34],\r\n",
        "       train_regret_winner_17[slice34],\r\n",
        "       train_regret_winner_18[slice34],\r\n",
        "       train_regret_winner_19[slice34],\r\n",
        "       train_regret_winner_20[slice34]]\r\n",
        "\r\n",
        "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\r\n",
        "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\r\n",
        "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\r\n",
        "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\r\n",
        "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\r\n",
        "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGKj-NR8w_ae"
      },
      "source": [
        "# Iteration44 :\r\n",
        "\r\n",
        "slice44 = 43\r\n",
        "\r\n",
        "loser44 = [train_regret_loser_1[slice44],\r\n",
        "       train_regret_loser_2[slice44],\r\n",
        "       train_regret_loser_3[slice44],\r\n",
        "       train_regret_loser_4[slice44],\r\n",
        "       train_regret_loser_5[slice44],\r\n",
        "       train_regret_loser_6[slice44],\r\n",
        "       train_regret_loser_7[slice44],\r\n",
        "       train_regret_loser_8[slice44],\r\n",
        "       train_regret_loser_9[slice44],\r\n",
        "       train_regret_loser_10[slice44],\r\n",
        "       train_regret_loser_11[slice44],\r\n",
        "       train_regret_loser_12[slice44],\r\n",
        "       train_regret_loser_13[slice44],\r\n",
        "       train_regret_loser_14[slice44],\r\n",
        "       train_regret_loser_15[slice44],\r\n",
        "       train_regret_loser_16[slice44],\r\n",
        "       train_regret_loser_17[slice44],\r\n",
        "       train_regret_loser_18[slice44],\r\n",
        "       train_regret_loser_19[slice44],\r\n",
        "       train_regret_loser_20[slice44]]\r\n",
        "\r\n",
        "winner44 = [train_regret_winner_1[slice44],\r\n",
        "       train_regret_winner_2[slice44],\r\n",
        "       train_regret_winner_3[slice44],\r\n",
        "       train_regret_winner_4[slice44],\r\n",
        "       train_regret_winner_5[slice44],\r\n",
        "       train_regret_winner_6[slice44],\r\n",
        "       train_regret_winner_7[slice44],\r\n",
        "       train_regret_winner_8[slice44],\r\n",
        "       train_regret_winner_9[slice44],\r\n",
        "       train_regret_winner_10[slice44],\r\n",
        "       train_regret_winner_11[slice44],\r\n",
        "       train_regret_winner_12[slice44],\r\n",
        "       train_regret_winner_13[slice44],\r\n",
        "       train_regret_winner_14[slice44],\r\n",
        "       train_regret_winner_15[slice44],\r\n",
        "       train_regret_winner_16[slice44],\r\n",
        "       train_regret_winner_17[slice44],\r\n",
        "       train_regret_winner_18[slice44],\r\n",
        "       train_regret_winner_19[slice44],\r\n",
        "       train_regret_winner_20[slice44]]\r\n",
        "\r\n",
        "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\r\n",
        "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\r\n",
        "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\r\n",
        "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\r\n",
        "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\r\n",
        "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJzx094Fw_c_"
      },
      "source": [
        "# Iteration54 :\r\n",
        "\r\n",
        "slice54 = 53\r\n",
        "\r\n",
        "loser54 = [train_regret_loser_1[slice54],\r\n",
        "       train_regret_loser_2[slice54],\r\n",
        "       train_regret_loser_3[slice54],\r\n",
        "       train_regret_loser_4[slice54],\r\n",
        "       train_regret_loser_5[slice54],\r\n",
        "       train_regret_loser_6[slice54],\r\n",
        "       train_regret_loser_7[slice54],\r\n",
        "       train_regret_loser_8[slice54],\r\n",
        "       train_regret_loser_9[slice54],\r\n",
        "       train_regret_loser_10[slice54],\r\n",
        "       train_regret_loser_11[slice54],\r\n",
        "       train_regret_loser_12[slice54],\r\n",
        "       train_regret_loser_13[slice54],\r\n",
        "       train_regret_loser_14[slice54],\r\n",
        "       train_regret_loser_15[slice54],\r\n",
        "       train_regret_loser_16[slice54],\r\n",
        "       train_regret_loser_17[slice54],\r\n",
        "       train_regret_loser_18[slice54],\r\n",
        "       train_regret_loser_19[slice54],\r\n",
        "       train_regret_loser_20[slice54]]\r\n",
        "\r\n",
        "winner54 = [train_regret_winner_1[slice54],\r\n",
        "       train_regret_winner_2[slice54],\r\n",
        "       train_regret_winner_3[slice54],\r\n",
        "       train_regret_winner_4[slice54],\r\n",
        "       train_regret_winner_5[slice54],\r\n",
        "       train_regret_winner_6[slice54],\r\n",
        "       train_regret_winner_7[slice54],\r\n",
        "       train_regret_winner_8[slice54],\r\n",
        "       train_regret_winner_9[slice54],\r\n",
        "       train_regret_winner_10[slice54],\r\n",
        "       train_regret_winner_11[slice54],\r\n",
        "       train_regret_winner_12[slice54],\r\n",
        "       train_regret_winner_13[slice54],\r\n",
        "       train_regret_winner_14[slice54],\r\n",
        "       train_regret_winner_15[slice54],\r\n",
        "       train_regret_winner_16[slice54],\r\n",
        "       train_regret_winner_17[slice54],\r\n",
        "       train_regret_winner_18[slice54],\r\n",
        "       train_regret_winner_19[slice54],\r\n",
        "       train_regret_winner_20[slice54]]\r\n",
        "\r\n",
        "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\r\n",
        "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\r\n",
        "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\r\n",
        "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\r\n",
        "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\r\n",
        "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEVYMqj_w_fr"
      },
      "source": [
        "# Iteration64 :\r\n",
        "\r\n",
        "slice64 = 63\r\n",
        "\r\n",
        "loser64 = [train_regret_loser_1[slice64],\r\n",
        "       train_regret_loser_2[slice64],\r\n",
        "       train_regret_loser_3[slice64],\r\n",
        "       train_regret_loser_4[slice64],\r\n",
        "       train_regret_loser_5[slice64],\r\n",
        "       train_regret_loser_6[slice64],\r\n",
        "       train_regret_loser_7[slice64],\r\n",
        "       train_regret_loser_8[slice64],\r\n",
        "       train_regret_loser_9[slice64],\r\n",
        "       train_regret_loser_10[slice64],\r\n",
        "       train_regret_loser_11[slice64],\r\n",
        "       train_regret_loser_12[slice64],\r\n",
        "       train_regret_loser_13[slice64],\r\n",
        "       train_regret_loser_14[slice64],\r\n",
        "       train_regret_loser_15[slice64],\r\n",
        "       train_regret_loser_16[slice64],\r\n",
        "       train_regret_loser_17[slice64],\r\n",
        "       train_regret_loser_18[slice64],\r\n",
        "       train_regret_loser_19[slice64],\r\n",
        "       train_regret_loser_20[slice64]]\r\n",
        "\r\n",
        "winner64 = [train_regret_winner_1[slice64],\r\n",
        "       train_regret_winner_2[slice64],\r\n",
        "       train_regret_winner_3[slice64],\r\n",
        "       train_regret_winner_4[slice64],\r\n",
        "       train_regret_winner_5[slice64],\r\n",
        "       train_regret_winner_6[slice64],\r\n",
        "       train_regret_winner_7[slice64],\r\n",
        "       train_regret_winner_8[slice64],\r\n",
        "       train_regret_winner_9[slice64],\r\n",
        "       train_regret_winner_10[slice64],\r\n",
        "       train_regret_winner_11[slice64],\r\n",
        "       train_regret_winner_12[slice64],\r\n",
        "       train_regret_winner_13[slice64],\r\n",
        "       train_regret_winner_14[slice64],\r\n",
        "       train_regret_winner_15[slice64],\r\n",
        "       train_regret_winner_16[slice64],\r\n",
        "       train_regret_winner_17[slice64],\r\n",
        "       train_regret_winner_18[slice64],\r\n",
        "       train_regret_winner_19[slice64],\r\n",
        "       train_regret_winner_20[slice64]]\r\n",
        "\r\n",
        "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\r\n",
        "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\r\n",
        "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\r\n",
        "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\r\n",
        "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\r\n",
        "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvr3S_p0w_ii"
      },
      "source": [
        "# Iteration74 :\r\n",
        "\r\n",
        "slice74 = 73\r\n",
        "\r\n",
        "loser74 = [train_regret_loser_1[slice74],\r\n",
        "       train_regret_loser_2[slice74],\r\n",
        "       train_regret_loser_3[slice74],\r\n",
        "       train_regret_loser_4[slice74],\r\n",
        "       train_regret_loser_5[slice74],\r\n",
        "       train_regret_loser_6[slice74],\r\n",
        "       train_regret_loser_7[slice74],\r\n",
        "       train_regret_loser_8[slice74],\r\n",
        "       train_regret_loser_9[slice74],\r\n",
        "       train_regret_loser_10[slice74],\r\n",
        "       train_regret_loser_11[slice74],\r\n",
        "       train_regret_loser_12[slice74],\r\n",
        "       train_regret_loser_13[slice74],\r\n",
        "       train_regret_loser_14[slice74],\r\n",
        "       train_regret_loser_15[slice74],\r\n",
        "       train_regret_loser_16[slice74],\r\n",
        "       train_regret_loser_17[slice74],\r\n",
        "       train_regret_loser_18[slice74],\r\n",
        "       train_regret_loser_19[slice74],\r\n",
        "       train_regret_loser_20[slice74]]\r\n",
        "\r\n",
        "winner74 = [train_regret_winner_1[slice74],\r\n",
        "       train_regret_winner_2[slice74],\r\n",
        "       train_regret_winner_3[slice74],\r\n",
        "       train_regret_winner_4[slice74],\r\n",
        "       train_regret_winner_5[slice74],\r\n",
        "       train_regret_winner_6[slice74],\r\n",
        "       train_regret_winner_7[slice74],\r\n",
        "       train_regret_winner_8[slice74],\r\n",
        "       train_regret_winner_9[slice74],\r\n",
        "       train_regret_winner_10[slice74],\r\n",
        "       train_regret_winner_11[slice74],\r\n",
        "       train_regret_winner_12[slice74],\r\n",
        "       train_regret_winner_13[slice74],\r\n",
        "       train_regret_winner_14[slice74],\r\n",
        "       train_regret_winner_15[slice74],\r\n",
        "       train_regret_winner_16[slice74],\r\n",
        "       train_regret_winner_17[slice74],\r\n",
        "       train_regret_winner_18[slice74],\r\n",
        "       train_regret_winner_19[slice74],\r\n",
        "       train_regret_winner_20[slice74]]\r\n",
        "\r\n",
        "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\r\n",
        "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\r\n",
        "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\r\n",
        "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\r\n",
        "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\r\n",
        "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEAvUVK7w_lO"
      },
      "source": [
        "# Iteration84 :\r\n",
        "\r\n",
        "slice84 = 83\r\n",
        "\r\n",
        "loser84 = [train_regret_loser_1[slice84],\r\n",
        "       train_regret_loser_2[slice84],\r\n",
        "       train_regret_loser_3[slice84],\r\n",
        "       train_regret_loser_4[slice84],\r\n",
        "       train_regret_loser_5[slice84],\r\n",
        "       train_regret_loser_6[slice84],\r\n",
        "       train_regret_loser_7[slice84],\r\n",
        "       train_regret_loser_8[slice84],\r\n",
        "       train_regret_loser_9[slice84],\r\n",
        "       train_regret_loser_10[slice84],\r\n",
        "       train_regret_loser_11[slice84],\r\n",
        "       train_regret_loser_12[slice84],\r\n",
        "       train_regret_loser_13[slice84],\r\n",
        "       train_regret_loser_14[slice84],\r\n",
        "       train_regret_loser_15[slice84],\r\n",
        "       train_regret_loser_16[slice84],\r\n",
        "       train_regret_loser_17[slice84],\r\n",
        "       train_regret_loser_18[slice84],\r\n",
        "       train_regret_loser_19[slice84],\r\n",
        "       train_regret_loser_20[slice84]]\r\n",
        "\r\n",
        "winner84 = [train_regret_winner_1[slice84],\r\n",
        "       train_regret_winner_2[slice84],\r\n",
        "       train_regret_winner_3[slice84],\r\n",
        "       train_regret_winner_4[slice84],\r\n",
        "       train_regret_winner_5[slice84],\r\n",
        "       train_regret_winner_6[slice84],\r\n",
        "       train_regret_winner_7[slice84],\r\n",
        "       train_regret_winner_8[slice84],\r\n",
        "       train_regret_winner_9[slice84],\r\n",
        "       train_regret_winner_10[slice84],\r\n",
        "       train_regret_winner_11[slice84],\r\n",
        "       train_regret_winner_12[slice84],\r\n",
        "       train_regret_winner_13[slice84],\r\n",
        "       train_regret_winner_14[slice84],\r\n",
        "       train_regret_winner_15[slice84],\r\n",
        "       train_regret_winner_16[slice84],\r\n",
        "       train_regret_winner_17[slice84],\r\n",
        "       train_regret_winner_18[slice84],\r\n",
        "       train_regret_winner_19[slice84],\r\n",
        "       train_regret_winner_20[slice84]]\r\n",
        "\r\n",
        "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\r\n",
        "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\r\n",
        "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\r\n",
        "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\r\n",
        "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\r\n",
        "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMZzhdGw_oC"
      },
      "source": [
        "# Iteration94 :\r\n",
        "\r\n",
        "slice94 = 93\r\n",
        "\r\n",
        "loser94 = [train_regret_loser_1[slice94],\r\n",
        "       train_regret_loser_2[slice94],\r\n",
        "       train_regret_loser_3[slice94],\r\n",
        "       train_regret_loser_4[slice94],\r\n",
        "       train_regret_loser_5[slice94],\r\n",
        "       train_regret_loser_6[slice94],\r\n",
        "       train_regret_loser_7[slice94],\r\n",
        "       train_regret_loser_8[slice94],\r\n",
        "       train_regret_loser_9[slice94],\r\n",
        "       train_regret_loser_10[slice94],\r\n",
        "       train_regret_loser_11[slice94],\r\n",
        "       train_regret_loser_12[slice94],\r\n",
        "       train_regret_loser_13[slice94],\r\n",
        "       train_regret_loser_14[slice94],\r\n",
        "       train_regret_loser_15[slice94],\r\n",
        "       train_regret_loser_16[slice94],\r\n",
        "       train_regret_loser_17[slice94],\r\n",
        "       train_regret_loser_18[slice94],\r\n",
        "       train_regret_loser_19[slice94],\r\n",
        "       train_regret_loser_20[slice94]]\r\n",
        "\r\n",
        "winner94 = [train_regret_winner_1[slice94],\r\n",
        "       train_regret_winner_2[slice94],\r\n",
        "       train_regret_winner_3[slice94],\r\n",
        "       train_regret_winner_4[slice94],\r\n",
        "       train_regret_winner_5[slice94],\r\n",
        "       train_regret_winner_6[slice94],\r\n",
        "       train_regret_winner_7[slice94],\r\n",
        "       train_regret_winner_8[slice94],\r\n",
        "       train_regret_winner_9[slice94],\r\n",
        "       train_regret_winner_10[slice94],\r\n",
        "       train_regret_winner_11[slice94],\r\n",
        "       train_regret_winner_12[slice94],\r\n",
        "       train_regret_winner_13[slice94],\r\n",
        "       train_regret_winner_14[slice94],\r\n",
        "       train_regret_winner_15[slice94],\r\n",
        "       train_regret_winner_16[slice94],\r\n",
        "       train_regret_winner_17[slice94],\r\n",
        "       train_regret_winner_18[slice94],\r\n",
        "       train_regret_winner_19[slice94],\r\n",
        "       train_regret_winner_20[slice94]]\r\n",
        "\r\n",
        "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\r\n",
        "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\r\n",
        "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\r\n",
        "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\r\n",
        "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\r\n",
        "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMxHQOgaw_qu"
      },
      "source": [
        "# Iteration5 :\r\n",
        "\r\n",
        "slice5 = 4\r\n",
        "\r\n",
        "loser5 = [train_regret_loser_1[slice5],\r\n",
        "       train_regret_loser_2[slice5],\r\n",
        "       train_regret_loser_3[slice5],\r\n",
        "       train_regret_loser_4[slice5],\r\n",
        "       train_regret_loser_5[slice5],\r\n",
        "       train_regret_loser_6[slice5],\r\n",
        "       train_regret_loser_7[slice5],\r\n",
        "       train_regret_loser_8[slice5],\r\n",
        "       train_regret_loser_9[slice5],\r\n",
        "       train_regret_loser_10[slice5],\r\n",
        "       train_regret_loser_11[slice5],\r\n",
        "       train_regret_loser_12[slice5],\r\n",
        "       train_regret_loser_13[slice5],\r\n",
        "       train_regret_loser_14[slice5],\r\n",
        "       train_regret_loser_15[slice5],\r\n",
        "       train_regret_loser_16[slice5],\r\n",
        "       train_regret_loser_17[slice5],\r\n",
        "       train_regret_loser_18[slice5],\r\n",
        "       train_regret_loser_19[slice5],\r\n",
        "       train_regret_loser_20[slice5]]\r\n",
        "\r\n",
        "winner5 = [train_regret_winner_1[slice5],\r\n",
        "       train_regret_winner_2[slice5],\r\n",
        "       train_regret_winner_3[slice5],\r\n",
        "       train_regret_winner_4[slice5],\r\n",
        "       train_regret_winner_5[slice5],\r\n",
        "       train_regret_winner_6[slice5],\r\n",
        "       train_regret_winner_7[slice5],\r\n",
        "       train_regret_winner_8[slice5],\r\n",
        "       train_regret_winner_9[slice5],\r\n",
        "       train_regret_winner_10[slice5],\r\n",
        "       train_regret_winner_11[slice5],\r\n",
        "       train_regret_winner_12[slice5],\r\n",
        "       train_regret_winner_13[slice5],\r\n",
        "       train_regret_winner_14[slice5],\r\n",
        "       train_regret_winner_15[slice5],\r\n",
        "       train_regret_winner_16[slice5],\r\n",
        "       train_regret_winner_17[slice5],\r\n",
        "       train_regret_winner_18[slice5],\r\n",
        "       train_regret_winner_19[slice5],\r\n",
        "       train_regret_winner_20[slice5]]\r\n",
        "\r\n",
        "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\r\n",
        "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\r\n",
        "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\r\n",
        "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\r\n",
        "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\r\n",
        "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPVFQTfNw_wF"
      },
      "source": [
        "# Iteration15 :\r\n",
        "\r\n",
        "slice15 = 14\r\n",
        "\r\n",
        "loser15 = [train_regret_loser_1[slice15],\r\n",
        "       train_regret_loser_2[slice15],\r\n",
        "       train_regret_loser_3[slice15],\r\n",
        "       train_regret_loser_4[slice15],\r\n",
        "       train_regret_loser_5[slice15],\r\n",
        "       train_regret_loser_6[slice15],\r\n",
        "       train_regret_loser_7[slice15],\r\n",
        "       train_regret_loser_8[slice15],\r\n",
        "       train_regret_loser_9[slice15],\r\n",
        "       train_regret_loser_10[slice15],\r\n",
        "       train_regret_loser_11[slice15],\r\n",
        "       train_regret_loser_12[slice15],\r\n",
        "       train_regret_loser_13[slice15],\r\n",
        "       train_regret_loser_14[slice15],\r\n",
        "       train_regret_loser_15[slice15],\r\n",
        "       train_regret_loser_16[slice15],\r\n",
        "       train_regret_loser_17[slice15],\r\n",
        "       train_regret_loser_18[slice15],\r\n",
        "       train_regret_loser_19[slice15],\r\n",
        "       train_regret_loser_20[slice15]]\r\n",
        "\r\n",
        "winner15 = [train_regret_winner_1[slice15],\r\n",
        "       train_regret_winner_2[slice15],\r\n",
        "       train_regret_winner_3[slice15],\r\n",
        "       train_regret_winner_4[slice15],\r\n",
        "       train_regret_winner_5[slice15],\r\n",
        "       train_regret_winner_6[slice15],\r\n",
        "       train_regret_winner_7[slice15],\r\n",
        "       train_regret_winner_8[slice15],\r\n",
        "       train_regret_winner_9[slice15],\r\n",
        "       train_regret_winner_10[slice15],\r\n",
        "       train_regret_winner_11[slice15],\r\n",
        "       train_regret_winner_12[slice15],\r\n",
        "       train_regret_winner_13[slice15],\r\n",
        "       train_regret_winner_14[slice15],\r\n",
        "       train_regret_winner_15[slice15],\r\n",
        "       train_regret_winner_16[slice15],\r\n",
        "       train_regret_winner_17[slice15],\r\n",
        "       train_regret_winner_18[slice15],\r\n",
        "       train_regret_winner_19[slice15],\r\n",
        "       train_regret_winner_20[slice15]]\r\n",
        "\r\n",
        "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\r\n",
        "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\r\n",
        "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\r\n",
        "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\r\n",
        "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\r\n",
        "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADBmBA3ew_zK"
      },
      "source": [
        "# Iteration25 :\r\n",
        "\r\n",
        "slice25 = 24\r\n",
        "\r\n",
        "loser25 = [train_regret_loser_1[slice25],\r\n",
        "       train_regret_loser_2[slice25],\r\n",
        "       train_regret_loser_3[slice25],\r\n",
        "       train_regret_loser_4[slice25],\r\n",
        "       train_regret_loser_5[slice25],\r\n",
        "       train_regret_loser_6[slice25],\r\n",
        "       train_regret_loser_7[slice25],\r\n",
        "       train_regret_loser_8[slice25],\r\n",
        "       train_regret_loser_9[slice25],\r\n",
        "       train_regret_loser_10[slice25],\r\n",
        "       train_regret_loser_11[slice25],\r\n",
        "       train_regret_loser_12[slice25],\r\n",
        "       train_regret_loser_13[slice25],\r\n",
        "       train_regret_loser_14[slice25],\r\n",
        "       train_regret_loser_15[slice25],\r\n",
        "       train_regret_loser_16[slice25],\r\n",
        "       train_regret_loser_17[slice25],\r\n",
        "       train_regret_loser_18[slice25],\r\n",
        "       train_regret_loser_19[slice25],\r\n",
        "       train_regret_loser_20[slice25]]\r\n",
        "\r\n",
        "winner25 = [train_regret_winner_1[slice25],\r\n",
        "       train_regret_winner_2[slice25],\r\n",
        "       train_regret_winner_3[slice25],\r\n",
        "       train_regret_winner_4[slice25],\r\n",
        "       train_regret_winner_5[slice25],\r\n",
        "       train_regret_winner_6[slice25],\r\n",
        "       train_regret_winner_7[slice25],\r\n",
        "       train_regret_winner_8[slice25],\r\n",
        "       train_regret_winner_9[slice25],\r\n",
        "       train_regret_winner_10[slice25],\r\n",
        "       train_regret_winner_11[slice25],\r\n",
        "       train_regret_winner_12[slice25],\r\n",
        "       train_regret_winner_13[slice25],\r\n",
        "       train_regret_winner_14[slice25],\r\n",
        "       train_regret_winner_15[slice25],\r\n",
        "       train_regret_winner_16[slice25],\r\n",
        "       train_regret_winner_17[slice25],\r\n",
        "       train_regret_winner_18[slice25],\r\n",
        "       train_regret_winner_19[slice25],\r\n",
        "       train_regret_winner_20[slice25]]\r\n",
        "\r\n",
        "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\r\n",
        "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\r\n",
        "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\r\n",
        "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\r\n",
        "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\r\n",
        "upper_winner25 = np.asarray(winner25_results[14:15][0])[0]"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUk9UtSTw_2N"
      },
      "source": [
        "# Iteration35 :\r\n",
        "\r\n",
        "slice35 = 34\r\n",
        "\r\n",
        "loser35 = [train_regret_loser_1[slice35],\r\n",
        "       train_regret_loser_2[slice35],\r\n",
        "       train_regret_loser_3[slice35],\r\n",
        "       train_regret_loser_4[slice35],\r\n",
        "       train_regret_loser_5[slice35],\r\n",
        "       train_regret_loser_6[slice35],\r\n",
        "       train_regret_loser_7[slice35],\r\n",
        "       train_regret_loser_8[slice35],\r\n",
        "       train_regret_loser_9[slice35],\r\n",
        "       train_regret_loser_10[slice35],\r\n",
        "       train_regret_loser_11[slice35],\r\n",
        "       train_regret_loser_12[slice35],\r\n",
        "       train_regret_loser_13[slice35],\r\n",
        "       train_regret_loser_14[slice35],\r\n",
        "       train_regret_loser_15[slice35],\r\n",
        "       train_regret_loser_16[slice35],\r\n",
        "       train_regret_loser_17[slice35],\r\n",
        "       train_regret_loser_18[slice35],\r\n",
        "       train_regret_loser_19[slice35],\r\n",
        "       train_regret_loser_20[slice35]]\r\n",
        "\r\n",
        "winner35 = [train_regret_winner_1[slice35],\r\n",
        "       train_regret_winner_2[slice35],\r\n",
        "       train_regret_winner_3[slice35],\r\n",
        "       train_regret_winner_4[slice35],\r\n",
        "       train_regret_winner_5[slice35],\r\n",
        "       train_regret_winner_6[slice35],\r\n",
        "       train_regret_winner_7[slice35],\r\n",
        "       train_regret_winner_8[slice35],\r\n",
        "       train_regret_winner_9[slice35],\r\n",
        "       train_regret_winner_10[slice35],\r\n",
        "       train_regret_winner_11[slice35],\r\n",
        "       train_regret_winner_12[slice35],\r\n",
        "       train_regret_winner_13[slice35],\r\n",
        "       train_regret_winner_14[slice35],\r\n",
        "       train_regret_winner_15[slice35],\r\n",
        "       train_regret_winner_16[slice35],\r\n",
        "       train_regret_winner_17[slice35],\r\n",
        "       train_regret_winner_18[slice35],\r\n",
        "       train_regret_winner_19[slice35],\r\n",
        "       train_regret_winner_20[slice35]]\r\n",
        "\r\n",
        "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\r\n",
        "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\r\n",
        "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\r\n",
        "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\r\n",
        "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\r\n",
        "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX3VgqD_w_4z"
      },
      "source": [
        "# Iteration45 :\r\n",
        "\r\n",
        "slice45 = 44\r\n",
        "\r\n",
        "loser45 = [train_regret_loser_1[slice45],\r\n",
        "       train_regret_loser_2[slice45],\r\n",
        "       train_regret_loser_3[slice45],\r\n",
        "       train_regret_loser_4[slice45],\r\n",
        "       train_regret_loser_5[slice45],\r\n",
        "       train_regret_loser_6[slice45],\r\n",
        "       train_regret_loser_7[slice45],\r\n",
        "       train_regret_loser_8[slice45],\r\n",
        "       train_regret_loser_9[slice45],\r\n",
        "       train_regret_loser_10[slice45],\r\n",
        "       train_regret_loser_11[slice45],\r\n",
        "       train_regret_loser_12[slice45],\r\n",
        "       train_regret_loser_13[slice45],\r\n",
        "       train_regret_loser_14[slice45],\r\n",
        "       train_regret_loser_15[slice45],\r\n",
        "       train_regret_loser_16[slice45],\r\n",
        "       train_regret_loser_17[slice45],\r\n",
        "       train_regret_loser_18[slice45],\r\n",
        "       train_regret_loser_19[slice45],\r\n",
        "       train_regret_loser_20[slice45]]\r\n",
        "\r\n",
        "winner45 = [train_regret_winner_1[slice45],\r\n",
        "       train_regret_winner_2[slice45],\r\n",
        "       train_regret_winner_3[slice45],\r\n",
        "       train_regret_winner_4[slice45],\r\n",
        "       train_regret_winner_5[slice45],\r\n",
        "       train_regret_winner_6[slice45],\r\n",
        "       train_regret_winner_7[slice45],\r\n",
        "       train_regret_winner_8[slice45],\r\n",
        "       train_regret_winner_9[slice45],\r\n",
        "       train_regret_winner_10[slice45],\r\n",
        "       train_regret_winner_11[slice45],\r\n",
        "       train_regret_winner_12[slice45],\r\n",
        "       train_regret_winner_13[slice45],\r\n",
        "       train_regret_winner_14[slice45],\r\n",
        "       train_regret_winner_15[slice45],\r\n",
        "       train_regret_winner_16[slice45],\r\n",
        "       train_regret_winner_17[slice45],\r\n",
        "       train_regret_winner_18[slice45],\r\n",
        "       train_regret_winner_19[slice45],\r\n",
        "       train_regret_winner_20[slice45]]\r\n",
        "\r\n",
        "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\r\n",
        "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\r\n",
        "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\r\n",
        "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\r\n",
        "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\r\n",
        "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o35W-AOzw_8F"
      },
      "source": [
        "# Iteration55 :\r\n",
        "\r\n",
        "slice55 = 54\r\n",
        "\r\n",
        "loser55 = [train_regret_loser_1[slice55],\r\n",
        "       train_regret_loser_2[slice55],\r\n",
        "       train_regret_loser_3[slice55],\r\n",
        "       train_regret_loser_4[slice55],\r\n",
        "       train_regret_loser_5[slice55],\r\n",
        "       train_regret_loser_6[slice55],\r\n",
        "       train_regret_loser_7[slice55],\r\n",
        "       train_regret_loser_8[slice55],\r\n",
        "       train_regret_loser_9[slice55],\r\n",
        "       train_regret_loser_10[slice55],\r\n",
        "       train_regret_loser_11[slice55],\r\n",
        "       train_regret_loser_12[slice55],\r\n",
        "       train_regret_loser_13[slice55],\r\n",
        "       train_regret_loser_14[slice55],\r\n",
        "       train_regret_loser_15[slice55],\r\n",
        "       train_regret_loser_16[slice55],\r\n",
        "       train_regret_loser_17[slice55],\r\n",
        "       train_regret_loser_18[slice55],\r\n",
        "       train_regret_loser_19[slice55],\r\n",
        "       train_regret_loser_20[slice55]]\r\n",
        "\r\n",
        "winner55 = [train_regret_winner_1[slice55],\r\n",
        "       train_regret_winner_2[slice55],\r\n",
        "       train_regret_winner_3[slice55],\r\n",
        "       train_regret_winner_4[slice55],\r\n",
        "       train_regret_winner_5[slice55],\r\n",
        "       train_regret_winner_6[slice55],\r\n",
        "       train_regret_winner_7[slice55],\r\n",
        "       train_regret_winner_8[slice55],\r\n",
        "       train_regret_winner_9[slice55],\r\n",
        "       train_regret_winner_10[slice55],\r\n",
        "       train_regret_winner_11[slice55],\r\n",
        "       train_regret_winner_12[slice55],\r\n",
        "       train_regret_winner_13[slice55],\r\n",
        "       train_regret_winner_14[slice55],\r\n",
        "       train_regret_winner_15[slice55],\r\n",
        "       train_regret_winner_16[slice55],\r\n",
        "       train_regret_winner_17[slice55],\r\n",
        "       train_regret_winner_18[slice55],\r\n",
        "       train_regret_winner_19[slice55],\r\n",
        "       train_regret_winner_20[slice55]]\r\n",
        "\r\n",
        "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\r\n",
        "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\r\n",
        "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\r\n",
        "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\r\n",
        "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\r\n",
        "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUAjKXWdw__J"
      },
      "source": [
        "# Iteration65 :\r\n",
        "\r\n",
        "slice65 = 64\r\n",
        "\r\n",
        "loser65 = [train_regret_loser_1[slice65],\r\n",
        "       train_regret_loser_2[slice65],\r\n",
        "       train_regret_loser_3[slice65],\r\n",
        "       train_regret_loser_4[slice65],\r\n",
        "       train_regret_loser_5[slice65],\r\n",
        "       train_regret_loser_6[slice65],\r\n",
        "       train_regret_loser_7[slice65],\r\n",
        "       train_regret_loser_8[slice65],\r\n",
        "       train_regret_loser_9[slice65],\r\n",
        "       train_regret_loser_10[slice65],\r\n",
        "       train_regret_loser_11[slice65],\r\n",
        "       train_regret_loser_12[slice65],\r\n",
        "       train_regret_loser_13[slice65],\r\n",
        "       train_regret_loser_14[slice65],\r\n",
        "       train_regret_loser_15[slice65],\r\n",
        "       train_regret_loser_16[slice65],\r\n",
        "       train_regret_loser_17[slice65],\r\n",
        "       train_regret_loser_18[slice65],\r\n",
        "       train_regret_loser_19[slice65],\r\n",
        "       train_regret_loser_20[slice65]]\r\n",
        "\r\n",
        "winner65 = [train_regret_winner_1[slice65],\r\n",
        "       train_regret_winner_2[slice65],\r\n",
        "       train_regret_winner_3[slice65],\r\n",
        "       train_regret_winner_4[slice65],\r\n",
        "       train_regret_winner_5[slice65],\r\n",
        "       train_regret_winner_6[slice65],\r\n",
        "       train_regret_winner_7[slice65],\r\n",
        "       train_regret_winner_8[slice65],\r\n",
        "       train_regret_winner_9[slice65],\r\n",
        "       train_regret_winner_10[slice65],\r\n",
        "       train_regret_winner_11[slice65],\r\n",
        "       train_regret_winner_12[slice65],\r\n",
        "       train_regret_winner_13[slice65],\r\n",
        "       train_regret_winner_14[slice65],\r\n",
        "       train_regret_winner_15[slice65],\r\n",
        "       train_regret_winner_16[slice65],\r\n",
        "       train_regret_winner_17[slice65],\r\n",
        "       train_regret_winner_18[slice65],\r\n",
        "       train_regret_winner_19[slice65],\r\n",
        "       train_regret_winner_20[slice65]]\r\n",
        "\r\n",
        "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\r\n",
        "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\r\n",
        "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\r\n",
        "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\r\n",
        "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\r\n",
        "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D00FcmZwxAB9"
      },
      "source": [
        "# Iteration75 :\r\n",
        "\r\n",
        "slice75 = 74\r\n",
        "\r\n",
        "loser75 = [train_regret_loser_1[slice75],\r\n",
        "       train_regret_loser_2[slice75],\r\n",
        "       train_regret_loser_3[slice75],\r\n",
        "       train_regret_loser_4[slice75],\r\n",
        "       train_regret_loser_5[slice75],\r\n",
        "       train_regret_loser_6[slice75],\r\n",
        "       train_regret_loser_7[slice75],\r\n",
        "       train_regret_loser_8[slice75],\r\n",
        "       train_regret_loser_9[slice75],\r\n",
        "       train_regret_loser_10[slice75],\r\n",
        "       train_regret_loser_11[slice75],\r\n",
        "       train_regret_loser_12[slice75],\r\n",
        "       train_regret_loser_13[slice75],\r\n",
        "       train_regret_loser_14[slice75],\r\n",
        "       train_regret_loser_15[slice75],\r\n",
        "       train_regret_loser_16[slice75],\r\n",
        "       train_regret_loser_17[slice75],\r\n",
        "       train_regret_loser_18[slice75],\r\n",
        "       train_regret_loser_19[slice75],\r\n",
        "       train_regret_loser_20[slice75]]\r\n",
        "\r\n",
        "winner75 = [train_regret_winner_1[slice75],\r\n",
        "       train_regret_winner_2[slice75],\r\n",
        "       train_regret_winner_3[slice75],\r\n",
        "       train_regret_winner_4[slice75],\r\n",
        "       train_regret_winner_5[slice75],\r\n",
        "       train_regret_winner_6[slice75],\r\n",
        "       train_regret_winner_7[slice75],\r\n",
        "       train_regret_winner_8[slice75],\r\n",
        "       train_regret_winner_9[slice75],\r\n",
        "       train_regret_winner_10[slice75],\r\n",
        "       train_regret_winner_11[slice75],\r\n",
        "       train_regret_winner_12[slice75],\r\n",
        "       train_regret_winner_13[slice75],\r\n",
        "       train_regret_winner_14[slice75],\r\n",
        "       train_regret_winner_15[slice75],\r\n",
        "       train_regret_winner_16[slice75],\r\n",
        "       train_regret_winner_17[slice75],\r\n",
        "       train_regret_winner_18[slice75],\r\n",
        "       train_regret_winner_19[slice75],\r\n",
        "       train_regret_winner_20[slice75]]\r\n",
        "\r\n",
        "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\r\n",
        "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\r\n",
        "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\r\n",
        "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\r\n",
        "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\r\n",
        "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnzmprxxAFB"
      },
      "source": [
        "# Iteration85 :\r\n",
        "\r\n",
        "slice85 = 84\r\n",
        "\r\n",
        "loser85 = [train_regret_loser_1[slice85],\r\n",
        "       train_regret_loser_2[slice85],\r\n",
        "       train_regret_loser_3[slice85],\r\n",
        "       train_regret_loser_4[slice85],\r\n",
        "       train_regret_loser_5[slice85],\r\n",
        "       train_regret_loser_6[slice85],\r\n",
        "       train_regret_loser_7[slice85],\r\n",
        "       train_regret_loser_8[slice85],\r\n",
        "       train_regret_loser_9[slice85],\r\n",
        "       train_regret_loser_10[slice85],\r\n",
        "       train_regret_loser_11[slice85],\r\n",
        "       train_regret_loser_12[slice85],\r\n",
        "       train_regret_loser_13[slice85],\r\n",
        "       train_regret_loser_14[slice85],\r\n",
        "       train_regret_loser_15[slice85],\r\n",
        "       train_regret_loser_16[slice85],\r\n",
        "       train_regret_loser_17[slice85],\r\n",
        "       train_regret_loser_18[slice85],\r\n",
        "       train_regret_loser_19[slice85],\r\n",
        "       train_regret_loser_20[slice85]]\r\n",
        "\r\n",
        "winner85 = [train_regret_winner_1[slice85],\r\n",
        "       train_regret_winner_2[slice85],\r\n",
        "       train_regret_winner_3[slice85],\r\n",
        "       train_regret_winner_4[slice85],\r\n",
        "       train_regret_winner_5[slice85],\r\n",
        "       train_regret_winner_6[slice85],\r\n",
        "       train_regret_winner_7[slice85],\r\n",
        "       train_regret_winner_8[slice85],\r\n",
        "       train_regret_winner_9[slice85],\r\n",
        "       train_regret_winner_10[slice85],\r\n",
        "       train_regret_winner_11[slice85],\r\n",
        "       train_regret_winner_12[slice85],\r\n",
        "       train_regret_winner_13[slice85],\r\n",
        "       train_regret_winner_14[slice85],\r\n",
        "       train_regret_winner_15[slice85],\r\n",
        "       train_regret_winner_16[slice85],\r\n",
        "       train_regret_winner_17[slice85],\r\n",
        "       train_regret_winner_18[slice85],\r\n",
        "       train_regret_winner_19[slice85],\r\n",
        "       train_regret_winner_20[slice85]]\r\n",
        "\r\n",
        "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\r\n",
        "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\r\n",
        "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\r\n",
        "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\r\n",
        "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\r\n",
        "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztvJhrN6xAH1"
      },
      "source": [
        "# Iteration95 :\r\n",
        "\r\n",
        "slice95 = 94\r\n",
        "\r\n",
        "loser95 = [train_regret_loser_1[slice95],\r\n",
        "       train_regret_loser_2[slice95],\r\n",
        "       train_regret_loser_3[slice95],\r\n",
        "       train_regret_loser_4[slice95],\r\n",
        "       train_regret_loser_5[slice95],\r\n",
        "       train_regret_loser_6[slice95],\r\n",
        "       train_regret_loser_7[slice95],\r\n",
        "       train_regret_loser_8[slice95],\r\n",
        "       train_regret_loser_9[slice95],\r\n",
        "       train_regret_loser_10[slice95],\r\n",
        "       train_regret_loser_11[slice95],\r\n",
        "       train_regret_loser_12[slice95],\r\n",
        "       train_regret_loser_13[slice95],\r\n",
        "       train_regret_loser_14[slice95],\r\n",
        "       train_regret_loser_15[slice95],\r\n",
        "       train_regret_loser_16[slice95],\r\n",
        "       train_regret_loser_17[slice95],\r\n",
        "       train_regret_loser_18[slice95],\r\n",
        "       train_regret_loser_19[slice95],\r\n",
        "       train_regret_loser_20[slice95]]\r\n",
        "\r\n",
        "winner95 = [train_regret_winner_1[slice95],\r\n",
        "       train_regret_winner_2[slice95],\r\n",
        "       train_regret_winner_3[slice95],\r\n",
        "       train_regret_winner_4[slice95],\r\n",
        "       train_regret_winner_5[slice95],\r\n",
        "       train_regret_winner_6[slice95],\r\n",
        "       train_regret_winner_7[slice95],\r\n",
        "       train_regret_winner_8[slice95],\r\n",
        "       train_regret_winner_9[slice95],\r\n",
        "       train_regret_winner_10[slice95],\r\n",
        "       train_regret_winner_11[slice95],\r\n",
        "       train_regret_winner_12[slice95],\r\n",
        "       train_regret_winner_13[slice95],\r\n",
        "       train_regret_winner_14[slice95],\r\n",
        "       train_regret_winner_15[slice95],\r\n",
        "       train_regret_winner_16[slice95],\r\n",
        "       train_regret_winner_17[slice95],\r\n",
        "       train_regret_winner_18[slice95],\r\n",
        "       train_regret_winner_19[slice95],\r\n",
        "       train_regret_winner_20[slice95]]\r\n",
        "\r\n",
        "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\r\n",
        "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\r\n",
        "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\r\n",
        "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\r\n",
        "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\r\n",
        "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK-4nr-OxALA"
      },
      "source": [
        "# Iteration6 :\r\n",
        "\r\n",
        "slice6 = 5\r\n",
        "\r\n",
        "loser6 = [train_regret_loser_1[slice6],\r\n",
        "       train_regret_loser_2[slice6],\r\n",
        "       train_regret_loser_3[slice6],\r\n",
        "       train_regret_loser_4[slice6],\r\n",
        "       train_regret_loser_5[slice6],\r\n",
        "       train_regret_loser_6[slice6],\r\n",
        "       train_regret_loser_7[slice6],\r\n",
        "       train_regret_loser_8[slice6],\r\n",
        "       train_regret_loser_9[slice6],\r\n",
        "       train_regret_loser_10[slice6],\r\n",
        "       train_regret_loser_11[slice6],\r\n",
        "       train_regret_loser_12[slice6],\r\n",
        "       train_regret_loser_13[slice6],\r\n",
        "       train_regret_loser_14[slice6],\r\n",
        "       train_regret_loser_15[slice6],\r\n",
        "       train_regret_loser_16[slice6],\r\n",
        "       train_regret_loser_17[slice6],\r\n",
        "       train_regret_loser_18[slice6],\r\n",
        "       train_regret_loser_19[slice6],\r\n",
        "       train_regret_loser_20[slice6]]\r\n",
        "\r\n",
        "winner6 = [train_regret_winner_1[slice6],\r\n",
        "       train_regret_winner_2[slice6],\r\n",
        "       train_regret_winner_3[slice6],\r\n",
        "       train_regret_winner_4[slice6],\r\n",
        "       train_regret_winner_5[slice6],\r\n",
        "       train_regret_winner_6[slice6],\r\n",
        "       train_regret_winner_7[slice6],\r\n",
        "       train_regret_winner_8[slice6],\r\n",
        "       train_regret_winner_9[slice6],\r\n",
        "       train_regret_winner_10[slice6],\r\n",
        "       train_regret_winner_11[slice6],\r\n",
        "       train_regret_winner_12[slice6],\r\n",
        "       train_regret_winner_13[slice6],\r\n",
        "       train_regret_winner_14[slice6],\r\n",
        "       train_regret_winner_15[slice6],\r\n",
        "       train_regret_winner_16[slice6],\r\n",
        "       train_regret_winner_17[slice6],\r\n",
        "       train_regret_winner_18[slice6],\r\n",
        "       train_regret_winner_19[slice6],\r\n",
        "       train_regret_winner_20[slice6]]\r\n",
        "\r\n",
        "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\r\n",
        "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\r\n",
        "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\r\n",
        "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\r\n",
        "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\r\n",
        "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSwfNX5ExANt"
      },
      "source": [
        "# Iteration16 :\r\n",
        "\r\n",
        "slice16 = 15\r\n",
        "\r\n",
        "loser16 = [train_regret_loser_1[slice16],\r\n",
        "       train_regret_loser_2[slice16],\r\n",
        "       train_regret_loser_3[slice16],\r\n",
        "       train_regret_loser_4[slice16],\r\n",
        "       train_regret_loser_5[slice16],\r\n",
        "       train_regret_loser_6[slice16],\r\n",
        "       train_regret_loser_7[slice16],\r\n",
        "       train_regret_loser_8[slice16],\r\n",
        "       train_regret_loser_9[slice16],\r\n",
        "       train_regret_loser_10[slice16],\r\n",
        "       train_regret_loser_11[slice16],\r\n",
        "       train_regret_loser_12[slice16],\r\n",
        "       train_regret_loser_13[slice16],\r\n",
        "       train_regret_loser_14[slice16],\r\n",
        "       train_regret_loser_15[slice16],\r\n",
        "       train_regret_loser_16[slice16],\r\n",
        "       train_regret_loser_17[slice16],\r\n",
        "       train_regret_loser_18[slice16],\r\n",
        "       train_regret_loser_19[slice16],\r\n",
        "       train_regret_loser_20[slice16]]\r\n",
        "\r\n",
        "winner16 = [train_regret_winner_1[slice16],\r\n",
        "       train_regret_winner_2[slice16],\r\n",
        "       train_regret_winner_3[slice16],\r\n",
        "       train_regret_winner_4[slice16],\r\n",
        "       train_regret_winner_5[slice16],\r\n",
        "       train_regret_winner_6[slice16],\r\n",
        "       train_regret_winner_7[slice16],\r\n",
        "       train_regret_winner_8[slice16],\r\n",
        "       train_regret_winner_9[slice16],\r\n",
        "       train_regret_winner_10[slice16],\r\n",
        "       train_regret_winner_11[slice16],\r\n",
        "       train_regret_winner_12[slice16],\r\n",
        "       train_regret_winner_13[slice16],\r\n",
        "       train_regret_winner_14[slice16],\r\n",
        "       train_regret_winner_15[slice16],\r\n",
        "       train_regret_winner_16[slice16],\r\n",
        "       train_regret_winner_17[slice16],\r\n",
        "       train_regret_winner_18[slice16],\r\n",
        "       train_regret_winner_19[slice16],\r\n",
        "       train_regret_winner_20[slice16]]\r\n",
        "\r\n",
        "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\r\n",
        "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\r\n",
        "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\r\n",
        "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\r\n",
        "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\r\n",
        "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOB3hOLIxAQy"
      },
      "source": [
        "# Iteration26 :\r\n",
        "\r\n",
        "slice26 = 25\r\n",
        "\r\n",
        "loser26 = [train_regret_loser_1[slice26],\r\n",
        "       train_regret_loser_2[slice26],\r\n",
        "       train_regret_loser_3[slice26],\r\n",
        "       train_regret_loser_4[slice26],\r\n",
        "       train_regret_loser_5[slice26],\r\n",
        "       train_regret_loser_6[slice26],\r\n",
        "       train_regret_loser_7[slice26],\r\n",
        "       train_regret_loser_8[slice26],\r\n",
        "       train_regret_loser_9[slice26],\r\n",
        "       train_regret_loser_10[slice26],\r\n",
        "       train_regret_loser_11[slice26],\r\n",
        "       train_regret_loser_12[slice26],\r\n",
        "       train_regret_loser_13[slice26],\r\n",
        "       train_regret_loser_14[slice26],\r\n",
        "       train_regret_loser_15[slice26],\r\n",
        "       train_regret_loser_16[slice26],\r\n",
        "       train_regret_loser_17[slice26],\r\n",
        "       train_regret_loser_18[slice26],\r\n",
        "       train_regret_loser_19[slice26],\r\n",
        "       train_regret_loser_20[slice26]]\r\n",
        "\r\n",
        "winner26 = [train_regret_winner_1[slice26],\r\n",
        "       train_regret_winner_2[slice26],\r\n",
        "       train_regret_winner_3[slice26],\r\n",
        "       train_regret_winner_4[slice26],\r\n",
        "       train_regret_winner_5[slice26],\r\n",
        "       train_regret_winner_6[slice26],\r\n",
        "       train_regret_winner_7[slice26],\r\n",
        "       train_regret_winner_8[slice26],\r\n",
        "       train_regret_winner_9[slice26],\r\n",
        "       train_regret_winner_10[slice26],\r\n",
        "       train_regret_winner_11[slice26],\r\n",
        "       train_regret_winner_12[slice26],\r\n",
        "       train_regret_winner_13[slice26],\r\n",
        "       train_regret_winner_14[slice26],\r\n",
        "       train_regret_winner_15[slice26],\r\n",
        "       train_regret_winner_16[slice26],\r\n",
        "       train_regret_winner_17[slice26],\r\n",
        "       train_regret_winner_18[slice26],\r\n",
        "       train_regret_winner_19[slice26],\r\n",
        "       train_regret_winner_20[slice26]]\r\n",
        "\r\n",
        "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\r\n",
        "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\r\n",
        "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\r\n",
        "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\r\n",
        "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\r\n",
        "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-0hQMtOxATh"
      },
      "source": [
        "# Iteration36 :\r\n",
        "\r\n",
        "slice36 = 35\r\n",
        "\r\n",
        "loser36 = [train_regret_loser_1[slice36],\r\n",
        "       train_regret_loser_2[slice36],\r\n",
        "       train_regret_loser_3[slice36],\r\n",
        "       train_regret_loser_4[slice36],\r\n",
        "       train_regret_loser_5[slice36],\r\n",
        "       train_regret_loser_6[slice36],\r\n",
        "       train_regret_loser_7[slice36],\r\n",
        "       train_regret_loser_8[slice36],\r\n",
        "       train_regret_loser_9[slice36],\r\n",
        "       train_regret_loser_10[slice36],\r\n",
        "       train_regret_loser_11[slice36],\r\n",
        "       train_regret_loser_12[slice36],\r\n",
        "       train_regret_loser_13[slice36],\r\n",
        "       train_regret_loser_14[slice36],\r\n",
        "       train_regret_loser_15[slice36],\r\n",
        "       train_regret_loser_16[slice36],\r\n",
        "       train_regret_loser_17[slice36],\r\n",
        "       train_regret_loser_18[slice36],\r\n",
        "       train_regret_loser_19[slice36],\r\n",
        "       train_regret_loser_20[slice36]]\r\n",
        "\r\n",
        "winner36 = [train_regret_winner_1[slice36],\r\n",
        "       train_regret_winner_2[slice36],\r\n",
        "       train_regret_winner_3[slice36],\r\n",
        "       train_regret_winner_4[slice36],\r\n",
        "       train_regret_winner_5[slice36],\r\n",
        "       train_regret_winner_6[slice36],\r\n",
        "       train_regret_winner_7[slice36],\r\n",
        "       train_regret_winner_8[slice36],\r\n",
        "       train_regret_winner_9[slice36],\r\n",
        "       train_regret_winner_10[slice36],\r\n",
        "       train_regret_winner_11[slice36],\r\n",
        "       train_regret_winner_12[slice36],\r\n",
        "       train_regret_winner_13[slice36],\r\n",
        "       train_regret_winner_14[slice36],\r\n",
        "       train_regret_winner_15[slice36],\r\n",
        "       train_regret_winner_16[slice36],\r\n",
        "       train_regret_winner_17[slice36],\r\n",
        "       train_regret_winner_18[slice36],\r\n",
        "       train_regret_winner_19[slice36],\r\n",
        "       train_regret_winner_20[slice36]]\r\n",
        "\r\n",
        "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\r\n",
        "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\r\n",
        "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\r\n",
        "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\r\n",
        "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\r\n",
        "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSqBLqTgxAWs"
      },
      "source": [
        "# Iteration46 :\r\n",
        "\r\n",
        "slice46 = 45\r\n",
        "\r\n",
        "loser46 = [train_regret_loser_1[slice46],\r\n",
        "       train_regret_loser_2[slice46],\r\n",
        "       train_regret_loser_3[slice46],\r\n",
        "       train_regret_loser_4[slice46],\r\n",
        "       train_regret_loser_5[slice46],\r\n",
        "       train_regret_loser_6[slice46],\r\n",
        "       train_regret_loser_7[slice46],\r\n",
        "       train_regret_loser_8[slice46],\r\n",
        "       train_regret_loser_9[slice46],\r\n",
        "       train_regret_loser_10[slice46],\r\n",
        "       train_regret_loser_11[slice46],\r\n",
        "       train_regret_loser_12[slice46],\r\n",
        "       train_regret_loser_13[slice46],\r\n",
        "       train_regret_loser_14[slice46],\r\n",
        "       train_regret_loser_15[slice46],\r\n",
        "       train_regret_loser_16[slice46],\r\n",
        "       train_regret_loser_17[slice46],\r\n",
        "       train_regret_loser_18[slice46],\r\n",
        "       train_regret_loser_19[slice46],\r\n",
        "       train_regret_loser_20[slice46]]\r\n",
        "\r\n",
        "winner46 = [train_regret_winner_1[slice46],\r\n",
        "       train_regret_winner_2[slice46],\r\n",
        "       train_regret_winner_3[slice46],\r\n",
        "       train_regret_winner_4[slice46],\r\n",
        "       train_regret_winner_5[slice46],\r\n",
        "       train_regret_winner_6[slice46],\r\n",
        "       train_regret_winner_7[slice46],\r\n",
        "       train_regret_winner_8[slice46],\r\n",
        "       train_regret_winner_9[slice46],\r\n",
        "       train_regret_winner_10[slice46],\r\n",
        "       train_regret_winner_11[slice46],\r\n",
        "       train_regret_winner_12[slice46],\r\n",
        "       train_regret_winner_13[slice46],\r\n",
        "       train_regret_winner_14[slice46],\r\n",
        "       train_regret_winner_15[slice46],\r\n",
        "       train_regret_winner_16[slice46],\r\n",
        "       train_regret_winner_17[slice46],\r\n",
        "       train_regret_winner_18[slice46],\r\n",
        "       train_regret_winner_19[slice46],\r\n",
        "       train_regret_winner_20[slice46]]\r\n",
        "\r\n",
        "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\r\n",
        "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\r\n",
        "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\r\n",
        "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\r\n",
        "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\r\n",
        "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFwO80w6xAZZ"
      },
      "source": [
        "# Iteration56 :\r\n",
        "\r\n",
        "slice56 = 55\r\n",
        "\r\n",
        "loser56 = [train_regret_loser_1[slice56],\r\n",
        "       train_regret_loser_2[slice56],\r\n",
        "       train_regret_loser_3[slice56],\r\n",
        "       train_regret_loser_4[slice56],\r\n",
        "       train_regret_loser_5[slice56],\r\n",
        "       train_regret_loser_6[slice56],\r\n",
        "       train_regret_loser_7[slice56],\r\n",
        "       train_regret_loser_8[slice56],\r\n",
        "       train_regret_loser_9[slice56],\r\n",
        "       train_regret_loser_10[slice56],\r\n",
        "       train_regret_loser_11[slice56],\r\n",
        "       train_regret_loser_12[slice56],\r\n",
        "       train_regret_loser_13[slice56],\r\n",
        "       train_regret_loser_14[slice56],\r\n",
        "       train_regret_loser_15[slice56],\r\n",
        "       train_regret_loser_16[slice56],\r\n",
        "       train_regret_loser_17[slice56],\r\n",
        "       train_regret_loser_18[slice56],\r\n",
        "       train_regret_loser_19[slice56],\r\n",
        "       train_regret_loser_20[slice56]]\r\n",
        "\r\n",
        "winner56 = [train_regret_winner_1[slice56],\r\n",
        "       train_regret_winner_2[slice56],\r\n",
        "       train_regret_winner_3[slice56],\r\n",
        "       train_regret_winner_4[slice56],\r\n",
        "       train_regret_winner_5[slice56],\r\n",
        "       train_regret_winner_6[slice56],\r\n",
        "       train_regret_winner_7[slice56],\r\n",
        "       train_regret_winner_8[slice56],\r\n",
        "       train_regret_winner_9[slice56],\r\n",
        "       train_regret_winner_10[slice56],\r\n",
        "       train_regret_winner_11[slice56],\r\n",
        "       train_regret_winner_12[slice56],\r\n",
        "       train_regret_winner_13[slice56],\r\n",
        "       train_regret_winner_14[slice56],\r\n",
        "       train_regret_winner_15[slice56],\r\n",
        "       train_regret_winner_16[slice56],\r\n",
        "       train_regret_winner_17[slice56],\r\n",
        "       train_regret_winner_18[slice56],\r\n",
        "       train_regret_winner_19[slice56],\r\n",
        "       train_regret_winner_20[slice56]]\r\n",
        "\r\n",
        "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\r\n",
        "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\r\n",
        "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\r\n",
        "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\r\n",
        "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\r\n",
        "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfVZBDP8xAcC"
      },
      "source": [
        "# Iteration66 :\r\n",
        "\r\n",
        "slice66 = 65\r\n",
        "\r\n",
        "loser66 = [train_regret_loser_1[slice66],\r\n",
        "       train_regret_loser_2[slice66],\r\n",
        "       train_regret_loser_3[slice66],\r\n",
        "       train_regret_loser_4[slice66],\r\n",
        "       train_regret_loser_5[slice66],\r\n",
        "       train_regret_loser_6[slice66],\r\n",
        "       train_regret_loser_7[slice66],\r\n",
        "       train_regret_loser_8[slice66],\r\n",
        "       train_regret_loser_9[slice66],\r\n",
        "       train_regret_loser_10[slice66],\r\n",
        "       train_regret_loser_11[slice66],\r\n",
        "       train_regret_loser_12[slice66],\r\n",
        "       train_regret_loser_13[slice66],\r\n",
        "       train_regret_loser_14[slice66],\r\n",
        "       train_regret_loser_15[slice66],\r\n",
        "       train_regret_loser_16[slice66],\r\n",
        "       train_regret_loser_17[slice66],\r\n",
        "       train_regret_loser_18[slice66],\r\n",
        "       train_regret_loser_19[slice66],\r\n",
        "       train_regret_loser_20[slice66]]\r\n",
        "\r\n",
        "winner66 = [train_regret_winner_1[slice66],\r\n",
        "       train_regret_winner_2[slice66],\r\n",
        "       train_regret_winner_3[slice66],\r\n",
        "       train_regret_winner_4[slice66],\r\n",
        "       train_regret_winner_5[slice66],\r\n",
        "       train_regret_winner_6[slice66],\r\n",
        "       train_regret_winner_7[slice66],\r\n",
        "       train_regret_winner_8[slice66],\r\n",
        "       train_regret_winner_9[slice66],\r\n",
        "       train_regret_winner_10[slice66],\r\n",
        "       train_regret_winner_11[slice66],\r\n",
        "       train_regret_winner_12[slice66],\r\n",
        "       train_regret_winner_13[slice66],\r\n",
        "       train_regret_winner_14[slice66],\r\n",
        "       train_regret_winner_15[slice66],\r\n",
        "       train_regret_winner_16[slice66],\r\n",
        "       train_regret_winner_17[slice66],\r\n",
        "       train_regret_winner_18[slice66],\r\n",
        "       train_regret_winner_19[slice66],\r\n",
        "       train_regret_winner_20[slice66]]\r\n",
        "\r\n",
        "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\r\n",
        "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\r\n",
        "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\r\n",
        "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\r\n",
        "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\r\n",
        "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnSYoG7bxAfL"
      },
      "source": [
        "# Iteration76 :\r\n",
        "\r\n",
        "slice76 = 75\r\n",
        "\r\n",
        "loser76 = [train_regret_loser_1[slice76],\r\n",
        "       train_regret_loser_2[slice76],\r\n",
        "       train_regret_loser_3[slice76],\r\n",
        "       train_regret_loser_4[slice76],\r\n",
        "       train_regret_loser_5[slice76],\r\n",
        "       train_regret_loser_6[slice76],\r\n",
        "       train_regret_loser_7[slice76],\r\n",
        "       train_regret_loser_8[slice76],\r\n",
        "       train_regret_loser_9[slice76],\r\n",
        "       train_regret_loser_10[slice76],\r\n",
        "       train_regret_loser_11[slice76],\r\n",
        "       train_regret_loser_12[slice76],\r\n",
        "       train_regret_loser_13[slice76],\r\n",
        "       train_regret_loser_14[slice76],\r\n",
        "       train_regret_loser_15[slice76],\r\n",
        "       train_regret_loser_16[slice76],\r\n",
        "       train_regret_loser_17[slice76],\r\n",
        "       train_regret_loser_18[slice76],\r\n",
        "       train_regret_loser_19[slice76],\r\n",
        "       train_regret_loser_20[slice76]]\r\n",
        "\r\n",
        "winner76 = [train_regret_winner_1[slice76],\r\n",
        "       train_regret_winner_2[slice76],\r\n",
        "       train_regret_winner_3[slice76],\r\n",
        "       train_regret_winner_4[slice76],\r\n",
        "       train_regret_winner_5[slice76],\r\n",
        "       train_regret_winner_6[slice76],\r\n",
        "       train_regret_winner_7[slice76],\r\n",
        "       train_regret_winner_8[slice76],\r\n",
        "       train_regret_winner_9[slice76],\r\n",
        "       train_regret_winner_10[slice76],\r\n",
        "       train_regret_winner_11[slice76],\r\n",
        "       train_regret_winner_12[slice76],\r\n",
        "       train_regret_winner_13[slice76],\r\n",
        "       train_regret_winner_14[slice76],\r\n",
        "       train_regret_winner_15[slice76],\r\n",
        "       train_regret_winner_16[slice76],\r\n",
        "       train_regret_winner_17[slice76],\r\n",
        "       train_regret_winner_18[slice76],\r\n",
        "       train_regret_winner_19[slice76],\r\n",
        "       train_regret_winner_20[slice76]]\r\n",
        "\r\n",
        "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\r\n",
        "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\r\n",
        "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\r\n",
        "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\r\n",
        "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\r\n",
        "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ze_Oy9mxAiO"
      },
      "source": [
        "# Iteration86 :\r\n",
        "\r\n",
        "slice86 = 85\r\n",
        "\r\n",
        "loser86 = [train_regret_loser_1[slice86],\r\n",
        "       train_regret_loser_2[slice86],\r\n",
        "       train_regret_loser_3[slice86],\r\n",
        "       train_regret_loser_4[slice86],\r\n",
        "       train_regret_loser_5[slice86],\r\n",
        "       train_regret_loser_6[slice86],\r\n",
        "       train_regret_loser_7[slice86],\r\n",
        "       train_regret_loser_8[slice86],\r\n",
        "       train_regret_loser_9[slice86],\r\n",
        "       train_regret_loser_10[slice86],\r\n",
        "       train_regret_loser_11[slice86],\r\n",
        "       train_regret_loser_12[slice86],\r\n",
        "       train_regret_loser_13[slice86],\r\n",
        "       train_regret_loser_14[slice86],\r\n",
        "       train_regret_loser_15[slice86],\r\n",
        "       train_regret_loser_16[slice86],\r\n",
        "       train_regret_loser_17[slice86],\r\n",
        "       train_regret_loser_18[slice86],\r\n",
        "       train_regret_loser_19[slice86],\r\n",
        "       train_regret_loser_20[slice86]]\r\n",
        "\r\n",
        "winner86 = [train_regret_winner_1[slice86],\r\n",
        "       train_regret_winner_2[slice86],\r\n",
        "       train_regret_winner_3[slice86],\r\n",
        "       train_regret_winner_4[slice86],\r\n",
        "       train_regret_winner_5[slice86],\r\n",
        "       train_regret_winner_6[slice86],\r\n",
        "       train_regret_winner_7[slice86],\r\n",
        "       train_regret_winner_8[slice86],\r\n",
        "       train_regret_winner_9[slice86],\r\n",
        "       train_regret_winner_10[slice86],\r\n",
        "       train_regret_winner_11[slice86],\r\n",
        "       train_regret_winner_12[slice86],\r\n",
        "       train_regret_winner_13[slice86],\r\n",
        "       train_regret_winner_14[slice86],\r\n",
        "       train_regret_winner_15[slice86],\r\n",
        "       train_regret_winner_16[slice86],\r\n",
        "       train_regret_winner_17[slice86],\r\n",
        "       train_regret_winner_18[slice86],\r\n",
        "       train_regret_winner_19[slice86],\r\n",
        "       train_regret_winner_20[slice86]]\r\n",
        "\r\n",
        "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\r\n",
        "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\r\n",
        "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\r\n",
        "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\r\n",
        "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\r\n",
        "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glsE9_RPxAlS"
      },
      "source": [
        "# Iteration96 :\r\n",
        "\r\n",
        "slice96 = 95\r\n",
        "\r\n",
        "loser96 = [train_regret_loser_1[slice96],\r\n",
        "       train_regret_loser_2[slice96],\r\n",
        "       train_regret_loser_3[slice96],\r\n",
        "       train_regret_loser_4[slice96],\r\n",
        "       train_regret_loser_5[slice96],\r\n",
        "       train_regret_loser_6[slice96],\r\n",
        "       train_regret_loser_7[slice96],\r\n",
        "       train_regret_loser_8[slice96],\r\n",
        "       train_regret_loser_9[slice96],\r\n",
        "       train_regret_loser_10[slice96],\r\n",
        "       train_regret_loser_11[slice96],\r\n",
        "       train_regret_loser_12[slice96],\r\n",
        "       train_regret_loser_13[slice96],\r\n",
        "       train_regret_loser_14[slice96],\r\n",
        "       train_regret_loser_15[slice96],\r\n",
        "       train_regret_loser_16[slice96],\r\n",
        "       train_regret_loser_17[slice96],\r\n",
        "       train_regret_loser_18[slice96],\r\n",
        "       train_regret_loser_19[slice96],\r\n",
        "       train_regret_loser_20[slice96]]\r\n",
        "\r\n",
        "winner96 = [train_regret_winner_1[slice96],\r\n",
        "       train_regret_winner_2[slice96],\r\n",
        "       train_regret_winner_3[slice96],\r\n",
        "       train_regret_winner_4[slice96],\r\n",
        "       train_regret_winner_5[slice96],\r\n",
        "       train_regret_winner_6[slice96],\r\n",
        "       train_regret_winner_7[slice96],\r\n",
        "       train_regret_winner_8[slice96],\r\n",
        "       train_regret_winner_9[slice96],\r\n",
        "       train_regret_winner_10[slice96],\r\n",
        "       train_regret_winner_11[slice96],\r\n",
        "       train_regret_winner_12[slice96],\r\n",
        "       train_regret_winner_13[slice96],\r\n",
        "       train_regret_winner_14[slice96],\r\n",
        "       train_regret_winner_15[slice96],\r\n",
        "       train_regret_winner_16[slice96],\r\n",
        "       train_regret_winner_17[slice96],\r\n",
        "       train_regret_winner_18[slice96],\r\n",
        "       train_regret_winner_19[slice96],\r\n",
        "       train_regret_winner_20[slice96]]\r\n",
        "\r\n",
        "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\r\n",
        "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\r\n",
        "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\r\n",
        "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\r\n",
        "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\r\n",
        "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhsEfl1RxAoA"
      },
      "source": [
        "# Iteration7 :\r\n",
        "\r\n",
        "slice7 = 6\r\n",
        "\r\n",
        "loser7 = [train_regret_loser_1[slice7],\r\n",
        "       train_regret_loser_2[slice7],\r\n",
        "       train_regret_loser_3[slice7],\r\n",
        "       train_regret_loser_4[slice7],\r\n",
        "       train_regret_loser_5[slice7],\r\n",
        "       train_regret_loser_6[slice7],\r\n",
        "       train_regret_loser_7[slice7],\r\n",
        "       train_regret_loser_8[slice7],\r\n",
        "       train_regret_loser_9[slice7],\r\n",
        "       train_regret_loser_10[slice7],\r\n",
        "       train_regret_loser_11[slice7],\r\n",
        "       train_regret_loser_12[slice7],\r\n",
        "       train_regret_loser_13[slice7],\r\n",
        "       train_regret_loser_14[slice7],\r\n",
        "       train_regret_loser_15[slice7],\r\n",
        "       train_regret_loser_16[slice7],\r\n",
        "       train_regret_loser_17[slice7],\r\n",
        "       train_regret_loser_18[slice7],\r\n",
        "       train_regret_loser_19[slice7],\r\n",
        "       train_regret_loser_20[slice7]]\r\n",
        "\r\n",
        "winner7 = [train_regret_winner_1[slice7],\r\n",
        "       train_regret_winner_2[slice7],\r\n",
        "       train_regret_winner_3[slice7],\r\n",
        "       train_regret_winner_4[slice7],\r\n",
        "       train_regret_winner_5[slice7],\r\n",
        "       train_regret_winner_6[slice7],\r\n",
        "       train_regret_winner_7[slice7],\r\n",
        "       train_regret_winner_8[slice7],\r\n",
        "       train_regret_winner_9[slice7],\r\n",
        "       train_regret_winner_10[slice7],\r\n",
        "       train_regret_winner_11[slice7],\r\n",
        "       train_regret_winner_12[slice7],\r\n",
        "       train_regret_winner_13[slice7],\r\n",
        "       train_regret_winner_14[slice7],\r\n",
        "       train_regret_winner_15[slice7],\r\n",
        "       train_regret_winner_16[slice7],\r\n",
        "       train_regret_winner_17[slice7],\r\n",
        "       train_regret_winner_18[slice7],\r\n",
        "       train_regret_winner_19[slice7],\r\n",
        "       train_regret_winner_20[slice7]]\r\n",
        "\r\n",
        "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\r\n",
        "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\r\n",
        "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\r\n",
        "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\r\n",
        "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\r\n",
        "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUCMheLKxArL"
      },
      "source": [
        "# Iteration17 :\r\n",
        "\r\n",
        "slice17 = 16\r\n",
        "\r\n",
        "loser17 = [train_regret_loser_1[slice17],\r\n",
        "       train_regret_loser_2[slice17],\r\n",
        "       train_regret_loser_3[slice17],\r\n",
        "       train_regret_loser_4[slice17],\r\n",
        "       train_regret_loser_5[slice17],\r\n",
        "       train_regret_loser_6[slice17],\r\n",
        "       train_regret_loser_7[slice17],\r\n",
        "       train_regret_loser_8[slice17],\r\n",
        "       train_regret_loser_9[slice17],\r\n",
        "       train_regret_loser_10[slice17],\r\n",
        "       train_regret_loser_11[slice17],\r\n",
        "       train_regret_loser_12[slice17],\r\n",
        "       train_regret_loser_13[slice17],\r\n",
        "       train_regret_loser_14[slice17],\r\n",
        "       train_regret_loser_15[slice17],\r\n",
        "       train_regret_loser_16[slice17],\r\n",
        "       train_regret_loser_17[slice17],\r\n",
        "       train_regret_loser_18[slice17],\r\n",
        "       train_regret_loser_19[slice17],\r\n",
        "       train_regret_loser_20[slice17]]\r\n",
        "\r\n",
        "winner17 = [train_regret_winner_1[slice17],\r\n",
        "       train_regret_winner_2[slice17],\r\n",
        "       train_regret_winner_3[slice17],\r\n",
        "       train_regret_winner_4[slice17],\r\n",
        "       train_regret_winner_5[slice17],\r\n",
        "       train_regret_winner_6[slice17],\r\n",
        "       train_regret_winner_7[slice17],\r\n",
        "       train_regret_winner_8[slice17],\r\n",
        "       train_regret_winner_9[slice17],\r\n",
        "       train_regret_winner_10[slice17],\r\n",
        "       train_regret_winner_11[slice17],\r\n",
        "       train_regret_winner_12[slice17],\r\n",
        "       train_regret_winner_13[slice17],\r\n",
        "       train_regret_winner_14[slice17],\r\n",
        "       train_regret_winner_15[slice17],\r\n",
        "       train_regret_winner_16[slice17],\r\n",
        "       train_regret_winner_17[slice17],\r\n",
        "       train_regret_winner_18[slice17],\r\n",
        "       train_regret_winner_19[slice17],\r\n",
        "       train_regret_winner_20[slice17]]\r\n",
        "\r\n",
        "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\r\n",
        "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\r\n",
        "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\r\n",
        "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\r\n",
        "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\r\n",
        "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYnBmq7_xAuB"
      },
      "source": [
        "# Iteration27 :\r\n",
        "\r\n",
        "slice27 = 26\r\n",
        "\r\n",
        "loser27 = [train_regret_loser_1[slice27],\r\n",
        "       train_regret_loser_2[slice27],\r\n",
        "       train_regret_loser_3[slice27],\r\n",
        "       train_regret_loser_4[slice27],\r\n",
        "       train_regret_loser_5[slice27],\r\n",
        "       train_regret_loser_6[slice27],\r\n",
        "       train_regret_loser_7[slice27],\r\n",
        "       train_regret_loser_8[slice27],\r\n",
        "       train_regret_loser_9[slice27],\r\n",
        "       train_regret_loser_10[slice27],\r\n",
        "       train_regret_loser_11[slice27],\r\n",
        "       train_regret_loser_12[slice27],\r\n",
        "       train_regret_loser_13[slice27],\r\n",
        "       train_regret_loser_14[slice27],\r\n",
        "       train_regret_loser_15[slice27],\r\n",
        "       train_regret_loser_16[slice27],\r\n",
        "       train_regret_loser_17[slice27],\r\n",
        "       train_regret_loser_18[slice27],\r\n",
        "       train_regret_loser_19[slice27],\r\n",
        "       train_regret_loser_20[slice27]]\r\n",
        "\r\n",
        "winner27 = [train_regret_winner_1[slice27],\r\n",
        "       train_regret_winner_2[slice27],\r\n",
        "       train_regret_winner_3[slice27],\r\n",
        "       train_regret_winner_4[slice27],\r\n",
        "       train_regret_winner_5[slice27],\r\n",
        "       train_regret_winner_6[slice27],\r\n",
        "       train_regret_winner_7[slice27],\r\n",
        "       train_regret_winner_8[slice27],\r\n",
        "       train_regret_winner_9[slice27],\r\n",
        "       train_regret_winner_10[slice27],\r\n",
        "       train_regret_winner_11[slice27],\r\n",
        "       train_regret_winner_12[slice27],\r\n",
        "       train_regret_winner_13[slice27],\r\n",
        "       train_regret_winner_14[slice27],\r\n",
        "       train_regret_winner_15[slice27],\r\n",
        "       train_regret_winner_16[slice27],\r\n",
        "       train_regret_winner_17[slice27],\r\n",
        "       train_regret_winner_18[slice27],\r\n",
        "       train_regret_winner_19[slice27],\r\n",
        "       train_regret_winner_20[slice27]]\r\n",
        "\r\n",
        "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\r\n",
        "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\r\n",
        "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\r\n",
        "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\r\n",
        "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\r\n",
        "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ChA8KOUxAw6"
      },
      "source": [
        "# Iteration37 :\r\n",
        "\r\n",
        "slice37 = 36\r\n",
        "\r\n",
        "loser37 = [train_regret_loser_1[slice37],\r\n",
        "       train_regret_loser_2[slice37],\r\n",
        "       train_regret_loser_3[slice37],\r\n",
        "       train_regret_loser_4[slice37],\r\n",
        "       train_regret_loser_5[slice37],\r\n",
        "       train_regret_loser_6[slice37],\r\n",
        "       train_regret_loser_7[slice37],\r\n",
        "       train_regret_loser_8[slice37],\r\n",
        "       train_regret_loser_9[slice37],\r\n",
        "       train_regret_loser_10[slice37],\r\n",
        "       train_regret_loser_11[slice37],\r\n",
        "       train_regret_loser_12[slice37],\r\n",
        "       train_regret_loser_13[slice37],\r\n",
        "       train_regret_loser_14[slice37],\r\n",
        "       train_regret_loser_15[slice37],\r\n",
        "       train_regret_loser_16[slice37],\r\n",
        "       train_regret_loser_17[slice37],\r\n",
        "       train_regret_loser_18[slice37],\r\n",
        "       train_regret_loser_19[slice37],\r\n",
        "       train_regret_loser_20[slice37]]\r\n",
        "\r\n",
        "winner37 = [train_regret_winner_1[slice37],\r\n",
        "       train_regret_winner_2[slice37],\r\n",
        "       train_regret_winner_3[slice37],\r\n",
        "       train_regret_winner_4[slice37],\r\n",
        "       train_regret_winner_5[slice37],\r\n",
        "       train_regret_winner_6[slice37],\r\n",
        "       train_regret_winner_7[slice37],\r\n",
        "       train_regret_winner_8[slice37],\r\n",
        "       train_regret_winner_9[slice37],\r\n",
        "       train_regret_winner_10[slice37],\r\n",
        "       train_regret_winner_11[slice37],\r\n",
        "       train_regret_winner_12[slice37],\r\n",
        "       train_regret_winner_13[slice37],\r\n",
        "       train_regret_winner_14[slice37],\r\n",
        "       train_regret_winner_15[slice37],\r\n",
        "       train_regret_winner_16[slice37],\r\n",
        "       train_regret_winner_17[slice37],\r\n",
        "       train_regret_winner_18[slice37],\r\n",
        "       train_regret_winner_19[slice37],\r\n",
        "       train_regret_winner_20[slice37]]\r\n",
        "\r\n",
        "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\r\n",
        "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\r\n",
        "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\r\n",
        "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\r\n",
        "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\r\n",
        "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVsY1buDxAzx"
      },
      "source": [
        "# Iteration47 :\r\n",
        "\r\n",
        "slice47 = 46\r\n",
        "\r\n",
        "loser47 = [train_regret_loser_1[slice47],\r\n",
        "       train_regret_loser_2[slice47],\r\n",
        "       train_regret_loser_3[slice47],\r\n",
        "       train_regret_loser_4[slice47],\r\n",
        "       train_regret_loser_5[slice47],\r\n",
        "       train_regret_loser_6[slice47],\r\n",
        "       train_regret_loser_7[slice47],\r\n",
        "       train_regret_loser_8[slice47],\r\n",
        "       train_regret_loser_9[slice47],\r\n",
        "       train_regret_loser_10[slice47],\r\n",
        "       train_regret_loser_11[slice47],\r\n",
        "       train_regret_loser_12[slice47],\r\n",
        "       train_regret_loser_13[slice47],\r\n",
        "       train_regret_loser_14[slice47],\r\n",
        "       train_regret_loser_15[slice47],\r\n",
        "       train_regret_loser_16[slice47],\r\n",
        "       train_regret_loser_17[slice47],\r\n",
        "       train_regret_loser_18[slice47],\r\n",
        "       train_regret_loser_19[slice47],\r\n",
        "       train_regret_loser_20[slice47]]\r\n",
        "\r\n",
        "winner47 = [train_regret_winner_1[slice47],\r\n",
        "       train_regret_winner_2[slice47],\r\n",
        "       train_regret_winner_3[slice47],\r\n",
        "       train_regret_winner_4[slice47],\r\n",
        "       train_regret_winner_5[slice47],\r\n",
        "       train_regret_winner_6[slice47],\r\n",
        "       train_regret_winner_7[slice47],\r\n",
        "       train_regret_winner_8[slice47],\r\n",
        "       train_regret_winner_9[slice47],\r\n",
        "       train_regret_winner_10[slice47],\r\n",
        "       train_regret_winner_11[slice47],\r\n",
        "       train_regret_winner_12[slice47],\r\n",
        "       train_regret_winner_13[slice47],\r\n",
        "       train_regret_winner_14[slice47],\r\n",
        "       train_regret_winner_15[slice47],\r\n",
        "       train_regret_winner_16[slice47],\r\n",
        "       train_regret_winner_17[slice47],\r\n",
        "       train_regret_winner_18[slice47],\r\n",
        "       train_regret_winner_19[slice47],\r\n",
        "       train_regret_winner_20[slice47]]\r\n",
        "\r\n",
        "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\r\n",
        "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\r\n",
        "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\r\n",
        "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\r\n",
        "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\r\n",
        "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fIy_EyTxA20"
      },
      "source": [
        "# Iteration57 :\r\n",
        "\r\n",
        "slice57 = 56\r\n",
        "\r\n",
        "loser57 = [train_regret_loser_1[slice57],\r\n",
        "       train_regret_loser_2[slice57],\r\n",
        "       train_regret_loser_3[slice57],\r\n",
        "       train_regret_loser_4[slice57],\r\n",
        "       train_regret_loser_5[slice57],\r\n",
        "       train_regret_loser_6[slice57],\r\n",
        "       train_regret_loser_7[slice57],\r\n",
        "       train_regret_loser_8[slice57],\r\n",
        "       train_regret_loser_9[slice57],\r\n",
        "       train_regret_loser_10[slice57],\r\n",
        "       train_regret_loser_11[slice57],\r\n",
        "       train_regret_loser_12[slice57],\r\n",
        "       train_regret_loser_13[slice57],\r\n",
        "       train_regret_loser_14[slice57],\r\n",
        "       train_regret_loser_15[slice57],\r\n",
        "       train_regret_loser_16[slice57],\r\n",
        "       train_regret_loser_17[slice57],\r\n",
        "       train_regret_loser_18[slice57],\r\n",
        "       train_regret_loser_19[slice57],\r\n",
        "       train_regret_loser_20[slice57]]\r\n",
        "\r\n",
        "winner57 = [train_regret_winner_1[slice57],\r\n",
        "       train_regret_winner_2[slice57],\r\n",
        "       train_regret_winner_3[slice57],\r\n",
        "       train_regret_winner_4[slice57],\r\n",
        "       train_regret_winner_5[slice57],\r\n",
        "       train_regret_winner_6[slice57],\r\n",
        "       train_regret_winner_7[slice57],\r\n",
        "       train_regret_winner_8[slice57],\r\n",
        "       train_regret_winner_9[slice57],\r\n",
        "       train_regret_winner_10[slice57],\r\n",
        "       train_regret_winner_11[slice57],\r\n",
        "       train_regret_winner_12[slice57],\r\n",
        "       train_regret_winner_13[slice57],\r\n",
        "       train_regret_winner_14[slice57],\r\n",
        "       train_regret_winner_15[slice57],\r\n",
        "       train_regret_winner_16[slice57],\r\n",
        "       train_regret_winner_17[slice57],\r\n",
        "       train_regret_winner_18[slice57],\r\n",
        "       train_regret_winner_19[slice57],\r\n",
        "       train_regret_winner_20[slice57]]\r\n",
        "\r\n",
        "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\r\n",
        "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\r\n",
        "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\r\n",
        "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\r\n",
        "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\r\n",
        "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umwdBl8XxA5r"
      },
      "source": [
        "# Iteration67 :\r\n",
        "\r\n",
        "slice67 = 66\r\n",
        "\r\n",
        "loser67 = [train_regret_loser_1[slice67],\r\n",
        "       train_regret_loser_2[slice67],\r\n",
        "       train_regret_loser_3[slice67],\r\n",
        "       train_regret_loser_4[slice67],\r\n",
        "       train_regret_loser_5[slice67],\r\n",
        "       train_regret_loser_6[slice67],\r\n",
        "       train_regret_loser_7[slice67],\r\n",
        "       train_regret_loser_8[slice67],\r\n",
        "       train_regret_loser_9[slice67],\r\n",
        "       train_regret_loser_10[slice67],\r\n",
        "       train_regret_loser_11[slice67],\r\n",
        "       train_regret_loser_12[slice67],\r\n",
        "       train_regret_loser_13[slice67],\r\n",
        "       train_regret_loser_14[slice67],\r\n",
        "       train_regret_loser_15[slice67],\r\n",
        "       train_regret_loser_16[slice67],\r\n",
        "       train_regret_loser_17[slice67],\r\n",
        "       train_regret_loser_18[slice67],\r\n",
        "       train_regret_loser_19[slice67],\r\n",
        "       train_regret_loser_20[slice67]]\r\n",
        "\r\n",
        "winner67 = [train_regret_winner_1[slice67],\r\n",
        "       train_regret_winner_2[slice67],\r\n",
        "       train_regret_winner_3[slice67],\r\n",
        "       train_regret_winner_4[slice67],\r\n",
        "       train_regret_winner_5[slice67],\r\n",
        "       train_regret_winner_6[slice67],\r\n",
        "       train_regret_winner_7[slice67],\r\n",
        "       train_regret_winner_8[slice67],\r\n",
        "       train_regret_winner_9[slice67],\r\n",
        "       train_regret_winner_10[slice67],\r\n",
        "       train_regret_winner_11[slice67],\r\n",
        "       train_regret_winner_12[slice67],\r\n",
        "       train_regret_winner_13[slice67],\r\n",
        "       train_regret_winner_14[slice67],\r\n",
        "       train_regret_winner_15[slice67],\r\n",
        "       train_regret_winner_16[slice67],\r\n",
        "       train_regret_winner_17[slice67],\r\n",
        "       train_regret_winner_18[slice67],\r\n",
        "       train_regret_winner_19[slice67],\r\n",
        "       train_regret_winner_20[slice67]]\r\n",
        "\r\n",
        "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\r\n",
        "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\r\n",
        "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\r\n",
        "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\r\n",
        "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\r\n",
        "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF3KFX-ZxA8q"
      },
      "source": [
        "# Iteration77 :\r\n",
        "\r\n",
        "slice77 = 76\r\n",
        "\r\n",
        "loser77 = [train_regret_loser_1[slice77],\r\n",
        "       train_regret_loser_2[slice77],\r\n",
        "       train_regret_loser_3[slice77],\r\n",
        "       train_regret_loser_4[slice77],\r\n",
        "       train_regret_loser_5[slice77],\r\n",
        "       train_regret_loser_6[slice77],\r\n",
        "       train_regret_loser_7[slice77],\r\n",
        "       train_regret_loser_8[slice77],\r\n",
        "       train_regret_loser_9[slice77],\r\n",
        "       train_regret_loser_10[slice77],\r\n",
        "       train_regret_loser_11[slice77],\r\n",
        "       train_regret_loser_12[slice77],\r\n",
        "       train_regret_loser_13[slice77],\r\n",
        "       train_regret_loser_14[slice77],\r\n",
        "       train_regret_loser_15[slice77],\r\n",
        "       train_regret_loser_16[slice77],\r\n",
        "       train_regret_loser_17[slice77],\r\n",
        "       train_regret_loser_18[slice77],\r\n",
        "       train_regret_loser_19[slice77],\r\n",
        "       train_regret_loser_20[slice77]]\r\n",
        "\r\n",
        "winner77 = [train_regret_winner_1[slice77],\r\n",
        "       train_regret_winner_2[slice77],\r\n",
        "       train_regret_winner_3[slice77],\r\n",
        "       train_regret_winner_4[slice77],\r\n",
        "       train_regret_winner_5[slice77],\r\n",
        "       train_regret_winner_6[slice77],\r\n",
        "       train_regret_winner_7[slice77],\r\n",
        "       train_regret_winner_8[slice77],\r\n",
        "       train_regret_winner_9[slice77],\r\n",
        "       train_regret_winner_10[slice77],\r\n",
        "       train_regret_winner_11[slice77],\r\n",
        "       train_regret_winner_12[slice77],\r\n",
        "       train_regret_winner_13[slice77],\r\n",
        "       train_regret_winner_14[slice77],\r\n",
        "       train_regret_winner_15[slice77],\r\n",
        "       train_regret_winner_16[slice77],\r\n",
        "       train_regret_winner_17[slice77],\r\n",
        "       train_regret_winner_18[slice77],\r\n",
        "       train_regret_winner_19[slice77],\r\n",
        "       train_regret_winner_20[slice77]]\r\n",
        "\r\n",
        "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\r\n",
        "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\r\n",
        "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\r\n",
        "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\r\n",
        "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\r\n",
        "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVYft39xA_t"
      },
      "source": [
        "# Iteration87 :\r\n",
        "\r\n",
        "slice87 = 86\r\n",
        "\r\n",
        "loser87 = [train_regret_loser_1[slice87],\r\n",
        "       train_regret_loser_2[slice87],\r\n",
        "       train_regret_loser_3[slice87],\r\n",
        "       train_regret_loser_4[slice87],\r\n",
        "       train_regret_loser_5[slice87],\r\n",
        "       train_regret_loser_6[slice87],\r\n",
        "       train_regret_loser_7[slice87],\r\n",
        "       train_regret_loser_8[slice87],\r\n",
        "       train_regret_loser_9[slice87],\r\n",
        "       train_regret_loser_10[slice87],\r\n",
        "       train_regret_loser_11[slice87],\r\n",
        "       train_regret_loser_12[slice87],\r\n",
        "       train_regret_loser_13[slice87],\r\n",
        "       train_regret_loser_14[slice87],\r\n",
        "       train_regret_loser_15[slice87],\r\n",
        "       train_regret_loser_16[slice87],\r\n",
        "       train_regret_loser_17[slice87],\r\n",
        "       train_regret_loser_18[slice87],\r\n",
        "       train_regret_loser_19[slice87],\r\n",
        "       train_regret_loser_20[slice87]]\r\n",
        "\r\n",
        "winner87 = [train_regret_winner_1[slice87],\r\n",
        "       train_regret_winner_2[slice87],\r\n",
        "       train_regret_winner_3[slice87],\r\n",
        "       train_regret_winner_4[slice87],\r\n",
        "       train_regret_winner_5[slice87],\r\n",
        "       train_regret_winner_6[slice87],\r\n",
        "       train_regret_winner_7[slice87],\r\n",
        "       train_regret_winner_8[slice87],\r\n",
        "       train_regret_winner_9[slice87],\r\n",
        "       train_regret_winner_10[slice87],\r\n",
        "       train_regret_winner_11[slice87],\r\n",
        "       train_regret_winner_12[slice87],\r\n",
        "       train_regret_winner_13[slice87],\r\n",
        "       train_regret_winner_14[slice87],\r\n",
        "       train_regret_winner_15[slice87],\r\n",
        "       train_regret_winner_16[slice87],\r\n",
        "       train_regret_winner_17[slice87],\r\n",
        "       train_regret_winner_18[slice87],\r\n",
        "       train_regret_winner_19[slice87],\r\n",
        "       train_regret_winner_20[slice87]]\r\n",
        "\r\n",
        "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\r\n",
        "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\r\n",
        "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\r\n",
        "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\r\n",
        "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\r\n",
        "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gPkLC0yxBCs"
      },
      "source": [
        "# Iteration97 :\r\n",
        "\r\n",
        "slice97 = 96\r\n",
        "\r\n",
        "loser97 = [train_regret_loser_1[slice97],\r\n",
        "       train_regret_loser_2[slice97],\r\n",
        "       train_regret_loser_3[slice97],\r\n",
        "       train_regret_loser_4[slice97],\r\n",
        "       train_regret_loser_5[slice97],\r\n",
        "       train_regret_loser_6[slice97],\r\n",
        "       train_regret_loser_7[slice97],\r\n",
        "       train_regret_loser_8[slice97],\r\n",
        "       train_regret_loser_9[slice97],\r\n",
        "       train_regret_loser_10[slice97],\r\n",
        "       train_regret_loser_11[slice97],\r\n",
        "       train_regret_loser_12[slice97],\r\n",
        "       train_regret_loser_13[slice97],\r\n",
        "       train_regret_loser_14[slice97],\r\n",
        "       train_regret_loser_15[slice97],\r\n",
        "       train_regret_loser_16[slice97],\r\n",
        "       train_regret_loser_17[slice97],\r\n",
        "       train_regret_loser_18[slice97],\r\n",
        "       train_regret_loser_19[slice97],\r\n",
        "       train_regret_loser_20[slice97]]\r\n",
        "\r\n",
        "winner97 = [train_regret_winner_1[slice97],\r\n",
        "       train_regret_winner_2[slice97],\r\n",
        "       train_regret_winner_3[slice97],\r\n",
        "       train_regret_winner_4[slice97],\r\n",
        "       train_regret_winner_5[slice97],\r\n",
        "       train_regret_winner_6[slice97],\r\n",
        "       train_regret_winner_7[slice97],\r\n",
        "       train_regret_winner_8[slice97],\r\n",
        "       train_regret_winner_9[slice97],\r\n",
        "       train_regret_winner_10[slice97],\r\n",
        "       train_regret_winner_11[slice97],\r\n",
        "       train_regret_winner_12[slice97],\r\n",
        "       train_regret_winner_13[slice97],\r\n",
        "       train_regret_winner_14[slice97],\r\n",
        "       train_regret_winner_15[slice97],\r\n",
        "       train_regret_winner_16[slice97],\r\n",
        "       train_regret_winner_17[slice97],\r\n",
        "       train_regret_winner_18[slice97],\r\n",
        "       train_regret_winner_19[slice97],\r\n",
        "       train_regret_winner_20[slice97]]\r\n",
        "\r\n",
        "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\r\n",
        "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\r\n",
        "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\r\n",
        "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\r\n",
        "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\r\n",
        "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbgYEpYUxBFr"
      },
      "source": [
        "# Iteration8 :\r\n",
        "\r\n",
        "slice8 = 7\r\n",
        "\r\n",
        "loser8 = [train_regret_loser_1[slice8],\r\n",
        "       train_regret_loser_2[slice8],\r\n",
        "       train_regret_loser_3[slice8],\r\n",
        "       train_regret_loser_4[slice8],\r\n",
        "       train_regret_loser_5[slice8],\r\n",
        "       train_regret_loser_6[slice8],\r\n",
        "       train_regret_loser_7[slice8],\r\n",
        "       train_regret_loser_8[slice8],\r\n",
        "       train_regret_loser_9[slice8],\r\n",
        "       train_regret_loser_10[slice8],\r\n",
        "       train_regret_loser_11[slice8],\r\n",
        "       train_regret_loser_12[slice8],\r\n",
        "       train_regret_loser_13[slice8],\r\n",
        "       train_regret_loser_14[slice8],\r\n",
        "       train_regret_loser_15[slice8],\r\n",
        "       train_regret_loser_16[slice8],\r\n",
        "       train_regret_loser_17[slice8],\r\n",
        "       train_regret_loser_18[slice8],\r\n",
        "       train_regret_loser_19[slice8],\r\n",
        "       train_regret_loser_20[slice8]]\r\n",
        "\r\n",
        "winner8 = [train_regret_winner_1[slice8],\r\n",
        "       train_regret_winner_2[slice8],\r\n",
        "       train_regret_winner_3[slice8],\r\n",
        "       train_regret_winner_4[slice8],\r\n",
        "       train_regret_winner_5[slice8],\r\n",
        "       train_regret_winner_6[slice8],\r\n",
        "       train_regret_winner_7[slice8],\r\n",
        "       train_regret_winner_8[slice8],\r\n",
        "       train_regret_winner_9[slice8],\r\n",
        "       train_regret_winner_10[slice8],\r\n",
        "       train_regret_winner_11[slice8],\r\n",
        "       train_regret_winner_12[slice8],\r\n",
        "       train_regret_winner_13[slice8],\r\n",
        "       train_regret_winner_14[slice8],\r\n",
        "       train_regret_winner_15[slice8],\r\n",
        "       train_regret_winner_16[slice8],\r\n",
        "       train_regret_winner_17[slice8],\r\n",
        "       train_regret_winner_18[slice8],\r\n",
        "       train_regret_winner_19[slice8],\r\n",
        "       train_regret_winner_20[slice8]]\r\n",
        "\r\n",
        "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\r\n",
        "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\r\n",
        "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\r\n",
        "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\r\n",
        "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\r\n",
        "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im9dHUqvxBH9"
      },
      "source": [
        "# Iteration18 :\r\n",
        "\r\n",
        "slice18 = 17\r\n",
        "\r\n",
        "loser18 = [train_regret_loser_1[slice18],\r\n",
        "       train_regret_loser_2[slice18],\r\n",
        "       train_regret_loser_3[slice18],\r\n",
        "       train_regret_loser_4[slice18],\r\n",
        "       train_regret_loser_5[slice18],\r\n",
        "       train_regret_loser_6[slice18],\r\n",
        "       train_regret_loser_7[slice18],\r\n",
        "       train_regret_loser_8[slice18],\r\n",
        "       train_regret_loser_9[slice18],\r\n",
        "       train_regret_loser_10[slice18],\r\n",
        "       train_regret_loser_11[slice18],\r\n",
        "       train_regret_loser_12[slice18],\r\n",
        "       train_regret_loser_13[slice18],\r\n",
        "       train_regret_loser_14[slice18],\r\n",
        "       train_regret_loser_15[slice18],\r\n",
        "       train_regret_loser_16[slice18],\r\n",
        "       train_regret_loser_17[slice18],\r\n",
        "       train_regret_loser_18[slice18],\r\n",
        "       train_regret_loser_19[slice18],\r\n",
        "       train_regret_loser_20[slice18]]\r\n",
        "\r\n",
        "winner18 = [train_regret_winner_1[slice18],\r\n",
        "       train_regret_winner_2[slice18],\r\n",
        "       train_regret_winner_3[slice18],\r\n",
        "       train_regret_winner_4[slice18],\r\n",
        "       train_regret_winner_5[slice18],\r\n",
        "       train_regret_winner_6[slice18],\r\n",
        "       train_regret_winner_7[slice18],\r\n",
        "       train_regret_winner_8[slice18],\r\n",
        "       train_regret_winner_9[slice18],\r\n",
        "       train_regret_winner_10[slice18],\r\n",
        "       train_regret_winner_11[slice18],\r\n",
        "       train_regret_winner_12[slice18],\r\n",
        "       train_regret_winner_13[slice18],\r\n",
        "       train_regret_winner_14[slice18],\r\n",
        "       train_regret_winner_15[slice18],\r\n",
        "       train_regret_winner_16[slice18],\r\n",
        "       train_regret_winner_17[slice18],\r\n",
        "       train_regret_winner_18[slice18],\r\n",
        "       train_regret_winner_19[slice18],\r\n",
        "       train_regret_winner_20[slice18]]\r\n",
        "\r\n",
        "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\r\n",
        "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\r\n",
        "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\r\n",
        "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\r\n",
        "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\r\n",
        "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvqFjlBDxBK4"
      },
      "source": [
        "# Iteration28 :\r\n",
        "\r\n",
        "slice28 = 27\r\n",
        "\r\n",
        "loser28 = [train_regret_loser_1[slice28],\r\n",
        "       train_regret_loser_2[slice28],\r\n",
        "       train_regret_loser_3[slice28],\r\n",
        "       train_regret_loser_4[slice28],\r\n",
        "       train_regret_loser_5[slice28],\r\n",
        "       train_regret_loser_6[slice28],\r\n",
        "       train_regret_loser_7[slice28],\r\n",
        "       train_regret_loser_8[slice28],\r\n",
        "       train_regret_loser_9[slice28],\r\n",
        "       train_regret_loser_10[slice28],\r\n",
        "       train_regret_loser_11[slice28],\r\n",
        "       train_regret_loser_12[slice28],\r\n",
        "       train_regret_loser_13[slice28],\r\n",
        "       train_regret_loser_14[slice28],\r\n",
        "       train_regret_loser_15[slice28],\r\n",
        "       train_regret_loser_16[slice28],\r\n",
        "       train_regret_loser_17[slice28],\r\n",
        "       train_regret_loser_18[slice28],\r\n",
        "       train_regret_loser_19[slice28],\r\n",
        "       train_regret_loser_20[slice28]]\r\n",
        "\r\n",
        "winner28 = [train_regret_winner_1[slice28],\r\n",
        "       train_regret_winner_2[slice28],\r\n",
        "       train_regret_winner_3[slice28],\r\n",
        "       train_regret_winner_4[slice28],\r\n",
        "       train_regret_winner_5[slice28],\r\n",
        "       train_regret_winner_6[slice28],\r\n",
        "       train_regret_winner_7[slice28],\r\n",
        "       train_regret_winner_8[slice28],\r\n",
        "       train_regret_winner_9[slice28],\r\n",
        "       train_regret_winner_10[slice28],\r\n",
        "       train_regret_winner_11[slice28],\r\n",
        "       train_regret_winner_12[slice28],\r\n",
        "       train_regret_winner_13[slice28],\r\n",
        "       train_regret_winner_14[slice28],\r\n",
        "       train_regret_winner_15[slice28],\r\n",
        "       train_regret_winner_16[slice28],\r\n",
        "       train_regret_winner_17[slice28],\r\n",
        "       train_regret_winner_18[slice28],\r\n",
        "       train_regret_winner_19[slice28],\r\n",
        "       train_regret_winner_20[slice28]]\r\n",
        "\r\n",
        "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\r\n",
        "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\r\n",
        "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\r\n",
        "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\r\n",
        "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\r\n",
        "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It5daJCEted3"
      },
      "source": [
        "# Iteration38 :\r\n",
        "\r\n",
        "slice38 = 37\r\n",
        "\r\n",
        "loser38 = [train_regret_loser_1[slice38],\r\n",
        "       train_regret_loser_2[slice38],\r\n",
        "       train_regret_loser_3[slice38],\r\n",
        "       train_regret_loser_4[slice38],\r\n",
        "       train_regret_loser_5[slice38],\r\n",
        "       train_regret_loser_6[slice38],\r\n",
        "       train_regret_loser_7[slice38],\r\n",
        "       train_regret_loser_8[slice38],\r\n",
        "       train_regret_loser_9[slice38],\r\n",
        "       train_regret_loser_10[slice38],\r\n",
        "       train_regret_loser_11[slice38],\r\n",
        "       train_regret_loser_12[slice38],\r\n",
        "       train_regret_loser_13[slice38],\r\n",
        "       train_regret_loser_14[slice38],\r\n",
        "       train_regret_loser_15[slice38],\r\n",
        "       train_regret_loser_16[slice38],\r\n",
        "       train_regret_loser_17[slice38],\r\n",
        "       train_regret_loser_18[slice38],\r\n",
        "       train_regret_loser_19[slice38],\r\n",
        "       train_regret_loser_20[slice38]]\r\n",
        "\r\n",
        "winner38 = [train_regret_winner_1[slice38],\r\n",
        "       train_regret_winner_2[slice38],\r\n",
        "       train_regret_winner_3[slice38],\r\n",
        "       train_regret_winner_4[slice38],\r\n",
        "       train_regret_winner_5[slice38],\r\n",
        "       train_regret_winner_6[slice38],\r\n",
        "       train_regret_winner_7[slice38],\r\n",
        "       train_regret_winner_8[slice38],\r\n",
        "       train_regret_winner_9[slice38],\r\n",
        "       train_regret_winner_10[slice38],\r\n",
        "       train_regret_winner_11[slice38],\r\n",
        "       train_regret_winner_12[slice38],\r\n",
        "       train_regret_winner_13[slice38],\r\n",
        "       train_regret_winner_14[slice38],\r\n",
        "       train_regret_winner_15[slice38],\r\n",
        "       train_regret_winner_16[slice38],\r\n",
        "       train_regret_winner_17[slice38],\r\n",
        "       train_regret_winner_18[slice38],\r\n",
        "       train_regret_winner_19[slice38],\r\n",
        "       train_regret_winner_20[slice38]]\r\n",
        "\r\n",
        "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\r\n",
        "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\r\n",
        "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\r\n",
        "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\r\n",
        "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\r\n",
        "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GekXOf_gtegw"
      },
      "source": [
        "# Iteration48 :\r\n",
        "\r\n",
        "slice48 = 47\r\n",
        "\r\n",
        "loser48 = [train_regret_loser_1[slice48],\r\n",
        "       train_regret_loser_2[slice48],\r\n",
        "       train_regret_loser_3[slice48],\r\n",
        "       train_regret_loser_4[slice48],\r\n",
        "       train_regret_loser_5[slice48],\r\n",
        "       train_regret_loser_6[slice48],\r\n",
        "       train_regret_loser_7[slice48],\r\n",
        "       train_regret_loser_8[slice48],\r\n",
        "       train_regret_loser_9[slice48],\r\n",
        "       train_regret_loser_10[slice48],\r\n",
        "       train_regret_loser_11[slice48],\r\n",
        "       train_regret_loser_12[slice48],\r\n",
        "       train_regret_loser_13[slice48],\r\n",
        "       train_regret_loser_14[slice48],\r\n",
        "       train_regret_loser_15[slice48],\r\n",
        "       train_regret_loser_16[slice48],\r\n",
        "       train_regret_loser_17[slice48],\r\n",
        "       train_regret_loser_18[slice48],\r\n",
        "       train_regret_loser_19[slice48],\r\n",
        "       train_regret_loser_20[slice48]]\r\n",
        "\r\n",
        "winner48 = [train_regret_winner_1[slice48],\r\n",
        "       train_regret_winner_2[slice48],\r\n",
        "       train_regret_winner_3[slice48],\r\n",
        "       train_regret_winner_4[slice48],\r\n",
        "       train_regret_winner_5[slice48],\r\n",
        "       train_regret_winner_6[slice48],\r\n",
        "       train_regret_winner_7[slice48],\r\n",
        "       train_regret_winner_8[slice48],\r\n",
        "       train_regret_winner_9[slice48],\r\n",
        "       train_regret_winner_10[slice48],\r\n",
        "       train_regret_winner_11[slice48],\r\n",
        "       train_regret_winner_12[slice48],\r\n",
        "       train_regret_winner_13[slice48],\r\n",
        "       train_regret_winner_14[slice48],\r\n",
        "       train_regret_winner_15[slice48],\r\n",
        "       train_regret_winner_16[slice48],\r\n",
        "       train_regret_winner_17[slice48],\r\n",
        "       train_regret_winner_18[slice48],\r\n",
        "       train_regret_winner_19[slice48],\r\n",
        "       train_regret_winner_20[slice48]]\r\n",
        "\r\n",
        "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\r\n",
        "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\r\n",
        "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\r\n",
        "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\r\n",
        "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\r\n",
        "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNpV6knOtejv"
      },
      "source": [
        "# Iteration58 :\r\n",
        "\r\n",
        "slice58 = 57\r\n",
        "\r\n",
        "loser58 = [train_regret_loser_1[slice58],\r\n",
        "       train_regret_loser_2[slice58],\r\n",
        "       train_regret_loser_3[slice58],\r\n",
        "       train_regret_loser_4[slice58],\r\n",
        "       train_regret_loser_5[slice58],\r\n",
        "       train_regret_loser_6[slice58],\r\n",
        "       train_regret_loser_7[slice58],\r\n",
        "       train_regret_loser_8[slice58],\r\n",
        "       train_regret_loser_9[slice58],\r\n",
        "       train_regret_loser_10[slice58],\r\n",
        "       train_regret_loser_11[slice58],\r\n",
        "       train_regret_loser_12[slice58],\r\n",
        "       train_regret_loser_13[slice58],\r\n",
        "       train_regret_loser_14[slice58],\r\n",
        "       train_regret_loser_15[slice58],\r\n",
        "       train_regret_loser_16[slice58],\r\n",
        "       train_regret_loser_17[slice58],\r\n",
        "       train_regret_loser_18[slice58],\r\n",
        "       train_regret_loser_19[slice58],\r\n",
        "       train_regret_loser_20[slice58]]\r\n",
        "\r\n",
        "winner58 = [train_regret_winner_1[slice58],\r\n",
        "       train_regret_winner_2[slice58],\r\n",
        "       train_regret_winner_3[slice58],\r\n",
        "       train_regret_winner_4[slice58],\r\n",
        "       train_regret_winner_5[slice58],\r\n",
        "       train_regret_winner_6[slice58],\r\n",
        "       train_regret_winner_7[slice58],\r\n",
        "       train_regret_winner_8[slice58],\r\n",
        "       train_regret_winner_9[slice58],\r\n",
        "       train_regret_winner_10[slice58],\r\n",
        "       train_regret_winner_11[slice58],\r\n",
        "       train_regret_winner_12[slice58],\r\n",
        "       train_regret_winner_13[slice58],\r\n",
        "       train_regret_winner_14[slice58],\r\n",
        "       train_regret_winner_15[slice58],\r\n",
        "       train_regret_winner_16[slice58],\r\n",
        "       train_regret_winner_17[slice58],\r\n",
        "       train_regret_winner_18[slice58],\r\n",
        "       train_regret_winner_19[slice58],\r\n",
        "       train_regret_winner_20[slice58]]\r\n",
        "\r\n",
        "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\r\n",
        "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\r\n",
        "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\r\n",
        "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\r\n",
        "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\r\n",
        "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VFsqaBqtemn"
      },
      "source": [
        "# Iteration68 :\r\n",
        "\r\n",
        "slice68 = 67\r\n",
        "\r\n",
        "loser68 = [train_regret_loser_1[slice68],\r\n",
        "       train_regret_loser_2[slice68],\r\n",
        "       train_regret_loser_3[slice68],\r\n",
        "       train_regret_loser_4[slice68],\r\n",
        "       train_regret_loser_5[slice68],\r\n",
        "       train_regret_loser_6[slice68],\r\n",
        "       train_regret_loser_7[slice68],\r\n",
        "       train_regret_loser_8[slice68],\r\n",
        "       train_regret_loser_9[slice68],\r\n",
        "       train_regret_loser_10[slice68],\r\n",
        "       train_regret_loser_11[slice68],\r\n",
        "       train_regret_loser_12[slice68],\r\n",
        "       train_regret_loser_13[slice68],\r\n",
        "       train_regret_loser_14[slice68],\r\n",
        "       train_regret_loser_15[slice68],\r\n",
        "       train_regret_loser_16[slice68],\r\n",
        "       train_regret_loser_17[slice68],\r\n",
        "       train_regret_loser_18[slice68],\r\n",
        "       train_regret_loser_19[slice68],\r\n",
        "       train_regret_loser_20[slice68]]\r\n",
        "\r\n",
        "winner68 = [train_regret_winner_1[slice68],\r\n",
        "       train_regret_winner_2[slice68],\r\n",
        "       train_regret_winner_3[slice68],\r\n",
        "       train_regret_winner_4[slice68],\r\n",
        "       train_regret_winner_5[slice68],\r\n",
        "       train_regret_winner_6[slice68],\r\n",
        "       train_regret_winner_7[slice68],\r\n",
        "       train_regret_winner_8[slice68],\r\n",
        "       train_regret_winner_9[slice68],\r\n",
        "       train_regret_winner_10[slice68],\r\n",
        "       train_regret_winner_11[slice68],\r\n",
        "       train_regret_winner_12[slice68],\r\n",
        "       train_regret_winner_13[slice68],\r\n",
        "       train_regret_winner_14[slice68],\r\n",
        "       train_regret_winner_15[slice68],\r\n",
        "       train_regret_winner_16[slice68],\r\n",
        "       train_regret_winner_17[slice68],\r\n",
        "       train_regret_winner_18[slice68],\r\n",
        "       train_regret_winner_19[slice68],\r\n",
        "       train_regret_winner_20[slice68]]\r\n",
        "\r\n",
        "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\r\n",
        "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\r\n",
        "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\r\n",
        "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\r\n",
        "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\r\n",
        "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XlFH0Wxtepo"
      },
      "source": [
        "# Iteration78 :\r\n",
        "\r\n",
        "slice78 = 77\r\n",
        "\r\n",
        "loser78 = [train_regret_loser_1[slice78],\r\n",
        "       train_regret_loser_2[slice78],\r\n",
        "       train_regret_loser_3[slice78],\r\n",
        "       train_regret_loser_4[slice78],\r\n",
        "       train_regret_loser_5[slice78],\r\n",
        "       train_regret_loser_6[slice78],\r\n",
        "       train_regret_loser_7[slice78],\r\n",
        "       train_regret_loser_8[slice78],\r\n",
        "       train_regret_loser_9[slice78],\r\n",
        "       train_regret_loser_10[slice78],\r\n",
        "       train_regret_loser_11[slice78],\r\n",
        "       train_regret_loser_12[slice78],\r\n",
        "       train_regret_loser_13[slice78],\r\n",
        "       train_regret_loser_14[slice78],\r\n",
        "       train_regret_loser_15[slice78],\r\n",
        "       train_regret_loser_16[slice78],\r\n",
        "       train_regret_loser_17[slice78],\r\n",
        "       train_regret_loser_18[slice78],\r\n",
        "       train_regret_loser_19[slice78],\r\n",
        "       train_regret_loser_20[slice78]]\r\n",
        "\r\n",
        "winner78 = [train_regret_winner_1[slice78],\r\n",
        "       train_regret_winner_2[slice78],\r\n",
        "       train_regret_winner_3[slice78],\r\n",
        "       train_regret_winner_4[slice78],\r\n",
        "       train_regret_winner_5[slice78],\r\n",
        "       train_regret_winner_6[slice78],\r\n",
        "       train_regret_winner_7[slice78],\r\n",
        "       train_regret_winner_8[slice78],\r\n",
        "       train_regret_winner_9[slice78],\r\n",
        "       train_regret_winner_10[slice78],\r\n",
        "       train_regret_winner_11[slice78],\r\n",
        "       train_regret_winner_12[slice78],\r\n",
        "       train_regret_winner_13[slice78],\r\n",
        "       train_regret_winner_14[slice78],\r\n",
        "       train_regret_winner_15[slice78],\r\n",
        "       train_regret_winner_16[slice78],\r\n",
        "       train_regret_winner_17[slice78],\r\n",
        "       train_regret_winner_18[slice78],\r\n",
        "       train_regret_winner_19[slice78],\r\n",
        "       train_regret_winner_20[slice78]]\r\n",
        "\r\n",
        "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\r\n",
        "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\r\n",
        "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\r\n",
        "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\r\n",
        "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\r\n",
        "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsL0PJVetesC"
      },
      "source": [
        "# Iteration88 :\r\n",
        "\r\n",
        "slice88 = 87\r\n",
        "\r\n",
        "loser88 = [train_regret_loser_1[slice88],\r\n",
        "       train_regret_loser_2[slice88],\r\n",
        "       train_regret_loser_3[slice88],\r\n",
        "       train_regret_loser_4[slice88],\r\n",
        "       train_regret_loser_5[slice88],\r\n",
        "       train_regret_loser_6[slice88],\r\n",
        "       train_regret_loser_7[slice88],\r\n",
        "       train_regret_loser_8[slice88],\r\n",
        "       train_regret_loser_9[slice88],\r\n",
        "       train_regret_loser_10[slice88],\r\n",
        "       train_regret_loser_11[slice88],\r\n",
        "       train_regret_loser_12[slice88],\r\n",
        "       train_regret_loser_13[slice88],\r\n",
        "       train_regret_loser_14[slice88],\r\n",
        "       train_regret_loser_15[slice88],\r\n",
        "       train_regret_loser_16[slice88],\r\n",
        "       train_regret_loser_17[slice88],\r\n",
        "       train_regret_loser_18[slice88],\r\n",
        "       train_regret_loser_19[slice88],\r\n",
        "       train_regret_loser_20[slice88]]\r\n",
        "\r\n",
        "winner88 = [train_regret_winner_1[slice88],\r\n",
        "       train_regret_winner_2[slice88],\r\n",
        "       train_regret_winner_3[slice88],\r\n",
        "       train_regret_winner_4[slice88],\r\n",
        "       train_regret_winner_5[slice88],\r\n",
        "       train_regret_winner_6[slice88],\r\n",
        "       train_regret_winner_7[slice88],\r\n",
        "       train_regret_winner_8[slice88],\r\n",
        "       train_regret_winner_9[slice88],\r\n",
        "       train_regret_winner_10[slice88],\r\n",
        "       train_regret_winner_11[slice88],\r\n",
        "       train_regret_winner_12[slice88],\r\n",
        "       train_regret_winner_13[slice88],\r\n",
        "       train_regret_winner_14[slice88],\r\n",
        "       train_regret_winner_15[slice88],\r\n",
        "       train_regret_winner_16[slice88],\r\n",
        "       train_regret_winner_17[slice88],\r\n",
        "       train_regret_winner_18[slice88],\r\n",
        "       train_regret_winner_19[slice88],\r\n",
        "       train_regret_winner_20[slice88]]\r\n",
        "\r\n",
        "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\r\n",
        "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\r\n",
        "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\r\n",
        "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\r\n",
        "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\r\n",
        "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYWEWGku1o_9"
      },
      "source": [
        "# Iteration98 :\r\n",
        "\r\n",
        "slice98 = 97\r\n",
        "\r\n",
        "loser98 = [train_regret_loser_1[slice98],\r\n",
        "       train_regret_loser_2[slice98],\r\n",
        "       train_regret_loser_3[slice98],\r\n",
        "       train_regret_loser_4[slice98],\r\n",
        "       train_regret_loser_5[slice98],\r\n",
        "       train_regret_loser_6[slice98],\r\n",
        "       train_regret_loser_7[slice98],\r\n",
        "       train_regret_loser_8[slice98],\r\n",
        "       train_regret_loser_9[slice98],\r\n",
        "       train_regret_loser_10[slice98],\r\n",
        "       train_regret_loser_11[slice98],\r\n",
        "       train_regret_loser_12[slice98],\r\n",
        "       train_regret_loser_13[slice98],\r\n",
        "       train_regret_loser_14[slice98],\r\n",
        "       train_regret_loser_15[slice98],\r\n",
        "       train_regret_loser_16[slice98],\r\n",
        "       train_regret_loser_17[slice98],\r\n",
        "       train_regret_loser_18[slice98],\r\n",
        "       train_regret_loser_19[slice98],\r\n",
        "       train_regret_loser_20[slice98]]\r\n",
        "\r\n",
        "winner98 = [train_regret_winner_1[slice98],\r\n",
        "       train_regret_winner_2[slice98],\r\n",
        "       train_regret_winner_3[slice98],\r\n",
        "       train_regret_winner_4[slice98],\r\n",
        "       train_regret_winner_5[slice98],\r\n",
        "       train_regret_winner_6[slice98],\r\n",
        "       train_regret_winner_7[slice98],\r\n",
        "       train_regret_winner_8[slice98],\r\n",
        "       train_regret_winner_9[slice98],\r\n",
        "       train_regret_winner_10[slice98],\r\n",
        "       train_regret_winner_11[slice98],\r\n",
        "       train_regret_winner_12[slice98],\r\n",
        "       train_regret_winner_13[slice98],\r\n",
        "       train_regret_winner_14[slice98],\r\n",
        "       train_regret_winner_15[slice98],\r\n",
        "       train_regret_winner_16[slice98],\r\n",
        "       train_regret_winner_17[slice98],\r\n",
        "       train_regret_winner_18[slice98],\r\n",
        "       train_regret_winner_19[slice98],\r\n",
        "       train_regret_winner_20[slice98]]\r\n",
        "\r\n",
        "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\r\n",
        "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\r\n",
        "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\r\n",
        "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\r\n",
        "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\r\n",
        "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guLGgcX91pDK"
      },
      "source": [
        "# Iteration9 :\r\n",
        "\r\n",
        "slice9 = 8\r\n",
        "\r\n",
        "loser9 = [train_regret_loser_1[slice9],\r\n",
        "       train_regret_loser_2[slice9],\r\n",
        "       train_regret_loser_3[slice9],\r\n",
        "       train_regret_loser_4[slice9],\r\n",
        "       train_regret_loser_5[slice9],\r\n",
        "       train_regret_loser_6[slice9],\r\n",
        "       train_regret_loser_7[slice9],\r\n",
        "       train_regret_loser_8[slice9],\r\n",
        "       train_regret_loser_9[slice9],\r\n",
        "       train_regret_loser_10[slice9],\r\n",
        "       train_regret_loser_11[slice9],\r\n",
        "       train_regret_loser_12[slice9],\r\n",
        "       train_regret_loser_13[slice9],\r\n",
        "       train_regret_loser_14[slice9],\r\n",
        "       train_regret_loser_15[slice9],\r\n",
        "       train_regret_loser_16[slice9],\r\n",
        "       train_regret_loser_17[slice9],\r\n",
        "       train_regret_loser_18[slice9],\r\n",
        "       train_regret_loser_19[slice9],\r\n",
        "       train_regret_loser_20[slice9]]\r\n",
        "\r\n",
        "winner9 = [train_regret_winner_1[slice9],\r\n",
        "       train_regret_winner_2[slice9],\r\n",
        "       train_regret_winner_3[slice9],\r\n",
        "       train_regret_winner_4[slice9],\r\n",
        "       train_regret_winner_5[slice9],\r\n",
        "       train_regret_winner_6[slice9],\r\n",
        "       train_regret_winner_7[slice9],\r\n",
        "       train_regret_winner_8[slice9],\r\n",
        "       train_regret_winner_9[slice9],\r\n",
        "       train_regret_winner_10[slice9],\r\n",
        "       train_regret_winner_11[slice9],\r\n",
        "       train_regret_winner_12[slice9],\r\n",
        "       train_regret_winner_13[slice9],\r\n",
        "       train_regret_winner_14[slice9],\r\n",
        "       train_regret_winner_15[slice9],\r\n",
        "       train_regret_winner_16[slice9],\r\n",
        "       train_regret_winner_17[slice9],\r\n",
        "       train_regret_winner_18[slice9],\r\n",
        "       train_regret_winner_19[slice9],\r\n",
        "       train_regret_winner_20[slice9]]\r\n",
        "\r\n",
        "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\r\n",
        "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\r\n",
        "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\r\n",
        "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\r\n",
        "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\r\n",
        "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3_9-eoT1pGd"
      },
      "source": [
        "# Iteration19 :\r\n",
        "\r\n",
        "slice19 = 18\r\n",
        "\r\n",
        "loser19 = [train_regret_loser_1[slice19],\r\n",
        "       train_regret_loser_2[slice19],\r\n",
        "       train_regret_loser_3[slice19],\r\n",
        "       train_regret_loser_4[slice19],\r\n",
        "       train_regret_loser_5[slice19],\r\n",
        "       train_regret_loser_6[slice19],\r\n",
        "       train_regret_loser_7[slice19],\r\n",
        "       train_regret_loser_8[slice19],\r\n",
        "       train_regret_loser_9[slice19],\r\n",
        "       train_regret_loser_10[slice19],\r\n",
        "       train_regret_loser_11[slice19],\r\n",
        "       train_regret_loser_12[slice19],\r\n",
        "       train_regret_loser_13[slice19],\r\n",
        "       train_regret_loser_14[slice19],\r\n",
        "       train_regret_loser_15[slice19],\r\n",
        "       train_regret_loser_16[slice19],\r\n",
        "       train_regret_loser_17[slice19],\r\n",
        "       train_regret_loser_18[slice19],\r\n",
        "       train_regret_loser_19[slice19],\r\n",
        "       train_regret_loser_20[slice19]]\r\n",
        "\r\n",
        "winner19 = [train_regret_winner_1[slice19],\r\n",
        "       train_regret_winner_2[slice19],\r\n",
        "       train_regret_winner_3[slice19],\r\n",
        "       train_regret_winner_4[slice19],\r\n",
        "       train_regret_winner_5[slice19],\r\n",
        "       train_regret_winner_6[slice19],\r\n",
        "       train_regret_winner_7[slice19],\r\n",
        "       train_regret_winner_8[slice19],\r\n",
        "       train_regret_winner_9[slice19],\r\n",
        "       train_regret_winner_10[slice19],\r\n",
        "       train_regret_winner_11[slice19],\r\n",
        "       train_regret_winner_12[slice19],\r\n",
        "       train_regret_winner_13[slice19],\r\n",
        "       train_regret_winner_14[slice19],\r\n",
        "       train_regret_winner_15[slice19],\r\n",
        "       train_regret_winner_16[slice19],\r\n",
        "       train_regret_winner_17[slice19],\r\n",
        "       train_regret_winner_18[slice19],\r\n",
        "       train_regret_winner_19[slice19],\r\n",
        "       train_regret_winner_20[slice19]]\r\n",
        "\r\n",
        "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\r\n",
        "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\r\n",
        "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\r\n",
        "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\r\n",
        "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\r\n",
        "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ae5x13M1pRp"
      },
      "source": [
        "# Iteration29 :\r\n",
        "\r\n",
        "slice29 = 28\r\n",
        "\r\n",
        "loser29 = [train_regret_loser_1[slice29],\r\n",
        "       train_regret_loser_2[slice29],\r\n",
        "       train_regret_loser_3[slice29],\r\n",
        "       train_regret_loser_4[slice29],\r\n",
        "       train_regret_loser_5[slice29],\r\n",
        "       train_regret_loser_6[slice29],\r\n",
        "       train_regret_loser_7[slice29],\r\n",
        "       train_regret_loser_8[slice29],\r\n",
        "       train_regret_loser_9[slice29],\r\n",
        "       train_regret_loser_10[slice29],\r\n",
        "       train_regret_loser_11[slice29],\r\n",
        "       train_regret_loser_12[slice29],\r\n",
        "       train_regret_loser_13[slice29],\r\n",
        "       train_regret_loser_14[slice29],\r\n",
        "       train_regret_loser_15[slice29],\r\n",
        "       train_regret_loser_16[slice29],\r\n",
        "       train_regret_loser_17[slice29],\r\n",
        "       train_regret_loser_18[slice29],\r\n",
        "       train_regret_loser_19[slice29],\r\n",
        "       train_regret_loser_20[slice29]]\r\n",
        "\r\n",
        "winner29 = [train_regret_winner_1[slice29],\r\n",
        "       train_regret_winner_2[slice29],\r\n",
        "       train_regret_winner_3[slice29],\r\n",
        "       train_regret_winner_4[slice29],\r\n",
        "       train_regret_winner_5[slice29],\r\n",
        "       train_regret_winner_6[slice29],\r\n",
        "       train_regret_winner_7[slice29],\r\n",
        "       train_regret_winner_8[slice29],\r\n",
        "       train_regret_winner_9[slice29],\r\n",
        "       train_regret_winner_10[slice29],\r\n",
        "       train_regret_winner_11[slice29],\r\n",
        "       train_regret_winner_12[slice29],\r\n",
        "       train_regret_winner_13[slice29],\r\n",
        "       train_regret_winner_14[slice29],\r\n",
        "       train_regret_winner_15[slice29],\r\n",
        "       train_regret_winner_16[slice29],\r\n",
        "       train_regret_winner_17[slice29],\r\n",
        "       train_regret_winner_18[slice29],\r\n",
        "       train_regret_winner_19[slice29],\r\n",
        "       train_regret_winner_20[slice29]]\r\n",
        "\r\n",
        "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\r\n",
        "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\r\n",
        "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\r\n",
        "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\r\n",
        "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\r\n",
        "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR9aPvBy1pUc"
      },
      "source": [
        "# Iteration39 :\r\n",
        "\r\n",
        "slice39 = 38\r\n",
        "\r\n",
        "loser39 = [train_regret_loser_1[slice39],\r\n",
        "       train_regret_loser_2[slice39],\r\n",
        "       train_regret_loser_3[slice39],\r\n",
        "       train_regret_loser_4[slice39],\r\n",
        "       train_regret_loser_5[slice39],\r\n",
        "       train_regret_loser_6[slice39],\r\n",
        "       train_regret_loser_7[slice39],\r\n",
        "       train_regret_loser_8[slice39],\r\n",
        "       train_regret_loser_9[slice39],\r\n",
        "       train_regret_loser_10[slice39],\r\n",
        "       train_regret_loser_11[slice39],\r\n",
        "       train_regret_loser_12[slice39],\r\n",
        "       train_regret_loser_13[slice39],\r\n",
        "       train_regret_loser_14[slice39],\r\n",
        "       train_regret_loser_15[slice39],\r\n",
        "       train_regret_loser_16[slice39],\r\n",
        "       train_regret_loser_17[slice39],\r\n",
        "       train_regret_loser_18[slice39],\r\n",
        "       train_regret_loser_19[slice39],\r\n",
        "       train_regret_loser_20[slice39]]\r\n",
        "\r\n",
        "winner39 = [train_regret_winner_1[slice39],\r\n",
        "       train_regret_winner_2[slice39],\r\n",
        "       train_regret_winner_3[slice39],\r\n",
        "       train_regret_winner_4[slice39],\r\n",
        "       train_regret_winner_5[slice39],\r\n",
        "       train_regret_winner_6[slice39],\r\n",
        "       train_regret_winner_7[slice39],\r\n",
        "       train_regret_winner_8[slice39],\r\n",
        "       train_regret_winner_9[slice39],\r\n",
        "       train_regret_winner_10[slice39],\r\n",
        "       train_regret_winner_11[slice39],\r\n",
        "       train_regret_winner_12[slice39],\r\n",
        "       train_regret_winner_13[slice39],\r\n",
        "       train_regret_winner_14[slice39],\r\n",
        "       train_regret_winner_15[slice39],\r\n",
        "       train_regret_winner_16[slice39],\r\n",
        "       train_regret_winner_17[slice39],\r\n",
        "       train_regret_winner_18[slice39],\r\n",
        "       train_regret_winner_19[slice39],\r\n",
        "       train_regret_winner_20[slice39]]\r\n",
        "\r\n",
        "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\r\n",
        "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\r\n",
        "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\r\n",
        "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\r\n",
        "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\r\n",
        "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vb7vh2K1pXp"
      },
      "source": [
        "# Iteration49 :\r\n",
        "\r\n",
        "slice49 = 48\r\n",
        "\r\n",
        "loser49 = [train_regret_loser_1[slice49],\r\n",
        "       train_regret_loser_2[slice49],\r\n",
        "       train_regret_loser_3[slice49],\r\n",
        "       train_regret_loser_4[slice49],\r\n",
        "       train_regret_loser_5[slice49],\r\n",
        "       train_regret_loser_6[slice49],\r\n",
        "       train_regret_loser_7[slice49],\r\n",
        "       train_regret_loser_8[slice49],\r\n",
        "       train_regret_loser_9[slice49],\r\n",
        "       train_regret_loser_10[slice49],\r\n",
        "       train_regret_loser_11[slice49],\r\n",
        "       train_regret_loser_12[slice49],\r\n",
        "       train_regret_loser_13[slice49],\r\n",
        "       train_regret_loser_14[slice49],\r\n",
        "       train_regret_loser_15[slice49],\r\n",
        "       train_regret_loser_16[slice49],\r\n",
        "       train_regret_loser_17[slice49],\r\n",
        "       train_regret_loser_18[slice49],\r\n",
        "       train_regret_loser_19[slice49],\r\n",
        "       train_regret_loser_20[slice49]]\r\n",
        "\r\n",
        "winner49 = [train_regret_winner_1[slice49],\r\n",
        "       train_regret_winner_2[slice49],\r\n",
        "       train_regret_winner_3[slice49],\r\n",
        "       train_regret_winner_4[slice49],\r\n",
        "       train_regret_winner_5[slice49],\r\n",
        "       train_regret_winner_6[slice49],\r\n",
        "       train_regret_winner_7[slice49],\r\n",
        "       train_regret_winner_8[slice49],\r\n",
        "       train_regret_winner_9[slice49],\r\n",
        "       train_regret_winner_10[slice49],\r\n",
        "       train_regret_winner_11[slice49],\r\n",
        "       train_regret_winner_12[slice49],\r\n",
        "       train_regret_winner_13[slice49],\r\n",
        "       train_regret_winner_14[slice49],\r\n",
        "       train_regret_winner_15[slice49],\r\n",
        "       train_regret_winner_16[slice49],\r\n",
        "       train_regret_winner_17[slice49],\r\n",
        "       train_regret_winner_18[slice49],\r\n",
        "       train_regret_winner_19[slice49],\r\n",
        "       train_regret_winner_20[slice49]]\r\n",
        "\r\n",
        "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\r\n",
        "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\r\n",
        "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\r\n",
        "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\r\n",
        "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\r\n",
        "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wN7DWwt1paM"
      },
      "source": [
        "# Iteration59 :\r\n",
        "\r\n",
        "slice59 = 58\r\n",
        "\r\n",
        "loser59 = [train_regret_loser_1[slice59],\r\n",
        "       train_regret_loser_2[slice59],\r\n",
        "       train_regret_loser_3[slice59],\r\n",
        "       train_regret_loser_4[slice59],\r\n",
        "       train_regret_loser_5[slice59],\r\n",
        "       train_regret_loser_6[slice59],\r\n",
        "       train_regret_loser_7[slice59],\r\n",
        "       train_regret_loser_8[slice59],\r\n",
        "       train_regret_loser_9[slice59],\r\n",
        "       train_regret_loser_10[slice59],\r\n",
        "       train_regret_loser_11[slice59],\r\n",
        "       train_regret_loser_12[slice59],\r\n",
        "       train_regret_loser_13[slice59],\r\n",
        "       train_regret_loser_14[slice59],\r\n",
        "       train_regret_loser_15[slice59],\r\n",
        "       train_regret_loser_16[slice59],\r\n",
        "       train_regret_loser_17[slice59],\r\n",
        "       train_regret_loser_18[slice59],\r\n",
        "       train_regret_loser_19[slice59],\r\n",
        "       train_regret_loser_20[slice59]]\r\n",
        "\r\n",
        "winner59 = [train_regret_winner_1[slice59],\r\n",
        "       train_regret_winner_2[slice59],\r\n",
        "       train_regret_winner_3[slice59],\r\n",
        "       train_regret_winner_4[slice59],\r\n",
        "       train_regret_winner_5[slice59],\r\n",
        "       train_regret_winner_6[slice59],\r\n",
        "       train_regret_winner_7[slice59],\r\n",
        "       train_regret_winner_8[slice59],\r\n",
        "       train_regret_winner_9[slice59],\r\n",
        "       train_regret_winner_10[slice59],\r\n",
        "       train_regret_winner_11[slice59],\r\n",
        "       train_regret_winner_12[slice59],\r\n",
        "       train_regret_winner_13[slice59],\r\n",
        "       train_regret_winner_14[slice59],\r\n",
        "       train_regret_winner_15[slice59],\r\n",
        "       train_regret_winner_16[slice59],\r\n",
        "       train_regret_winner_17[slice59],\r\n",
        "       train_regret_winner_18[slice59],\r\n",
        "       train_regret_winner_19[slice59],\r\n",
        "       train_regret_winner_20[slice59]]\r\n",
        "\r\n",
        "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\r\n",
        "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\r\n",
        "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\r\n",
        "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\r\n",
        "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\r\n",
        "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqtgekml1pde"
      },
      "source": [
        "# Iteration69 :\r\n",
        "\r\n",
        "slice69 = 68\r\n",
        "\r\n",
        "loser69 = [train_regret_loser_1[slice69],\r\n",
        "       train_regret_loser_2[slice69],\r\n",
        "       train_regret_loser_3[slice69],\r\n",
        "       train_regret_loser_4[slice69],\r\n",
        "       train_regret_loser_5[slice69],\r\n",
        "       train_regret_loser_6[slice69],\r\n",
        "       train_regret_loser_7[slice69],\r\n",
        "       train_regret_loser_8[slice69],\r\n",
        "       train_regret_loser_9[slice69],\r\n",
        "       train_regret_loser_10[slice69],\r\n",
        "       train_regret_loser_11[slice69],\r\n",
        "       train_regret_loser_12[slice69],\r\n",
        "       train_regret_loser_13[slice69],\r\n",
        "       train_regret_loser_14[slice69],\r\n",
        "       train_regret_loser_15[slice69],\r\n",
        "       train_regret_loser_16[slice69],\r\n",
        "       train_regret_loser_17[slice69],\r\n",
        "       train_regret_loser_18[slice69],\r\n",
        "       train_regret_loser_19[slice69],\r\n",
        "       train_regret_loser_20[slice69]]\r\n",
        "\r\n",
        "winner69 = [train_regret_winner_1[slice69],\r\n",
        "       train_regret_winner_2[slice69],\r\n",
        "       train_regret_winner_3[slice69],\r\n",
        "       train_regret_winner_4[slice69],\r\n",
        "       train_regret_winner_5[slice69],\r\n",
        "       train_regret_winner_6[slice69],\r\n",
        "       train_regret_winner_7[slice69],\r\n",
        "       train_regret_winner_8[slice69],\r\n",
        "       train_regret_winner_9[slice69],\r\n",
        "       train_regret_winner_10[slice69],\r\n",
        "       train_regret_winner_11[slice69],\r\n",
        "       train_regret_winner_12[slice69],\r\n",
        "       train_regret_winner_13[slice69],\r\n",
        "       train_regret_winner_14[slice69],\r\n",
        "       train_regret_winner_15[slice69],\r\n",
        "       train_regret_winner_16[slice69],\r\n",
        "       train_regret_winner_17[slice69],\r\n",
        "       train_regret_winner_18[slice69],\r\n",
        "       train_regret_winner_19[slice69],\r\n",
        "       train_regret_winner_20[slice69]]\r\n",
        "\r\n",
        "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\r\n",
        "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\r\n",
        "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\r\n",
        "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\r\n",
        "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\r\n",
        "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPFTa1171pgF"
      },
      "source": [
        "# Iteration79 :\r\n",
        "\r\n",
        "slice79 = 78\r\n",
        "\r\n",
        "loser79 = [train_regret_loser_1[slice79],\r\n",
        "       train_regret_loser_2[slice79],\r\n",
        "       train_regret_loser_3[slice79],\r\n",
        "       train_regret_loser_4[slice79],\r\n",
        "       train_regret_loser_5[slice79],\r\n",
        "       train_regret_loser_6[slice79],\r\n",
        "       train_regret_loser_7[slice79],\r\n",
        "       train_regret_loser_8[slice79],\r\n",
        "       train_regret_loser_9[slice79],\r\n",
        "       train_regret_loser_10[slice79],\r\n",
        "       train_regret_loser_11[slice79],\r\n",
        "       train_regret_loser_12[slice79],\r\n",
        "       train_regret_loser_13[slice79],\r\n",
        "       train_regret_loser_14[slice79],\r\n",
        "       train_regret_loser_15[slice79],\r\n",
        "       train_regret_loser_16[slice79],\r\n",
        "       train_regret_loser_17[slice79],\r\n",
        "       train_regret_loser_18[slice79],\r\n",
        "       train_regret_loser_19[slice79],\r\n",
        "       train_regret_loser_20[slice79]]\r\n",
        "\r\n",
        "winner79 = [train_regret_winner_1[slice79],\r\n",
        "       train_regret_winner_2[slice79],\r\n",
        "       train_regret_winner_3[slice79],\r\n",
        "       train_regret_winner_4[slice79],\r\n",
        "       train_regret_winner_5[slice79],\r\n",
        "       train_regret_winner_6[slice79],\r\n",
        "       train_regret_winner_7[slice79],\r\n",
        "       train_regret_winner_8[slice79],\r\n",
        "       train_regret_winner_9[slice79],\r\n",
        "       train_regret_winner_10[slice79],\r\n",
        "       train_regret_winner_11[slice79],\r\n",
        "       train_regret_winner_12[slice79],\r\n",
        "       train_regret_winner_13[slice79],\r\n",
        "       train_regret_winner_14[slice79],\r\n",
        "       train_regret_winner_15[slice79],\r\n",
        "       train_regret_winner_16[slice79],\r\n",
        "       train_regret_winner_17[slice79],\r\n",
        "       train_regret_winner_18[slice79],\r\n",
        "       train_regret_winner_19[slice79],\r\n",
        "       train_regret_winner_20[slice79]]\r\n",
        "\r\n",
        "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\r\n",
        "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\r\n",
        "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\r\n",
        "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\r\n",
        "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\r\n",
        "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTnRF4lx1pio"
      },
      "source": [
        "# Iteration89 :\r\n",
        "\r\n",
        "slice89 = 88\r\n",
        "\r\n",
        "loser89 = [train_regret_loser_1[slice89],\r\n",
        "       train_regret_loser_2[slice89],\r\n",
        "       train_regret_loser_3[slice89],\r\n",
        "       train_regret_loser_4[slice89],\r\n",
        "       train_regret_loser_5[slice89],\r\n",
        "       train_regret_loser_6[slice89],\r\n",
        "       train_regret_loser_7[slice89],\r\n",
        "       train_regret_loser_8[slice89],\r\n",
        "       train_regret_loser_9[slice89],\r\n",
        "       train_regret_loser_10[slice89],\r\n",
        "       train_regret_loser_11[slice89],\r\n",
        "       train_regret_loser_12[slice89],\r\n",
        "       train_regret_loser_13[slice89],\r\n",
        "       train_regret_loser_14[slice89],\r\n",
        "       train_regret_loser_15[slice89],\r\n",
        "       train_regret_loser_16[slice89],\r\n",
        "       train_regret_loser_17[slice89],\r\n",
        "       train_regret_loser_18[slice89],\r\n",
        "       train_regret_loser_19[slice89],\r\n",
        "       train_regret_loser_20[slice89]]\r\n",
        "\r\n",
        "winner89 = [train_regret_winner_1[slice89],\r\n",
        "       train_regret_winner_2[slice89],\r\n",
        "       train_regret_winner_3[slice89],\r\n",
        "       train_regret_winner_4[slice89],\r\n",
        "       train_regret_winner_5[slice89],\r\n",
        "       train_regret_winner_6[slice89],\r\n",
        "       train_regret_winner_7[slice89],\r\n",
        "       train_regret_winner_8[slice89],\r\n",
        "       train_regret_winner_9[slice89],\r\n",
        "       train_regret_winner_10[slice89],\r\n",
        "       train_regret_winner_11[slice89],\r\n",
        "       train_regret_winner_12[slice89],\r\n",
        "       train_regret_winner_13[slice89],\r\n",
        "       train_regret_winner_14[slice89],\r\n",
        "       train_regret_winner_15[slice89],\r\n",
        "       train_regret_winner_16[slice89],\r\n",
        "       train_regret_winner_17[slice89],\r\n",
        "       train_regret_winner_18[slice89],\r\n",
        "       train_regret_winner_19[slice89],\r\n",
        "       train_regret_winner_20[slice89]]\r\n",
        "\r\n",
        "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\r\n",
        "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\r\n",
        "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\r\n",
        "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\r\n",
        "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\r\n",
        "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiLAF38i1plO"
      },
      "source": [
        "# Iteration99 :\r\n",
        "\r\n",
        "slice99 = 98\r\n",
        "\r\n",
        "loser99 = [train_regret_loser_1[slice99],\r\n",
        "       train_regret_loser_2[slice99],\r\n",
        "       train_regret_loser_3[slice99],\r\n",
        "       train_regret_loser_4[slice99],\r\n",
        "       train_regret_loser_5[slice99],\r\n",
        "       train_regret_loser_6[slice99],\r\n",
        "       train_regret_loser_7[slice99],\r\n",
        "       train_regret_loser_8[slice99],\r\n",
        "       train_regret_loser_9[slice99],\r\n",
        "       train_regret_loser_10[slice99],\r\n",
        "       train_regret_loser_11[slice99],\r\n",
        "       train_regret_loser_12[slice99],\r\n",
        "       train_regret_loser_13[slice99],\r\n",
        "       train_regret_loser_14[slice99],\r\n",
        "       train_regret_loser_15[slice99],\r\n",
        "       train_regret_loser_16[slice99],\r\n",
        "       train_regret_loser_17[slice99],\r\n",
        "       train_regret_loser_18[slice99],\r\n",
        "       train_regret_loser_19[slice99],\r\n",
        "       train_regret_loser_20[slice99]]\r\n",
        "\r\n",
        "winner99 = [train_regret_winner_1[slice99],\r\n",
        "       train_regret_winner_2[slice99],\r\n",
        "       train_regret_winner_3[slice99],\r\n",
        "       train_regret_winner_4[slice99],\r\n",
        "       train_regret_winner_5[slice99],\r\n",
        "       train_regret_winner_6[slice99],\r\n",
        "       train_regret_winner_7[slice99],\r\n",
        "       train_regret_winner_8[slice99],\r\n",
        "       train_regret_winner_9[slice99],\r\n",
        "       train_regret_winner_10[slice99],\r\n",
        "       train_regret_winner_11[slice99],\r\n",
        "       train_regret_winner_12[slice99],\r\n",
        "       train_regret_winner_13[slice99],\r\n",
        "       train_regret_winner_14[slice99],\r\n",
        "       train_regret_winner_15[slice99],\r\n",
        "       train_regret_winner_16[slice99],\r\n",
        "       train_regret_winner_17[slice99],\r\n",
        "       train_regret_winner_18[slice99],\r\n",
        "       train_regret_winner_19[slice99],\r\n",
        "       train_regret_winner_20[slice99]]\r\n",
        "\r\n",
        "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\r\n",
        "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\r\n",
        "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\r\n",
        "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\r\n",
        "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\r\n",
        "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBd_N0F21pn0"
      },
      "source": [
        "# Iteration10 :\r\n",
        "\r\n",
        "slice10 = 9\r\n",
        "\r\n",
        "loser10 = [train_regret_loser_1[slice10],\r\n",
        "       train_regret_loser_2[slice10],\r\n",
        "       train_regret_loser_3[slice10],\r\n",
        "       train_regret_loser_4[slice10],\r\n",
        "       train_regret_loser_5[slice10],\r\n",
        "       train_regret_loser_6[slice10],\r\n",
        "       train_regret_loser_7[slice10],\r\n",
        "       train_regret_loser_8[slice10],\r\n",
        "       train_regret_loser_9[slice10],\r\n",
        "       train_regret_loser_10[slice10],\r\n",
        "       train_regret_loser_11[slice10],\r\n",
        "       train_regret_loser_12[slice10],\r\n",
        "       train_regret_loser_13[slice10],\r\n",
        "       train_regret_loser_14[slice10],\r\n",
        "       train_regret_loser_15[slice10],\r\n",
        "       train_regret_loser_16[slice10],\r\n",
        "       train_regret_loser_17[slice10],\r\n",
        "       train_regret_loser_18[slice10],\r\n",
        "       train_regret_loser_19[slice10],\r\n",
        "       train_regret_loser_20[slice10]]\r\n",
        "\r\n",
        "winner10 = [train_regret_winner_1[slice10],\r\n",
        "       train_regret_winner_2[slice10],\r\n",
        "       train_regret_winner_3[slice10],\r\n",
        "       train_regret_winner_4[slice10],\r\n",
        "       train_regret_winner_5[slice10],\r\n",
        "       train_regret_winner_6[slice10],\r\n",
        "       train_regret_winner_7[slice10],\r\n",
        "       train_regret_winner_8[slice10],\r\n",
        "       train_regret_winner_9[slice10],\r\n",
        "       train_regret_winner_10[slice10],\r\n",
        "       train_regret_winner_11[slice10],\r\n",
        "       train_regret_winner_12[slice10],\r\n",
        "       train_regret_winner_13[slice10],\r\n",
        "       train_regret_winner_14[slice10],\r\n",
        "       train_regret_winner_15[slice10],\r\n",
        "       train_regret_winner_16[slice10],\r\n",
        "       train_regret_winner_17[slice10],\r\n",
        "       train_regret_winner_18[slice10],\r\n",
        "       train_regret_winner_19[slice10],\r\n",
        "       train_regret_winner_20[slice10]]\r\n",
        "\r\n",
        "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\r\n",
        "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\r\n",
        "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\r\n",
        "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\r\n",
        "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\r\n",
        "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXjdqc4s1pqb"
      },
      "source": [
        "# Iteration20 :\r\n",
        "\r\n",
        "slice20 = 19\r\n",
        "\r\n",
        "loser20 = [train_regret_loser_1[slice20],\r\n",
        "       train_regret_loser_2[slice20],\r\n",
        "       train_regret_loser_3[slice20],\r\n",
        "       train_regret_loser_4[slice20],\r\n",
        "       train_regret_loser_5[slice20],\r\n",
        "       train_regret_loser_6[slice20],\r\n",
        "       train_regret_loser_7[slice20],\r\n",
        "       train_regret_loser_8[slice20],\r\n",
        "       train_regret_loser_9[slice20],\r\n",
        "       train_regret_loser_10[slice20],\r\n",
        "       train_regret_loser_11[slice20],\r\n",
        "       train_regret_loser_12[slice20],\r\n",
        "       train_regret_loser_13[slice20],\r\n",
        "       train_regret_loser_14[slice20],\r\n",
        "       train_regret_loser_15[slice20],\r\n",
        "       train_regret_loser_16[slice20],\r\n",
        "       train_regret_loser_17[slice20],\r\n",
        "       train_regret_loser_18[slice20],\r\n",
        "       train_regret_loser_19[slice20],\r\n",
        "       train_regret_loser_20[slice20]]\r\n",
        "\r\n",
        "winner20 = [train_regret_winner_1[slice20],\r\n",
        "       train_regret_winner_2[slice20],\r\n",
        "       train_regret_winner_3[slice20],\r\n",
        "       train_regret_winner_4[slice20],\r\n",
        "       train_regret_winner_5[slice20],\r\n",
        "       train_regret_winner_6[slice20],\r\n",
        "       train_regret_winner_7[slice20],\r\n",
        "       train_regret_winner_8[slice20],\r\n",
        "       train_regret_winner_9[slice20],\r\n",
        "       train_regret_winner_10[slice20],\r\n",
        "       train_regret_winner_11[slice20],\r\n",
        "       train_regret_winner_12[slice20],\r\n",
        "       train_regret_winner_13[slice20],\r\n",
        "       train_regret_winner_14[slice20],\r\n",
        "       train_regret_winner_15[slice20],\r\n",
        "       train_regret_winner_16[slice20],\r\n",
        "       train_regret_winner_17[slice20],\r\n",
        "       train_regret_winner_18[slice20],\r\n",
        "       train_regret_winner_19[slice20],\r\n",
        "       train_regret_winner_20[slice20]]\r\n",
        "\r\n",
        "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\r\n",
        "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\r\n",
        "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\r\n",
        "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\r\n",
        "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\r\n",
        "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQBVS6dx1ptB"
      },
      "source": [
        "# Iteration30 :\r\n",
        "\r\n",
        "slice30 = 29\r\n",
        "\r\n",
        "loser30 = [train_regret_loser_1[slice30],\r\n",
        "       train_regret_loser_2[slice30],\r\n",
        "       train_regret_loser_3[slice30],\r\n",
        "       train_regret_loser_4[slice30],\r\n",
        "       train_regret_loser_5[slice30],\r\n",
        "       train_regret_loser_6[slice30],\r\n",
        "       train_regret_loser_7[slice30],\r\n",
        "       train_regret_loser_8[slice30],\r\n",
        "       train_regret_loser_9[slice30],\r\n",
        "       train_regret_loser_10[slice30],\r\n",
        "       train_regret_loser_11[slice30],\r\n",
        "       train_regret_loser_12[slice30],\r\n",
        "       train_regret_loser_13[slice30],\r\n",
        "       train_regret_loser_14[slice30],\r\n",
        "       train_regret_loser_15[slice30],\r\n",
        "       train_regret_loser_16[slice30],\r\n",
        "       train_regret_loser_17[slice30],\r\n",
        "       train_regret_loser_18[slice30],\r\n",
        "       train_regret_loser_19[slice30],\r\n",
        "       train_regret_loser_20[slice30]]\r\n",
        "\r\n",
        "winner30 = [train_regret_winner_1[slice30],\r\n",
        "       train_regret_winner_2[slice30],\r\n",
        "       train_regret_winner_3[slice30],\r\n",
        "       train_regret_winner_4[slice30],\r\n",
        "       train_regret_winner_5[slice30],\r\n",
        "       train_regret_winner_6[slice30],\r\n",
        "       train_regret_winner_7[slice30],\r\n",
        "       train_regret_winner_8[slice30],\r\n",
        "       train_regret_winner_9[slice30],\r\n",
        "       train_regret_winner_10[slice30],\r\n",
        "       train_regret_winner_11[slice30],\r\n",
        "       train_regret_winner_12[slice30],\r\n",
        "       train_regret_winner_13[slice30],\r\n",
        "       train_regret_winner_14[slice30],\r\n",
        "       train_regret_winner_15[slice30],\r\n",
        "       train_regret_winner_16[slice30],\r\n",
        "       train_regret_winner_17[slice30],\r\n",
        "       train_regret_winner_18[slice30],\r\n",
        "       train_regret_winner_19[slice30],\r\n",
        "       train_regret_winner_20[slice30]]\r\n",
        "\r\n",
        "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\r\n",
        "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\r\n",
        "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\r\n",
        "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\r\n",
        "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\r\n",
        "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-iWfLkj1pvn"
      },
      "source": [
        "# Iteration40 :\r\n",
        "\r\n",
        "slice40 = 39\r\n",
        "\r\n",
        "loser40 = [train_regret_loser_1[slice40],\r\n",
        "       train_regret_loser_2[slice40],\r\n",
        "       train_regret_loser_3[slice40],\r\n",
        "       train_regret_loser_4[slice40],\r\n",
        "       train_regret_loser_5[slice40],\r\n",
        "       train_regret_loser_6[slice40],\r\n",
        "       train_regret_loser_7[slice40],\r\n",
        "       train_regret_loser_8[slice40],\r\n",
        "       train_regret_loser_9[slice40],\r\n",
        "       train_regret_loser_10[slice40],\r\n",
        "       train_regret_loser_11[slice40],\r\n",
        "       train_regret_loser_12[slice40],\r\n",
        "       train_regret_loser_13[slice40],\r\n",
        "       train_regret_loser_14[slice40],\r\n",
        "       train_regret_loser_15[slice40],\r\n",
        "       train_regret_loser_16[slice40],\r\n",
        "       train_regret_loser_17[slice40],\r\n",
        "       train_regret_loser_18[slice40],\r\n",
        "       train_regret_loser_19[slice40],\r\n",
        "       train_regret_loser_20[slice40]]\r\n",
        "\r\n",
        "winner40 = [train_regret_winner_1[slice40],\r\n",
        "       train_regret_winner_2[slice40],\r\n",
        "       train_regret_winner_3[slice40],\r\n",
        "       train_regret_winner_4[slice40],\r\n",
        "       train_regret_winner_5[slice40],\r\n",
        "       train_regret_winner_6[slice40],\r\n",
        "       train_regret_winner_7[slice40],\r\n",
        "       train_regret_winner_8[slice40],\r\n",
        "       train_regret_winner_9[slice40],\r\n",
        "       train_regret_winner_10[slice40],\r\n",
        "       train_regret_winner_11[slice40],\r\n",
        "       train_regret_winner_12[slice40],\r\n",
        "       train_regret_winner_13[slice40],\r\n",
        "       train_regret_winner_14[slice40],\r\n",
        "       train_regret_winner_15[slice40],\r\n",
        "       train_regret_winner_16[slice40],\r\n",
        "       train_regret_winner_17[slice40],\r\n",
        "       train_regret_winner_18[slice40],\r\n",
        "       train_regret_winner_19[slice40],\r\n",
        "       train_regret_winner_20[slice40]]\r\n",
        "\r\n",
        "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\r\n",
        "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\r\n",
        "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\r\n",
        "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\r\n",
        "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\r\n",
        "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G5i-Cjw2U7K"
      },
      "source": [
        "# Iteration50 :\r\n",
        "\r\n",
        "slice50 = 49\r\n",
        "\r\n",
        "loser50 = [train_regret_loser_1[slice50],\r\n",
        "       train_regret_loser_2[slice50],\r\n",
        "       train_regret_loser_3[slice50],\r\n",
        "       train_regret_loser_4[slice50],\r\n",
        "       train_regret_loser_5[slice50],\r\n",
        "       train_regret_loser_6[slice50],\r\n",
        "       train_regret_loser_7[slice50],\r\n",
        "       train_regret_loser_8[slice50],\r\n",
        "       train_regret_loser_9[slice50],\r\n",
        "       train_regret_loser_10[slice50],\r\n",
        "       train_regret_loser_11[slice50],\r\n",
        "       train_regret_loser_12[slice50],\r\n",
        "       train_regret_loser_13[slice50],\r\n",
        "       train_regret_loser_14[slice50],\r\n",
        "       train_regret_loser_15[slice50],\r\n",
        "       train_regret_loser_16[slice50],\r\n",
        "       train_regret_loser_17[slice50],\r\n",
        "       train_regret_loser_18[slice50],\r\n",
        "       train_regret_loser_19[slice50],\r\n",
        "       train_regret_loser_20[slice50]]\r\n",
        "\r\n",
        "winner50 = [train_regret_winner_1[slice50],\r\n",
        "       train_regret_winner_2[slice50],\r\n",
        "       train_regret_winner_3[slice50],\r\n",
        "       train_regret_winner_4[slice50],\r\n",
        "       train_regret_winner_5[slice50],\r\n",
        "       train_regret_winner_6[slice50],\r\n",
        "       train_regret_winner_7[slice50],\r\n",
        "       train_regret_winner_8[slice50],\r\n",
        "       train_regret_winner_9[slice50],\r\n",
        "       train_regret_winner_10[slice50],\r\n",
        "       train_regret_winner_11[slice50],\r\n",
        "       train_regret_winner_12[slice50],\r\n",
        "       train_regret_winner_13[slice50],\r\n",
        "       train_regret_winner_14[slice50],\r\n",
        "       train_regret_winner_15[slice50],\r\n",
        "       train_regret_winner_16[slice50],\r\n",
        "       train_regret_winner_17[slice50],\r\n",
        "       train_regret_winner_18[slice50],\r\n",
        "       train_regret_winner_19[slice50],\r\n",
        "       train_regret_winner_20[slice50]]\r\n",
        "\r\n",
        "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\r\n",
        "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\r\n",
        "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\r\n",
        "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\r\n",
        "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\r\n",
        "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY6qxZQA2U-r"
      },
      "source": [
        "# Iteration60 :\r\n",
        "\r\n",
        "slice60 = 59\r\n",
        "\r\n",
        "loser60 = [train_regret_loser_1[slice60],\r\n",
        "       train_regret_loser_2[slice60],\r\n",
        "       train_regret_loser_3[slice60],\r\n",
        "       train_regret_loser_4[slice60],\r\n",
        "       train_regret_loser_5[slice60],\r\n",
        "       train_regret_loser_6[slice60],\r\n",
        "       train_regret_loser_7[slice60],\r\n",
        "       train_regret_loser_8[slice60],\r\n",
        "       train_regret_loser_9[slice60],\r\n",
        "       train_regret_loser_10[slice60],\r\n",
        "       train_regret_loser_11[slice60],\r\n",
        "       train_regret_loser_12[slice60],\r\n",
        "       train_regret_loser_13[slice60],\r\n",
        "       train_regret_loser_14[slice60],\r\n",
        "       train_regret_loser_15[slice60],\r\n",
        "       train_regret_loser_16[slice60],\r\n",
        "       train_regret_loser_17[slice60],\r\n",
        "       train_regret_loser_18[slice60],\r\n",
        "       train_regret_loser_19[slice60],\r\n",
        "       train_regret_loser_20[slice60]]\r\n",
        "\r\n",
        "winner60 = [train_regret_winner_1[slice60],\r\n",
        "       train_regret_winner_2[slice60],\r\n",
        "       train_regret_winner_3[slice60],\r\n",
        "       train_regret_winner_4[slice60],\r\n",
        "       train_regret_winner_5[slice60],\r\n",
        "       train_regret_winner_6[slice60],\r\n",
        "       train_regret_winner_7[slice60],\r\n",
        "       train_regret_winner_8[slice60],\r\n",
        "       train_regret_winner_9[slice60],\r\n",
        "       train_regret_winner_10[slice60],\r\n",
        "       train_regret_winner_11[slice60],\r\n",
        "       train_regret_winner_12[slice60],\r\n",
        "       train_regret_winner_13[slice60],\r\n",
        "       train_regret_winner_14[slice60],\r\n",
        "       train_regret_winner_15[slice60],\r\n",
        "       train_regret_winner_16[slice60],\r\n",
        "       train_regret_winner_17[slice60],\r\n",
        "       train_regret_winner_18[slice60],\r\n",
        "       train_regret_winner_19[slice60],\r\n",
        "       train_regret_winner_20[slice60]]\r\n",
        "\r\n",
        "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\r\n",
        "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\r\n",
        "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\r\n",
        "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\r\n",
        "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\r\n",
        "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqgVxVH2VCU"
      },
      "source": [
        "# Iteration70 :\r\n",
        "\r\n",
        "slice70 = 69\r\n",
        "\r\n",
        "loser70 = [train_regret_loser_1[slice70],\r\n",
        "       train_regret_loser_2[slice70],\r\n",
        "       train_regret_loser_3[slice70],\r\n",
        "       train_regret_loser_4[slice70],\r\n",
        "       train_regret_loser_5[slice70],\r\n",
        "       train_regret_loser_6[slice70],\r\n",
        "       train_regret_loser_7[slice70],\r\n",
        "       train_regret_loser_8[slice70],\r\n",
        "       train_regret_loser_9[slice70],\r\n",
        "       train_regret_loser_10[slice70],\r\n",
        "       train_regret_loser_11[slice70],\r\n",
        "       train_regret_loser_12[slice70],\r\n",
        "       train_regret_loser_13[slice70],\r\n",
        "       train_regret_loser_14[slice70],\r\n",
        "       train_regret_loser_15[slice70],\r\n",
        "       train_regret_loser_16[slice70],\r\n",
        "       train_regret_loser_17[slice70],\r\n",
        "       train_regret_loser_18[slice70],\r\n",
        "       train_regret_loser_19[slice70],\r\n",
        "       train_regret_loser_20[slice70]]\r\n",
        "\r\n",
        "winner70 = [train_regret_winner_1[slice70],\r\n",
        "       train_regret_winner_2[slice70],\r\n",
        "       train_regret_winner_3[slice70],\r\n",
        "       train_regret_winner_4[slice70],\r\n",
        "       train_regret_winner_5[slice70],\r\n",
        "       train_regret_winner_6[slice70],\r\n",
        "       train_regret_winner_7[slice70],\r\n",
        "       train_regret_winner_8[slice70],\r\n",
        "       train_regret_winner_9[slice70],\r\n",
        "       train_regret_winner_10[slice70],\r\n",
        "       train_regret_winner_11[slice70],\r\n",
        "       train_regret_winner_12[slice70],\r\n",
        "       train_regret_winner_13[slice70],\r\n",
        "       train_regret_winner_14[slice70],\r\n",
        "       train_regret_winner_15[slice70],\r\n",
        "       train_regret_winner_16[slice70],\r\n",
        "       train_regret_winner_17[slice70],\r\n",
        "       train_regret_winner_18[slice70],\r\n",
        "       train_regret_winner_19[slice70],\r\n",
        "       train_regret_winner_20[slice70]]\r\n",
        "\r\n",
        "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\r\n",
        "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\r\n",
        "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\r\n",
        "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\r\n",
        "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\r\n",
        "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF7mlDZL1pyN"
      },
      "source": [
        "# Iteration80 :\r\n",
        "\r\n",
        "slice80 = 79\r\n",
        "\r\n",
        "loser80 = [train_regret_loser_1[slice80],\r\n",
        "       train_regret_loser_2[slice80],\r\n",
        "       train_regret_loser_3[slice80],\r\n",
        "       train_regret_loser_4[slice80],\r\n",
        "       train_regret_loser_5[slice80],\r\n",
        "       train_regret_loser_6[slice80],\r\n",
        "       train_regret_loser_7[slice80],\r\n",
        "       train_regret_loser_8[slice80],\r\n",
        "       train_regret_loser_9[slice80],\r\n",
        "       train_regret_loser_10[slice80],\r\n",
        "       train_regret_loser_11[slice80],\r\n",
        "       train_regret_loser_12[slice80],\r\n",
        "       train_regret_loser_13[slice80],\r\n",
        "       train_regret_loser_14[slice80],\r\n",
        "       train_regret_loser_15[slice80],\r\n",
        "       train_regret_loser_16[slice80],\r\n",
        "       train_regret_loser_17[slice80],\r\n",
        "       train_regret_loser_18[slice80],\r\n",
        "       train_regret_loser_19[slice80],\r\n",
        "       train_regret_loser_20[slice80]]\r\n",
        "\r\n",
        "winner80 = [train_regret_winner_1[slice80],\r\n",
        "       train_regret_winner_2[slice80],\r\n",
        "       train_regret_winner_3[slice80],\r\n",
        "       train_regret_winner_4[slice80],\r\n",
        "       train_regret_winner_5[slice80],\r\n",
        "       train_regret_winner_6[slice80],\r\n",
        "       train_regret_winner_7[slice80],\r\n",
        "       train_regret_winner_8[slice80],\r\n",
        "       train_regret_winner_9[slice80],\r\n",
        "       train_regret_winner_10[slice80],\r\n",
        "       train_regret_winner_11[slice80],\r\n",
        "       train_regret_winner_12[slice80],\r\n",
        "       train_regret_winner_13[slice80],\r\n",
        "       train_regret_winner_14[slice80],\r\n",
        "       train_regret_winner_15[slice80],\r\n",
        "       train_regret_winner_16[slice80],\r\n",
        "       train_regret_winner_17[slice80],\r\n",
        "       train_regret_winner_18[slice80],\r\n",
        "       train_regret_winner_19[slice80],\r\n",
        "       train_regret_winner_20[slice80]]\r\n",
        "\r\n",
        "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\r\n",
        "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\r\n",
        "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\r\n",
        "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\r\n",
        "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\r\n",
        "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Rw8IkD1p0z"
      },
      "source": [
        "# Iteration90 :\r\n",
        "\r\n",
        "slice90 = 89\r\n",
        "\r\n",
        "loser90 = [train_regret_loser_1[slice90],\r\n",
        "       train_regret_loser_2[slice90],\r\n",
        "       train_regret_loser_3[slice90],\r\n",
        "       train_regret_loser_4[slice90],\r\n",
        "       train_regret_loser_5[slice90],\r\n",
        "       train_regret_loser_6[slice90],\r\n",
        "       train_regret_loser_7[slice90],\r\n",
        "       train_regret_loser_8[slice90],\r\n",
        "       train_regret_loser_9[slice90],\r\n",
        "       train_regret_loser_10[slice90],\r\n",
        "       train_regret_loser_11[slice90],\r\n",
        "       train_regret_loser_12[slice90],\r\n",
        "       train_regret_loser_13[slice90],\r\n",
        "       train_regret_loser_14[slice90],\r\n",
        "       train_regret_loser_15[slice90],\r\n",
        "       train_regret_loser_16[slice90],\r\n",
        "       train_regret_loser_17[slice90],\r\n",
        "       train_regret_loser_18[slice90],\r\n",
        "       train_regret_loser_19[slice90],\r\n",
        "       train_regret_loser_20[slice90]]\r\n",
        "\r\n",
        "winner90 = [train_regret_winner_1[slice90],\r\n",
        "       train_regret_winner_2[slice90],\r\n",
        "       train_regret_winner_3[slice90],\r\n",
        "       train_regret_winner_4[slice90],\r\n",
        "       train_regret_winner_5[slice90],\r\n",
        "       train_regret_winner_6[slice90],\r\n",
        "       train_regret_winner_7[slice90],\r\n",
        "       train_regret_winner_8[slice90],\r\n",
        "       train_regret_winner_9[slice90],\r\n",
        "       train_regret_winner_10[slice90],\r\n",
        "       train_regret_winner_11[slice90],\r\n",
        "       train_regret_winner_12[slice90],\r\n",
        "       train_regret_winner_13[slice90],\r\n",
        "       train_regret_winner_14[slice90],\r\n",
        "       train_regret_winner_15[slice90],\r\n",
        "       train_regret_winner_16[slice90],\r\n",
        "       train_regret_winner_17[slice90],\r\n",
        "       train_regret_winner_18[slice90],\r\n",
        "       train_regret_winner_19[slice90],\r\n",
        "       train_regret_winner_20[slice90]]\r\n",
        "\r\n",
        "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\r\n",
        "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\r\n",
        "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\r\n",
        "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\r\n",
        "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\r\n",
        "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiljCrq21p3a"
      },
      "source": [
        "# Iteration100 :\r\n",
        "\r\n",
        "slice100 = 99\r\n",
        "\r\n",
        "loser100 = [train_regret_loser_1[slice100],\r\n",
        "       train_regret_loser_2[slice100],\r\n",
        "       train_regret_loser_3[slice100],\r\n",
        "       train_regret_loser_4[slice100],\r\n",
        "       train_regret_loser_5[slice100],\r\n",
        "       train_regret_loser_6[slice100],\r\n",
        "       train_regret_loser_7[slice100],\r\n",
        "       train_regret_loser_8[slice100],\r\n",
        "       train_regret_loser_9[slice100],\r\n",
        "       train_regret_loser_10[slice100],\r\n",
        "       train_regret_loser_11[slice100],\r\n",
        "       train_regret_loser_12[slice100],\r\n",
        "       train_regret_loser_13[slice100],\r\n",
        "       train_regret_loser_14[slice100],\r\n",
        "       train_regret_loser_15[slice100],\r\n",
        "       train_regret_loser_16[slice100],\r\n",
        "       train_regret_loser_17[slice100],\r\n",
        "       train_regret_loser_18[slice100],\r\n",
        "       train_regret_loser_19[slice100],\r\n",
        "       train_regret_loser_20[slice100]]\r\n",
        "\r\n",
        "winner100 = [train_regret_winner_1[slice100],\r\n",
        "       train_regret_winner_2[slice100],\r\n",
        "       train_regret_winner_3[slice100],\r\n",
        "       train_regret_winner_4[slice100],\r\n",
        "       train_regret_winner_5[slice100],\r\n",
        "       train_regret_winner_6[slice100],\r\n",
        "       train_regret_winner_7[slice100],\r\n",
        "       train_regret_winner_8[slice100],\r\n",
        "       train_regret_winner_9[slice100],\r\n",
        "       train_regret_winner_10[slice100],\r\n",
        "       train_regret_winner_11[slice100],\r\n",
        "       train_regret_winner_12[slice100],\r\n",
        "       train_regret_winner_13[slice100],\r\n",
        "       train_regret_winner_14[slice100],\r\n",
        "       train_regret_winner_15[slice100],\r\n",
        "       train_regret_winner_16[slice100],\r\n",
        "       train_regret_winner_17[slice100],\r\n",
        "       train_regret_winner_18[slice100],\r\n",
        "       train_regret_winner_19[slice100],\r\n",
        "       train_regret_winner_20[slice100]]\r\n",
        "\r\n",
        "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\r\n",
        "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\r\n",
        "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\r\n",
        "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\r\n",
        "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\r\n",
        "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OItJgqUT2oNb"
      },
      "source": [
        "### Summarize arrays: 'Loser'\r\n",
        "\r\n",
        "lower_loser = [lower_loser1,\r\n",
        "            lower_loser2,\r\n",
        "            lower_loser3,\r\n",
        "            lower_loser4,\r\n",
        "            lower_loser5,\r\n",
        "            lower_loser6,\r\n",
        "            lower_loser7,\r\n",
        "            lower_loser8,\r\n",
        "            lower_loser9,\r\n",
        "            lower_loser10,\r\n",
        "            lower_loser11,\r\n",
        "            lower_loser12,\r\n",
        "            lower_loser13,\r\n",
        "            lower_loser14,\r\n",
        "            lower_loser15,\r\n",
        "            lower_loser16,\r\n",
        "            lower_loser17,\r\n",
        "            lower_loser18,\r\n",
        "            lower_loser19,\r\n",
        "            lower_loser20,\r\n",
        "            lower_loser21,\r\n",
        "            lower_loser22,\r\n",
        "            lower_loser23,\r\n",
        "            lower_loser24,\r\n",
        "            lower_loser25,\r\n",
        "            lower_loser26,\r\n",
        "            lower_loser27,\r\n",
        "            lower_loser28,\r\n",
        "            lower_loser29,\r\n",
        "            lower_loser30,\r\n",
        "            lower_loser31,\r\n",
        "            lower_loser32,\r\n",
        "            lower_loser33,\r\n",
        "            lower_loser34,\r\n",
        "            lower_loser35,\r\n",
        "            lower_loser36,\r\n",
        "            lower_loser37,\r\n",
        "            lower_loser38,\r\n",
        "            lower_loser39,\r\n",
        "            lower_loser40,\r\n",
        "            lower_loser41,\r\n",
        "            lower_loser42,\r\n",
        "            lower_loser43,\r\n",
        "            lower_loser44,\r\n",
        "            lower_loser45,\r\n",
        "            lower_loser46,\r\n",
        "            lower_loser47,\r\n",
        "            lower_loser48,\r\n",
        "            lower_loser49,\r\n",
        "            lower_loser50,\r\n",
        "            lower_loser51,\r\n",
        "            lower_loser52,\r\n",
        "            lower_loser53,\r\n",
        "            lower_loser54,\r\n",
        "            lower_loser55,\r\n",
        "            lower_loser56,\r\n",
        "            lower_loser57,\r\n",
        "            lower_loser58,\r\n",
        "            lower_loser59,\r\n",
        "            lower_loser60,\r\n",
        "            lower_loser61,\r\n",
        "            lower_loser62,\r\n",
        "            lower_loser63,\r\n",
        "            lower_loser64,\r\n",
        "            lower_loser65,\r\n",
        "            lower_loser66,\r\n",
        "            lower_loser67,\r\n",
        "            lower_loser68,\r\n",
        "            lower_loser69,\r\n",
        "            lower_loser70,\r\n",
        "            lower_loser71,\r\n",
        "            lower_loser72,\r\n",
        "            lower_loser73,\r\n",
        "            lower_loser74,\r\n",
        "            lower_loser75,\r\n",
        "            lower_loser76,\r\n",
        "            lower_loser77,\r\n",
        "            lower_loser78,\r\n",
        "            lower_loser79,\r\n",
        "            lower_loser80,\r\n",
        "            lower_loser81,\r\n",
        "            lower_loser82,\r\n",
        "            lower_loser83,\r\n",
        "            lower_loser84,\r\n",
        "            lower_loser85,\r\n",
        "            lower_loser86,\r\n",
        "            lower_loser87,\r\n",
        "            lower_loser88,\r\n",
        "            lower_loser89,\r\n",
        "            lower_loser90,\r\n",
        "            lower_loser91,\r\n",
        "            lower_loser92,\r\n",
        "            lower_loser93,\r\n",
        "            lower_loser94,\r\n",
        "            lower_loser95,\r\n",
        "            lower_loser96,\r\n",
        "            lower_loser97,\r\n",
        "            lower_loser98,\r\n",
        "            lower_loser99,\r\n",
        "            lower_loser100,\r\n",
        "            lower_loser101]\r\n",
        "\r\n",
        "median_loser = [median_loser1,\r\n",
        "            median_loser2,\r\n",
        "            median_loser3,\r\n",
        "            median_loser4,\r\n",
        "            median_loser5,\r\n",
        "            median_loser6,\r\n",
        "            median_loser7,\r\n",
        "            median_loser8,\r\n",
        "            median_loser9,\r\n",
        "            median_loser10,\r\n",
        "            median_loser11,\r\n",
        "            median_loser12,\r\n",
        "            median_loser13,\r\n",
        "            median_loser14,\r\n",
        "            median_loser15,\r\n",
        "            median_loser16,\r\n",
        "            median_loser17,\r\n",
        "            median_loser18,\r\n",
        "            median_loser19,\r\n",
        "            median_loser20,\r\n",
        "            median_loser21,\r\n",
        "            median_loser22,\r\n",
        "            median_loser23,\r\n",
        "            median_loser24,\r\n",
        "            median_loser25,\r\n",
        "            median_loser26,\r\n",
        "            median_loser27,\r\n",
        "            median_loser28,\r\n",
        "            median_loser29,\r\n",
        "            median_loser30,\r\n",
        "            median_loser31,\r\n",
        "            median_loser32,\r\n",
        "            median_loser33,\r\n",
        "            median_loser34,\r\n",
        "            median_loser35,\r\n",
        "            median_loser36,\r\n",
        "            median_loser37,\r\n",
        "            median_loser38,\r\n",
        "            median_loser39,\r\n",
        "            median_loser40,\r\n",
        "            median_loser41,\r\n",
        "            median_loser42,\r\n",
        "            median_loser43,\r\n",
        "            median_loser44,\r\n",
        "            median_loser45,\r\n",
        "            median_loser46,\r\n",
        "            median_loser47,\r\n",
        "            median_loser48,\r\n",
        "            median_loser49,\r\n",
        "            median_loser50,\r\n",
        "            median_loser51,\r\n",
        "            median_loser52,\r\n",
        "            median_loser53,\r\n",
        "            median_loser54,\r\n",
        "            median_loser55,\r\n",
        "            median_loser56,\r\n",
        "            median_loser57,\r\n",
        "            median_loser58,\r\n",
        "            median_loser59,\r\n",
        "            median_loser60,\r\n",
        "            median_loser61,\r\n",
        "            median_loser62,\r\n",
        "            median_loser63,\r\n",
        "            median_loser64,\r\n",
        "            median_loser65,\r\n",
        "            median_loser66,\r\n",
        "            median_loser67,\r\n",
        "            median_loser68,\r\n",
        "            median_loser69,\r\n",
        "            median_loser70,\r\n",
        "            median_loser71,\r\n",
        "            median_loser72,\r\n",
        "            median_loser73,\r\n",
        "            median_loser74,\r\n",
        "            median_loser75,\r\n",
        "            median_loser76,\r\n",
        "            median_loser77,\r\n",
        "            median_loser78,\r\n",
        "            median_loser79,\r\n",
        "            median_loser80,\r\n",
        "            median_loser81,\r\n",
        "            median_loser82,\r\n",
        "            median_loser83,\r\n",
        "            median_loser84,\r\n",
        "            median_loser85,\r\n",
        "            median_loser86,\r\n",
        "            median_loser87,\r\n",
        "            median_loser88,\r\n",
        "            median_loser89,\r\n",
        "            median_loser90,\r\n",
        "            median_loser91,\r\n",
        "            median_loser92,\r\n",
        "            median_loser93,\r\n",
        "            median_loser94,\r\n",
        "            median_loser95,\r\n",
        "            median_loser96,\r\n",
        "            median_loser97,\r\n",
        "            median_loser98,\r\n",
        "            median_loser99,\r\n",
        "            median_loser100,\r\n",
        "            median_loser101]\r\n",
        "\r\n",
        "upper_loser = [upper_loser1,\r\n",
        "            upper_loser2,\r\n",
        "            upper_loser3,\r\n",
        "            upper_loser4,\r\n",
        "            upper_loser5,\r\n",
        "            upper_loser6,\r\n",
        "            upper_loser7,\r\n",
        "            upper_loser8,\r\n",
        "            upper_loser9,\r\n",
        "            upper_loser10,\r\n",
        "            upper_loser11,\r\n",
        "            upper_loser12,\r\n",
        "            upper_loser13,\r\n",
        "            upper_loser14,\r\n",
        "            upper_loser15,\r\n",
        "            upper_loser16,\r\n",
        "            upper_loser17,\r\n",
        "            upper_loser18,\r\n",
        "            upper_loser19,\r\n",
        "            upper_loser20,\r\n",
        "            upper_loser21,\r\n",
        "            upper_loser22,\r\n",
        "            upper_loser23,\r\n",
        "            upper_loser24,\r\n",
        "            upper_loser25,\r\n",
        "            upper_loser26,\r\n",
        "            upper_loser27,\r\n",
        "            upper_loser28,\r\n",
        "            upper_loser29,\r\n",
        "            upper_loser30,\r\n",
        "            upper_loser31,\r\n",
        "            upper_loser32,\r\n",
        "            upper_loser33,\r\n",
        "            upper_loser34,\r\n",
        "            upper_loser35,\r\n",
        "            upper_loser36,\r\n",
        "            upper_loser37,\r\n",
        "            upper_loser38,\r\n",
        "            upper_loser39,\r\n",
        "            upper_loser40,\r\n",
        "            upper_loser41,\r\n",
        "            upper_loser42,\r\n",
        "            upper_loser43,\r\n",
        "            upper_loser44,\r\n",
        "            upper_loser45,\r\n",
        "            upper_loser46,\r\n",
        "            upper_loser47,\r\n",
        "            upper_loser48,\r\n",
        "            upper_loser49,\r\n",
        "            upper_loser50,\r\n",
        "            upper_loser51,\r\n",
        "            upper_loser52,\r\n",
        "            upper_loser53,\r\n",
        "            upper_loser54,\r\n",
        "            upper_loser55,\r\n",
        "            upper_loser56,\r\n",
        "            upper_loser57,\r\n",
        "            upper_loser58,\r\n",
        "            upper_loser59,\r\n",
        "            upper_loser60,\r\n",
        "            upper_loser61,\r\n",
        "            upper_loser62,\r\n",
        "            upper_loser63,\r\n",
        "            upper_loser64,\r\n",
        "            upper_loser65,\r\n",
        "            upper_loser66,\r\n",
        "            upper_loser67,\r\n",
        "            upper_loser68,\r\n",
        "            upper_loser69,\r\n",
        "            upper_loser70,\r\n",
        "            upper_loser71,\r\n",
        "            upper_loser72,\r\n",
        "            upper_loser73,\r\n",
        "            upper_loser74,\r\n",
        "            upper_loser75,\r\n",
        "            upper_loser76,\r\n",
        "            upper_loser77,\r\n",
        "            upper_loser78,\r\n",
        "            upper_loser79,\r\n",
        "            upper_loser80,\r\n",
        "            upper_loser81,\r\n",
        "            upper_loser82,\r\n",
        "            upper_loser83,\r\n",
        "            upper_loser84,\r\n",
        "            upper_loser85,\r\n",
        "            upper_loser86,\r\n",
        "            upper_loser87,\r\n",
        "            upper_loser88,\r\n",
        "            upper_loser89,\r\n",
        "            upper_loser90,\r\n",
        "            upper_loser91,\r\n",
        "            upper_loser92,\r\n",
        "            upper_loser93,\r\n",
        "            upper_loser94,\r\n",
        "            upper_loser95,\r\n",
        "            upper_loser96,\r\n",
        "            upper_loser97,\r\n",
        "            upper_loser98,\r\n",
        "            upper_loser99,\r\n",
        "            upper_loser100,\r\n",
        "            upper_loser101]"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u19FndLT2oU3"
      },
      "source": [
        "### Summarize arrays: 'Winner'\r\n",
        "\r\n",
        "lower_winner = [lower_winner1,\r\n",
        "            lower_winner2,\r\n",
        "            lower_winner3,\r\n",
        "            lower_winner4,\r\n",
        "            lower_winner5,\r\n",
        "            lower_winner6,\r\n",
        "            lower_winner7,\r\n",
        "            lower_winner8,\r\n",
        "            lower_winner9,\r\n",
        "            lower_winner10,\r\n",
        "            lower_winner11,\r\n",
        "            lower_winner12,\r\n",
        "            lower_winner13,\r\n",
        "            lower_winner14,\r\n",
        "            lower_winner15,\r\n",
        "            lower_winner16,\r\n",
        "            lower_winner17,\r\n",
        "            lower_winner18,\r\n",
        "            lower_winner19,\r\n",
        "            lower_winner20,\r\n",
        "            lower_winner21,\r\n",
        "            lower_winner22,\r\n",
        "            lower_winner23,\r\n",
        "            lower_winner24,\r\n",
        "            lower_winner25,\r\n",
        "            lower_winner26,\r\n",
        "            lower_winner27,\r\n",
        "            lower_winner28,\r\n",
        "            lower_winner29,\r\n",
        "            lower_winner30,\r\n",
        "            lower_winner31,\r\n",
        "            lower_winner32,\r\n",
        "            lower_winner33,\r\n",
        "            lower_winner34,\r\n",
        "            lower_winner35,\r\n",
        "            lower_winner36,\r\n",
        "            lower_winner37,\r\n",
        "            lower_winner38,\r\n",
        "            lower_winner39,\r\n",
        "            lower_winner40,\r\n",
        "            lower_winner41,\r\n",
        "            lower_winner42,\r\n",
        "            lower_winner43,\r\n",
        "            lower_winner44,\r\n",
        "            lower_winner45,\r\n",
        "            lower_winner46,\r\n",
        "            lower_winner47,\r\n",
        "            lower_winner48,\r\n",
        "            lower_winner49,\r\n",
        "            lower_winner50,\r\n",
        "            lower_winner51,\r\n",
        "            lower_winner52,\r\n",
        "            lower_winner53,\r\n",
        "            lower_winner54,\r\n",
        "            lower_winner55,\r\n",
        "            lower_winner56,\r\n",
        "            lower_winner57,\r\n",
        "            lower_winner58,\r\n",
        "            lower_winner59,\r\n",
        "            lower_winner60,\r\n",
        "            lower_winner61,\r\n",
        "            lower_winner62,\r\n",
        "            lower_winner63,\r\n",
        "            lower_winner64,\r\n",
        "            lower_winner65,\r\n",
        "            lower_winner66,\r\n",
        "            lower_winner67,\r\n",
        "            lower_winner68,\r\n",
        "            lower_winner69,\r\n",
        "            lower_winner70,\r\n",
        "            lower_winner71,\r\n",
        "            lower_winner72,\r\n",
        "            lower_winner73,\r\n",
        "            lower_winner74,\r\n",
        "            lower_winner75,\r\n",
        "            lower_winner76,\r\n",
        "            lower_winner77,\r\n",
        "            lower_winner78,\r\n",
        "            lower_winner79,\r\n",
        "            lower_winner80,\r\n",
        "            lower_winner81,\r\n",
        "            lower_winner82,\r\n",
        "            lower_winner83,\r\n",
        "            lower_winner84,\r\n",
        "            lower_winner85,\r\n",
        "            lower_winner86,\r\n",
        "            lower_winner87,\r\n",
        "            lower_winner88,\r\n",
        "            lower_winner89,\r\n",
        "            lower_winner90,\r\n",
        "            lower_winner91,\r\n",
        "            lower_winner92,\r\n",
        "            lower_winner93,\r\n",
        "            lower_winner94,\r\n",
        "            lower_winner95,\r\n",
        "            lower_winner96,\r\n",
        "            lower_winner97,\r\n",
        "            lower_winner98,\r\n",
        "            lower_winner99,\r\n",
        "            lower_winner100,\r\n",
        "            lower_winner101]\r\n",
        "\r\n",
        "median_winner = [median_winner1,\r\n",
        "            median_winner2,\r\n",
        "            median_winner3,\r\n",
        "            median_winner4,\r\n",
        "            median_winner5,\r\n",
        "            median_winner6,\r\n",
        "            median_winner7,\r\n",
        "            median_winner8,\r\n",
        "            median_winner9,\r\n",
        "            median_winner10,\r\n",
        "            median_winner11,\r\n",
        "            median_winner12,\r\n",
        "            median_winner13,\r\n",
        "            median_winner14,\r\n",
        "            median_winner15,\r\n",
        "            median_winner16,\r\n",
        "            median_winner17,\r\n",
        "            median_winner18,\r\n",
        "            median_winner19,\r\n",
        "            median_winner20,\r\n",
        "            median_winner21,\r\n",
        "            median_winner22,\r\n",
        "            median_winner23,\r\n",
        "            median_winner24,\r\n",
        "            median_winner25,\r\n",
        "            median_winner26,\r\n",
        "            median_winner27,\r\n",
        "            median_winner28,\r\n",
        "            median_winner29,\r\n",
        "            median_winner30,\r\n",
        "            median_winner31,\r\n",
        "            median_winner32,\r\n",
        "            median_winner33,\r\n",
        "            median_winner34,\r\n",
        "            median_winner35,\r\n",
        "            median_winner36,\r\n",
        "            median_winner37,\r\n",
        "            median_winner38,\r\n",
        "            median_winner39,\r\n",
        "            median_winner40,\r\n",
        "            median_winner41,\r\n",
        "            median_winner42,\r\n",
        "            median_winner43,\r\n",
        "            median_winner44,\r\n",
        "            median_winner45,\r\n",
        "            median_winner46,\r\n",
        "            median_winner47,\r\n",
        "            median_winner48,\r\n",
        "            median_winner49,\r\n",
        "            median_winner50,\r\n",
        "            median_winner51,\r\n",
        "            median_winner52,\r\n",
        "            median_winner53,\r\n",
        "            median_winner54,\r\n",
        "            median_winner55,\r\n",
        "            median_winner56,\r\n",
        "            median_winner57,\r\n",
        "            median_winner58,\r\n",
        "            median_winner59,\r\n",
        "            median_winner60,\r\n",
        "            median_winner61,\r\n",
        "            median_winner62,\r\n",
        "            median_winner63,\r\n",
        "            median_winner64,\r\n",
        "            median_winner65,\r\n",
        "            median_winner66,\r\n",
        "            median_winner67,\r\n",
        "            median_winner68,\r\n",
        "            median_winner69,\r\n",
        "            median_winner70,\r\n",
        "            median_winner71,\r\n",
        "            median_winner72,\r\n",
        "            median_winner73,\r\n",
        "            median_winner74,\r\n",
        "            median_winner75,\r\n",
        "            median_winner76,\r\n",
        "            median_winner77,\r\n",
        "            median_winner78,\r\n",
        "            median_winner79,\r\n",
        "            median_winner80,\r\n",
        "            median_winner81,\r\n",
        "            median_winner82,\r\n",
        "            median_winner83,\r\n",
        "            median_winner84,\r\n",
        "            median_winner85,\r\n",
        "            median_winner86,\r\n",
        "            median_winner87,\r\n",
        "            median_winner88,\r\n",
        "            median_winner89,\r\n",
        "            median_winner90,\r\n",
        "            median_winner91,\r\n",
        "            median_winner92,\r\n",
        "            median_winner93,\r\n",
        "            median_winner94,\r\n",
        "            median_winner95,\r\n",
        "            median_winner96,\r\n",
        "            median_winner97,\r\n",
        "            median_winner98,\r\n",
        "            median_winner99,\r\n",
        "            median_winner100,\r\n",
        "            median_winner101]\r\n",
        "\r\n",
        "upper_winner = [upper_winner1,\r\n",
        "            upper_winner2,\r\n",
        "            upper_winner3,\r\n",
        "            upper_winner4,\r\n",
        "            upper_winner5,\r\n",
        "            upper_winner6,\r\n",
        "            upper_winner7,\r\n",
        "            upper_winner8,\r\n",
        "            upper_winner9,\r\n",
        "            upper_winner10,\r\n",
        "            upper_winner11,\r\n",
        "            upper_winner12,\r\n",
        "            upper_winner13,\r\n",
        "            upper_winner14,\r\n",
        "            upper_winner15,\r\n",
        "            upper_winner16,\r\n",
        "            upper_winner17,\r\n",
        "            upper_winner18,\r\n",
        "            upper_winner19,\r\n",
        "            upper_winner20,\r\n",
        "            upper_winner21,\r\n",
        "            upper_winner22,\r\n",
        "            upper_winner23,\r\n",
        "            upper_winner24,\r\n",
        "            upper_winner25,\r\n",
        "            upper_winner26,\r\n",
        "            upper_winner27,\r\n",
        "            upper_winner28,\r\n",
        "            upper_winner29,\r\n",
        "            upper_winner30,\r\n",
        "            upper_winner31,\r\n",
        "            upper_winner32,\r\n",
        "            upper_winner33,\r\n",
        "            upper_winner34,\r\n",
        "            upper_winner35,\r\n",
        "            upper_winner36,\r\n",
        "            upper_winner37,\r\n",
        "            upper_winner38,\r\n",
        "            upper_winner39,\r\n",
        "            upper_winner40,\r\n",
        "            upper_winner41,\r\n",
        "            upper_winner42,\r\n",
        "            upper_winner43,\r\n",
        "            upper_winner44,\r\n",
        "            upper_winner45,\r\n",
        "            upper_winner46,\r\n",
        "            upper_winner47,\r\n",
        "            upper_winner48,\r\n",
        "            upper_winner49,\r\n",
        "            upper_winner50,\r\n",
        "            upper_winner51,\r\n",
        "            upper_winner52,\r\n",
        "            upper_winner53,\r\n",
        "            upper_winner54,\r\n",
        "            upper_winner55,\r\n",
        "            upper_winner56,\r\n",
        "            upper_winner57,\r\n",
        "            upper_winner58,\r\n",
        "            upper_winner59,\r\n",
        "            upper_winner60,\r\n",
        "            upper_winner61,\r\n",
        "            upper_winner62,\r\n",
        "            upper_winner63,\r\n",
        "            upper_winner64,\r\n",
        "            upper_winner65,\r\n",
        "            upper_winner66,\r\n",
        "            upper_winner67,\r\n",
        "            upper_winner68,\r\n",
        "            upper_winner69,\r\n",
        "            upper_winner70,\r\n",
        "            upper_winner71,\r\n",
        "            upper_winner72,\r\n",
        "            upper_winner73,\r\n",
        "            upper_winner74,\r\n",
        "            upper_winner75,\r\n",
        "            upper_winner76,\r\n",
        "            upper_winner77,\r\n",
        "            upper_winner78,\r\n",
        "            upper_winner79,\r\n",
        "            upper_winner80,\r\n",
        "            upper_winner81,\r\n",
        "            upper_winner82,\r\n",
        "            upper_winner83,\r\n",
        "            upper_winner84,\r\n",
        "            upper_winner85,\r\n",
        "            upper_winner86,\r\n",
        "            upper_winner87,\r\n",
        "            upper_winner88,\r\n",
        "            upper_winner89,\r\n",
        "            upper_winner90,\r\n",
        "            upper_winner91,\r\n",
        "            upper_winner92,\r\n",
        "            upper_winner93,\r\n",
        "            upper_winner94,\r\n",
        "            upper_winner95,\r\n",
        "            upper_winner96,\r\n",
        "            upper_winner97,\r\n",
        "            upper_winner98,\r\n",
        "            upper_winner99,\r\n",
        "            upper_winner100,\r\n",
        "            upper_winner101]"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84XIzmWD2oba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "ea7f7bf6-6c93-47ea-9e9e-116e524de748"
      },
      "source": [
        "### Visualize!\r\n",
        "\r\n",
        "title = obj_func\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "\r\n",
        "plt.plot(median_loser, color = 'Green')\r\n",
        "plt.plot(median_winner, color = 'Brown')\r\n",
        "\r\n",
        "xstar = np.arange(0, max_iter+1, step=1)\r\n",
        "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Green', alpha=0.4, label='GP EI Regret IQR: dEI')\r\n",
        "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Brown', alpha=0.4, label='GP CBM Regret IQR: dCBM')\r\n",
        "\r\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\r\n",
        "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') \r\n",
        "plt.ylabel('log(Regret)', weight = 'bold', family = 'Arial') \r\n",
        "plt.legend(loc=1) # add plot legend\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5dkR2Id2oiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f938318-7fe1-4960-9843-2228eeed2abd"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3417.914459466934, 22324.984003543854)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNLBF57p2opi"
      },
      "source": [
        ""
      ],
      "execution_count": 181,
      "outputs": []
    }
  ]
}