{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rosenbrock__GP__dERM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQo_XsusPbm"
      },
      "source": [
        "Rosenbrock synthetic function:\r\n",
        "\r\n",
        "GP EI: (exact GP EI gradients) vs. GP ERM: (exact GP ERM gradients)\r\n",
        "\r\n",
        "https://www.sfu.ca/~ssurjano/rosen.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhgucjJgsYfm",
        "outputId": "2378889d-2d0f-4892-b0ab-ad4fb8992fbb"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyGPGO\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/87/a113c91ba014708114f7635d5c0f6a5e5c773480c5f0a537b257a02d180d/pyGPGO-0.4.0.dev1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n",
            "Building wheels for collected packages: pyGPGO\n",
            "  Building wheel for pyGPGO (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyGPGO: filename=pyGPGO-0.4.0.dev1-cp37-none-any.whl size=19866 sha256=c4361ae17b2cc79a305ba9397d07bc350465bf9b351077802b1950b4499ca489\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/27/04/c4fa3bfe194d36e3cd51542132f43415a6813114a5e8301acb\n",
            "Successfully built pyGPGO\n",
            "Installing collected packages: pyGPGO\n",
            "Successfully installed pyGPGO-0.4.0.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E99qMZverfrP"
      },
      "source": [
        "### Import modules:\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import scipy as sp\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import warnings\r\n",
        "import time\r\n",
        "\r\n",
        "from pyGPGO.logger import EventLogger\r\n",
        "from pyGPGO.GPGO import GPGO\r\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\r\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\r\n",
        "from pyGPGO.acquisition import Acquisition\r\n",
        "from pyGPGO.covfunc import squaredExponential\r\n",
        "\r\n",
        "from collections import OrderedDict\r\n",
        "from joblib import Parallel, delayed\r\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\r\n",
        "from scipy.optimize import minimize\r\n",
        "from scipy.spatial.distance import cdist\r\n",
        "from scipy.special import gamma\r\n",
        "from scipy.stats import norm, t\r\n",
        "from matplotlib.pyplot import rc\r\n",
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiJSpz2P9qXK"
      },
      "source": [
        "n_start_AcqFunc = 250 #multi-start iterations to avoid local optima in AcqFunc optimization"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzPUbCs3s9nu"
      },
      "source": [
        "### Inputs:\r\n",
        "\r\n",
        "obj_func = 'Rosenbrock'\r\n",
        "n_test = n_start_AcqFunc # test points\r\n",
        "df = 3 # nu\r\n",
        "\r\n",
        "util_loser = 'dEI_GP'\r\n",
        "util_winner = 'dERM_GP'\r\n",
        "n_init = 5 # random initialisations"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AMqjmpbtAb1"
      },
      "source": [
        "### Objective function:\r\n",
        "\r\n",
        "if obj_func == 'Rosenbrock':\r\n",
        "            \r\n",
        "    # True y bounds:\r\n",
        "    y_lb = 0\r\n",
        "    operator = -1 # targets global minimum \r\n",
        "    y_global_orig = y_lb * operator # targets global minimum\r\n",
        "            \r\n",
        "# Constraints:\r\n",
        "    lb = -2.048 \r\n",
        "    ub = +2.048 \r\n",
        "    \r\n",
        "# Input array dimension(s):\r\n",
        "    dim = 2\r\n",
        "\r\n",
        "# 2-D inputs' parameter bounds:\r\n",
        "    param = {'x1_training': ('cont', [lb, ub]),\r\n",
        "             'x2_training': ('cont', [lb, ub])}\r\n",
        "    \r\n",
        "    max_iter = 100  # iterations of Bayesian optimisation\r\n",
        "    \r\n",
        "# Test data:\r\n",
        "    x1_test = np.linspace(lb, ub, n_test)\r\n",
        "    x2_test = np.linspace(lb, ub, n_test)\r\n",
        "    Xstar_d = np.column_stack((x1_test, x2_test))\r\n",
        "    \r\n",
        "    def f_syn_polarity(x1_training, x2_training):\r\n",
        "        return operator * (100 * (x2_training - x1_training ** 2) ** 2 + (x1_training - 1) ** 2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyQX-59DtC7-"
      },
      "source": [
        "### Cumulative Regret Calculator:\r\n",
        "\r\n",
        "def min_max_array(x):\r\n",
        "    new_list = []\r\n",
        "    for i, num in enumerate(x):\r\n",
        "            new_list.append(np.min(x[0:i+1]))\r\n",
        "    return new_list"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE_I_YSntHxl"
      },
      "source": [
        "### Set-seeds:\r\n",
        "\r\n",
        "run_num_1 = 111\r\n",
        "run_num_2 = 113\r\n",
        "run_num_3 = 3333\r\n",
        "run_num_4 = 444\r\n",
        "run_num_5 = 5555\r\n",
        "run_num_6 = 6\r\n",
        "run_num_7 = 777\r\n",
        "run_num_8 = 887\r\n",
        "run_num_9 = 999\r\n",
        "run_num_10 = 1000\r\n",
        "run_num_11 = 1113\r\n",
        "run_num_12 = 1234\r\n",
        "run_num_13 = 2345\r\n",
        "run_num_14 = 88\r\n",
        "run_num_15 = 1556\r\n",
        "run_num_16 = 1666\r\n",
        "run_num_17 = 717\r\n",
        "run_num_18 = 8\r\n",
        "run_num_19 = 1998\r\n",
        "run_num_20 = 2000"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe29v5N5tKjF"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\r\n",
        "\r\n",
        "def l2norm_(X, Xstar):\r\n",
        "    \r\n",
        "    return cdist(X, Xstar)\r\n",
        "\r\n",
        "def kronDelta(X, Xstar):\r\n",
        "\r\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\r\n",
        "\r\n",
        "class squaredExponentialDeriv(squaredExponential):\r\n",
        "    \r\n",
        "    def K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\r\n",
        "        return K\r\n",
        "    \r\n",
        "    def dK(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\r\n",
        "        return dK\r\n",
        "    \r\n",
        "        \r\n",
        "    def d2K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (r**2-1)\r\n",
        "        return d2K\r\n",
        "    \r\n",
        "cov_func = squaredExponential()\r\n",
        "d_cov_func = squaredExponentialDeriv()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEBX5l7TtNSN"
      },
      "source": [
        "class Acquisition_new(Acquisition):    \r\n",
        "    def __init__(self, mode, eps=1e-08, **params):\r\n",
        "        \r\n",
        "        self.params = params\r\n",
        "        self.eps = eps\r\n",
        "\r\n",
        "        mode_dict = {\r\n",
        "            'dEI_GP': self.dEI_GP,\r\n",
        "            'dERM_GP': self.dERM_GP\r\n",
        "        }\r\n",
        "\r\n",
        "        self.f = mode_dict[mode]\r\n",
        "    \r\n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\r\n",
        "        \r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\r\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\r\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\r\n",
        "        \r\n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\r\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\r\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\r\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\r\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\r\n",
        "            \r\n",
        "        return f, df, d2f\r\n",
        "\r\n",
        "    def dERM_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "        z = -1 * (y_global_orig - mean - self.eps) / (std + self.eps)\r\n",
        "        \r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\r\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\r\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\r\n",
        "        \r\n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\r\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\r\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\r\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\r\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\r\n",
        "            \r\n",
        "        return f, df, d2f\r\n",
        "    \r\n",
        "    def _eval(self, tau, mean, std):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, **self.params)\r\n",
        "    \r\n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, **self.params)\r\n",
        "\r\n",
        "    def d_eval_stp(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0, **self.params)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SdY8rHwtRJ_"
      },
      "source": [
        "### Surrogate derivatives: \r\n",
        "\r\n",
        "from scipy.linalg import cholesky, solve\r\n",
        "\r\n",
        "class dGaussianProcess(GaussianProcess):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1\r\n",
        "    sigman = 1e-6\r\n",
        "\r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\r\n",
        "        L = cholesky(K).T\r\n",
        "        alpha = solve(L.T, solve(L, self.y))\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(self.L, Kstar.T)\r\n",
        "        dv = solve(self.L, dKstar.T)\r\n",
        "        d2v = solve(self.L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -2 * np.dot(dv.T, v)\r\n",
        "        dvdv = np.dot(dv.T, dv)\r\n",
        "        d2s = -2 * (dvdv + d2v)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha)\r\n",
        "        d2m = np.dot(d2Kstar, alpha)\r\n",
        "        return ds, dm, dvdv, d2v, d2m\r\n",
        "\r\n",
        "class dtStudentProcess(tStudentProcess):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1\r\n",
        "    sigman = 1e-6\r\n",
        "    \r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\r\n",
        "        L = cholesky(self.K11).T\r\n",
        "        alpha = solve(L.T, solve(L, self.y))\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(L, Kstar.T)\r\n",
        "        dv = solve(L, dKstar.T)\r\n",
        "        d2v = solve(L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -2 * np.dot(dv.T, v)\r\n",
        "        dvdv = np.dot(dv.T, dv)\r\n",
        "        d2s = -2 * (dvdv + d2v)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha)\r\n",
        "        d2m = np.dot(d2Kstar, alpha)\r\n",
        "        return ds, dm, dvdv, d2v, d2m"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNv4Z0EqtUi2"
      },
      "source": [
        "class dGPGO(GPGO):  \r\n",
        "    n_start = n_start_AcqFunc\r\n",
        "    eps = 1e-08\r\n",
        "        \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7vea4uj-GOi"
      },
      "source": [
        "## dGPGO_stp - BayesOpt derivatives' class: Student's-t\r\n",
        "\r\n",
        "class dGPGO_stp(GPGO):  \r\n",
        "    n_start = 100\r\n",
        "        \r\n",
        "    def func_stp(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[0]\r\n",
        "        df = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq_stp(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func_stp,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq_stp()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expMSaSUtcB-"
      },
      "source": [
        "### d2GPGO - BayesOpt class: \r\n",
        "\r\n",
        "class d2GPGO(GPGO):  \r\n",
        "    n_start = 100\r\n",
        "    p = np.full((n_start,1),1)\r\n",
        "    \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2s, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "    \r\n",
        "    def hessp_nonzero(self, xnew, p):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2s, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        df2 = np.empty((self.n_start,))\r\n",
        "        df2 = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[2]\r\n",
        "        H2 = np.empty((self.n_start,))\r\n",
        "        df2 = np.asarray(df2)\r\n",
        "        p = np.asarray(p)\r\n",
        "        H2 = np.multiply(df2,p)\r\n",
        "        return H2\r\n",
        "    \r\n",
        "    def d_optimizeAcq(self, method='Newton-CG', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 hessp = self.hessp_nonzero,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        \r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.best = x_best[np.argmin(f_best)]     \r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umKPgDuEtcxH",
        "outputId": "f0ca0ca3-dab1-418f-c40a-696fe1763172"
      },
      "source": [
        "start_lose = time.time()\r\n",
        "start_lose"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1615129800.7592247"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfEuNIqvtdSv",
        "outputId": "05de3d15-80eb-4d8c-f311-a055f7bc953a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_loser_1 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_1 = dGPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.45944904 -1.35549029]. \t  -245.71064611316496 \t -108.5713485785257\n",
            "init   \t [-0.26190226  1.10289909]. \t  -108.5713485785257 \t -108.5713485785257\n",
            "init   \t [-0.83834755 -1.43702853]. \t  -461.2775269355244 \t -108.5713485785257\n",
            "init   \t [-1.95592878 -0.32676048]. \t  -1732.9949421003257 \t -108.5713485785257\n",
            "init   \t [-1.07035795 -0.66496024]. \t  -332.12317010404104 \t -108.5713485785257\n",
            "1      \t [-0.16418989 -0.35655496]. \t  \u001b[92m-16.063581388629835\u001b[0m \t -16.063581388629835\n",
            "2      \t [-0.46725478 -0.31805013]. \t  -30.922881903337014 \t -16.063581388629835\n",
            "3      \t [0.36410567 1.99516816]. \t  -347.33045841529025 \t -16.063581388629835\n",
            "4      \t [ 1.88433773 -0.57897707]. \t  -1706.2290269490966 \t -16.063581388629835\n",
            "5      \t [-0.2797263  -0.65083633]. \t  -54.79392119360623 \t -16.063581388629835\n",
            "6      \t [-0.36967736 -0.35543443]. \t  -26.09184200624848 \t -16.063581388629835\n",
            "7      \t [ 0.02629338 -0.17265926]. \t  \u001b[92m-3.953147754267091\u001b[0m \t -3.953147754267091\n",
            "8      \t [0.115618   0.25596482]. \t  -6.667476486235974 \t -3.953147754267091\n",
            "9      \t [ 0.08916241 -0.04709341]. \t  \u001b[92m-1.132602051393015\u001b[0m \t -1.132602051393015\n",
            "10     \t [-1.62320868  1.91504656]. \t  -58.686647613096916 \t -1.132602051393015\n",
            "11     \t [-0.08235995  0.39477545]. \t  -16.22530470798019 \t -1.132602051393015\n",
            "12     \t [-0.73826796  0.85450126]. \t  -12.598228932592956 \t -1.132602051393015\n",
            "13     \t [-1.9487096   1.65113376]. \t  -469.37043159536927 \t -1.132602051393015\n",
            "14     \t [-1.12645133  1.928153  ]. \t  -47.984224062261326 \t -1.132602051393015\n",
            "15     \t [-0.52027153  0.35508588]. \t  -3.0236191451316823 \t -1.132602051393015\n",
            "16     \t [-1.38434577  2.00911604]. \t  -6.544486382798326 \t -1.132602051393015\n",
            "17     \t [0.11931237 0.03484829]. \t  \u001b[92m-0.8180996773800518\u001b[0m \t -0.8180996773800518\n",
            "18     \t [-0.28710502  1.95755726]. \t  -353.26712743307354 \t -0.8180996773800518\n",
            "19     \t [-2.01047749 -0.74297611]. \t  -2298.681505002316 \t -0.8180996773800518\n",
            "20     \t [1.82737506 1.96720994]. \t  -188.947559440074 \t -0.8180996773800518\n",
            "21     \t [-0.01733915  0.01617608]. \t  -1.0601818852729488 \t -0.8180996773800518\n",
            "22     \t [-1.21732315  1.76122934]. \t  -12.72037112638002 \t -0.8180996773800518\n",
            "23     \t [-0.95070309  1.48236624]. \t  -37.27492459488175 \t -0.8180996773800518\n",
            "24     \t [-1.41896608  0.20289253]. \t  -333.66856468368997 \t -0.8180996773800518\n",
            "25     \t [-0.54492688 -0.24007982]. \t  -31.226397664342578 \t -0.8180996773800518\n",
            "26     \t [-0.7271155   0.48673977]. \t  -3.1589684286668533 \t -0.8180996773800518\n",
            "27     \t [-0.05750687 -0.24363503]. \t  -7.216359370591485 \t -0.8180996773800518\n",
            "28     \t [1.97325415 0.90319927]. \t  -895.2757944749616 \t -0.8180996773800518\n",
            "29     \t [1.27468998 0.27710306]. \t  -181.71346766613016 \t -0.8180996773800518\n",
            "30     \t [0.75854931 0.56201665]. \t  \u001b[92m-0.07620196038707869\u001b[0m \t -0.07620196038707869\n",
            "31     \t [0.69211926 0.40956748]. \t  -0.5772817795001087 \t -0.07620196038707869\n",
            "32     \t [0.9018657  0.56261731]. \t  -6.296907159700716 \t -0.07620196038707869\n",
            "33     \t [ 0.66521893 -0.23347934]. \t  -45.80907842624932 \t -0.07620196038707869\n",
            "34     \t [-0.08322602 -1.64656607]. \t  -274.5771703903309 \t -0.07620196038707869\n",
            "35     \t [ 0.74271154 -1.83825245]. \t  -571.2154314512803 \t -0.07620196038707869\n",
            "36     \t [0.03658015 0.7696289 ]. \t  -59.95525251309522 \t -0.07620196038707869\n",
            "37     \t [0.26039215 0.32972883]. \t  -7.407477600251035 \t -0.07620196038707869\n",
            "38     \t [ 0.37248426 -1.61622801]. \t  -308.38663721223264 \t -0.07620196038707869\n",
            "39     \t [-1.65372101 -1.45526157]. \t  -1762.698113551249 \t -0.07620196038707869\n",
            "40     \t [-0.47088606 -0.46262524]. \t  -48.99821853353074 \t -0.07620196038707869\n",
            "41     \t [-0.3125598   0.45186613]. \t  -14.266629745783442 \t -0.07620196038707869\n",
            "42     \t [0.38598463 0.51471049]. \t  -13.752591818790338 \t -0.07620196038707869\n",
            "43     \t [0.72190023 0.52150295]. \t  -0.07735265867424165 \t -0.07620196038707869\n",
            "44     \t [ 1.73016164 -1.2227373 ]. \t  -1778.1645165098817 \t -0.07620196038707869\n",
            "45     \t [ 1.64398144 -1.05553562]. \t  -1412.8294011552719 \t -0.07620196038707869\n",
            "46     \t [0.27098297 1.72492339]. \t  -273.27392385300084 \t -0.07620196038707869\n",
            "47     \t [-1.96679809  0.3621967 ]. \t  -1238.0742281602288 \t -0.07620196038707869\n",
            "48     \t [0.6870367  0.46579552]. \t  -0.10181973365367795 \t -0.07620196038707869\n",
            "49     \t [1.23422419 1.1855391 ]. \t  -11.463734691460713 \t -0.07620196038707869\n",
            "50     \t [1.16602856 1.44326202]. \t  -0.7271208806517206 \t -0.07620196038707869\n",
            "51     \t [1.01272581 1.19254546]. \t  -2.78678767066262 \t -0.07620196038707869\n",
            "52     \t [-0.08642683  1.66645759]. \t  -276.40443835653934 \t -0.07620196038707869\n",
            "53     \t [ 0.2793997 -1.2676269]. \t  -181.60771680116625 \t -0.07620196038707869\n",
            "54     \t [-1.52130991  1.28473634]. \t  -112.37440040662757 \t -0.07620196038707869\n",
            "55     \t [ 0.89555147 -0.5238285 ]. \t  -175.79632639733111 \t -0.07620196038707869\n",
            "56     \t [0.59567427 0.35122253]. \t  -0.16477912069810416 \t -0.07620196038707869\n",
            "57     \t [-0.2340737   0.17313989]. \t  -2.923595782136139 \t -0.07620196038707869\n",
            "58     \t [-1.13926972  1.3171951 ]. \t  -4.613568196613076 \t -0.07620196038707869\n",
            "59     \t [1.1589209  1.28792886]. \t  -0.329615494928996 \t -0.07620196038707869\n",
            "60     \t [1.54581875 1.85457943]. \t  -28.917870132042722 \t -0.07620196038707869\n",
            "61     \t [1.34128166 1.87657041]. \t  -0.717623902851532 \t -0.07620196038707869\n",
            "62     \t [-1.24554527 -0.53065282]. \t  -438.5297977056196 \t -0.07620196038707869\n",
            "63     \t [1.18809787 1.37499203]. \t  -0.1692234542156798 \t -0.07620196038707869\n",
            "64     \t [1.7002555  0.27668319]. \t  -683.8869845466562 \t -0.07620196038707869\n",
            "65     \t [1.99383884 1.10766416]. \t  -823.3747770666051 \t -0.07620196038707869\n",
            "66     \t [1.37808234 1.81106111]. \t  -0.9182236498656533 \t -0.07620196038707869\n",
            "67     \t [0.35317376 0.11662955]. \t  -0.4249486786488104 \t -0.07620196038707869\n",
            "68     \t [1.03606703 1.05837255]. \t  \u001b[92m-0.023988235066214003\u001b[0m \t -0.023988235066214003\n",
            "69     \t [-1.50630826 -0.16398703]. \t  -598.206928062259 \t -0.023988235066214003\n",
            "70     \t [ 1.68333128 -1.97788986]. \t  -2315.5144414799224 \t -0.023988235066214003\n",
            "71     \t [1.34551784 1.78423272]. \t  -0.1879508149799955 \t -0.023988235066214003\n",
            "72     \t [1.29225532 1.67688582]. \t  -0.09026012074033712 \t -0.023988235066214003\n",
            "73     \t [ 0.74592129 -0.2301352 ]. \t  -61.928093339755414 \t -0.023988235066214003\n",
            "74     \t [-1.87109385  1.76343759]. \t  -310.15278416471114 \t -0.023988235066214003\n",
            "75     \t [-0.93266155 -1.55003675]. \t  -589.324028504928 \t -0.023988235066214003\n",
            "76     \t [-1.68024478  0.26353533]. \t  -662.383562847581 \t -0.023988235066214003\n",
            "77     \t [0.4746603  0.23465048]. \t  -0.28472045823755326 \t -0.023988235066214003\n",
            "78     \t [1.32834068 1.7406204 ]. \t  -0.1647784105535939 \t -0.023988235066214003\n",
            "79     \t [-0.05339432 -1.69392365]. \t  -289.01404418626623 \t -0.023988235066214003\n",
            "80     \t [-0.94283968 -1.63797583]. \t  -642.3083516225248 \t -0.023988235066214003\n",
            "81     \t [ 1.05683909 -0.15691659]. \t  -162.26635865176712 \t -0.023988235066214003\n",
            "82     \t [0.55274964 0.23507388]. \t  -0.6964698420642352 \t -0.023988235066214003\n",
            "83     \t [1.3296437  0.28016115]. \t  -221.46093307441774 \t -0.023988235066214003\n",
            "84     \t [-2.02364883 -1.77656229]. \t  -3456.8483534725187 \t -0.023988235066214003\n",
            "85     \t [-1.27770636  1.26153307]. \t  -18.95208179070812 \t -0.023988235066214003\n",
            "86     \t [0.51038623 0.20598007]. \t  -0.5368996443132265 \t -0.023988235066214003\n",
            "87     \t [ 0.50612368 -1.16357471]. \t  -201.8089132793395 \t -0.023988235066214003\n",
            "88     \t [-0.88460821  0.61773558]. \t  -6.267523692053022 \t -0.023988235066214003\n",
            "89     \t [0.50228244 0.23501356]. \t  -0.2775621902900523 \t -0.023988235066214003\n",
            "90     \t [-0.44738973 -1.32343723]. \t  -234.22904880534765 \t -0.023988235066214003\n",
            "91     \t [-0.63994283  0.08808947]. \t  -13.021610016776167 \t -0.023988235066214003\n",
            "92     \t [1.0383734  1.04010109]. \t  -0.1467723720377292 \t -0.023988235066214003\n",
            "93     \t [1.06284503 1.13140222]. \t  \u001b[92m-0.004260196964201298\u001b[0m \t -0.004260196964201298\n",
            "94     \t [-1.15969297  0.44127987]. \t  -86.31500129402497 \t -0.004260196964201298\n",
            "95     \t [-0.60324118  1.28584672]. \t  -87.56897214586364 \t -0.004260196964201298\n",
            "96     \t [0.40836656 0.16078733]. \t  -0.35360128622997594 \t -0.004260196964201298\n",
            "97     \t [1.4073171 2.0264264]. \t  -0.3764504237923546 \t -0.004260196964201298\n",
            "98     \t [1.35693448 1.64384049]. \t  -4.0252899496503955 \t -0.004260196964201298\n",
            "99     \t [0.88133044 0.79074803]. \t  -0.03369559239437238 \t -0.004260196964201298\n",
            "100    \t [0.90711595 0.79505496]. \t  -0.08593583231557217 \t -0.004260196964201298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j6UBhkdtdVe",
        "outputId": "408888c5-0e5a-4f24-cd51-1c1ff2e325ae"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_loser_2 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_2 = dGPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.44173258 -1.74529086]. \t  -1462.4037722675753 \t -1.3013277264983028\n",
            "init   \t [ 1.6176405  -0.26012243]. \t  -828.0271838688934 \t -1.3013277264983028\n",
            "init   \t [-1.525032    0.31071385]. \t  -412.40181251788414 \t -1.3013277264983028\n",
            "init   \t [ 1.39456889 -0.26574622]. \t  -488.8170397507174 \t -1.3013277264983028\n",
            "init   \t [0.80244966 0.75627765]. \t  -1.3013277264983028 \t -1.3013277264983028\n",
            "1      \t [ 0.33906095 -0.26492231]. \t  -14.868074192108132 \t -1.3013277264983028\n",
            "2      \t [ 0.63761197 -0.00466461]. \t  -17.040990776479866 \t -1.3013277264983028\n",
            "3      \t [0.97082692 1.75663077]. \t  -66.28094057138775 \t -1.3013277264983028\n",
            "4      \t [ 0.2824696  -0.71012934]. \t  -62.91195966330248 \t -1.3013277264983028\n",
            "5      \t [0.83023894 1.20141882]. \t  -26.25572497486854 \t -1.3013277264983028\n",
            "6      \t [1.1566147  1.12418388]. \t  -4.5859003313837405 \t -1.3013277264983028\n",
            "7      \t [-0.74554464 -0.41245325]. \t  -96.80549233062297 \t -1.3013277264983028\n",
            "8      \t [0.97465444 0.82884679]. \t  -1.4672722002650613 \t -1.3013277264983028\n",
            "9      \t [1.61837733 2.00595213]. \t  -37.98296228972681 \t -1.3013277264983028\n",
            "10     \t [0.82477171 0.79542092]. \t  -1.3571765467732686 \t -1.3013277264983028\n",
            "11     \t [-0.29464919  0.72430263]. \t  -42.31476276292781 \t -1.3013277264983028\n",
            "12     \t [0.8943991  0.84073397]. \t  \u001b[92m-0.1774868517993569\u001b[0m \t -0.1774868517993569\n",
            "13     \t [ 0.99179266 -0.2276715 ]. \t  -146.7306964172385 \t -0.1774868517993569\n",
            "14     \t [-0.56286493 -1.72824109]. \t  -420.66877543293583 \t -0.1774868517993569\n",
            "15     \t [-0.10093385 -0.19015513]. \t  -5.225777817449663 \t -0.1774868517993569\n",
            "16     \t [-1.06905002  1.88119765]. \t  -58.79404261214023 \t -0.1774868517993569\n",
            "17     \t [-0.34106247 -0.75024631]. \t  -76.89279157666216 \t -0.1774868517993569\n",
            "18     \t [0.17184324 0.13281523]. \t  -1.7526255309303058 \t -0.1774868517993569\n",
            "19     \t [0.79806246 0.64724868]. \t  \u001b[92m-0.051480636842797745\u001b[0m \t -0.051480636842797745\n",
            "20     \t [-0.74325245 -0.41707328]. \t  -97.03146624563828 \t -0.051480636842797745\n",
            "21     \t [ 0.14093629 -0.04595891]. \t  -1.1712433634712784 \t -0.051480636842797745\n",
            "22     \t [0.81522103 0.6769212 ]. \t  \u001b[92m-0.04936063201465491\u001b[0m \t -0.04936063201465491\n",
            "23     \t [0.72451938 0.50584602]. \t  -0.11230301877260293 \t -0.04936063201465491\n",
            "24     \t [ 0.56866235 -0.90474152]. \t  -151.01353122800091 \t -0.04936063201465491\n",
            "25     \t [0.44397812 0.30176866]. \t  -1.4043662848067613 \t -0.04936063201465491\n",
            "26     \t [0.88264132 0.79574464]. \t  \u001b[92m-0.04162511363239053\u001b[0m \t -0.04162511363239053\n",
            "27     \t [0.68156913 0.41441561]. \t  -0.35260827284530105 \t -0.04162511363239053\n",
            "28     \t [0.51158158 0.29053007]. \t  -0.3215792826944577 \t -0.04162511363239053\n",
            "29     \t [-1.69076328  0.3014827 ]. \t  -661.1662444475091 \t -0.04162511363239053\n",
            "30     \t [-0.92530255  0.67130482]. \t  -7.1248506505111795 \t -0.04162511363239053\n",
            "31     \t [-0.43040611  1.99670407]. \t  -330.18285815833576 \t -0.04162511363239053\n",
            "32     \t [-0.6314424   0.18749216]. \t  -7.123303427193068 \t -0.04162511363239053\n",
            "33     \t [-1.58293138  2.02745107]. \t  -29.54103678051261 \t -0.04162511363239053\n",
            "34     \t [0.32747641 0.09066291]. \t  -0.4797706335825145 \t -0.04162511363239053\n",
            "35     \t [1.46932108 1.16443248]. \t  -99.11770622554333 \t -0.04162511363239053\n",
            "36     \t [0.93821415 0.86251887]. \t  \u001b[92m-0.035241894498354374\u001b[0m \t -0.035241894498354374\n",
            "37     \t [-0.98007518  1.8084631 ]. \t  -75.81680806136724 \t -0.035241894498354374\n",
            "38     \t [ 1.32130641 -1.83018897]. \t  -1278.909164298691 \t -0.035241894498354374\n",
            "39     \t [1.16111548 1.37459576]. \t  -0.09568906685019224 \t -0.035241894498354374\n",
            "40     \t [-0.79520927  0.44741328]. \t  -6.643223185924455 \t -0.035241894498354374\n",
            "41     \t [-1.33829421  1.62006885]. \t  -8.39043860228765 \t -0.035241894498354374\n",
            "42     \t [-1.0567372  -0.45304023]. \t  -250.63656771094728 \t -0.035241894498354374\n",
            "43     \t [ 2.02301052 -1.55702126]. \t  -3192.836445995665 \t -0.035241894498354374\n",
            "44     \t [1.0177217  1.05191344]. \t  \u001b[92m-0.026415621071020308\u001b[0m \t -0.026415621071020308\n",
            "45     \t [-1.15938681  1.23036401]. \t  -5.958308524783243 \t -0.026415621071020308\n",
            "46     \t [1.16378936 1.38101067]. \t  -0.09760953665351375 \t -0.026415621071020308\n",
            "47     \t [1.50263101 1.01997912]. \t  -153.4974360773491 \t -0.026415621071020308\n",
            "48     \t [1.84746402 0.69827684]. \t  -737.75732880183 \t -0.026415621071020308\n",
            "49     \t [-0.79787615 -1.07553245]. \t  -296.3742858915344 \t -0.026415621071020308\n",
            "50     \t [1.32469683 1.95092639]. \t  -3.9511331739041573 \t -0.026415621071020308\n",
            "51     \t [1.35307803 1.778157  ]. \t  -0.4020048738638888 \t -0.026415621071020308\n",
            "52     \t [-0.75825752  1.91406987]. \t  -182.4144761163101 \t -0.026415621071020308\n",
            "53     \t [-0.80497207  1.85963067]. \t  -150.06764795856756 \t -0.026415621071020308\n",
            "54     \t [1.26328001 1.56576959]. \t  -0.1599583044778113 \t -0.026415621071020308\n",
            "55     \t [-1.90268391  0.96809939]. \t  -711.7925471201771 \t -0.026415621071020308\n",
            "56     \t [1.25385    1.52764835]. \t  -0.26238898360948615 \t -0.026415621071020308\n",
            "57     \t [0.3879084  1.76656536]. \t  -261.55013351865534 \t -0.026415621071020308\n",
            "58     \t [-0.18045394 -1.23520567]. \t  -162.11737031666118 \t -0.026415621071020308\n",
            "59     \t [1.54251158 0.31288154]. \t  -427.32018454078906 \t -0.026415621071020308\n",
            "60     \t [1.85648927 1.71578724]. \t  -300.2883765100222 \t -0.026415621071020308\n",
            "61     \t [-1.24963833 -1.51014022]. \t  -948.6171836162908 \t -0.026415621071020308\n",
            "62     \t [-1.69061122 -0.69050267]. \t  -1266.5445430312402 \t -0.026415621071020308\n",
            "63     \t [ 0.2998765  -1.89503628]. \t  -394.4976661783674 \t -0.026415621071020308\n",
            "64     \t [1.19907741 1.45971508]. \t  -0.08771754024096773 \t -0.026415621071020308\n",
            "65     \t [ 0.12167434 -1.732526  ]. \t  -306.08789281014197 \t -0.026415621071020308\n",
            "66     \t [0.80727433 0.58756804]. \t  -0.4483292996493487 \t -0.026415621071020308\n",
            "67     \t [ 0.32900738 -0.38466485]. \t  -24.746327383062937 \t -0.026415621071020308\n",
            "68     \t [ 0.88967876 -1.29655175]. \t  -436.0199975541458 \t -0.026415621071020308\n",
            "69     \t [1.1981682  1.44214661]. \t  -0.04354724987751669 \t -0.026415621071020308\n",
            "70     \t [0.79004918 0.58112711]. \t  -0.22941477244356115 \t -0.026415621071020308\n",
            "71     \t [-1.84477672  1.67970458]. \t  -305.1367924344605 \t -0.026415621071020308\n",
            "72     \t [1.24686003 1.56358902]. \t  -0.06891275469397362 \t -0.026415621071020308\n",
            "73     \t [-0.84108376  1.7411032 ]. \t  -110.23929448243285 \t -0.026415621071020308\n",
            "74     \t [0.75483372 0.56410819]. \t  -0.06331658876375948 \t -0.026415621071020308\n",
            "75     \t [-1.57882819 -0.776232  ]. \t  -1075.2409896985714 \t -0.026415621071020308\n",
            "76     \t [ 1.34076278 -1.96365453]. \t  -1414.853401573814 \t -0.026415621071020308\n",
            "77     \t [0.75337646 0.53567887]. \t  -0.16256640192346394 \t -0.026415621071020308\n",
            "78     \t [1.28609931 1.66618478]. \t  -0.09657462710522058 \t -0.026415621071020308\n",
            "79     \t [-2.0382743   0.10376635]. \t  -1650.1257476316432 \t -0.026415621071020308\n",
            "80     \t [1.21830229 1.46689116]. \t  -0.07782520260960916 \t -0.026415621071020308\n",
            "81     \t [-0.19118443  0.05462748]. \t  -1.4515944904185003 \t -0.026415621071020308\n",
            "82     \t [0.81707117 0.64759812]. \t  -0.07349168672564534 \t -0.026415621071020308\n",
            "83     \t [-0.35186844  1.22744145]. \t  -123.62747676309935 \t -0.026415621071020308\n",
            "84     \t [0.02188484 1.06933684]. \t  -115.20242968324177 \t -0.026415621071020308\n",
            "85     \t [-1.41357471 -1.37068528]. \t  -1140.7597357766265 \t -0.026415621071020308\n",
            "86     \t [0.0417646  0.31435926]. \t  -10.69102757382465 \t -0.026415621071020308\n",
            "87     \t [1.29509121 1.6760604 ]. \t  -0.08722302883666529 \t -0.026415621071020308\n",
            "88     \t [ 0.78679977 -0.75523225]. \t  -188.91169055958073 \t -0.026415621071020308\n",
            "89     \t [ 2.04698854 -0.95916468]. \t  -2652.652812167113 \t -0.026415621071020308\n",
            "90     \t [-0.86567468 -1.04638044]. \t  -325.96084184091194 \t -0.026415621071020308\n",
            "91     \t [-0.31919473 -0.90157516]. \t  -102.43355936094454 \t -0.026415621071020308\n",
            "92     \t [1.78776824 1.57337201]. \t  -263.9501550654721 \t -0.026415621071020308\n",
            "93     \t [1.28798869 1.65471811]. \t  -0.08469875407276624 \t -0.026415621071020308\n",
            "94     \t [0.82881531 0.64333576]. \t  -0.21939204741303808 \t -0.026415621071020308\n",
            "95     \t [-0.20726081  0.57599578]. \t  -29.87050810289775 \t -0.026415621071020308\n",
            "96     \t [1.37586746 1.95449868]. \t  -0.5193465183474737 \t -0.026415621071020308\n",
            "97     \t [0.41936547 0.26065522]. \t  -1.056033835945708 \t -0.026415621071020308\n",
            "98     \t [-1.70982569  0.65088814]. \t  -523.821384062963 \t -0.026415621071020308\n",
            "99     \t [ 0.23906176 -1.33358848]. \t  -193.99452545400055 \t -0.026415621071020308\n",
            "100    \t [0.65717454 1.15221083]. \t  -52.00541436633597 \t -0.026415621071020308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yw0oJ-TpiN_",
        "outputId": "47a14225-d450-4272-9de6-71171371c776"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_loser_3 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_3 = dGPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.03045369 -1.60050678]. \t  -708.8071948417274 \t -1.118465165857483\n",
            "init   \t [-0.05684724 -0.0006915 ]. \t  -1.118465165857483 \t -1.118465165857483\n",
            "init   \t [ 0.64352256 -1.08181622]. \t  -223.90997740226942 \t -1.118465165857483\n",
            "init   \t [ 0.46200787 -1.55790376]. \t  -314.05929962166397 \t -1.118465165857483\n",
            "init   \t [ 0.86918061 -0.52199202]. \t  -163.20929387450548 \t -1.118465165857483\n",
            "1      \t [-0.47032198 -0.98469614]. \t  -147.5810642040628 \t -1.118465165857483\n",
            "2      \t [ 0.19510124 -0.39571949]. \t  -19.464716969441064 \t -1.118465165857483\n",
            "3      \t [-0.48374833  1.21060744]. \t  -97.57528626382148 \t -1.118465165857483\n",
            "4      \t [-0.04142004  0.27493655]. \t  -8.549523360099188 \t -1.118465165857483\n",
            "5      \t [-0.75508265  0.36868271]. \t  -7.139214187351264 \t -1.118465165857483\n",
            "6      \t [-0.37659969  0.1941843 ]. \t  -2.169151970106639 \t -1.118465165857483\n",
            "7      \t [-2.00120028  0.75454632]. \t  -1065.4237754889868 \t -1.118465165857483\n",
            "8      \t [0.29432639 0.0138672 ]. \t  \u001b[92m-1.0273890250980249\u001b[0m \t -1.0273890250980249\n",
            "9      \t [-1.93971307  0.59809255]. \t  -1009.9810140546972 \t -1.0273890250980249\n",
            "10     \t [-0.49691934  0.33305786]. \t  -2.9825885043834797 \t -1.0273890250980249\n",
            "11     \t [0.11487674 0.05912955]. \t  \u001b[92m-0.9944261838126743\u001b[0m \t -0.9944261838126743\n",
            "12     \t [1.20192189 0.99338095]. \t  -20.402100015337346 \t -0.9944261838126743\n",
            "13     \t [0.74861108 0.64386018]. \t  \u001b[92m-0.7594469688952155\u001b[0m \t -0.7594469688952155\n",
            "14     \t [2.048 2.048]. \t  -461.7603900415353 \t -0.7594469688952155\n",
            "15     \t [1.76285049 0.63025447]. \t  -614.3267727264987 \t -0.7594469688952155\n",
            "16     \t [0.96375751 1.56754617]. \t  -40.79733469171215 \t -0.7594469688952155\n",
            "17     \t [1.03601665 1.15966006]. \t  \u001b[92m-0.7465766926129433\u001b[0m \t -0.7465766926129433\n",
            "18     \t [0.51055878 0.3178585 ]. \t  \u001b[92m-0.5666020698356665\u001b[0m \t -0.5666020698356665\n",
            "19     \t [0.95991252 0.86549378]. \t  \u001b[92m-0.3145160316585125\u001b[0m \t -0.3145160316585125\n",
            "20     \t [0.93855127 0.88859853]. \t  \u001b[92m-0.009735844765363858\u001b[0m \t -0.009735844765363858\n",
            "21     \t [-1.54677484 -1.42960587]. \t  -1467.344875545267 \t -0.009735844765363858\n",
            "22     \t [1.13269135 1.26759145]. \t  -0.04131755720586214 \t -0.009735844765363858\n",
            "23     \t [0.38547819 0.21072274]. \t  -0.7636421454044093 \t -0.009735844765363858\n",
            "24     \t [-0.51379233  0.32424217]. \t  -2.6546893059535472 \t -0.009735844765363858\n",
            "25     \t [-0.53104766 -1.92376744]. \t  -488.89022912611455 \t -0.009735844765363858\n",
            "26     \t [0.98391655 0.95875632]. \t  \u001b[92m-0.008973762773523547\u001b[0m \t -0.008973762773523547\n",
            "27     \t [1.04668274 1.04339526]. \t  -0.2741362116411211 \t -0.008973762773523547\n",
            "28     \t [1.08242906 1.20393354]. \t  -0.11100004184662911 \t -0.008973762773523547\n",
            "29     \t [-0.33250964 -0.39777081]. \t  -27.615873630731052 \t -0.008973762773523547\n",
            "30     \t [1.07504385 1.15794208]. \t  \u001b[92m-0.006125661922954601\u001b[0m \t -0.006125661922954601\n",
            "31     \t [1.06176258 1.1392823 ]. \t  -0.018077009257239162 \t -0.006125661922954601\n",
            "32     \t [ 0.52575715 -0.44393616]. \t  -52.11629023706623 \t -0.006125661922954601\n",
            "33     \t [-0.28444708 -0.11423451]. \t  -5.457947591478899 \t -0.006125661922954601\n",
            "34     \t [-1.28321751  0.98886312]. \t  -48.48106860740266 \t -0.006125661922954601\n",
            "35     \t [-1.24396408  1.61208391]. \t  -5.4531725633735615 \t -0.006125661922954601\n",
            "36     \t [-1.1282022  1.2002165]. \t  -5.056665030592668 \t -0.006125661922954601\n",
            "37     \t [0.51069402 0.31521316]. \t  -0.5354083256716863 \t -0.006125661922954601\n",
            "38     \t [-1.31746446  1.97848047]. \t  -11.264265296218156 \t -0.006125661922954601\n",
            "39     \t [ 1.7242307  -0.10450261]. \t  -947.6092010408778 \t -0.006125661922954601\n",
            "40     \t [-0.84831635  0.64795332]. \t  -3.9301804066432773 \t -0.006125661922954601\n",
            "41     \t [ 0.69436079 -1.97932458]. \t  -605.9726815722216 \t -0.006125661922954601\n",
            "42     \t [1.04146909 1.03730469]. \t  -0.22595209065942243 \t -0.006125661922954601\n",
            "43     \t [1.03419323 1.05570699]. \t  -0.02034766142365846 \t -0.006125661922954601\n",
            "44     \t [-1.42549166  1.8991367 ]. \t  -7.648979004438684 \t -0.006125661922954601\n",
            "45     \t [-0.25567451 -0.60585552]. \t  -46.63101579411829 \t -0.006125661922954601\n",
            "46     \t [1.62153724 1.77767364]. \t  -72.9271970608067 \t -0.006125661922954601\n",
            "47     \t [1.14907196 1.24566204]. \t  -0.5802961480496691 \t -0.006125661922954601\n",
            "48     \t [-0.14167054 -1.84125797]. \t  -347.75779274568123 \t -0.006125661922954601\n",
            "49     \t [1.1378145  1.25246812]. \t  -0.19668640197016854 \t -0.006125661922954601\n",
            "50     \t [0.08799344 0.14240424]. \t  -2.6451251959418776 \t -0.006125661922954601\n",
            "51     \t [-1.86423292  1.96849273]. \t  -235.27004747386428 \t -0.006125661922954601\n",
            "52     \t [-1.14761215 -0.61437996]. \t  -377.64036514287966 \t -0.006125661922954601\n",
            "53     \t [1.08165771 1.16910576]. \t  -0.006745005607482398 \t -0.006125661922954601\n",
            "54     \t [0.8669972  0.78603518]. \t  -0.13568908083082817 \t -0.006125661922954601\n",
            "55     \t [-1.17155945  1.30088295]. \t  -5.229309276185716 \t -0.006125661922954601\n",
            "56     \t [1.45691194 0.1092011 ]. \t  -405.5832286366827 \t -0.006125661922954601\n",
            "57     \t [0.67907159 0.23444503]. \t  -5.241975604509687 \t -0.006125661922954601\n",
            "58     \t [1.35079852 1.737578  ]. \t  -0.881328652925095 \t -0.006125661922954601\n",
            "59     \t [ 0.82191645 -0.39087832]. \t  -113.7579362329471 \t -0.006125661922954601\n",
            "60     \t [1.12075149 1.24792408]. \t  -0.021239188950571825 \t -0.006125661922954601\n",
            "61     \t [1.28769073 1.63789832]. \t  -0.12376855714934631 \t -0.006125661922954601\n",
            "62     \t [1.26599189 1.58100526]. \t  -0.11797189318565926 \t -0.006125661922954601\n",
            "63     \t [-1.89418234 -1.62116101]. \t  -2721.8358303256878 \t -0.006125661922954601\n",
            "64     \t [1.26056682 1.57394954]. \t  -0.09063321711292399 \t -0.006125661922954601\n",
            "65     \t [-0.33670335  1.87999363]. \t  -313.88298237180817 \t -0.006125661922954601\n",
            "66     \t [0.9427676  0.89343169]. \t  \u001b[92m-0.0054108541640425275\u001b[0m \t -0.0054108541640425275\n",
            "67     \t [0.20204697 0.38969872]. \t  -12.80815708669491 \t -0.0054108541640425275\n",
            "68     \t [1.50003658 0.3734927 ]. \t  -352.4191905238935 \t -0.0054108541640425275\n",
            "69     \t [-0.29527149  1.70664909]. \t  -263.9440407563509 \t -0.0054108541640425275\n",
            "70     \t [1.05446509 1.13105095]. \t  -0.03965521490137981 \t -0.0054108541640425275\n",
            "71     \t [-2.00348166  0.78190776]. \t  -1053.623340234436 \t -0.0054108541640425275\n",
            "72     \t [0.99356435 0.96510804]. \t  -0.04871492118397859 \t -0.0054108541640425275\n",
            "73     \t [0.17664647 0.85788352]. \t  -69.01781709494013 \t -0.0054108541640425275\n",
            "74     \t [0.99287609 0.96134527]. \t  -0.05986847761864009 \t -0.0054108541640425275\n",
            "75     \t [0.83096098 0.66821642]. \t  -0.07821283108443079 \t -0.0054108541640425275\n",
            "76     \t [0.88918186 0.77418265]. \t  -0.03937953542494712 \t -0.0054108541640425275\n",
            "77     \t [-0.76123896 -0.88227645]. \t  -216.7765445007591 \t -0.0054108541640425275\n",
            "78     \t [0.34422813 0.4797669 ]. \t  -13.481919620506176 \t -0.0054108541640425275\n",
            "79     \t [-0.3496611   1.99775184]. \t  -353.5674665544358 \t -0.0054108541640425275\n",
            "80     \t [0.92627648 0.83861158]. \t  -0.04298017453543089 \t -0.0054108541640425275\n",
            "81     \t [ 0.39394077 -1.05235458]. \t  -146.1835367236264 \t -0.0054108541640425275\n",
            "82     \t [1.31075222 1.7167406 ]. \t  -0.09674403923068131 \t -0.0054108541640425275\n",
            "83     \t [0.30981187 0.87976317]. \t  -61.907432340113104 \t -0.0054108541640425275\n",
            "84     \t [0.98621654 0.99359697]. \t  -0.04418048304730919 \t -0.0054108541640425275\n",
            "85     \t [ 1.51087379 -1.52307762]. \t  -1448.6854653269693 \t -0.0054108541640425275\n",
            "86     \t [ 0.04136922 -0.11238201]. \t  -2.2207039332235077 \t -0.0054108541640425275\n",
            "87     \t [ 2.00030005 -1.1263103 ]. \t  -2630.137104728218 \t -0.0054108541640425275\n",
            "88     \t [1.42267845 0.33450296]. \t  -285.62340308224816 \t -0.0054108541640425275\n",
            "89     \t [0.99560463 0.98250413]. \t  -0.007630923643092643 \t -0.0054108541640425275\n",
            "90     \t [ 0.85116426 -0.33166719]. \t  -111.56696821035466 \t -0.0054108541640425275\n",
            "91     \t [-0.55705471  1.56959819]. \t  -161.0051040713034 \t -0.0054108541640425275\n",
            "92     \t [1.34185465 0.11229551]. \t  -285.14525980682635 \t -0.0054108541640425275\n",
            "93     \t [ 0.34866918 -0.0873866 ]. \t  -4.7905262893301686 \t -0.0054108541640425275\n",
            "94     \t [0.88414022 0.7804986 ]. \t  -0.013568770554377583 \t -0.0054108541640425275\n",
            "95     \t [-1.0219889  -0.74463384]. \t  -324.17458727864135 \t -0.0054108541640425275\n",
            "96     \t [-1.97904207 -1.1990717 ]. \t  -2625.8920644414125 \t -0.0054108541640425275\n",
            "97     \t [0.88678747 0.76523517]. \t  -0.05757829370212176 \t -0.0054108541640425275\n",
            "98     \t [-0.84088724  1.7432666 ]. \t  -110.75478073212808 \t -0.0054108541640425275\n",
            "99     \t [ 0.25338118 -0.64784029]. \t  -51.25786484737453 \t -0.0054108541640425275\n",
            "100    \t [-1.33438605  0.77689646]. \t  -106.18865265597901 \t -0.0054108541640425275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zciLjgNltda2",
        "outputId": "8322ec80-62cf-4923-fdff-ee4dd8ceacda"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_loser_4 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_4 = dGPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.39002285 1.31602418]. \t  -38.11488737154776 \t -12.122423820878506\n",
            "init   \t [0.58248055 0.68494384]. \t  -12.122423820878506 \t -12.122423820878506\n",
            "init   \t [-1.88768295 -1.7790059 ]. \t  -2862.4120613667637 \t -12.122423820878506\n",
            "init   \t [-0.91545338 -0.75218738]. \t  -256.5560079635026 \t -12.122423820878506\n",
            "init   \t [-1.25504548 -1.24070756]. \t  -797.98450090518 \t -12.122423820878506\n",
            "1      \t [-0.07724009 -1.60241953]. \t  -259.85085705484 \t -12.122423820878506\n",
            "2      \t [0.50582555 1.31540574]. \t  -112.5080347477725 \t -12.122423820878506\n",
            "3      \t [1.38124473 0.39077049]. \t  -230.29442671843938 \t -12.122423820878506\n",
            "4      \t [1.97887267 1.83898365]. \t  -432.3317389383738 \t -12.122423820878506\n",
            "5      \t [0.84233915 0.9518293 ]. \t  \u001b[92m-5.89549813715424\u001b[0m \t -5.89549813715424\n",
            "6      \t [1.0556916  1.08009465]. \t  \u001b[92m-0.12136948890890542\u001b[0m \t -0.12136948890890542\n",
            "7      \t [-0.18308617 -1.34190572]. \t  -190.579434904392 \t -0.12136948890890542\n",
            "8      \t [1.05222041 1.13392694]. \t  \u001b[92m-0.0743321857598434\u001b[0m \t -0.0743321857598434\n",
            "9      \t [-0.13107399 -0.84726098]. \t  -76.00521578930851 \t -0.0743321857598434\n",
            "10     \t [ 0.37177912 -0.23395445]. \t  -14.2460219313725 \t -0.0743321857598434\n",
            "11     \t [1.057716   1.11667387]. \t  \u001b[92m-0.003767644645393139\u001b[0m \t -0.003767644645393139\n",
            "12     \t [-1.78443008  1.4136824 ]. \t  -321.22301968113425 \t -0.003767644645393139\n",
            "13     \t [ 1.48633283 -0.77964418]. \t  -893.5466803786309 \t -0.003767644645393139\n",
            "14     \t [0.25372219 0.06764368]. \t  -0.5579990295176024 \t -0.003767644645393139\n",
            "15     \t [0.3352143  0.15908183]. \t  -0.6601523487531911 \t -0.003767644645393139\n",
            "16     \t [0.29189838 0.01864103]. \t  -0.9444796585289186 \t -0.003767644645393139\n",
            "17     \t [-0.25533525 -0.95263609]. \t  -105.17410174748937 \t -0.003767644645393139\n",
            "18     \t [-1.22702171  0.03516077]. \t  -221.173568600708 \t -0.003767644645393139\n",
            "19     \t [-0.23860944 -1.69672867]. \t  -309.06759363010303 \t -0.003767644645393139\n",
            "20     \t [1.43289088 1.93136973]. \t  -1.6710779543216412 \t -0.003767644645393139\n",
            "21     \t [1.07946734 2.048     ]. \t  -77.93111768811308 \t -0.003767644645393139\n",
            "22     \t [-0.84972588  0.45175101]. \t  -10.726779538532615 \t -0.003767644645393139\n",
            "23     \t [-0.67310233  0.83762191]. \t  -17.587539091298325 \t -0.003767644645393139\n",
            "24     \t [1.28079797 1.82400176]. \t  -3.448213660017752 \t -0.003767644645393139\n",
            "25     \t [-0.81246147  0.63327293]. \t  -3.356951636991144 \t -0.003767644645393139\n",
            "26     \t [-0.71350369  0.59260605]. \t  -3.633629427727572 \t -0.003767644645393139\n",
            "27     \t [-0.75099975 -0.36212237]. \t  -88.83637904884068 \t -0.003767644645393139\n",
            "28     \t [-0.49949511  0.06754364]. \t  -5.559128586272248 \t -0.003767644645393139\n",
            "29     \t [-0.60496585  0.88407464]. \t  -29.41773966484951 \t -0.003767644645393139\n",
            "30     \t [-1.39511098 -1.07364137]. \t  -917.7620618067973 \t -0.003767644645393139\n",
            "31     \t [1.44351854 2.00896715]. \t  -0.7558928709733288 \t -0.003767644645393139\n",
            "32     \t [0.59607989 0.36541864]. \t  -0.17336742519789472 \t -0.003767644645393139\n",
            "33     \t [1.08616483 1.14274229]. \t  -0.14441133113668123 \t -0.003767644645393139\n",
            "34     \t [-0.96255788 -0.02208449]. \t  -93.83624093464555 \t -0.003767644645393139\n",
            "35     \t [1.37553468 1.96569251]. \t  -0.6826760286825106 \t -0.003767644645393139\n",
            "36     \t [1.22974468 1.44907648]. \t  -0.4521496094356799 \t -0.003767644645393139\n",
            "37     \t [2.04206484 1.39503674]. \t  -771.1439943589678 \t -0.003767644645393139\n",
            "38     \t [0.97817863 0.96346351]. \t  -0.0048719716429606475 \t -0.003767644645393139\n",
            "39     \t [ 1.35705638 -0.0890798 ]. \t  -372.880721250297 \t -0.003767644645393139\n",
            "40     \t [1.24019056 1.50517118]. \t  -0.165942040598321 \t -0.003767644645393139\n",
            "41     \t [1.32821362 1.71954028]. \t  -0.3067397043779968 \t -0.003767644645393139\n",
            "42     \t [-0.48263409 -0.17790873]. \t  -19.077516014712117 \t -0.003767644645393139\n",
            "43     \t [0.42976678 0.13721056]. \t  -0.550685701238865 \t -0.003767644645393139\n",
            "44     \t [ 1.9474966  -0.04153141]. \t  -1471.0637656833974 \t -0.003767644645393139\n",
            "45     \t [-0.63915313 -1.52523522]. \t  -376.62647934729335 \t -0.003767644645393139\n",
            "46     \t [0.59967352 0.26573875]. \t  -1.0414111735529614 \t -0.003767644645393139\n",
            "47     \t [ 0.16957906 -1.30817191]. \t  -179.42750437290832 \t -0.003767644645393139\n",
            "48     \t [-0.02410094 -0.51242169]. \t  -27.365943875563282 \t -0.003767644645393139\n",
            "49     \t [1.39700274 1.91958205]. \t  -0.2602327845162954 \t -0.003767644645393139\n",
            "50     \t [ 0.50426436 -1.43702746]. \t  -286.2987090020451 \t -0.003767644645393139\n",
            "51     \t [ 1.1489613  -0.40819872]. \t  -298.7280091007735 \t -0.003767644645393139\n",
            "52     \t [ 1.30973476 -1.95863185]. \t  -1349.9507199683198 \t -0.003767644645393139\n",
            "53     \t [-0.61209944  1.81939613]. \t  -211.32345951362092 \t -0.003767644645393139\n",
            "54     \t [0.87875814 0.75969817]. \t  -0.030368838694919645 \t -0.003767644645393139\n",
            "55     \t [1.08383245 1.1236438 ]. \t  -0.26762777491811934 \t -0.003767644645393139\n",
            "56     \t [1.41162934 1.99620681]. \t  -0.17067032175005348 \t -0.003767644645393139\n",
            "57     \t [1.9173578  0.61140306]. \t  -940.1769340714045 \t -0.003767644645393139\n",
            "58     \t [ 1.94491488 -1.47738315]. \t  -2767.733922681714 \t -0.003767644645393139\n",
            "59     \t [0.61405767 2.03367284]. \t  -274.5833037339654 \t -0.003767644645393139\n",
            "60     \t [-0.11891331  0.13662719]. \t  -2.752269057151917 \t -0.003767644645393139\n",
            "61     \t [-0.10786281  1.00686775]. \t  -100.27630475842861 \t -0.003767644645393139\n",
            "62     \t [1.97698163 1.70213882]. \t  -487.7381985083474 \t -0.003767644645393139\n",
            "63     \t [0.53734859 1.65356498]. \t  -186.48780951709446 \t -0.003767644645393139\n",
            "64     \t [-1.14297065  1.86649181]. \t  -35.964633455673976 \t -0.003767644645393139\n",
            "65     \t [-0.96246214 -0.02709468]. \t  -94.75376211511318 \t -0.003767644645393139\n",
            "66     \t [1.25126118 0.0571559 ]. \t  -227.6199426633258 \t -0.003767644645393139\n",
            "67     \t [-0.6937863  -1.40106436]. \t  -357.2133150989895 \t -0.003767644645393139\n",
            "68     \t [0.95549784 0.86604783]. \t  -0.2222068591982532 \t -0.003767644645393139\n",
            "69     \t [1.28225655 1.6353635 ]. \t  -0.08744509852916786 \t -0.003767644645393139\n",
            "70     \t [-1.80768649 -1.70179769]. \t  -2477.50407993395 \t -0.003767644645393139\n",
            "71     \t [ 0.30088331 -1.44707546]. \t  -236.9120555068589 \t -0.003767644645393139\n",
            "72     \t [0.80834368 0.6273686 ]. \t  -0.10459707926781348 \t -0.003767644645393139\n",
            "73     \t [0.61443815 1.81916878]. \t  -207.97967276656087 \t -0.003767644645393139\n",
            "74     \t [1.33216717 1.71759687]. \t  -0.4360620630523842 \t -0.003767644645393139\n",
            "75     \t [ 1.11552486 -0.4361074 ]. \t  -282.4224218613643 \t -0.003767644645393139\n",
            "76     \t [0.98267101 1.95397144]. \t  -97.67974536163663 \t -0.003767644645393139\n",
            "77     \t [-0.19775693  1.97289223]. \t  -375.38684111000026 \t -0.003767644645393139\n",
            "78     \t [-1.23013135  0.9694735 ]. \t  -34.53985251506643 \t -0.003767644645393139\n",
            "79     \t [ 1.6256887 -0.6896703]. \t  -1110.969801925948 \t -0.003767644645393139\n",
            "80     \t [0.93273914 0.86062769]. \t  -0.013312362338648295 \t -0.003767644645393139\n",
            "81     \t [2.04120098 0.77758227]. \t  -1149.5614262260044 \t -0.003767644645393139\n",
            "82     \t [ 0.3824406  -0.99422429]. \t  -130.45200646480208 \t -0.003767644645393139\n",
            "83     \t [-0.82565706 -0.22225113]. \t  -85.04752133209 \t -0.003767644645393139\n",
            "84     \t [1.24169556 0.05195789]. \t  -222.02371025462764 \t -0.003767644645393139\n",
            "85     \t [0.33993996 1.91512803]. \t  -324.2804853053065 \t -0.003767644645393139\n",
            "86     \t [0.98453716 0.98003788]. \t  -0.011740524325372582 \t -0.003767644645393139\n",
            "87     \t [ 0.30211268 -0.94085493]. \t  -107.01566089528842 \t -0.003767644645393139\n",
            "88     \t [1.80504874 0.0468575 ]. \t  -1031.9207747589403 \t -0.003767644645393139\n",
            "89     \t [ 1.9476981  -1.06044231]. \t  -2357.0008076212116 \t -0.003767644645393139\n",
            "90     \t [1.17513285 0.41271901]. \t  -93.77532296883037 \t -0.003767644645393139\n",
            "91     \t [-1.98501009 -0.04967342]. \t  -1600.8712085231891 \t -0.003767644645393139\n",
            "92     \t [-0.17324745 -0.76910253]. \t  -65.23534049934628 \t -0.003767644645393139\n",
            "93     \t [0.58963501 0.34054937]. \t  -0.17346897888556995 \t -0.003767644645393139\n",
            "94     \t [0.67907186 0.43314691]. \t  -0.18134828203992925 \t -0.003767644645393139\n",
            "95     \t [-0.83907006  0.07181086]. \t  -43.35336521664036 \t -0.003767644645393139\n",
            "96     \t [0.76718452 0.61299269]. \t  -0.11383958913844518 \t -0.003767644645393139\n",
            "97     \t [0.67066997 0.44211967]. \t  -0.11435426825749294 \t -0.003767644645393139\n",
            "98     \t [-0.63542945  1.47879097]. \t  -118.24151322059333 \t -0.003767644645393139\n",
            "99     \t [0.76584676 0.57666316]. \t  -0.06454594618863316 \t -0.003767644645393139\n",
            "100    \t [0.75534477 0.5600731 ]. \t  -0.0708237557011021 \t -0.003767644645393139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5Hnw4war1af",
        "outputId": "c567349b-b95f-4220-e55d-ce001d21a92d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_loser_5 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_5 = dGPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.23491901 0.17105153]. \t  -1.9278091788796494 \t -1.9278091788796494\n",
            "init   \t [ 0.24749582 -0.48313101]. \t  -30.201785940713883 \t -1.9278091788796494\n",
            "init   \t [-1.56019024  1.41721238]. \t  -109.97965348801837 \t -1.9278091788796494\n",
            "init   \t [0.98198164 1.77093658]. \t  -65.06852657321838 \t -1.9278091788796494\n",
            "init   \t [1.47180752 0.85239655]. \t  -172.83511538815276 \t -1.9278091788796494\n",
            "1      \t [-0.22001069  0.22846888]. \t  -4.730736857945067 \t -1.9278091788796494\n",
            "2      \t [-0.04631006  0.34471263]. \t  -12.830048576914852 \t -1.9278091788796494\n",
            "3      \t [-1.24874396 -0.40718067]. \t  -391.7856478877513 \t -1.9278091788796494\n",
            "4      \t [ 0.02295425 -0.00498953]. \t  \u001b[92m-0.9576614962030955\u001b[0m \t -0.9576614962030955\n",
            "5      \t [ 0.9852452  -0.41754642]. \t  -192.72527827698983 \t -0.9576614962030955\n",
            "6      \t [ 0.17833193 -1.94470778]. \t  -391.33433945839664 \t -0.9576614962030955\n",
            "7      \t [-1.02601459  0.70105493]. \t  -16.470578116788527 \t -0.9576614962030955\n",
            "8      \t [0.1645955  0.02668441]. \t  \u001b[92m-0.6979172642698882\u001b[0m \t -0.6979172642698882\n",
            "9      \t [ 0.43346255 -0.75667882]. \t  -89.54194982904822 \t -0.6979172642698882\n",
            "10     \t [0.05775992 0.01737802]. \t  -0.9075335979803729 \t -0.6979172642698882\n",
            "11     \t [-1.29511157  1.01406091]. \t  -49.258002513003866 \t -0.6979172642698882\n",
            "12     \t [-0.10514936 -0.21837704]. \t  -6.485324879184702 \t -0.6979172642698882\n",
            "13     \t [-0.7467318   0.65803919]. \t  -4.059706617026702 \t -0.6979172642698882\n",
            "14     \t [-0.60173605  1.09331657]. \t  -56.0353319781476 \t -0.6979172642698882\n",
            "15     \t [-0.5033492  -1.35122097]. \t  -259.7282014742476 \t -0.6979172642698882\n",
            "16     \t [1.80992487 0.13819377]. \t  -985.1308628648286 \t -0.6979172642698882\n",
            "17     \t [1.10914658 1.20798517]. \t  \u001b[92m-0.06129011369415375\u001b[0m \t -0.06129011369415375\n",
            "18     \t [2.048 2.048]. \t  -461.7603900415999 \t -0.06129011369415375\n",
            "19     \t [0.87491994 0.83113376]. \t  -0.446622240028497 \t -0.06129011369415375\n",
            "20     \t [1.08286286 1.1766518 ]. \t  \u001b[92m-0.008514483059359458\u001b[0m \t -0.008514483059359458\n",
            "21     \t [1.04133418 1.11839936]. \t  -0.1174615126135806 \t -0.008514483059359458\n",
            "22     \t [0.49963123 0.23086254]. \t  -0.2855958144241081 \t -0.008514483059359458\n",
            "23     \t [0.93196297 0.90382006]. \t  -0.1289916174950372 \t -0.008514483059359458\n",
            "24     \t [1.03662994 0.97811476]. \t  -0.9323134917241241 \t -0.008514483059359458\n",
            "25     \t [-0.81535936  0.58043406]. \t  -4.007474413946899 \t -0.008514483059359458\n",
            "26     \t [-0.78857247 -1.59982509]. \t  -496.78147157734134 \t -0.008514483059359458\n",
            "27     \t [0.99036749 0.33968709]. \t  -41.10622914377682 \t -0.008514483059359458\n",
            "28     \t [0.63718644 0.30046434]. \t  -1.245549493064055 \t -0.008514483059359458\n",
            "29     \t [ 1.47354817 -0.45096802]. \t  -687.8763878924361 \t -0.008514483059359458\n",
            "30     \t [-0.11339345 -0.71322107]. \t  -53.95873790462344 \t -0.008514483059359458\n",
            "31     \t [ 0.25685491 -1.73171831]. \t  -323.72218821205206 \t -0.008514483059359458\n",
            "32     \t [-1.94162085  1.78946102]. \t  -400.8636249015539 \t -0.008514483059359458\n",
            "33     \t [-1.23235411  1.84002276]. \t  -15.308451918145744 \t -0.008514483059359458\n",
            "34     \t [-1.12852027  1.37561898]. \t  -5.572243123919866 \t -0.008514483059359458\n",
            "35     \t [0.80808532 0.58624571]. \t  -0.4824698690405001 \t -0.008514483059359458\n",
            "36     \t [-1.40731081 -1.74373723]. \t  -1392.8070946925925 \t -0.008514483059359458\n",
            "37     \t [1.06391531 1.16313568]. \t  -0.10155338780814412 \t -0.008514483059359458\n",
            "38     \t [1.36081877 0.32800021]. \t  -232.33521879013418 \t -0.008514483059359458\n",
            "39     \t [1.62807101 0.40241991]. \t  -505.8326830375496 \t -0.008514483059359458\n",
            "40     \t [0.72551944 0.50675545]. \t  -0.11384581870317573 \t -0.008514483059359458\n",
            "41     \t [ 1.45800013 -0.78186987]. \t  -845.6434516684446 \t -0.008514483059359458\n",
            "42     \t [-0.95981259  1.55133123]. \t  -43.54233524581199 \t -0.008514483059359458\n",
            "43     \t [0.70930071 0.50522015]. \t  -0.08495240605275833 \t -0.008514483059359458\n",
            "44     \t [0.7620922  0.60467104]. \t  -0.11365667830682971 \t -0.008514483059359458\n",
            "45     \t [0.18326765 0.89863262]. \t  -75.49743886501514 \t -0.008514483059359458\n",
            "46     \t [ 1.2578951  -0.78115751]. \t  -558.6596858541424 \t -0.008514483059359458\n",
            "47     \t [-0.70498112  1.35567505]. \t  -76.63952263722884 \t -0.008514483059359458\n",
            "48     \t [ 1.59433827 -1.56934331]. \t  -1690.597343715668 \t -0.008514483059359458\n",
            "49     \t [1.51693909 0.89691712]. \t  -197.4413596981743 \t -0.008514483059359458\n",
            "50     \t [1.00746689 1.02233169]. \t  \u001b[92m-0.005446487424419977\u001b[0m \t -0.005446487424419977\n",
            "51     \t [-1.42388075  0.1146593 ]. \t  -371.7468143613834 \t -0.005446487424419977\n",
            "52     \t [ 0.1085647  -0.58892905]. \t  -36.880548666447616 \t -0.005446487424419977\n",
            "53     \t [1.01245249 1.03492609]. \t  -0.009888952603299361 \t -0.005446487424419977\n",
            "54     \t [1.01931537 1.03345034]. \t  \u001b[92m-0.0034571917858046007\u001b[0m \t -0.0034571917858046007\n",
            "55     \t [-1.65362243  1.55016785]. \t  -147.29819343735738 \t -0.0034571917858046007\n",
            "56     \t [-1.45956907 -0.7097167 ]. \t  -812.6427464783716 \t -0.0034571917858046007\n",
            "57     \t [ 0.54610405 -0.11064588]. \t  -16.923939619141667 \t -0.0034571917858046007\n",
            "58     \t [ 0.98175078 -1.36234828]. \t  -541.1130116542306 \t -0.0034571917858046007\n",
            "59     \t [1.0751185  1.20090496]. \t  -0.20836943972817076 \t -0.0034571917858046007\n",
            "60     \t [1.0504432  1.11387854]. \t  -0.013459781296602602 \t -0.0034571917858046007\n",
            "61     \t [1.35617493 0.55961046]. \t  -163.86447197354195 \t -0.0034571917858046007\n",
            "62     \t [0.45552316 0.21897706]. \t  -0.30962423274284356 \t -0.0034571917858046007\n",
            "63     \t [-0.16196487 -1.7443089 ]. \t  -314.83188763669534 \t -0.0034571917858046007\n",
            "64     \t [1.37328284 1.6831954 ]. \t  -4.248488633261746 \t -0.0034571917858046007\n",
            "65     \t [1.23348108 1.77303889]. \t  -6.382923344952589 \t -0.0034571917858046007\n",
            "66     \t [-0.62266337  1.87911137]. \t  -225.06093923446917 \t -0.0034571917858046007\n",
            "67     \t [1.18500214 1.46119513]. \t  -0.3587274565223503 \t -0.0034571917858046007\n",
            "68     \t [0.19064963 0.18377161]. \t  -2.8284411978450037 \t -0.0034571917858046007\n",
            "69     \t [-1.33577761  0.72523685]. \t  -117.61771581899634 \t -0.0034571917858046007\n",
            "70     \t [0.42332497 1.62320704]. \t  -208.8470227456658 \t -0.0034571917858046007\n",
            "71     \t [-2.02702739 -0.26349559]. \t  -1920.894768467468 \t -0.0034571917858046007\n",
            "72     \t [0.39183501 0.50543384]. \t  -12.753166403029297 \t -0.0034571917858046007\n",
            "73     \t [1.27394533 1.66709006]. \t  -0.26999797717969853 \t -0.0034571917858046007\n",
            "74     \t [ 0.76614337 -1.31814465]. \t  -363.00302733841585 \t -0.0034571917858046007\n",
            "75     \t [1.28292561 1.63333802]. \t  -0.09582253221044269 \t -0.0034571917858046007\n",
            "76     \t [1.29637884 1.65048082]. \t  -0.17854547250380065 \t -0.0034571917858046007\n",
            "77     \t [0.79176731 1.25677505]. \t  -39.71818790076598 \t -0.0034571917858046007\n",
            "78     \t [0.99025131 1.00097006]. \t  -0.04159853630295643 \t -0.0034571917858046007\n",
            "79     \t [0.85512775 0.72113603]. \t  -0.031203980078682414 \t -0.0034571917858046007\n",
            "80     \t [-0.59155645 -0.47791717]. \t  -71.06764146316972 \t -0.0034571917858046007\n",
            "81     \t [ 1.53401241 -1.94544776]. \t  -1848.117337600054 \t -0.0034571917858046007\n",
            "82     \t [-1.26090414 -1.76207818]. \t  -1128.6735459557462 \t -0.0034571917858046007\n",
            "83     \t [-1.68651501 -1.61570474]. \t  -1996.4109351925408 \t -0.0034571917858046007\n",
            "84     \t [1.46877044 0.82530088]. \t  -177.6383431187912 \t -0.0034571917858046007\n",
            "85     \t [-1.16358746 -0.31670177]. \t  -283.784094866158 \t -0.0034571917858046007\n",
            "86     \t [1.03028438 1.04660832]. \t  -0.023051409914086236 \t -0.0034571917858046007\n",
            "87     \t [0.3867384  0.02623288]. \t  -1.897210205768112 \t -0.0034571917858046007\n",
            "88     \t [-1.48794505  1.79929581]. \t  -23.386208377647066 \t -0.0034571917858046007\n",
            "89     \t [1.24054662 1.52465116]. \t  -0.07832531223517429 \t -0.0034571917858046007\n",
            "90     \t [ 0.26647502 -0.99000205]. \t  -113.11248998845508 \t -0.0034571917858046007\n",
            "91     \t [0.9410594  0.87881259]. \t  -0.008071103041009397 \t -0.0034571917858046007\n",
            "92     \t [-1.42508924 -1.98741794]. \t  -1620.5523613714354 \t -0.0034571917858046007\n",
            "93     \t [-1.25859021  1.86885188]. \t  -13.212480352712507 \t -0.0034571917858046007\n",
            "94     \t [-1.65480693  0.49966047]. \t  -508.2371864994538 \t -0.0034571917858046007\n",
            "95     \t [-0.00691463  0.37379988]. \t  -14.9829378826118 \t -0.0034571917858046007\n",
            "96     \t [1.24404156 1.55830734]. \t  -0.07093675417988167 \t -0.0034571917858046007\n",
            "97     \t [-0.73715664 -1.36234614]. \t  -366.2045166787597 \t -0.0034571917858046007\n",
            "98     \t [1.27097076 1.63619241]. \t  -0.1167962764405455 \t -0.0034571917858046007\n",
            "99     \t [ 0.77014904 -0.30280835]. \t  -80.32330169562528 \t -0.0034571917858046007\n",
            "100    \t [-0.40242189  0.70870106]. \t  -31.861182802358318 \t -0.0034571917858046007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh_Amb8TtdgO",
        "outputId": "8218c815-b8de-4e02-9d5d-679966d9ba2e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_loser_6 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_6 = dGPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.60915518 -0.68821072]. \t  -1074.6314195925434 \t -3.0269049669752817\n",
            "init   \t [ 1.31575449 -1.87721062]. \t  -1302.169546932896 \t -3.0269049669752817\n",
            "init   \t [-1.60703824  0.38933325]. \t  -487.8262244570432 \t -3.0269049669752817\n",
            "init   \t [ 0.12213192 -0.33256477]. \t  -12.844955340992902 \t -3.0269049669752817\n",
            "init   \t [-0.67416945  0.50183959]. \t  -3.0269049669752817 \t -3.0269049669752817\n",
            "1      \t [-0.06756934  0.5044222 ]. \t  -26.125364719936726 \t -3.0269049669752817\n",
            "2      \t [-0.46927273 -0.51516379]. \t  -56.23723640401884 \t -3.0269049669752817\n",
            "3      \t [-0.27427447  0.04468444]. \t  \u001b[92m-1.7170570344409537\u001b[0m \t -1.7170570344409537\n",
            "4      \t [-0.7005749   1.50074021]. \t  -104.88882839307404 \t -1.7170570344409537\n",
            "5      \t [-0.46613674  0.64777826]. \t  -20.68213437104052 \t -1.7170570344409537\n",
            "6      \t [1.16259188 1.02707415]. \t  -10.559428954318465 \t -1.7170570344409537\n",
            "7      \t [0.26282348 0.02154315]. \t  \u001b[92m-0.7693680794170482\u001b[0m \t -0.7693680794170482\n",
            "8      \t [0.13742794 0.05100134]. \t  -0.8471672326655502 \t -0.7693680794170482\n",
            "9      \t [0.76291309 0.57541653]. \t  \u001b[92m-0.06059244450758462\u001b[0m \t -0.06059244450758462\n",
            "10     \t [0.9717123  1.30814843]. \t  -13.244841950407265 \t -0.06059244450758462\n",
            "11     \t [0.74995669 0.75504438]. \t  -3.7723574144839005 \t -0.06059244450758462\n",
            "12     \t [1.86881989 2.02915792]. \t  -214.88827624604227 \t -0.06059244450758462\n",
            "13     \t [-0.46121134  0.22456258]. \t  -2.1491729768093766 \t -0.06059244450758462\n",
            "14     \t [-1.64886576  2.02848959]. \t  -54.663578807527045 \t -0.06059244450758462\n",
            "15     \t [0.55463831 0.35702693]. \t  -0.44241535558223744 \t -0.06059244450758462\n",
            "16     \t [0.78138218 1.84221429]. \t  -151.74548715531026 \t -0.06059244450758462\n",
            "17     \t [ 0.05896132 -0.01172031]. \t  -0.9086479056578121 \t -0.06059244450758462\n",
            "18     \t [-1.11205613  1.15532179]. \t  -5.12251527619631 \t -0.06059244450758462\n",
            "19     \t [0.6083543  0.34598655]. \t  -0.21150784087130964 \t -0.06059244450758462\n",
            "20     \t [0.60534978 0.36004684]. \t  -0.15984674139762628 \t -0.06059244450758462\n",
            "21     \t [-0.77698697  1.56001318]. \t  -94.60949982977071 \t -0.06059244450758462\n",
            "22     \t [-0.55078469 -0.83989102]. \t  -133.10808530049047 \t -0.06059244450758462\n",
            "23     \t [-1.31227773  2.03619975]. \t  -15.214200315819033 \t -0.06059244450758462\n",
            "24     \t [ 0.80570692 -1.02979778]. \t  -281.92889771006395 \t -0.06059244450758462\n",
            "25     \t [-0.94696839  0.86842292]. \t  -3.8709233696267025 \t -0.06059244450758462\n",
            "26     \t [-0.15693252  0.26323614]. \t  -7.031886120412313 \t -0.06059244450758462\n",
            "27     \t [1.47166778 0.25534753]. \t  -365.20765281558084 \t -0.06059244450758462\n",
            "28     \t [0.68127202 0.48442364]. \t  -0.142764383238895 \t -0.06059244450758462\n",
            "29     \t [-1.52580274 -1.09844367]. \t  -1180.4820266939364 \t -0.06059244450758462\n",
            "30     \t [0.48343844 0.21728772]. \t  -0.29381391000716506 \t -0.06059244450758462\n",
            "31     \t [1.59467551 0.79854923]. \t  -304.6609988132639 \t -0.06059244450758462\n",
            "32     \t [1.0002996  1.10496461]. \t  -1.0892119646126375 \t -0.06059244450758462\n",
            "33     \t [0.66975161 0.45602927]. \t  -0.11463222060463235 \t -0.06059244450758462\n",
            "34     \t [0.85061931 0.72472308]. \t  \u001b[92m-0.022451450323285087\u001b[0m \t -0.022451450323285087\n",
            "35     \t [0.8763174  0.77042807]. \t  \u001b[92m-0.015920326692339105\u001b[0m \t -0.015920326692339105\n",
            "36     \t [1.0304445  1.05936354]. \t  \u001b[92m-0.0015282576420380743\u001b[0m \t -0.0015282576420380743\n",
            "37     \t [-1.68156932 -1.93210104]. \t  -2272.737961281629 \t -0.0015282576420380743\n",
            "38     \t [1.18306743 1.43030098]. \t  -0.12747094498765982 \t -0.0015282576420380743\n",
            "39     \t [1.05524058 1.18053815]. \t  -0.4520248243555432 \t -0.0015282576420380743\n",
            "40     \t [1.16802336 1.32788128]. \t  -0.16070816763035162 \t -0.0015282576420380743\n",
            "41     \t [0.09480507 1.95906405]. \t  -381.0990390354115 \t -0.0015282576420380743\n",
            "42     \t [-1.16850434  1.4108355 ]. \t  -4.908827732761238 \t -0.0015282576420380743\n",
            "43     \t [1.03157812 1.04914732]. \t  -0.02351549960770606 \t -0.0015282576420380743\n",
            "44     \t [ 1.22316586 -1.80538469]. \t  -1090.0528432604024 \t -0.0015282576420380743\n",
            "45     \t [ 0.18623279 -1.93678777]. \t  -389.3317792113233 \t -0.0015282576420380743\n",
            "46     \t [-1.2323997  -1.29908191]. \t  -799.0345393152076 \t -0.0015282576420380743\n",
            "47     \t [1.16545637 1.32089235]. \t  -0.16722330840851782 \t -0.0015282576420380743\n",
            "48     \t [0.65243029 1.06414155]. \t  -40.88599954011427 \t -0.0015282576420380743\n",
            "49     \t [1.0706259  1.12852903]. \t  -0.03635522814658004 \t -0.0015282576420380743\n",
            "50     \t [-0.03454204 -0.1886721 ]. \t  -4.675158818828568 \t -0.0015282576420380743\n",
            "51     \t [0.77334622 0.61511475]. \t  -0.08044346238140163 \t -0.0015282576420380743\n",
            "52     \t [1.39524932 1.49131097]. \t  -20.896020069571428 \t -0.0015282576420380743\n",
            "53     \t [-1.45946305  0.86133243]. \t  -167.00891750251822 \t -0.0015282576420380743\n",
            "54     \t [1.15947831 1.31691205]. \t  -0.10093679401279845 \t -0.0015282576420380743\n",
            "55     \t [ 1.0966438  -1.78210956]. \t  -890.8749525146312 \t -0.0015282576420380743\n",
            "56     \t [0.81645276 0.67938207]. \t  -0.05004022079805938 \t -0.0015282576420380743\n",
            "57     \t [1.10644041 1.25330942]. \t  -0.09600497454499114 \t -0.0015282576420380743\n",
            "58     \t [ 0.05426691 -2.01085777]. \t  -406.43452858749674 \t -0.0015282576420380743\n",
            "59     \t [1.16675717 1.36857024]. \t  -0.03306121760283245 \t -0.0015282576420380743\n",
            "60     \t [0.8648525  0.74363049]. \t  -0.020147848026052854 \t -0.0015282576420380743\n",
            "61     \t [-0.89282563  0.73921926]. \t  -3.918242355093483 \t -0.0015282576420380743\n",
            "62     \t [ 1.23043616 -1.71564361]. \t  -1043.095545084003 \t -0.0015282576420380743\n",
            "63     \t [0.85032242 0.70890977]. \t  -0.04239296622262545 \t -0.0015282576420380743\n",
            "64     \t [1.98349663 0.05164635]. \t  -1508.4352567550156 \t -0.0015282576420380743\n",
            "65     \t [0.71146786 0.39093788]. \t  -1.4114756191988476 \t -0.0015282576420380743\n",
            "66     \t [ 0.15070243 -0.35797581]. \t  -15.213567966194207 \t -0.0015282576420380743\n",
            "67     \t [-0.84196325 -0.03742189]. \t  -59.09278066675081 \t -0.0015282576420380743\n",
            "68     \t [0.73029918 1.53562134]. \t  -100.53015091470867 \t -0.0015282576420380743\n",
            "69     \t [-0.93643438 -1.58272987]. \t  -608.7322847330377 \t -0.0015282576420380743\n",
            "70     \t [1.3120196  2.00453818]. \t  -8.114337895616032 \t -0.0015282576420380743\n",
            "71     \t [0.9293458  0.85768369]. \t  -0.008591935989947641 \t -0.0015282576420380743\n",
            "72     \t [-1.64153317 -0.05783136]. \t  -764.5826832498624 \t -0.0015282576420380743\n",
            "73     \t [0.0951594  1.35191321]. \t  -181.1454700941001 \t -0.0015282576420380743\n",
            "74     \t [-0.47475672 -0.9289252 ]. \t  -135.4201751947115 \t -0.0015282576420380743\n",
            "75     \t [-0.67246471 -0.39538125]. \t  -74.63802490155976 \t -0.0015282576420380743\n",
            "76     \t [-0.54687729 -1.66262826]. \t  -387.2207058781848 \t -0.0015282576420380743\n",
            "77     \t [0.18670875 0.77461356]. \t  -55.38495183637282 \t -0.0015282576420380743\n",
            "78     \t [-1.87549053 -0.06118832]. \t  -1288.944200275795 \t -0.0015282576420380743\n",
            "79     \t [1.82501194 1.48588699]. \t  -341.0025557765546 \t -0.0015282576420380743\n",
            "80     \t [ 0.05802756 -1.76728631]. \t  -314.4086978048547 \t -0.0015282576420380743\n",
            "81     \t [1.32318137 1.80428107]. \t  -0.39037303414753044 \t -0.0015282576420380743\n",
            "82     \t [1.14603764 1.3159491 ]. \t  -0.021975630904855405 \t -0.0015282576420380743\n",
            "83     \t [1.16723617 1.96839226]. \t  -36.74574817607512 \t -0.0015282576420380743\n",
            "84     \t [1.11620229 1.25273114]. \t  -0.018159103179929793 \t -0.0015282576420380743\n",
            "85     \t [-1.20320127 -0.35699766]. \t  -330.54503505181793 \t -0.0015282576420380743\n",
            "86     \t [1.82389133 1.28038113]. \t  -419.37161019000627 \t -0.0015282576420380743\n",
            "87     \t [ 0.88625922 -0.3468704 ]. \t  -128.2291097907271 \t -0.0015282576420380743\n",
            "88     \t [0.85864023 0.04165531]. \t  -48.406994899042076 \t -0.0015282576420380743\n",
            "89     \t [0.46181679 0.9575149 ]. \t  -55.678982222462196 \t -0.0015282576420380743\n",
            "90     \t [-0.7935673   1.42080154]. \t  -65.79328558574495 \t -0.0015282576420380743\n",
            "91     \t [ 2.04234106 -0.12966452]. \t  -1850.7930646007785 \t -0.0015282576420380743\n",
            "92     \t [1.23923152 1.54383138]. \t  -0.06385218695013092 \t -0.0015282576420380743\n",
            "93     \t [1.27932336 1.66748409]. \t  -0.1729830223847136 \t -0.0015282576420380743\n",
            "94     \t [ 2.02538509 -0.76140625]. \t  -2366.5031536246265 \t -0.0015282576420380743\n",
            "95     \t [1.25603595 1.60487613]. \t  -0.13980967564426916 \t -0.0015282576420380743\n",
            "96     \t [-1.95689662 -1.63522269]. \t  -2995.0018570387247 \t -0.0015282576420380743\n",
            "97     \t [0.95008464 0.89464807]. \t  -0.00891196172657524 \t -0.0015282576420380743\n",
            "98     \t [-2.01222674  0.53070006]. \t  -1246.9566845983406 \t -0.0015282576420380743\n",
            "99     \t [0.70205516 0.48460947]. \t  -0.09561367871903 \t -0.0015282576420380743\n",
            "100    \t [-0.90499114  1.91485776]. \t  -123.71745106321148 \t -0.0015282576420380743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNmoDTuUtdi3",
        "outputId": "13555842-4b11-429a-aa68-630d3d71a625"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_loser_7 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_7 = dGPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.42268934 -0.80954733]. \t  -808.7939502616841 \t -2.0077595729598063\n",
            "init   \t [-1.79389885 -0.16441204]. \t  -1151.9264213711547 \t -2.0077595729598063\n",
            "init   \t [1.37319786 1.74897991]. \t  -2.0077595729598063 \t -2.0077595729598063\n",
            "init   \t [0.92974688 1.09976052]. \t  -5.543015908052858 \t -2.0077595729598063\n",
            "init   \t [-0.94533605  0.58994398]. \t  -13.008689168416776 \t -2.0077595729598063\n",
            "1      \t [-0.37625117  1.06266898]. \t  -86.73733152643135 \t -2.0077595729598063\n",
            "2      \t [ 0.03555281 -0.06681774]. \t  \u001b[92m-1.3936707114630122\u001b[0m \t -1.3936707114630122\n",
            "3      \t [1.70981563 0.62552629]. \t  -528.5581328002694 \t -1.3936707114630122\n",
            "4      \t [1.00342219 1.66585077]. \t  -43.427410777261855 \t -1.3936707114630122\n",
            "5      \t [-0.28030133  0.4011523 ]. \t  -12.045180746113228 \t -1.3936707114630122\n",
            "6      \t [-1.54963585  1.50677465]. \t  -86.5309531618843 \t -1.3936707114630122\n",
            "7      \t [ 0.11233166 -1.46148808]. \t  -218.0869481450979 \t -1.3936707114630122\n",
            "8      \t [-0.95822245  0.95071318]. \t  -3.940409222089954 \t -1.3936707114630122\n",
            "9      \t [1.92585728 1.93941244]. \t  -313.9751288778981 \t -1.3936707114630122\n",
            "10     \t [-0.7289408   0.54875501]. \t  -3.019513411687114 \t -1.3936707114630122\n",
            "11     \t [1.20067738 1.581771  ]. \t  -2.0043289107835234 \t -1.3936707114630122\n",
            "12     \t [0.46213398 0.26553063]. \t  \u001b[92m-0.5593132411126935\u001b[0m \t -0.5593132411126935\n",
            "13     \t [1.31763586 1.87891628]. \t  -2.138706571027679 \t -0.5593132411126935\n",
            "14     \t [0.45604776 0.59219017]. \t  -15.057663906077446 \t -0.5593132411126935\n",
            "15     \t [-0.22276621  0.11703406]. \t  -1.9495582474432847 \t -0.5593132411126935\n",
            "16     \t [ 0.29565231 -0.03211627]. \t  -1.9247653511248326 \t -0.5593132411126935\n",
            "17     \t [0.17355264 0.02926456]. \t  -0.6830885034792047 \t -0.5593132411126935\n",
            "18     \t [0.3951644  0.16699149]. \t  \u001b[92m-0.37756925347923925\u001b[0m \t -0.37756925347923925\n",
            "19     \t [1.27246343 1.60397822]. \t  \u001b[92m-0.09729457969826462\u001b[0m \t -0.09729457969826462\n",
            "20     \t [-0.94093813  0.72810265]. \t  -6.2403716851369 \t -0.09729457969826462\n",
            "21     \t [-0.54832657  1.90584102]. \t  -260.05727709114376 \t -0.09729457969826462\n",
            "22     \t [0.63186756 0.48432176]. \t  -0.8591294559071819 \t -0.09729457969826462\n",
            "23     \t [ 0.96780316 -0.78044245]. \t  -294.8392690826092 \t -0.09729457969826462\n",
            "24     \t [-1.97561109 -1.33580864]. \t  -2753.4069170851544 \t -0.09729457969826462\n",
            "25     \t [-0.49292243 -0.7916302 ]. \t  -109.26909728905902 \t -0.09729457969826462\n",
            "26     \t [0.81115052 0.70978983]. \t  -0.3042436276168672 \t -0.09729457969826462\n",
            "27     \t [-0.588465    0.05940153]. \t  -10.753781252257108 \t -0.09729457969826462\n",
            "28     \t [0.72600312 0.60635585]. \t  -0.7035319805615363 \t -0.09729457969826462\n",
            "29     \t [1.55375794 0.04422647]. \t  -561.9669176444024 \t -0.09729457969826462\n",
            "30     \t [1.32791237 1.75857392]. \t  -0.10980881641314008 \t -0.09729457969826462\n",
            "31     \t [ 1.84142169 -0.68915833]. \t  -1665.3415997891302 \t -0.09729457969826462\n",
            "32     \t [0.70587209 0.46014561]. \t  -0.2317468799219714 \t -0.09729457969826462\n",
            "33     \t [1.29468303 1.72357606]. \t  -0.31124794977871817 \t -0.09729457969826462\n",
            "34     \t [ 2.01308506 -1.1081972 ]. \t  -2664.317705524392 \t -0.09729457969826462\n",
            "35     \t [-1.15538548 -0.32404392]. \t  -279.8603611900302 \t -0.09729457969826462\n",
            "36     \t [1.27476812 1.62866836]. \t  \u001b[92m-0.0768185504021595\u001b[0m \t -0.0768185504021595\n",
            "37     \t [ 0.9721382  -0.41205174]. \t  -184.17401640929336 \t -0.0768185504021595\n",
            "38     \t [-0.02246719  1.02914007]. \t  -106.85449638551168 \t -0.0768185504021595\n",
            "39     \t [-1.20940169 -1.15883084]. \t  -692.0989167941194 \t -0.0768185504021595\n",
            "40     \t [-1.75043739 -0.96288583]. \t  -1629.1708668895085 \t -0.0768185504021595\n",
            "41     \t [ 0.18646567 -0.01091241]. \t  -0.8705212881455077 \t -0.0768185504021595\n",
            "42     \t [-1.57895555  1.65991668]. \t  -76.07055806729942 \t -0.0768185504021595\n",
            "43     \t [ 1.22473655 -0.02664716]. \t  -233.10943775648116 \t -0.0768185504021595\n",
            "44     \t [ 0.35130358 -0.51042541]. \t  -40.596073343757254 \t -0.0768185504021595\n",
            "45     \t [1.31169037 1.71205742]. \t  -0.10433210379994093 \t -0.0768185504021595\n",
            "46     \t [-1.59405765 -0.72663747]. \t  -1074.487542263908 \t -0.0768185504021595\n",
            "47     \t [-0.28651539 -0.574408  ]. \t  -44.75422470822018 \t -0.0768185504021595\n",
            "48     \t [ 1.17786294 -1.63653741]. \t  -914.4278621791691 \t -0.0768185504021595\n",
            "49     \t [-1.73635555  0.31082404]. \t  -738.7068672809747 \t -0.0768185504021595\n",
            "50     \t [-1.59047368  0.71679317]. \t  -335.3397783815587 \t -0.0768185504021595\n",
            "51     \t [1.24558232 1.59362825]. \t  -0.23799774150302205 \t -0.0768185504021595\n",
            "52     \t [-0.74435884  0.34039411]. \t  -7.6085296637269195 \t -0.0768185504021595\n",
            "53     \t [0.7140417  0.44870733]. \t  -0.45568258475893825 \t -0.0768185504021595\n",
            "54     \t [0.70850424 0.45035585]. \t  -0.3514570134726549 \t -0.0768185504021595\n",
            "55     \t [1.12644203 1.83016155]. \t  -31.520623750753167 \t -0.0768185504021595\n",
            "56     \t [0.66971685 0.45191973]. \t  -0.11024233161666784 \t -0.0768185504021595\n",
            "57     \t [-1.24954666  1.78525415]. \t  -10.073012518900768 \t -0.0768185504021595\n",
            "58     \t [0.75999203 0.54763979]. \t  -0.14729271932259913 \t -0.0768185504021595\n",
            "59     \t [1.26920987 1.64225928]. \t  -0.17085392230467644 \t -0.0768185504021595\n",
            "60     \t [1.26662319 1.61163542]. \t  \u001b[92m-0.07641853550900016\u001b[0m \t -0.07641853550900016\n",
            "61     \t [-1.37789349 -0.22158635]. \t  -455.1693559042023 \t -0.07641853550900016\n",
            "62     \t [-1.95087914  0.75283349]. \t  -940.8471583586562 \t -0.07641853550900016\n",
            "63     \t [0.61700267 0.30621819]. \t  -0.7013262445509651 \t -0.07641853550900016\n",
            "64     \t [0.67047068 0.44595488]. \t  -0.10986838832854515 \t -0.07641853550900016\n",
            "65     \t [0.71131079 0.51730102]. \t  -0.09619642912615073 \t -0.07641853550900016\n",
            "66     \t [-1.98057804  0.15332739]. \t  -1429.6928273167414 \t -0.07641853550900016\n",
            "67     \t [-1.21300802  1.51226535]. \t  -5.06449648828758 \t -0.07641853550900016\n",
            "68     \t [ 0.37605334 -0.813153  ]. \t  -91.50952887424619 \t -0.07641853550900016\n",
            "69     \t [-0.99219442  1.58641447]. \t  -40.204989906806524 \t -0.07641853550900016\n",
            "70     \t [-0.86162263 -0.11474692]. \t  -76.93461829347547 \t -0.07641853550900016\n",
            "71     \t [-1.05647768 -0.98787153]. \t  -446.9176886955735 \t -0.07641853550900016\n",
            "72     \t [-0.75778259 -0.25072129]. \t  -71.14499647851899 \t -0.07641853550900016\n",
            "73     \t [0.71481062 0.50334247]. \t  -0.08712687188637122 \t -0.07641853550900016\n",
            "74     \t [0.76422544 0.59494797]. \t  \u001b[92m-0.06748687884423658\u001b[0m \t -0.06748687884423658\n",
            "75     \t [1.65163192 0.53075901]. \t  -483.1621969903884 \t -0.06748687884423658\n",
            "76     \t [1.01202069 0.95936413]. \t  -0.42033022406164844 \t -0.06748687884423658\n",
            "77     \t [0.84517478 0.73987668]. \t  -0.08928319617884767 \t -0.06748687884423658\n",
            "78     \t [0.75157005 0.54179966]. \t  -0.11488399313826303 \t -0.06748687884423658\n",
            "79     \t [-1.02331335 -1.98831652]. \t  -925.511763560693 \t -0.06748687884423658\n",
            "80     \t [-1.76585423  0.51846661]. \t  -683.5327188082996 \t -0.06748687884423658\n",
            "81     \t [0.75010484 0.5213793 ]. \t  -0.232834681612964 \t -0.06748687884423658\n",
            "82     \t [-0.34050634 -1.25170618]. \t  -188.8438156755429 \t -0.06748687884423658\n",
            "83     \t [-0.06717539 -0.55957888]. \t  -32.958775452655466 \t -0.06748687884423658\n",
            "84     \t [0.94887324 0.87272592]. \t  -0.07898054832118488 \t -0.06748687884423658\n",
            "85     \t [-1.0565167   0.00792408]. \t  -127.06291560199584 \t -0.06748687884423658\n",
            "86     \t [0.84915878 0.7528633 ]. \t  -0.12383040631323093 \t -0.06748687884423658\n",
            "87     \t [-1.93157144 -1.29004162]. \t  -2529.648099431939 \t -0.06748687884423658\n",
            "88     \t [-0.56364059  1.63132091]. \t  -175.00740333734777 \t -0.06748687884423658\n",
            "89     \t [1.30276769 1.72131991]. \t  -0.14982766937968078 \t -0.06748687884423658\n",
            "90     \t [-0.80306651 -1.303614  ]. \t  -382.92789483486376 \t -0.06748687884423658\n",
            "91     \t [ 1.98490407 -1.38419481]. \t  -2835.5091279274216 \t -0.06748687884423658\n",
            "92     \t [ 0.24002072 -1.48468418]. \t  -238.44468467100532 \t -0.06748687884423658\n",
            "93     \t [1.30623684 1.71433512]. \t  -0.1003103558526618 \t -0.06748687884423658\n",
            "94     \t [ 1.46606788 -1.9522138 ]. \t  -1682.5038950182677 \t -0.06748687884423658\n",
            "95     \t [1.38696035 1.33125903]. \t  -35.24351314917995 \t -0.06748687884423658\n",
            "96     \t [0.98938011 0.98397349]. \t  \u001b[92m-0.002714288849242109\u001b[0m \t -0.002714288849242109\n",
            "97     \t [-0.33165955  0.04899537]. \t  -2.1454500004474717 \t -0.002714288849242109\n",
            "98     \t [-0.33682144 -0.00609563]. \t  -3.2161758712036335 \t -0.002714288849242109\n",
            "99     \t [0.03997086 1.39923583]. \t  -196.26089887888634 \t -0.002714288849242109\n",
            "100    \t [0.95960873 0.93171848]. \t  -0.01344619104426845 \t -0.002714288849242109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_21yVprtdlu",
        "outputId": "16dd61dd-6677-4018-db4e-5ed2446ec8ce"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_loser_8 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_8 = dGPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.59948851 -1.8750873 ]. \t  -1972.3059423071145 \t -7.238275380208551\n",
            "init   \t [ 0.40834083 -0.81972959]. \t  -97.66272647293047 \t -7.238275380208551\n",
            "init   \t [ 1.21492186 -0.54806067]. \t  -409.7425700197761 \t -7.238275380208551\n",
            "init   \t [ 1.55897178 -0.82804067]. \t  -1062.051446884513 \t -7.238275380208551\n",
            "init   \t [-0.69999303  0.28146449]. \t  -7.238275380208551 \t -7.238275380208551\n",
            "1      \t [0.61360238 0.32771785]. \t  \u001b[92m-0.38734981524274686\u001b[0m \t -0.38734981524274686\n",
            "2      \t [ 0.44957683 -0.14950718]. \t  -12.667085696146009 \t -0.38734981524274686\n",
            "3      \t [-1.39188014  0.88003045]. \t  -117.50939285923893 \t -0.38734981524274686\n",
            "4      \t [1.41566408 0.84930114]. \t  -133.52992403681057 \t -0.38734981524274686\n",
            "5      \t [-0.6916621   0.64393699]. \t  -5.60208758511781 \t -0.38734981524274686\n",
            "6      \t [0.75922499 0.1685607 ]. \t  -16.69310442272994 \t -0.38734981524274686\n",
            "7      \t [0.35625505 1.43304671]. \t  -171.0117160875193 \t -0.38734981524274686\n",
            "8      \t [-0.01553477  0.50788672]. \t  -26.801695313336594 \t -0.38734981524274686\n",
            "9      \t [-0.8962121   0.44792833]. \t  -16.217141274308645 \t -0.38734981524274686\n",
            "10     \t [0.71806711 0.70823025]. \t  -3.7893425127670963 \t -0.38734981524274686\n",
            "11     \t [-1.45841171  2.02404169]. \t  -7.103102879159884 \t -0.38734981524274686\n",
            "12     \t [0.5643111  0.40904053]. \t  -1.0105432891995718 \t -0.38734981524274686\n",
            "13     \t [-0.56707816  0.41821546]. \t  -3.3896207753230425 \t -0.38734981524274686\n",
            "14     \t [-1.01385586  1.62704583]. \t  -39.952744159584476 \t -0.38734981524274686\n",
            "15     \t [0.58428637 0.40839102]. \t  -0.6217238959189965 \t -0.38734981524274686\n",
            "16     \t [-2.03615452  1.53171002]. \t  -692.6303498825225 \t -0.38734981524274686\n",
            "17     \t [1.4484407  1.88703496]. \t  -4.650898799351691 \t -0.38734981524274686\n",
            "18     \t [-1.16952629  1.98801246]. \t  -43.1742175698741 \t -0.38734981524274686\n",
            "19     \t [-1.02906948  1.10283435]. \t  -4.309408343334651 \t -0.38734981524274686\n",
            "20     \t [1.14713505 1.38075619]. \t  -0.4420372642961823 \t -0.38734981524274686\n",
            "21     \t [1.03799728 1.22229839]. \t  -2.0998868809733406 \t -0.38734981524274686\n",
            "22     \t [1.23015975 1.56784857]. \t  \u001b[92m-0.3506044600393513\u001b[0m \t -0.3506044600393513\n",
            "23     \t [1.22577866 1.45245473]. \t  \u001b[92m-0.30176262008602805\u001b[0m \t -0.30176262008602805\n",
            "24     \t [-1.90600432  0.6011805 ]. \t  -927.5483607203025 \t -0.30176262008602805\n",
            "25     \t [1.22988235 1.51959968]. \t  \u001b[92m-0.05773061435510794\u001b[0m \t -0.05773061435510794\n",
            "26     \t [-1.17538182  1.46656695]. \t  -5.45554343087837 \t -0.05773061435510794\n",
            "27     \t [-2.02223264 -0.51849968]. \t  -2132.4307428489506 \t -0.05773061435510794\n",
            "28     \t [1.92951462 1.09425467]. \t  -691.9082143721105 \t -0.05773061435510794\n",
            "29     \t [0.83603421 0.69123358]. \t  \u001b[92m-0.03284404679566778\u001b[0m \t -0.03284404679566778\n",
            "30     \t [1.37616    1.87961861]. \t  -0.16165392097749176 \t -0.03284404679566778\n",
            "31     \t [0.99454833 0.91627153]. \t  -0.5308124556940027 \t -0.03284404679566778\n",
            "32     \t [1.4028137  2.04755223]. \t  -0.796925196137526 \t -0.03284404679566778\n",
            "33     \t [1.05311543 1.08099774]. \t  -0.08152592616659253 \t -0.03284404679566778\n",
            "34     \t [ 0.42324543 -2.01100952]. \t  -480.00669093860853 \t -0.03284404679566778\n",
            "35     \t [ 0.190941   -0.04878897]. \t  -1.3812889957609409 \t -0.03284404679566778\n",
            "36     \t [-0.18657001 -0.10636657]. \t  -3.400984813924212 \t -0.03284404679566778\n",
            "37     \t [-1.33951156  1.72886835]. \t  -5.901329482203336 \t -0.03284404679566778\n",
            "38     \t [1.28676527 0.96379415]. \t  -47.96458146455062 \t -0.03284404679566778\n",
            "39     \t [1.131244   1.31148382]. \t  -0.11816351914173326 \t -0.03284404679566778\n",
            "40     \t [-0.75348494  1.99447316]. \t  -206.631586007862 \t -0.03284404679566778\n",
            "41     \t [-0.83236489 -1.3406397 ]. \t  -416.8579921277826 \t -0.03284404679566778\n",
            "42     \t [ 2.01538058 -0.33779095]. \t  -1936.6348722404737 \t -0.03284404679566778\n",
            "43     \t [0.33010469 0.19124598]. \t  -1.1257081671619646 \t -0.03284404679566778\n",
            "44     \t [1.40714957 1.94921237]. \t  -0.26098948963885904 \t -0.03284404679566778\n",
            "45     \t [1.19510526 1.45322224]. \t  -0.10029464005430952 \t -0.03284404679566778\n",
            "46     \t [1.13987265 1.29452607]. \t  \u001b[92m-0.021852623826453442\u001b[0m \t -0.021852623826453442\n",
            "47     \t [ 1.04555863 -0.33677927]. \t  -204.48410237553725 \t -0.021852623826453442\n",
            "48     \t [-0.4324462   1.50517663]. \t  -175.80830243387348 \t -0.021852623826453442\n",
            "49     \t [-0.23047777  1.43019956]. \t  -191.14888552417455 \t -0.021852623826453442\n",
            "50     \t [-1.17614757  1.22149043]. \t  -7.354599934031947 \t -0.021852623826453442\n",
            "51     \t [-0.27721953  1.68451558]. \t  -260.0899357890036 \t -0.021852623826453442\n",
            "52     \t [-0.68574931 -1.57082399]. \t  -419.44092006851207 \t -0.021852623826453442\n",
            "53     \t [1.2542339  1.61970987]. \t  -0.28185796598021506 \t -0.021852623826453442\n",
            "54     \t [ 0.21553377 -1.99468377]. \t  -417.2400550622069 \t -0.021852623826453442\n",
            "55     \t [1.91542278 1.13366555]. \t  -643.5511920107601 \t -0.021852623826453442\n",
            "56     \t [ 0.3149918  -0.37616106]. \t  -23.06793576005066 \t -0.021852623826453442\n",
            "57     \t [-0.34460371  1.04071351]. \t  -86.80931352169951 \t -0.021852623826453442\n",
            "58     \t [-1.98374815  1.75520272]. \t  -484.16630206685227 \t -0.021852623826453442\n",
            "59     \t [-1.8061815  -0.50959029]. \t  -1430.583972383175 \t -0.021852623826453442\n",
            "60     \t [0.78421657 0.57042093]. \t  -0.245252848489033 \t -0.021852623826453442\n",
            "61     \t [0.70564023 0.50051481]. \t  -0.08731675873544986 \t -0.021852623826453442\n",
            "62     \t [ 0.64167029 -0.50889445]. \t  -84.88531997530704 \t -0.021852623826453442\n",
            "63     \t [1.01977412 1.05765385]. \t  -0.03177169933083389 \t -0.021852623826453442\n",
            "64     \t [0.21991589 1.36604332]. \t  -174.23667298663855 \t -0.021852623826453442\n",
            "65     \t [0.99899032 1.43852549]. \t  -19.40788724532415 \t -0.021852623826453442\n",
            "66     \t [0.93405663 0.88546978]. \t  \u001b[92m-0.02126932659159693\u001b[0m \t -0.02126932659159693\n",
            "67     \t [-0.34263104 -1.21563687]. \t  -179.50032890794742 \t -0.02126932659159693\n",
            "68     \t [ 1.10108511 -0.64982308]. \t  -346.793388504788 \t -0.02126932659159693\n",
            "69     \t [ 1.55417844 -1.95204842]. \t  -1907.8293870643543 \t -0.02126932659159693\n",
            "70     \t [ 0.45966197 -0.08303173]. \t  -8.954441874760606 \t -0.02126932659159693\n",
            "71     \t [-0.69710604  0.61700195]. \t  -4.5974511549804715 \t -0.02126932659159693\n",
            "72     \t [-2.02537435  0.93380206]. \t  -1012.9902227465138 \t -0.02126932659159693\n",
            "73     \t [ 1.23113088 -1.6068377 ]. \t  -975.0671205123277 \t -0.02126932659159693\n",
            "74     \t [0.82086512 0.65526865]. \t  -0.0665028936378506 \t -0.02126932659159693\n",
            "75     \t [-1.44041432  1.4358205 ]. \t  -46.7842596086021 \t -0.02126932659159693\n",
            "76     \t [-1.33185859  0.89627932]. \t  -82.45012243198344 \t -0.02126932659159693\n",
            "77     \t [1.69438806 1.87624578]. \t  -99.42600253272379 \t -0.02126932659159693\n",
            "78     \t [1.39078969 0.55250285]. \t  -191.08793337017656 \t -0.02126932659159693\n",
            "79     \t [-0.51996186  0.82887034]. \t  -33.50362549718966 \t -0.02126932659159693\n",
            "80     \t [-0.38195512  1.28093926]. \t  -130.7435472768706 \t -0.02126932659159693\n",
            "81     \t [-1.238806    0.24087278]. \t  -172.39569306997012 \t -0.02126932659159693\n",
            "82     \t [0.80841935 0.6439636 ]. \t  -0.04587743648830525 \t -0.02126932659159693\n",
            "83     \t [ 0.51250767 -0.81361155]. \t  -116.07457999615696 \t -0.02126932659159693\n",
            "84     \t [0.79209725 0.62267504]. \t  -0.045473168314503036 \t -0.02126932659159693\n",
            "85     \t [0.97075693 0.9335548 ]. \t  \u001b[92m-0.008624202437991018\u001b[0m \t -0.008624202437991018\n",
            "86     \t [-0.66521432  0.24617109]. \t  -6.627839046105652 \t -0.008624202437991018\n",
            "87     \t [1.24207642 1.55105691]. \t  -0.06549511174024994 \t -0.008624202437991018\n",
            "88     \t [0.94742611 1.29214479]. \t  -15.568041789437807 \t -0.008624202437991018\n",
            "89     \t [1.35748863 1.84057215]. \t  -0.12828354308515053 \t -0.008624202437991018\n",
            "90     \t [-1.71028388  0.04698636]. \t  -835.6827274435514 \t -0.008624202437991018\n",
            "91     \t [0.33503966 1.87471175]. \t  -311.0687599752305 \t -0.008624202437991018\n",
            "92     \t [-1.71383293  1.05001275]. \t  -363.5212559628972 \t -0.008624202437991018\n",
            "93     \t [-0.11151438 -0.26141094]. \t  -8.734648863820025 \t -0.008624202437991018\n",
            "94     \t [0.93555506 0.47855305]. \t  -15.742052906702769 \t -0.008624202437991018\n",
            "95     \t [0.6228158  0.32330682]. \t  -0.5594896262848823 \t -0.008624202437991018\n",
            "96     \t [-1.58165864  1.13644603]. \t  -193.0415290939626 \t -0.008624202437991018\n",
            "97     \t [ 0.73390107 -1.9012    ]. \t  -595.3384730621333 \t -0.008624202437991018\n",
            "98     \t [ 0.54241415 -0.67884865]. \t  -94.89430414020869 \t -0.008624202437991018\n",
            "99     \t [1.01065966 1.0096387 ]. \t  -0.014024063131267398 \t -0.008624202437991018\n",
            "100    \t [1.36675047 1.90482644]. \t  -0.27007425547960934 \t -0.008624202437991018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkq32q1utdoO",
        "outputId": "8bb32e15-f601-4e73-d4d1-e70a4b9a8528"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_loser_9 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_9 = dGPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.24284125 0.11273132]. \t  -205.0993354093405 \t -3.486729021084656\n",
            "init   \t [-1.56011944  0.5721352 ]. \t  -353.19808767458017 \t -3.486729021084656\n",
            "init   \t [-1.67557012 -0.68720361]. \t  -1228.4786390381805 \t -3.486729021084656\n",
            "init   \t [-0.29744764  0.22276429]. \t  -3.486729021084656 \t -3.486729021084656\n",
            "init   \t [0.52480624 0.8085215 ]. \t  -28.645360944397154 \t -3.486729021084656\n",
            "1      \t [-0.42084996  1.1040236 ]. \t  -87.93482613667966 \t -3.486729021084656\n",
            "2      \t [0.09130205 0.2771249 ]. \t  -8.050476078634611 \t -3.486729021084656\n",
            "3      \t [1.34027586 1.76760892]. \t  \u001b[92m-0.19833152756144756\u001b[0m \t -0.19833152756144756\n",
            "4      \t [-0.18756391  0.42579889]. \t  -16.66860272011216 \t -0.19833152756144756\n",
            "5      \t [0.88246179 1.50452626]. \t  -52.69055737972176 \t -0.19833152756144756\n",
            "6      \t [2.048 2.048]. \t  -461.7603900415999 \t -0.19833152756144756\n",
            "7      \t [1.27982732 1.09230691]. \t  -29.851811719281347 \t -0.19833152756144756\n",
            "8      \t [-1.44807573  1.90157698]. \t  -9.809094279930342 \t -0.19833152756144756\n",
            "9      \t [1.25353455 2.01222292]. \t  -19.501272900917254 \t -0.19833152756144756\n",
            "10     \t [-0.08309162 -0.58835563]. \t  -36.606516095015934 \t -0.19833152756144756\n",
            "11     \t [1.17058719 1.45790505]. \t  -0.7970136470260041 \t -0.19833152756144756\n",
            "12     \t [-0.28673025  0.01206275]. \t  -2.147797915596625 \t -0.19833152756144756\n",
            "13     \t [-1.05613481  1.95822733]. \t  -75.25998743816882 \t -0.19833152756144756\n",
            "14     \t [-2.02019481  1.97518937]. \t  -452.6442141264078 \t -0.19833152756144756\n",
            "15     \t [-1.10078946  1.10156659]. \t  -5.627078020735778 \t -0.19833152756144756\n",
            "16     \t [-1.23057701  1.36250531]. \t  -7.2802368441914975 \t -0.19833152756144756\n",
            "17     \t [1.2635343 1.6311403]. \t  \u001b[92m-0.1893143427914994\u001b[0m \t -0.1893143427914994\n",
            "18     \t [-0.09037941 -0.14651037]. \t  -3.5814806229643863 \t -0.1893143427914994\n",
            "19     \t [-0.02731033 -1.89361341]. \t  -359.91506875937273 \t -0.1893143427914994\n",
            "20     \t [1.25599916 1.7389015 ]. \t  -2.6694859406391584 \t -0.1893143427914994\n",
            "21     \t [-0.11723884  0.01041056]. \t  -1.2493344301130864 \t -0.1893143427914994\n",
            "22     \t [1.05813244 0.2848522 ]. \t  -69.69115789460176 \t -0.1893143427914994\n",
            "23     \t [0.40881591 0.13015602]. \t  -0.486209457228743 \t -0.1893143427914994\n",
            "24     \t [0.54315476 0.38594919]. \t  -1.0355720404930138 \t -0.1893143427914994\n",
            "25     \t [0.48449582 0.22766394]. \t  -0.27074624249731616 \t -0.1893143427914994\n",
            "26     \t [-1.85090769 -0.12665691]. \t  -1270.1647887856454 \t -0.1893143427914994\n",
            "27     \t [0.60526501 0.37829678]. \t  \u001b[92m-0.1700984758265328\u001b[0m \t -0.1700984758265328\n",
            "28     \t [0.55059748 0.34801746]. \t  -0.40320350989624154 \t -0.1700984758265328\n",
            "29     \t [0.47891703 0.23013544]. \t  -0.2715873610350972 \t -0.1700984758265328\n",
            "30     \t [0.18541668 2.02241846]. \t  -395.8934976899818 \t -0.1700984758265328\n",
            "31     \t [0.52642259 0.28736236]. \t  -0.23476464372462252 \t -0.1700984758265328\n",
            "32     \t [0.75375726 1.95812448]. \t  -193.26353764808724 \t -0.1700984758265328\n",
            "33     \t [-1.33865525  1.99377279]. \t  -9.540619700456945 \t -0.1700984758265328\n",
            "34     \t [-0.73129346  0.5964421 ]. \t  -3.377473617663559 \t -0.1700984758265328\n",
            "35     \t [0.48521557 0.25912616]. \t  -0.3211341401470047 \t -0.1700984758265328\n",
            "36     \t [0.56146413 1.02155538]. \t  -50.080178024431504 \t -0.1700984758265328\n",
            "37     \t [0.82231768 0.67927012]. \t  \u001b[92m-0.03250966188882625\u001b[0m \t -0.03250966188882625\n",
            "38     \t [-1.26709962  1.64306065]. \t  -5.28050981937662 \t -0.03250966188882625\n",
            "39     \t [1.26642229 1.5848537 ]. \t  -0.10697346879458314 \t -0.03250966188882625\n",
            "40     \t [-0.33311469  0.41650083]. \t  -11.11238496663284 \t -0.03250966188882625\n",
            "41     \t [1.26354807 1.59274078]. \t  -0.07091144896219523 \t -0.03250966188882625\n",
            "42     \t [-0.0081328  -1.64996496]. \t  -273.2765953041409 \t -0.03250966188882625\n",
            "43     \t [0.24031165 0.7048205 ]. \t  -42.4471901057517 \t -0.03250966188882625\n",
            "44     \t [ 1.61631112 -1.48418493]. \t  -1678.6311613573498 \t -0.03250966188882625\n",
            "45     \t [-1.69413816  0.3032082 ]. \t  -666.1538340808597 \t -0.03250966188882625\n",
            "46     \t [0.83432269 0.70029004]. \t  \u001b[92m-0.029209353426092856\u001b[0m \t -0.029209353426092856\n",
            "47     \t [-0.84626561  0.11723745]. \t  -39.28017645872388 \t -0.029209353426092856\n",
            "48     \t [0.29733632 0.12212438]. \t  -0.6074096774597729 \t -0.029209353426092856\n",
            "49     \t [1.29295007 1.69839462]. \t  -0.15697385171704595 \t -0.029209353426092856\n",
            "50     \t [ 1.60159055 -1.12281436]. \t  -1360.4274655446186 \t -0.029209353426092856\n",
            "51     \t [0.67147936 0.42168057]. \t  -0.19321292091175873 \t -0.029209353426092856\n",
            "52     \t [1.91380379 0.04697279]. \t  -1308.1435369146268 \t -0.029209353426092856\n",
            "53     \t [0.49652228 0.17909307]. \t  -0.7083227934445969 \t -0.029209353426092856\n",
            "54     \t [1.1134421  1.25669822]. \t  -0.04158213219869683 \t -0.029209353426092856\n",
            "55     \t [-1.76315685  0.68887059]. \t  -593.2031575612671 \t -0.029209353426092856\n",
            "56     \t [1.23161183 1.54593794]. \t  -0.13815188777512266 \t -0.029209353426092856\n",
            "57     \t [1.30376312 1.67846651]. \t  -0.1377764649882903 \t -0.029209353426092856\n",
            "58     \t [-1.73618415 -0.26471173]. \t  -1082.701722128939 \t -0.029209353426092856\n",
            "59     \t [1.00231572 1.01679636]. \t  \u001b[92m-0.014790859610477337\u001b[0m \t -0.014790859610477337\n",
            "60     \t [1.11052663 1.22428929]. \t  -0.020280356071885546 \t -0.014790859610477337\n",
            "61     \t [0.59093498 0.33493549]. \t  -0.18769367281958008 \t -0.014790859610477337\n",
            "62     \t [-2.03594066 -1.14187835]. \t  -2804.382699699828 \t -0.014790859610477337\n",
            "63     \t [0.5867128  0.25919336]. \t  -0.8939618291042228 \t -0.014790859610477337\n",
            "64     \t [-1.24991045  1.5834155 ]. \t  -5.106784346627112 \t -0.014790859610477337\n",
            "65     \t [1.50125143 1.62555719]. \t  -39.714607462353506 \t -0.014790859610477337\n",
            "66     \t [-0.22080734  1.68418939]. \t  -268.9546463000948 \t -0.014790859610477337\n",
            "67     \t [1.08240709 1.4462156 ]. \t  -7.547883620562839 \t -0.014790859610477337\n",
            "68     \t [-0.19707086  1.02216595]. \t  -98.1265751377523 \t -0.014790859610477337\n",
            "69     \t [0.98686568 1.00639444]. \t  -0.10573630645673177 \t -0.014790859610477337\n",
            "70     \t [1.25117247 1.55795791]. \t  -0.06867464607512438 \t -0.014790859610477337\n",
            "71     \t [1.28082171 1.63700312]. \t  -0.08008663068279206 \t -0.014790859610477337\n",
            "72     \t [ 0.78084473 -1.0355336 ]. \t  -270.73347291450534 \t -0.014790859610477337\n",
            "73     \t [1.24548825 1.57593378]. \t  -0.12123790820979732 \t -0.014790859610477337\n",
            "74     \t [1.57236249 1.21634884]. \t  -158.0749092992291 \t -0.014790859610477337\n",
            "75     \t [0.94808783 0.88381763]. \t  -0.025353876616298517 \t -0.014790859610477337\n",
            "76     \t [-0.53765381 -0.87006796]. \t  -136.72483573616964 \t -0.014790859610477337\n",
            "77     \t [ 1.71771102 -1.06836806]. \t  -1615.6701892856786 \t -0.014790859610477337\n",
            "78     \t [1.99507809 0.19555372]. \t  -1433.4483271429306 \t -0.014790859610477337\n",
            "79     \t [0.96865643 0.9610416 ]. \t  -0.05272190712852584 \t -0.014790859610477337\n",
            "80     \t [1.2212461  1.46544867]. \t  -0.11651529469768873 \t -0.014790859610477337\n",
            "81     \t [0.39197243 0.75268142]. \t  -36.25447406670638 \t -0.014790859610477337\n",
            "82     \t [-1.39694674  1.90175093]. \t  -5.99245488403269 \t -0.014790859610477337\n",
            "83     \t [0.59889046 0.3338352 ]. \t  -0.22256449472436432 \t -0.014790859610477337\n",
            "84     \t [-0.03631004 -0.17967916]. \t  -4.349950937713276 \t -0.014790859610477337\n",
            "85     \t [0.96054307 0.9495561 ]. \t  -0.0739883614669761 \t -0.014790859610477337\n",
            "86     \t [ 0.74943733 -1.54421299]. \t  -443.5313342993164 \t -0.014790859610477337\n",
            "87     \t [1.06260063 1.13759807]. \t  \u001b[92m-0.01110642749342224\u001b[0m \t -0.01110642749342224\n",
            "88     \t [-1.45703678  0.68005051]. \t  -214.23470547517417 \t -0.01110642749342224\n",
            "89     \t [0.94694736 0.90103115]. \t  \u001b[92m-0.004682410240807627\u001b[0m \t -0.004682410240807627\n",
            "90     \t [ 0.23336998 -0.50785557]. \t  -32.207775776733314 \t -0.004682410240807627\n",
            "91     \t [1.25023007 1.52237014]. \t  -0.22830557401902302 \t -0.004682410240807627\n",
            "92     \t [1.23135719 1.48862354]. \t  -0.1297960097379552 \t -0.004682410240807627\n",
            "93     \t [0.83485889 0.71998434]. \t  -0.0801484523398668 \t -0.004682410240807627\n",
            "94     \t [-0.00623463  0.74020108]. \t  -55.796518282112864 \t -0.004682410240807627\n",
            "95     \t [-0.99623645 -0.80853576]. \t  -328.3532776023888 \t -0.004682410240807627\n",
            "96     \t [-0.92802601  0.40819344]. \t  -24.241702126559957 \t -0.004682410240807627\n",
            "97     \t [-0.17331047  1.9859159 ]. \t  -383.9230711749065 \t -0.004682410240807627\n",
            "98     \t [-0.21997478  1.33713612]. \t  -167.57527888398772 \t -0.004682410240807627\n",
            "99     \t [0.88425004 0.74788956]. \t  -0.12905644906559613 \t -0.004682410240807627\n",
            "100    \t [1.00490746 0.99039352]. \t  -0.03783678669409362 \t -0.004682410240807627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Aur-aRtdq-",
        "outputId": "18c6883a-b685-4361-e3c2-7b3fb7eb7f4f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_loser_10 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_10 = dGPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.62910294 -1.57693156]. \t  -389.29291138113445 \t -8.580376531587937\n",
            "init   \t [ 1.84435861 -0.07294402]. \t  -1207.999341247548 \t -8.580376531587937\n",
            "init   \t [ 1.5256557  -1.17828534]. \t  -1229.4172568797705 \t -8.580376531587937\n",
            "init   \t [-1.88125338 -0.42109149]. \t  -1576.6245828972787 \t -8.580376531587937\n",
            "init   \t [-1.09309052  1.39977001]. \t  -8.580376531587937 \t -8.580376531587937\n",
            "1      \t [-0.86019812  2.03750805]. \t  -171.82841453589126 \t -8.580376531587937\n",
            "2      \t [-0.09725394 -1.98440571]. \t  -398.75334498229967 \t -8.580376531587937\n",
            "3      \t [-0.71787532  1.26192598]. \t  -58.689416526596894 \t -8.580376531587937\n",
            "4      \t [-1.66940512  1.65929749]. \t  -134.27750112819044 \t -8.580376531587937\n",
            "5      \t [-0.21590304  1.35197276]. \t  -171.8745365573776 \t -8.580376531587937\n",
            "6      \t [ 0.55073144 -1.79055537]. \t  -438.6270149029219 \t -8.580376531587937\n",
            "7      \t [0.98226372 1.2697215 ]. \t  -9.295464012565606 \t -8.580376531587937\n",
            "8      \t [1.31261675 1.94600611]. \t  \u001b[92m-5.072563300257509\u001b[0m \t -5.072563300257509\n",
            "9      \t [1.33885437 1.58232492]. \t  \u001b[92m-4.533482903732608\u001b[0m \t -4.533482903732608\n",
            "10     \t [0.99101191 1.48541347]. \t  -25.332062695007775 \t -4.533482903732608\n",
            "11     \t [2.048 2.048]. \t  -461.7603900415999 \t -4.533482903732608\n",
            "12     \t [0.33393033 0.2144096 ]. \t  \u001b[92m-1.5024927073103536\u001b[0m \t -1.5024927073103536\n",
            "13     \t [ 0.19059185 -0.64759425]. \t  -47.42973036097653 \t -1.5024927073103536\n",
            "14     \t [0.33229456 0.37591818]. \t  -7.494776076220637 \t -1.5024927073103536\n",
            "15     \t [1.16362594 1.2521774 ]. \t  \u001b[92m-1.0640735852769558\u001b[0m \t -1.0640735852769558\n",
            "16     \t [0.46308579 0.16444882]. \t  \u001b[92m-0.5382731530325593\u001b[0m \t -0.5382731530325593\n",
            "17     \t [0.41415073 0.21195831]. \t  \u001b[92m-0.506738360174596\u001b[0m \t -0.506738360174596\n",
            "18     \t [ 0.39018098 -0.28753843]. \t  -19.7124915961744 \t -0.506738360174596\n",
            "19     \t [1.2346141 1.5506564]. \t  \u001b[92m-0.1246575074161505\u001b[0m \t -0.1246575074161505\n",
            "20     \t [-1.87856098  1.30352094]. \t  -503.5579671203956 \t -0.1246575074161505\n",
            "21     \t [0.41637339 0.15656252]. \t  -0.3688584008621456 \t -0.1246575074161505\n",
            "22     \t [-1.06254814  1.55142248]. \t  -22.09745810159913 \t -0.1246575074161505\n",
            "23     \t [1.20862894 1.43169714]. \t  -0.12813013041855423 \t -0.1246575074161505\n",
            "24     \t [0.43119568 0.17727596]. \t  -0.3310270984721544 \t -0.1246575074161505\n",
            "25     \t [0.76153104 0.72322349]. \t  -2.110183427272423 \t -0.1246575074161505\n",
            "26     \t [1.18456536 1.3504118 ]. \t  -0.3126719000090379 \t -0.1246575074161505\n",
            "27     \t [1.23787765 1.47346854]. \t  -0.4031832918887638 \t -0.1246575074161505\n",
            "28     \t [0.35048281 1.73211457]. \t  -259.39891582077297 \t -0.1246575074161505\n",
            "29     \t [-1.5112475   2.02381614]. \t  -13.069112565943167 \t -0.1246575074161505\n",
            "30     \t [-1.81413059 -1.87982595]. \t  -2681.735608840056 \t -0.1246575074161505\n",
            "31     \t [ 0.66620021 -1.06959045]. \t  -229.15336766252585 \t -0.1246575074161505\n",
            "32     \t [0.43068812 0.20771118]. \t  -0.3734840417386905 \t -0.1246575074161505\n",
            "33     \t [ 2.02131551 -1.66183259]. \t  -3304.4750197844164 \t -0.1246575074161505\n",
            "34     \t [ 0.25121189 -0.1424468 ]. \t  -4.785937184173313 \t -0.1246575074161505\n",
            "35     \t [ 0.22527611 -1.7089182 ]. \t  -310.24317612133376 \t -0.1246575074161505\n",
            "36     \t [1.18556672 1.35932009]. \t  -0.24832596302329496 \t -0.1246575074161505\n",
            "37     \t [1.22762998 1.53356027]. \t  \u001b[92m-0.12196035733472323\u001b[0m \t -0.12196035733472323\n",
            "38     \t [1.37993224 0.76530838]. \t  -129.85472110547894 \t -0.12196035733472323\n",
            "39     \t [-0.81057464  1.07467568]. \t  -20.720868440719848 \t -0.12196035733472323\n",
            "40     \t [ 1.36429407 -1.81891743]. \t  -1354.5315001039999 \t -0.12196035733472323\n",
            "41     \t [-0.98372483  0.52651721]. \t  -23.400672927577297 \t -0.12196035733472323\n",
            "42     \t [1.17305896 0.13752364]. \t  -153.42899481728517 \t -0.12196035733472323\n",
            "43     \t [1.22825442 1.45330983]. \t  -0.3578988943997122 \t -0.12196035733472323\n",
            "44     \t [-1.09851212 -1.00399318]. \t  -493.1329517742785 \t -0.12196035733472323\n",
            "45     \t [-0.51145232 -0.40888856]. \t  -47.2377621751766 \t -0.12196035733472323\n",
            "46     \t [-0.40623635 -1.12689812]. \t  -168.8848033846799 \t -0.12196035733472323\n",
            "47     \t [ 2.00666098 -0.03070804]. \t  -1647.2598439717965 \t -0.12196035733472323\n",
            "48     \t [-0.41532174 -0.47551646]. \t  -43.99465085536307 \t -0.12196035733472323\n",
            "49     \t [ 1.28532725 -1.43822542]. \t  -955.0715968483435 \t -0.12196035733472323\n",
            "50     \t [ 0.57893847 -1.48140375]. \t  -330.1712234659606 \t -0.12196035733472323\n",
            "51     \t [-0.95174761  0.94130511]. \t  -3.9352130961413714 \t -0.12196035733472323\n",
            "52     \t [-0.01548409  0.04620613]. \t  -1.242498702837128 \t -0.12196035733472323\n",
            "53     \t [-0.55717195  0.2036472 ]. \t  -3.5652671206950464 \t -0.12196035733472323\n",
            "54     \t [ 0.13208979 -0.01989839]. \t  -0.8927412743957782 \t -0.12196035733472323\n",
            "55     \t [0.35284375 0.14920397]. \t  -0.47984618984024335 \t -0.12196035733472323\n",
            "56     \t [0.55084584 1.55763767]. \t  -157.50514222288103 \t -0.12196035733472323\n",
            "57     \t [-1.98002668 -1.70712373]. \t  -3175.901794188777 \t -0.12196035733472323\n",
            "58     \t [0.18179443 0.00102903]. \t  -0.7719895520120091 \t -0.12196035733472323\n",
            "59     \t [1.93029386 0.8663766 ]. \t  -818.6297032102251 \t -0.12196035733472323\n",
            "60     \t [-0.58186118  0.32100713]. \t  -2.533103648209388 \t -0.12196035733472323\n",
            "61     \t [1.26917016 1.63733241]. \t  -0.14288716136459284 \t -0.12196035733472323\n",
            "62     \t [-1.53711804  0.59339743]. \t  -319.4914002029559 \t -0.12196035733472323\n",
            "63     \t [1.03984855 1.01302511]. \t  -0.46752924928229944 \t -0.12196035733472323\n",
            "64     \t [ 0.34578262 -0.99307802]. \t  -124.22558745727085 \t -0.12196035733472323\n",
            "65     \t [ 1.22527161 -0.36433001]. \t  -348.1047442233006 \t -0.12196035733472323\n",
            "66     \t [0.9582054  0.88846632]. \t  \u001b[92m-0.08990390927979725\u001b[0m \t -0.08990390927979725\n",
            "67     \t [0.97253808 0.96107658]. \t  \u001b[92m-0.023999031551614876\u001b[0m \t -0.023999031551614876\n",
            "68     \t [-0.37329274  1.59657522]. \t  -214.23720569315782 \t -0.023999031551614876\n",
            "69     \t [0.97148866 0.93859269]. \t  \u001b[92m-0.0035143294462421076\u001b[0m \t -0.0035143294462421076\n",
            "70     \t [0.87729541 0.79974483]. \t  -0.10564292944820287 \t -0.0035143294462421076\n",
            "71     \t [0.84160655 0.70851516]. \t  -0.025093047031433585 \t -0.0035143294462421076\n",
            "72     \t [-1.72421319 -0.65417702]. \t  -1322.9981744186289 \t -0.0035143294462421076\n",
            "73     \t [ 1.24898057 -1.24694444]. \t  -787.9290060785235 \t -0.0035143294462421076\n",
            "74     \t [1.24007303 1.54893508]. \t  -0.07007612892122705 \t -0.0035143294462421076\n",
            "75     \t [-1.2773907  -0.23007098]. \t  -351.8156843349986 \t -0.0035143294462421076\n",
            "76     \t [-1.28250973 -1.78120988]. \t  -1178.9856005689219 \t -0.0035143294462421076\n",
            "77     \t [0.93846077 0.86695317]. \t  -0.0227083060154697 \t -0.0035143294462421076\n",
            "78     \t [0.87519568 0.78968233]. \t  -0.0718155250039523 \t -0.0035143294462421076\n",
            "79     \t [-1.17896272  0.67580822]. \t  -55.74816867150558 \t -0.0035143294462421076\n",
            "80     \t [1.01591254 1.71707928]. \t  -46.92288984327722 \t -0.0035143294462421076\n",
            "81     \t [-0.09053564 -1.38041038]. \t  -194.0122314118666 \t -0.0035143294462421076\n",
            "82     \t [-1.44375349  0.00707512]. \t  -437.5098304110983 \t -0.0035143294462421076\n",
            "83     \t [-1.76670334 -0.90938185]. \t  -1632.2464505607536 \t -0.0035143294462421076\n",
            "84     \t [ 1.70713285 -0.89835968]. \t  -1454.1393775178587 \t -0.0035143294462421076\n",
            "85     \t [0.99343787 1.01420368]. \t  -0.07448954947368197 \t -0.0035143294462421076\n",
            "86     \t [-0.796771    1.73565395]. \t  -124.40663419788791 \t -0.0035143294462421076\n",
            "87     \t [1.19319732 1.40742892]. \t  -0.06386465037581994 \t -0.0035143294462421076\n",
            "88     \t [-0.32412935 -0.05427506]. \t  -4.292079571359494 \t -0.0035143294462421076\n",
            "89     \t [-1.45123758  0.69030709]. \t  -206.45283409422657 \t -0.0035143294462421076\n",
            "90     \t [-1.81082405 -1.93748806]. \t  -2729.162850805099 \t -0.0035143294462421076\n",
            "91     \t [1.45357638 0.68649884]. \t  -203.6632813385702 \t -0.0035143294462421076\n",
            "92     \t [ 1.11334257 -1.4045816 ]. \t  -699.1463474533937 \t -0.0035143294462421076\n",
            "93     \t [-1.84946302 -1.03142848]. \t  -1990.0981513159281 \t -0.0035143294462421076\n",
            "94     \t [1.54089267 1.39196953]. \t  -96.79974380386078 \t -0.0035143294462421076\n",
            "95     \t [ 0.56901041 -0.44080524]. \t  -58.64371641368977 \t -0.0035143294462421076\n",
            "96     \t [-0.84258106  1.8072523 ]. \t  -123.80390894280549 \t -0.0035143294462421076\n",
            "97     \t [-0.36971649 -1.66008565]. \t  -324.71649979111015 \t -0.0035143294462421076\n",
            "98     \t [1.42308353 0.96051045]. \t  -113.5282996267748 \t -0.0035143294462421076\n",
            "99     \t [0.67810219 1.92564012]. \t  -214.96572407533358 \t -0.0035143294462421076\n",
            "100    \t [1.43707224 0.96877898]. \t  -120.39981096582514 \t -0.0035143294462421076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMu7U9NNtdtn",
        "outputId": "8b53ef61-2e0b-4e11-e0fb-c802889f15a5"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_loser_11 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_11 = dGPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.97032207 1.31583119]. \t  -659.5505379977991 \t -45.00854175284071\n",
            "init   \t [ 0.59758024 -0.3125739 ]. \t  -45.00854175284071 \t -45.00854175284071\n",
            "init   \t [-1.21933423 -0.03314987]. \t  -235.94289844157058 \t -45.00854175284071\n",
            "init   \t [-1.48046507 -0.19447384]. \t  -575.5719351651762 \t -45.00854175284071\n",
            "init   \t [-1.58214297 -2.0360213 ]. \t  -2067.0989998073865 \t -45.00854175284071\n",
            "1      \t [-0.59782787  0.66030788]. \t  \u001b[92m-11.728483422473923\u001b[0m \t -11.728483422473923\n",
            "2      \t [-0.44273339  0.04021619]. \t  \u001b[92m-4.508739622784907\u001b[0m \t -4.508739622784907\n",
            "3      \t [-1.11955987  0.65043597]. \t  -40.85082114690448 \t -4.508739622784907\n",
            "4      \t [ 1.30792565 -1.05782521]. \t  -766.5511117868322 \t -4.508739622784907\n",
            "5      \t [0.38613963 0.10451944]. \t  \u001b[92m-0.5756012078214707\u001b[0m \t -0.5756012078214707\n",
            "6      \t [-0.80037406  0.38397636]. \t  -9.826845764091287 \t -0.5756012078214707\n",
            "7      \t [-0.36159767  1.94598938]. \t  -331.36230478725497 \t -0.5756012078214707\n",
            "8      \t [ 0.0915573  -0.33597592]. \t  -12.68355659131007 \t -0.5756012078214707\n",
            "9      \t [ 0.49826633 -0.0275608 ]. \t  -7.859963003684763 \t -0.5756012078214707\n",
            "10     \t [0.05276126 0.71526245]. \t  -51.659851309855156 \t -0.5756012078214707\n",
            "11     \t [ 0.20543015 -0.0638274 ]. \t  -1.7555549345227295 \t -0.5756012078214707\n",
            "12     \t [-1.45073686  1.55274697]. \t  -36.46442125185375 \t -0.5756012078214707\n",
            "13     \t [-1.09782657 -0.40581759]. \t  -263.94610895647355 \t -0.5756012078214707\n",
            "14     \t [-0.88857968  0.87009475]. \t  -4.215094951276874 \t -0.5756012078214707\n",
            "15     \t [-0.88702882 -0.79713069]. \t  -254.45089311807683 \t -0.5756012078214707\n",
            "16     \t [ 1.92259004 -0.29129619]. \t  -1590.985338494926 \t -0.5756012078214707\n",
            "17     \t [-0.52399875 -0.16859288]. \t  -21.962321500844066 \t -0.5756012078214707\n",
            "18     \t [ 0.22555161 -1.59901435]. \t  -272.81276944773344 \t -0.5756012078214707\n",
            "19     \t [-1.17547357  1.23700797]. \t  -6.827366201300569 \t -0.5756012078214707\n",
            "20     \t [0.10088775 1.46281324]. \t  -211.8232198151821 \t -0.5756012078214707\n",
            "21     \t [0.61794618 1.65841838]. \t  -163.10673822296363 \t -0.5756012078214707\n",
            "22     \t [ 0.0099395  -1.73764558]. \t  -302.9557719589955 \t -0.5756012078214707\n",
            "23     \t [0.29559594 0.04927424]. \t  -0.6413667749220707 \t -0.5756012078214707\n",
            "24     \t [0.72442873 0.75438542]. \t  -5.3470247263402015 \t -0.5756012078214707\n",
            "25     \t [-1.61216036 -0.94126898]. \t  -1260.2170238403203 \t -0.5756012078214707\n",
            "26     \t [-1.33775942 -1.84923898]. \t  -1329.5802276803001 \t -0.5756012078214707\n",
            "27     \t [0.64497691 0.54156911]. \t  -1.7029216576256063 \t -0.5756012078214707\n",
            "28     \t [ 1.75863482 -1.59610798]. \t  -2199.157970132603 \t -0.5756012078214707\n",
            "29     \t [-1.67254638  0.24145778]. \t  -660.4323974788158 \t -0.5756012078214707\n",
            "30     \t [-1.67066864 -1.91418671]. \t  -2221.136489962148 \t -0.5756012078214707\n",
            "31     \t [0.48590246 0.22234361]. \t  \u001b[92m-0.28322340436136556\u001b[0m \t -0.28322340436136556\n",
            "32     \t [-0.96901408 -0.41602442]. \t  -187.4829578926612 \t -0.28322340436136556\n",
            "33     \t [ 0.28557859 -0.87338187]. \t  -91.7008658277363 \t -0.28322340436136556\n",
            "34     \t [1.694842   1.68393527]. \t  -141.74889602888092 \t -0.28322340436136556\n",
            "35     \t [1.33505181 1.86729779]. \t  -0.8336459762839883 \t -0.28322340436136556\n",
            "36     \t [ 0.94980219 -0.49364355]. \t  -194.81928355041182 \t -0.28322340436136556\n",
            "37     \t [1.4656905  2.04725089]. \t  -1.2369219481070255 \t -0.28322340436136556\n",
            "38     \t [1.75118987 0.07716607]. \t  -894.2752412813545 \t -0.28322340436136556\n",
            "39     \t [0.77561893 0.44447291]. \t  -2.5187588695303353 \t -0.28322340436136556\n",
            "40     \t [1.18672693 1.5916093 ]. \t  -3.394333717925701 \t -0.28322340436136556\n",
            "41     \t [-0.84178006  1.97879725]. \t  -164.73386512539443 \t -0.28322340436136556\n",
            "42     \t [1.45709914 1.99815482]. \t  -1.7710167983531853 \t -0.28322340436136556\n",
            "43     \t [-0.0834664   1.76338305]. \t  -309.67376133433913 \t -0.28322340436136556\n",
            "44     \t [-0.56907761 -0.55733849]. \t  -80.11120082104294 \t -0.28322340436136556\n",
            "45     \t [ 0.13646305 -0.627146  ]. \t  -42.4473482539538 \t -0.28322340436136556\n",
            "46     \t [0.62933856 0.32434989]. \t  -0.6517246168226211 \t -0.28322340436136556\n",
            "47     \t [-0.61562099 -0.90126674]. \t  -166.51575971908775 \t -0.28322340436136556\n",
            "48     \t [0.55772191 0.29649741]. \t  \u001b[92m-0.21679854666071596\u001b[0m \t -0.21679854666071596\n",
            "49     \t [-1.1296874  -0.20714311]. \t  -224.56435014526477 \t -0.21679854666071596\n",
            "50     \t [0.59493351 0.34989462]. \t  \u001b[92m-0.16572013193456184\u001b[0m \t -0.16572013193456184\n",
            "51     \t [-0.02905022  0.00453234]. \t  -1.0603048036272553 \t -0.16572013193456184\n",
            "52     \t [-2.00016163  1.27511611]. \t  -751.8525811840013 \t -0.16572013193456184\n",
            "53     \t [-1.39269204  1.89928462]. \t  -5.887436572264647 \t -0.16572013193456184\n",
            "54     \t [-1.34635848  0.99460828]. \t  -72.42972137593217 \t -0.16572013193456184\n",
            "55     \t [0.71509222 0.4660443 ]. \t  -0.28649553412861256 \t -0.16572013193456184\n",
            "56     \t [ 0.78222375 -0.59035082]. \t  -144.58187812082383 \t -0.16572013193456184\n",
            "57     \t [-0.33202664 -1.0029603 ]. \t  -125.69616167866137 \t -0.16572013193456184\n",
            "58     \t [-0.99139494  1.52759432]. \t  -33.63877408109947 \t -0.16572013193456184\n",
            "59     \t [-1.20585533 -0.44728188]. \t  -366.38619263800217 \t -0.16572013193456184\n",
            "60     \t [ 1.36458872 -0.59491836]. \t  -603.8280138637097 \t -0.16572013193456184\n",
            "61     \t [0.30952834 0.29868557]. \t  -4.592690227278553 \t -0.16572013193456184\n",
            "62     \t [-0.02256099 -1.87738528]. \t  -353.69432248362006 \t -0.16572013193456184\n",
            "63     \t [-0.52472305  1.61408781]. \t  -181.55088101018254 \t -0.16572013193456184\n",
            "64     \t [0.87856461 0.76935759]. \t  \u001b[92m-0.01538067563656944\u001b[0m \t -0.01538067563656944\n",
            "65     \t [0.34149461 1.87892436]. \t  -311.0057994969181 \t -0.01538067563656944\n",
            "66     \t [-1.82360152 -0.2660919 ]. \t  -1297.942128509376 \t -0.01538067563656944\n",
            "67     \t [-1.13713511 -0.21115239]. \t  -230.83773110929698 \t -0.01538067563656944\n",
            "68     \t [-0.0171681   1.28735797]. \t  -166.6878061711111 \t -0.01538067563656944\n",
            "69     \t [-0.52678452  0.87257878]. \t  -37.74271649707644 \t -0.01538067563656944\n",
            "70     \t [0.04898973 0.33712364]. \t  -12.10841225009147 \t -0.01538067563656944\n",
            "71     \t [-0.01404161 -0.22677107]. \t  -6.179738497208102 \t -0.01538067563656944\n",
            "72     \t [0.83353528 0.68758315]. \t  -0.032891494022426415 \t -0.01538067563656944\n",
            "73     \t [0.82208687 1.0993916 ]. \t  -17.972365520672504 \t -0.01538067563656944\n",
            "74     \t [-0.49806933  1.27422427]. \t  -107.54284260181514 \t -0.01538067563656944\n",
            "75     \t [0.77319092 0.92358938]. \t  -10.663737077866921 \t -0.01538067563656944\n",
            "76     \t [ 0.72625056 -1.8458145 ]. \t  -563.308570745773 \t -0.01538067563656944\n",
            "77     \t [-1.44839121  0.77855851]. \t  -180.04421945941743 \t -0.01538067563656944\n",
            "78     \t [ 0.28985867 -1.50123496]. \t  -251.80701272330782 \t -0.01538067563656944\n",
            "79     \t [ 1.59307307 -1.7643583 ]. \t  -1851.2787304984117 \t -0.01538067563656944\n",
            "80     \t [-2.03817801  1.59554601]. \t  -663.8859957234097 \t -0.01538067563656944\n",
            "81     \t [ 0.50290089 -0.10287362]. \t  -12.90525671117939 \t -0.01538067563656944\n",
            "82     \t [ 1.519587   -0.68308356]. \t  -895.6129325439816 \t -0.01538067563656944\n",
            "83     \t [0.25956166 1.4313478 ]. \t  -186.59117783448178 \t -0.01538067563656944\n",
            "84     \t [ 1.256649   -1.63260162]. \t  -1031.6114516563475 \t -0.01538067563656944\n",
            "85     \t [-1.53789396 -0.57477465]. \t  -870.7376845511368 \t -0.01538067563656944\n",
            "86     \t [0.29759459 1.07304245]. \t  -97.4134417901508 \t -0.01538067563656944\n",
            "87     \t [-1.78556404 -1.65965977]. \t  -2357.9715377964612 \t -0.01538067563656944\n",
            "88     \t [-1.01750388 -0.1158688 ]. \t  -136.5925399122599 \t -0.01538067563656944\n",
            "89     \t [0.36842466 1.79975852]. \t  -277.29573833523074 \t -0.01538067563656944\n",
            "90     \t [-1.0695059   0.02320873]. \t  -129.8649439623663 \t -0.01538067563656944\n",
            "91     \t [0.78007427 0.62671844]. \t  -0.08150071301785675 \t -0.01538067563656944\n",
            "92     \t [-1.79594354  1.39508404]. \t  -342.82778427199776 \t -0.01538067563656944\n",
            "93     \t [0.84468995 0.67635742]. \t  -0.1620866730966448 \t -0.01538067563656944\n",
            "94     \t [ 0.10746123 -1.0786201 ]. \t  -119.64325677315229 \t -0.01538067563656944\n",
            "95     \t [0.68608758 1.26055941]. \t  -62.48377473932666 \t -0.01538067563656944\n",
            "96     \t [0.91835148 0.76807707]. \t  -0.5735606511488468 \t -0.01538067563656944\n",
            "97     \t [-1.81455988 -0.57823758]. \t  -1506.2814452342047 \t -0.01538067563656944\n",
            "98     \t [0.6486892  1.18761898]. \t  -58.92490993311827 \t -0.01538067563656944\n",
            "99     \t [-1.09863269  0.1074321 ]. \t  -125.3078511226666 \t -0.01538067563656944\n",
            "100    \t [-0.61278378 -0.73685194]. \t  -126.33463761962125 \t -0.01538067563656944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjKllHqtdwW",
        "outputId": "9e758c81-286a-4a91-cf7c-ba2539c26e9c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_loser_12 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_12 = dGPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.26353633  0.50015753]. \t  -125.32555445527122 \t -19.52113145175031\n",
            "init   \t [-0.25506718  1.16882876]. \t  -123.40590278793995 \t -19.52113145175031\n",
            "init   \t [ 1.14678091 -0.93146069]. \t  -504.72793807789463 \t -19.52113145175031\n",
            "init   \t [-0.91560241  1.23646844]. \t  -19.52113145175031 \t -19.52113145175031\n",
            "init   \t [1.87653879 1.53982007]. \t  -393.43336554061835 \t -19.52113145175031\n",
            "1      \t [-1.50623477  1.94347507]. \t  \u001b[92m-16.861146488467053\u001b[0m \t -16.861146488467053\n",
            "2      \t [-1.17298972  1.69858754]. \t  \u001b[92m-15.134293586652838\u001b[0m \t -15.134293586652838\n",
            "3      \t [-1.88219337  1.81269391]. \t  -307.58250276319126 \t -15.134293586652838\n",
            "4      \t [-0.74187204  1.95815573]. \t  -201.21902316932247 \t -15.134293586652838\n",
            "5      \t [-0.46123599  0.48505636]. \t  \u001b[92m-9.550904750813473\u001b[0m \t -9.550904750813473\n",
            "6      \t [-0.71357184  0.79843935]. \t  -11.303149762215897 \t -9.550904750813473\n",
            "7      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -9.550904750813473\n",
            "8      \t [-1.53349463  2.03551595]. \t  -16.409873778429322 \t -9.550904750813473\n",
            "9      \t [-1.11646688  1.08871131]. \t  \u001b[92m-6.969105605243006\u001b[0m \t -6.969105605243006\n",
            "10     \t [-1.01357959  1.07085675]. \t  \u001b[92m-4.243842336959228\u001b[0m \t -4.243842336959228\n",
            "11     \t [-0.12044632 -0.03853364]. \t  \u001b[92m-1.5367342684524132\u001b[0m \t -1.5367342684524132\n",
            "12     \t [-0.14909959  0.03215739]. \t  \u001b[92m-1.3302838181961192\u001b[0m \t -1.3302838181961192\n",
            "13     \t [-0.38858978  0.14104994]. \t  -1.9380859667320358 \t -1.3302838181961192\n",
            "14     \t [-0.26504228  0.09323753]. \t  -1.6531865199500224 \t -1.3302838181961192\n",
            "15     \t [-1.53137776 -1.39676552]. \t  -1406.5769775535598 \t -1.3302838181961192\n",
            "16     \t [-0.85576725 -0.68931784]. \t  -205.55428566200078 \t -1.3302838181961192\n",
            "17     \t [-1.60659348  0.49208595]. \t  -443.21010900556564 \t -1.3302838181961192\n",
            "18     \t [-0.80151381  0.52969875]. \t  -4.516158736222404 \t -1.3302838181961192\n",
            "19     \t [ 0.03298517 -0.06661061]. \t  -1.3934281580334764 \t -1.3302838181961192\n",
            "20     \t [1.57587961 0.46549742]. \t  -407.5233249662723 \t -1.3302838181961192\n",
            "21     \t [-0.604194    0.34618626]. \t  -2.609023949170781 \t -1.3302838181961192\n",
            "22     \t [-0.46813722  0.56682228]. \t  -14.242857785593285 \t -1.3302838181961192\n",
            "23     \t [-0.00347882 -0.04844968]. \t  \u001b[92m-1.2418241604000273\u001b[0m \t -1.2418241604000273\n",
            "24     \t [1.15984121 0.29531421]. \t  -110.25821090265927 \t -1.2418241604000273\n",
            "25     \t [-0.64286013  0.47116103]. \t  -3.0341363514341926 \t -1.2418241604000273\n",
            "26     \t [ 0.01950453 -0.01516482]. \t  \u001b[92m-0.9855368393852267\u001b[0m \t -0.9855368393852267\n",
            "27     \t [ 0.62192199 -1.03710214]. \t  -202.88896043583838 \t -0.9855368393852267\n",
            "28     \t [-0.78944214  1.03138771]. \t  -19.862281627288276 \t -0.9855368393852267\n",
            "29     \t [0.57550973 0.16635393]. \t  -2.897992216024607 \t -0.9855368393852267\n",
            "30     \t [-0.19222577  0.17077657]. \t  -3.212337453204319 \t -0.9855368393852267\n",
            "31     \t [1.35840484 0.76388173]. \t  -117.06715361980729 \t -0.9855368393852267\n",
            "32     \t [0.86590902 0.71663559]. \t  \u001b[92m-0.12795775949515567\u001b[0m \t -0.12795775949515567\n",
            "33     \t [-0.05827082 -1.51731853]. \t  -232.3770506578901 \t -0.12795775949515567\n",
            "34     \t [0.79417208 0.57440766]. \t  -0.359352620047412 \t -0.12795775949515567\n",
            "35     \t [0.94324962 0.71135666]. \t  -3.18456323762495 \t -0.12795775949515567\n",
            "36     \t [ 2.04163797 -0.03123183]. \t  -1764.679667609521 \t -0.12795775949515567\n",
            "37     \t [-0.65059558 -0.51476499]. \t  -90.71629548434643 \t -0.12795775949515567\n",
            "38     \t [0.51607066 1.36921625]. \t  -121.8702319388875 \t -0.12795775949515567\n",
            "39     \t [ 0.21458814 -1.62712584]. \t  -280.5679665819224 \t -0.12795775949515567\n",
            "40     \t [-0.18489852  0.72966304]. \t  -49.77261283862049 \t -0.12795775949515567\n",
            "41     \t [0.81858808 0.63245375]. \t  -0.17453225259032354 \t -0.12795775949515567\n",
            "42     \t [-0.11661194 -1.42082376]. \t  -207.00350025199268 \t -0.12795775949515567\n",
            "43     \t [ 1.21019142 -0.02411951]. \t  -221.66182284279884 \t -0.12795775949515567\n",
            "44     \t [-1.5448088  -0.08848165]. \t  -618.996911885718 \t -0.12795775949515567\n",
            "45     \t [0.67551616 0.98433008]. \t  -27.984534946266628 \t -0.12795775949515567\n",
            "46     \t [1.63519227 0.98986453]. \t  -283.98544470693525 \t -0.12795775949515567\n",
            "47     \t [ 1.49332523 -0.11835829]. \t  -551.7315501101823 \t -0.12795775949515567\n",
            "48     \t [0.68674736 0.50070317]. \t  -0.1826990343353264 \t -0.12795775949515567\n",
            "49     \t [1.83379956 0.83301368]. \t  -640.6876431922506 \t -0.12795775949515567\n",
            "50     \t [1.03717996 1.56330399]. \t  -23.773025968055446 \t -0.12795775949515567\n",
            "51     \t [1.11283519 1.3223822 ]. \t  -0.7179965892568682 \t -0.12795775949515567\n",
            "52     \t [0.64973467 0.43817563]. \t  -0.14835138713011253 \t -0.12795775949515567\n",
            "53     \t [0.32815066 0.12835009]. \t  -0.49409502814461187 \t -0.12795775949515567\n",
            "54     \t [0.61157689 0.43333692]. \t  -0.5026475610597363 \t -0.12795775949515567\n",
            "55     \t [1.01104666 1.04797638]. \t  \u001b[92m-0.06648509470256328\u001b[0m \t -0.06648509470256328\n",
            "56     \t [1.0717823  1.22198261]. \t  -0.541933135027493 \t -0.06648509470256328\n",
            "57     \t [1.08715175 1.13027436]. \t  -0.2741049722866009 \t -0.06648509470256328\n",
            "58     \t [-1.16058688 -1.8209149 ]. \t  -1008.212481886966 \t -0.06648509470256328\n",
            "59     \t [0.95891728 0.94429174]. \t  \u001b[92m-0.06304005132943308\u001b[0m \t -0.06304005132943308\n",
            "60     \t [1.09426025 1.18271284]. \t  \u001b[92m-0.030472405474027683\u001b[0m \t -0.030472405474027683\n",
            "61     \t [0.92135239 0.85127538]. \t  \u001b[92m-0.006754344787098047\u001b[0m \t -0.006754344787098047\n",
            "62     \t [1.97180658 0.20350603]. \t  -1358.509591995981 \t -0.006754344787098047\n",
            "63     \t [0.94717848 0.87696423]. \t  -0.04352485045747276 \t -0.006754344787098047\n",
            "64     \t [0.62819858 0.44843553]. \t  -0.4277026147132267 \t -0.006754344787098047\n",
            "65     \t [1.01651248 1.00777454]. \t  -0.06541545522848757 \t -0.006754344787098047\n",
            "66     \t [-0.82463195 -1.82694224]. \t  -631.8141740341131 \t -0.006754344787098047\n",
            "67     \t [-0.88416439  0.69309902]. \t  -4.335916078337832 \t -0.006754344787098047\n",
            "68     \t [-0.20230161  1.30356964]. \t  -160.8724390811152 \t -0.006754344787098047\n",
            "69     \t [0.58790828 0.74704427]. \t  -16.28266764067562 \t -0.006754344787098047\n",
            "70     \t [-0.15007015 -1.21747465]. \t  -155.08159606040635 \t -0.006754344787098047\n",
            "71     \t [1.09669711 1.19545764]. \t  -0.014660256667027569 \t -0.006754344787098047\n",
            "72     \t [1.19891441 1.3948363 ]. \t  -0.22069774735029318 \t -0.006754344787098047\n",
            "73     \t [0.59368062 0.84720693]. \t  -24.64287629699933 \t -0.006754344787098047\n",
            "74     \t [1.09560986 1.22984276]. \t  -0.09605886646729314 \t -0.006754344787098047\n",
            "75     \t [0.84403032 0.72180358]. \t  -0.03319341543954549 \t -0.006754344787098047\n",
            "76     \t [-1.59583951 -1.82759243]. \t  -1920.1850770306987 \t -0.006754344787098047\n",
            "77     \t [0.12714667 0.02881252]. \t  -0.7778656902381174 \t -0.006754344787098047\n",
            "78     \t [1.10110496 1.15181613]. \t  -0.37765210595269305 \t -0.006754344787098047\n",
            "79     \t [0.277836   0.11895751]. \t  -0.6959495754691443 \t -0.006754344787098047\n",
            "80     \t [-0.19349692 -1.86509339]. \t  -363.38816596376915 \t -0.006754344787098047\n",
            "81     \t [1.01175657 1.00353431]. \t  -0.040607732046360645 \t -0.006754344787098047\n",
            "82     \t [1.42966273 1.46328414]. \t  -33.90021318514245 \t -0.006754344787098047\n",
            "83     \t [0.54479177 0.32961654]. \t  -0.3149197295522605 \t -0.006754344787098047\n",
            "84     \t [-1.08094176  1.62134052]. \t  -24.84265147854368 \t -0.006754344787098047\n",
            "85     \t [-0.18566239 -0.61609005]. \t  -43.72870176139577 \t -0.006754344787098047\n",
            "86     \t [1.16523437 1.33844182]. \t  -0.064664642663035 \t -0.006754344787098047\n",
            "87     \t [1.08539507 1.1961194 ]. \t  -0.03982544057882036 \t -0.006754344787098047\n",
            "88     \t [0.13530442 0.06395526]. \t  -0.9560722181291014 \t -0.006754344787098047\n",
            "89     \t [ 0.7760306  -1.36997905]. \t  -389.0084461128361 \t -0.006754344787098047\n",
            "90     \t [-1.22173971  1.70618719]. \t  -9.49602906924913 \t -0.006754344787098047\n",
            "91     \t [1.33601794 1.2571533 ]. \t  -27.969202793068458 \t -0.006754344787098047\n",
            "92     \t [-0.39760952  0.73843906]. \t  -35.633428818966976 \t -0.006754344787098047\n",
            "93     \t [-0.76961064  1.86473579]. \t  -165.04066698652198 \t -0.006754344787098047\n",
            "94     \t [0.74943408 0.58941087]. \t  -0.13984186202876087 \t -0.006754344787098047\n",
            "95     \t [-1.25939561 -0.5085482 ]. \t  -443.8504713348479 \t -0.006754344787098047\n",
            "96     \t [-1.85875106  1.24298154]. \t  -497.45533978245345 \t -0.006754344787098047\n",
            "97     \t [-1.13354993  2.00624636]. \t  -56.58097840260251 \t -0.006754344787098047\n",
            "98     \t [1.13592812 1.28181978]. \t  -0.02572342235004445 \t -0.006754344787098047\n",
            "99     \t [-1.88399263  0.56730864]. \t  -897.6211402026098 \t -0.006754344787098047\n",
            "100    \t [2.00461881 1.6776865 ]. \t  -548.9484404727244 \t -0.006754344787098047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoONg4VEtdy-",
        "outputId": "7aa52f67-d664-4fe6-f2e0-99407303c08c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_loser_13 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_13 = dGPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.65799909 -0.35389173]. \t  -62.0309701572776 \t -62.0309701572776\n",
            "init   \t [ 0.68854808 -1.10615174]. \t  -249.81607110978882 \t -62.0309701572776\n",
            "init   \t [1.26025046 0.56040843]. \t  -105.7097019264073 \t -62.0309701572776\n",
            "init   \t [-1.34269398 -0.98145948]. \t  -780.713409392333 \t -62.0309701572776\n",
            "init   \t [ 1.70115068 -0.15230767]. \t  -928.4380405275942 \t -62.0309701572776\n",
            "1      \t [0.41169197 0.76902621]. \t  \u001b[92m-36.290440627725864\u001b[0m \t -36.290440627725864\n",
            "2      \t [1.56732557 1.62159952]. \t  -70.02931566657841 \t -36.290440627725864\n",
            "3      \t [0.44708238 0.08707663]. \t  \u001b[92m-1.578237817391173\u001b[0m \t -1.578237817391173\n",
            "4      \t [0.92702252 0.86460314]. \t  \u001b[92m-0.008063513247388177\u001b[0m \t -0.008063513247388177\n",
            "5      \t [-0.90131356  1.97423118]. \t  -138.6080292653433 \t -0.008063513247388177\n",
            "6      \t [0.67987852 0.5941335 ]. \t  -1.8422045489389927 \t -0.008063513247388177\n",
            "7      \t [ 0.85956863 -0.72063444]. \t  -213.0316067957737 \t -0.008063513247388177\n",
            "8      \t [1.19067678 1.85217997]. \t  -18.91266804836645 \t -0.008063513247388177\n",
            "9      \t [1.09067098 2.00432563]. \t  -66.3920042074066 \t -0.008063513247388177\n",
            "10     \t [-0.76612754  0.98864126]. \t  -19.254680648051 \t -0.008063513247388177\n",
            "11     \t [1.17575603 1.29780787]. \t  -0.7465109927057603 \t -0.008063513247388177\n",
            "12     \t [0.72586704 0.37811578]. \t  -2.2883160295173526 \t -0.008063513247388177\n",
            "13     \t [0.55322139 0.20508206]. \t  -1.2191423564326112 \t -0.008063513247388177\n",
            "14     \t [-1.33206651 -0.77282734]. \t  -654.2758580168237 \t -0.008063513247388177\n",
            "15     \t [-0.59369595  0.53069577]. \t  -5.716135202780976 \t -0.008063513247388177\n",
            "16     \t [-2.02415053  1.81206522]. \t  -531.3228965450492 \t -0.008063513247388177\n",
            "17     \t [0.58018287 0.31688201]. \t  -0.21517432057523567 \t -0.008063513247388177\n",
            "18     \t [1.19393231 1.53699067]. \t  -1.2811985513383701 \t -0.008063513247388177\n",
            "19     \t [-1.54062542  0.92282972]. \t  -216.90694503923066 \t -0.008063513247388177\n",
            "20     \t [0.99154575 0.94236248]. \t  -0.16653947694844695 \t -0.008063513247388177\n",
            "21     \t [-0.62056197 -1.76656905]. \t  -465.5929716936383 \t -0.008063513247388177\n",
            "22     \t [-1.17388695 -0.00878645]. \t  -197.0463823630232 \t -0.008063513247388177\n",
            "23     \t [0.85951184 0.66720746]. \t  -0.5317220809777736 \t -0.008063513247388177\n",
            "24     \t [ 0.53430913 -1.11507241]. \t  -196.37332171209164 \t -0.008063513247388177\n",
            "25     \t [-2.04602583 -0.00310691]. \t  -1764.3256972698305 \t -0.008063513247388177\n",
            "26     \t [-0.57281853 -0.60662065]. \t  -89.84796575492754 \t -0.008063513247388177\n",
            "27     \t [-0.52017856 -0.05716044]. \t  -13.052697998264273 \t -0.008063513247388177\n",
            "28     \t [-0.93027194  0.59447042]. \t  -11.066552780660691 \t -0.008063513247388177\n",
            "29     \t [0.79168256 1.72100923]. \t  -119.78125431659336 \t -0.008063513247388177\n",
            "30     \t [-0.41162842  0.99998671]. \t  -70.97381704737913 \t -0.008063513247388177\n",
            "31     \t [ 1.10650968 -0.76968109]. \t  -397.6327937907719 \t -0.008063513247388177\n",
            "32     \t [0.48552354 0.24405069]. \t  -0.27160425875247024 \t -0.008063513247388177\n",
            "33     \t [0.55486651 0.34667633]. \t  -0.3486838395435433 \t -0.008063513247388177\n",
            "34     \t [ 1.19751534 -0.68056059]. \t  -447.1938388605762 \t -0.008063513247388177\n",
            "35     \t [-1.77977978  0.80529652]. \t  -565.7825354395953 \t -0.008063513247388177\n",
            "36     \t [-2.00269429 -0.03686543]. \t  -1647.363110952309 \t -0.008063513247388177\n",
            "37     \t [-0.05723731 -0.00916061]. \t  -1.1332179262815876 \t -0.008063513247388177\n",
            "38     \t [0.83671297 1.25651821]. \t  -30.988055192015178 \t -0.008063513247388177\n",
            "39     \t [0.55016118 0.30517732]. \t  -0.20297995737585128 \t -0.008063513247388177\n",
            "40     \t [1.0275188  1.04328329]. \t  -0.016411293445117516 \t -0.008063513247388177\n",
            "41     \t [ 2.02097812 -0.56767553]. \t  -2165.1789393183 \t -0.008063513247388177\n",
            "42     \t [1.06708043 1.1464565 ]. \t  -0.010577331352798142 \t -0.008063513247388177\n",
            "43     \t [ 0.44738142 -1.75945227]. \t  -384.3095459218289 \t -0.008063513247388177\n",
            "44     \t [1.0461974  1.46347809]. \t  -13.614477131329513 \t -0.008063513247388177\n",
            "45     \t [ 0.23220122 -0.07927605]. \t  -2.363564553793089 \t -0.008063513247388177\n",
            "46     \t [1.04925422 1.13268521]. \t  -0.1032373244539513 \t -0.008063513247388177\n",
            "47     \t [1.02962678 1.11592012]. \t  -0.3121167904891954 \t -0.008063513247388177\n",
            "48     \t [ 0.19960659 -1.90024749]. \t  -377.0356578945621 \t -0.008063513247388177\n",
            "49     \t [0.35094864 0.10836632]. \t  -0.44316761714796343 \t -0.008063513247388177\n",
            "50     \t [1.01273991 1.08052579]. \t  -0.30138398593510035 \t -0.008063513247388177\n",
            "51     \t [-0.69694881  1.95944562]. \t  -220.0611550680011 \t -0.008063513247388177\n",
            "52     \t [-1.05415735 -1.03439792]. \t  -464.5990871560298 \t -0.008063513247388177\n",
            "53     \t [0.92359552 0.57127259]. \t  -7.944488047024072 \t -0.008063513247388177\n",
            "54     \t [-0.3029216   0.63837222]. \t  -31.575932970358767 \t -0.008063513247388177\n",
            "55     \t [0.88439215 0.80153632]. \t  -0.050950171115834816 \t -0.008063513247388177\n",
            "56     \t [ 0.11572348 -0.02973209]. \t  -0.9679130526096114 \t -0.008063513247388177\n",
            "57     \t [-0.54087444 -1.81978742]. \t  -448.5691858428467 \t -0.008063513247388177\n",
            "58     \t [0.98889748 0.97856586]. \t  \u001b[92m-0.00016520705189970985\u001b[0m \t -0.00016520705189970985\n",
            "59     \t [1.49541888 1.13688769]. \t  -121.1112632568946 \t -0.00016520705189970985\n",
            "60     \t [0.53232314 0.29460774]. \t  -0.23135499598464895 \t -0.00016520705189970985\n",
            "61     \t [ 1.18729071 -1.45128176]. \t  -818.5334177883941 \t -0.00016520705189970985\n",
            "62     \t [0.86514858 0.74837069]. \t  -0.01818614590869873 \t -0.00016520705189970985\n",
            "63     \t [-0.38430323 -0.14393198]. \t  -10.420573371876165 \t -0.00016520705189970985\n",
            "64     \t [-0.63342095  0.81723983]. \t  -19.975139704498243 \t -0.00016520705189970985\n",
            "65     \t [-1.07156387  1.22114234]. \t  -4.8227190361964345 \t -0.00016520705189970985\n",
            "66     \t [0.92379814 0.87338444]. \t  -0.045732531894914434 \t -0.00016520705189970985\n",
            "67     \t [ 0.51757545 -1.3693464 ]. \t  -268.28518421901623 \t -0.00016520705189970985\n",
            "68     \t [ 0.26761975 -0.19500353]. \t  -7.645209130738282 \t -0.00016520705189970985\n",
            "69     \t [0.11058556 0.28852397]. \t  -8.42493972861492 \t -0.00016520705189970985\n",
            "70     \t [-0.85830098 -1.40888248]. \t  -463.7973652642426 \t -0.00016520705189970985\n",
            "71     \t [ 1.79482222 -1.31531882]. \t  -2058.8015441070706 \t -0.00016520705189970985\n",
            "72     \t [-1.27769429  0.11797437]. \t  -234.5674956047715 \t -0.00016520705189970985\n",
            "73     \t [-0.19177921  1.56395293]. \t  -234.64627761659645 \t -0.00016520705189970985\n",
            "74     \t [ 0.79547643 -1.96309743]. \t  -673.9012232044952 \t -0.00016520705189970985\n",
            "75     \t [1.06935728 1.1365664 ]. \t  -0.009652645072618482 \t -0.00016520705189970985\n",
            "76     \t [-1.50435297 -0.48764829]. \t  -762.9212151557622 \t -0.00016520705189970985\n",
            "77     \t [-1.1056022   1.75222029]. \t  -32.509153433479376 \t -0.00016520705189970985\n",
            "78     \t [ 1.93059903 -0.51219088]. \t  -1798.120198187391 \t -0.00016520705189970985\n",
            "79     \t [1.37079816 1.93049847]. \t  -0.40179896032702306 \t -0.00016520705189970985\n",
            "80     \t [0.8268194  0.66066641]. \t  -0.08272558750586026 \t -0.00016520705189970985\n",
            "81     \t [1.40751444 2.00447791]. \t  -0.220735180708408 \t -0.00016520705189970985\n",
            "82     \t [-0.6357308  -1.23168039]. \t  -270.2709147453694 \t -0.00016520705189970985\n",
            "83     \t [-0.19146048 -0.65550324]. \t  -49.32817336411976 \t -0.00016520705189970985\n",
            "84     \t [1.34792586 0.78290787]. \t  -107.03587823357041 \t -0.00016520705189970985\n",
            "85     \t [0.97552344 0.9380648 ]. \t  -0.019043964394079054 \t -0.00016520705189970985\n",
            "86     \t [ 1.22373925 -0.18190517]. \t  -282.102907398439 \t -0.00016520705189970985\n",
            "87     \t [-1.95040851 -1.82662026]. \t  -3179.1984984099763 \t -0.00016520705189970985\n",
            "88     \t [-1.08662028  1.66416685]. \t  -27.723784719500078 \t -0.00016520705189970985\n",
            "89     \t [1.09151392 1.17762453]. \t  -0.027358400318382357 \t -0.00016520705189970985\n",
            "90     \t [0.05516837 0.74965998]. \t  -56.63631662900497 \t -0.00016520705189970985\n",
            "91     \t [-2.03381803 -0.29522442]. \t  -1973.147534557621 \t -0.00016520705189970985\n",
            "92     \t [0.88204445 0.54686205]. \t  -5.35650045213236 \t -0.00016520705189970985\n",
            "93     \t [-0.73415056 -0.96868031]. \t  -230.31034784815657 \t -0.00016520705189970985\n",
            "94     \t [0.89875558 1.62848863]. \t  -67.36953814857557 \t -0.00016520705189970985\n",
            "95     \t [-1.59789089 -1.06368423]. \t  -1314.9741894616814 \t -0.00016520705189970985\n",
            "96     \t [-0.07486746 -1.21481139]. \t  -150.0969901918518 \t -0.00016520705189970985\n",
            "97     \t [-0.10064113 -0.19189334]. \t  -5.292698725182635 \t -0.00016520705189970985\n",
            "98     \t [-0.40566945  0.12810339]. \t  -2.1088712429628327 \t -0.00016520705189970985\n",
            "99     \t [-1.95537468 -1.02301032]. \t  -2357.5909153464518 \t -0.00016520705189970985\n",
            "100    \t [ 1.8715921  -0.93847778]. \t  -1973.3051177606942 \t -0.00016520705189970985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQHSe2Dtd1m",
        "outputId": "fc3b0d7e-ee77-4d4f-87e9-e5682ce585ba"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_loser_14 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_14 = dGPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.6043691  0.02928512]. \t  -11.444570460525577 \t -4.306489127802793\n",
            "init   \t [0.11608629 1.6231842 ]. \t  -259.89734211112767 \t -4.306489127802793\n",
            "init   \t [0.81916392 0.87776093]. \t  -4.306489127802793 \t -4.306489127802793\n",
            "init   \t [ 0.89021801 -1.13533148]. \t  -371.66089065122344 \t -4.306489127802793\n",
            "init   \t [-1.33056707 -0.17677726]. \t  -384.58487399689074 \t -4.306489127802793\n",
            "1      \t [1.20014634 0.52224222]. \t  -84.33247643037394 \t -4.306489127802793\n",
            "2      \t [0.3990925  0.43403903]. \t  -7.910626836147134 \t -4.306489127802793\n",
            "3      \t [0.51527608 0.27463977]. \t  \u001b[92m-0.2432935527026396\u001b[0m \t -0.2432935527026396\n",
            "4      \t [1.53137544 1.8120952 ]. \t  -28.692917519939073 \t -0.2432935527026396\n",
            "5      \t [1.11969363 1.26054942]. \t  \u001b[92m-0.018999105716718055\u001b[0m \t -0.018999105716718055\n",
            "6      \t [1.02729802 1.2165308 ]. \t  -2.5989529034397827 \t -0.018999105716718055\n",
            "7      \t [1.44865337 1.35082366]. \t  -56.1177263844637 \t -0.018999105716718055\n",
            "8      \t [1.29740297 1.73971131]. \t  -0.40718596900658544 \t -0.018999105716718055\n",
            "9      \t [-1.24309755  1.69366197]. \t  -7.232865704168191 \t -0.018999105716718055\n",
            "10     \t [0.56958754 0.36912861]. \t  -0.3850517103361976 \t -0.018999105716718055\n",
            "11     \t [1.35949135 1.98432447]. \t  -1.981765697419812 \t -0.018999105716718055\n",
            "12     \t [-1.40611301  1.95482855]. \t  -5.839221403425711 \t -0.018999105716718055\n",
            "13     \t [-1.08945402 -0.16533092]. \t  -187.22138818008403 \t -0.018999105716718055\n",
            "14     \t [-0.57478424 -0.55700642]. \t  -81.22486524113543 \t -0.018999105716718055\n",
            "15     \t [-0.51640756  0.2882748 ]. \t  -2.346139372290676 \t -0.018999105716718055\n",
            "16     \t [-0.22249554 -0.07239387]. \t  -2.9804109652583604 \t -0.018999105716718055\n",
            "17     \t [-0.26347808  0.12501395]. \t  -1.905437828416229 \t -0.018999105716718055\n",
            "18     \t [-0.3753506  -0.00103897]. \t  -3.9059178913438064 \t -0.018999105716718055\n",
            "19     \t [0.05181263 0.00983458]. \t  -0.9041715685947159 \t -0.018999105716718055\n",
            "20     \t [-1.22536923  1.96285784]. \t  -26.234628732976525 \t -0.018999105716718055\n",
            "21     \t [-0.70708707  0.91569728]. \t  -20.196886272705065 \t -0.018999105716718055\n",
            "22     \t [-1.91539776  1.95894083]. \t  -300.8437993746776 \t -0.018999105716718055\n",
            "23     \t [ 0.33061555 -1.7446117 ]. \t  -344.14939823471684 \t -0.018999105716718055\n",
            "24     \t [ 0.2889832  -0.02511481]. \t  -1.6855078173979487 \t -0.018999105716718055\n",
            "25     \t [-0.98387027 -1.93324359]. \t  -845.6575912879017 \t -0.018999105716718055\n",
            "26     \t [-0.15521305  1.6408169 ]. \t  -262.7147527326001 \t -0.018999105716718055\n",
            "27     \t [-1.38319376 -0.62574996]. \t  -650.3189806290884 \t -0.018999105716718055\n",
            "28     \t [-0.22258774  0.77729379]. \t  -54.45650740111608 \t -0.018999105716718055\n",
            "29     \t [0.91423748 0.91937701]. \t  -0.705362561033248 \t -0.018999105716718055\n",
            "30     \t [-1.56710626  1.52314372]. \t  -93.57892023928483 \t -0.018999105716718055\n",
            "31     \t [ 1.68856139 -1.69422831]. \t  -2066.601950465006 \t -0.018999105716718055\n",
            "32     \t [0.7269338  0.53346647]. \t  -0.07709898912627434 \t -0.018999105716718055\n",
            "33     \t [-1.49436433  0.52273651]. \t  -298.7646462028464 \t -0.018999105716718055\n",
            "34     \t [ 1.21221907 -1.24380709]. \t  -736.2350470514691 \t -0.018999105716718055\n",
            "35     \t [1.01643062 0.60629059]. \t  -18.21956078873848 \t -0.018999105716718055\n",
            "36     \t [-1.71144866 -0.58798446]. \t  -1244.3096860073397 \t -0.018999105716718055\n",
            "37     \t [1.00698279 1.00630881]. \t  \u001b[92m-0.00598628680283637\u001b[0m \t -0.00598628680283637\n",
            "38     \t [0.95087013 0.82176906]. \t  -0.6811414992149321 \t -0.00598628680283637\n",
            "39     \t [ 1.19759156 -0.75635892]. \t  -479.90507167496264 \t -0.00598628680283637\n",
            "40     \t [0.62304424 0.3997443 ]. \t  -0.1554593865596887 \t -0.00598628680283637\n",
            "41     \t [-1.80520355  1.80372094]. \t  -219.58299236338456 \t -0.00598628680283637\n",
            "42     \t [0.93605721 0.85563527]. \t  -0.046392233932515146 \t -0.00598628680283637\n",
            "43     \t [0.77488254 0.55568842]. \t  -0.2509746947640763 \t -0.00598628680283637\n",
            "44     \t [0.46962302 0.20124042]. \t  -0.3185694417005745 \t -0.00598628680283637\n",
            "45     \t [0.73522446 0.54528247]. \t  -0.07234097869880866 \t -0.00598628680283637\n",
            "46     \t [1.01767863 1.05705422]. \t  -0.04604193359000726 \t -0.00598628680283637\n",
            "47     \t [0.95406635 0.9434474 ]. \t  -0.11236581250489598 \t -0.00598628680283637\n",
            "48     \t [0.47079852 0.21945611]. \t  -0.28053607208673287 \t -0.00598628680283637\n",
            "49     \t [ 1.85063588 -0.97466119]. \t  -1936.2962367024556 \t -0.00598628680283637\n",
            "50     \t [-1.94939435  1.05330205]. \t  -763.2098910205044 \t -0.00598628680283637\n",
            "51     \t [-0.92324205  0.71701112]. \t  -5.531221754433577 \t -0.00598628680283637\n",
            "52     \t [-1.60149383 -0.62832208]. \t  -1026.359445503927 \t -0.00598628680283637\n",
            "53     \t [0.88335428 0.76682488]. \t  -0.03180399498064435 \t -0.00598628680283637\n",
            "54     \t [-1.23617305 -1.44542477]. \t  -889.1995863710406 \t -0.00598628680283637\n",
            "55     \t [-0.78117471 -1.60706685]. \t  -494.8148560291321 \t -0.00598628680283637\n",
            "56     \t [ 1.71130173 -0.6721661 ]. \t  -1297.0242076040583 \t -0.00598628680283637\n",
            "57     \t [-0.29868956 -1.07522585]. \t  -137.27895007309056 \t -0.00598628680283637\n",
            "58     \t [0.67230061 0.46398472]. \t  -0.12177873235102721 \t -0.00598628680283637\n",
            "59     \t [ 0.36157136 -1.50268131]. \t  -267.2120990859399 \t -0.00598628680283637\n",
            "60     \t [0.74820506 0.55446216]. \t  -0.06626149664293017 \t -0.00598628680283637\n",
            "61     \t [0.92088371 0.844365  ]. \t  -0.0076002705336947985 \t -0.00598628680283637\n",
            "62     \t [-1.03233629 -1.32807551]. \t  -577.1552290395983 \t -0.00598628680283637\n",
            "63     \t [0.87139754 0.72822947]. \t  -0.11328576034017876 \t -0.00598628680283637\n",
            "64     \t [0.51171294 0.69297598]. \t  -18.825373352716998 \t -0.00598628680283637\n",
            "65     \t [ 0.01102164 -0.63187888]. \t  -40.92052333757982 \t -0.00598628680283637\n",
            "66     \t [ 1.08362544 -1.74324689]. \t  -851.1823555090484 \t -0.00598628680283637\n",
            "67     \t [-1.97621475  0.19571888]. \t  -1385.049603529039 \t -0.00598628680283637\n",
            "68     \t [-0.41089759  0.62934004]. \t  -23.196952287907333 \t -0.00598628680283637\n",
            "69     \t [ 0.39130643 -1.49916247]. \t  -273.37448175824903 \t -0.00598628680283637\n",
            "70     \t [-1.07184414 -0.90691365]. \t  -426.9089002026663 \t -0.00598628680283637\n",
            "71     \t [-1.07872013  1.14281955]. \t  -4.364414492511502 \t -0.00598628680283637\n",
            "72     \t [-0.84180355 -0.03393875]. \t  -58.53355289837961 \t -0.00598628680283637\n",
            "73     \t [0.8845345 0.779401 ]. \t  -0.014232449126398566 \t -0.00598628680283637\n",
            "74     \t [-1.3150643  -0.37131301]. \t  -446.6565677912945 \t -0.00598628680283637\n",
            "75     \t [-0.71785847  0.56052251]. \t  -3.1553573574022105 \t -0.00598628680283637\n",
            "76     \t [0.62777772 0.37392982]. \t  -0.17925267818251503 \t -0.00598628680283637\n",
            "77     \t [ 0.42264889 -0.6119495 ]. \t  -62.83525877699036 \t -0.00598628680283637\n",
            "78     \t [-0.74381035 -0.01336308]. \t  -35.14634796034531 \t -0.00598628680283637\n",
            "79     \t [1.01339524 1.03386855]. \t  \u001b[92m-0.004938559968799545\u001b[0m \t -0.004938559968799545\n",
            "80     \t [-1.01837067 -1.92221297]. \t  -879.8146046778985 \t -0.004938559968799545\n",
            "81     \t [-1.50431446 -1.46030715]. \t  -1392.5449028268836 \t -0.004938559968799545\n",
            "82     \t [0.27956274 0.79907521]. \t  -52.49157805656037 \t -0.004938559968799545\n",
            "83     \t [ 0.52086063 -1.67252572]. \t  -378.0737835953263 \t -0.004938559968799545\n",
            "84     \t [-0.94978618  0.4780828 ]. \t  -21.78019844628485 \t -0.004938559968799545\n",
            "85     \t [ 0.12877742 -1.95684838]. \t  -390.20241691311753 \t -0.004938559968799545\n",
            "86     \t [-2.00204856 -1.87930626]. \t  -3475.283431726853 \t -0.004938559968799545\n",
            "87     \t [ 0.88669579 -1.57734109]. \t  -558.659399419193 \t -0.004938559968799545\n",
            "88     \t [-0.44596638  1.84690402]. \t  -273.6871541784703 \t -0.004938559968799545\n",
            "89     \t [-1.11911962 -1.18457007]. \t  -598.3869779339484 \t -0.004938559968799545\n",
            "90     \t [0.90197033 0.79343018]. \t  -0.050092473861474356 \t -0.004938559968799545\n",
            "91     \t [0.98566747 0.99043632]. \t  -0.03591112795888479 \t -0.004938559968799545\n",
            "92     \t [-1.61856374  1.72133048]. \t  -87.57238376284555 \t -0.004938559968799545\n",
            "93     \t [ 1.3943274  -1.43945206]. \t  -1145.0310449248936 \t -0.004938559968799545\n",
            "94     \t [-1.98353862  1.55387886]. \t  -575.6017084221132 \t -0.004938559968799545\n",
            "95     \t [-1.14234013  0.82687416]. \t  -27.444408404006722 \t -0.004938559968799545\n",
            "96     \t [ 0.1251758  -1.38381926]. \t  -196.62205066614248 \t -0.004938559968799545\n",
            "97     \t [-0.29259507  0.5531349 ]. \t  -23.528579551977003 \t -0.004938559968799545\n",
            "98     \t [-1.35419313 -0.7584376 ]. \t  -677.5320326314333 \t -0.004938559968799545\n",
            "99     \t [-0.40607436 -1.07110048]. \t  -154.74586914162197 \t -0.004938559968799545\n",
            "100    \t [-0.50624145 -0.19383328]. \t  -22.528996627536284 \t -0.004938559968799545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq4J_FTytd4O",
        "outputId": "7687b179-1413-4197-e9b8-f59735b2f9db"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_loser_15 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_15 = dGPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.31900409 -1.00146655]. \t  -122.17543449532224 \t -6.867717811955245\n",
            "init   \t [-1.01364994  0.85976824]. \t  -6.867717811955245 \t -6.867717811955245\n",
            "init   \t [-0.21482552 -1.11844164]. \t  -137.1031718367486 \t -6.867717811955245\n",
            "init   \t [-0.40259762  1.56572768]. \t  -198.98860193376754 \t -6.867717811955245\n",
            "init   \t [-0.25717364  1.55002954]. \t  -221.77381285664336 \t -6.867717811955245\n",
            "1      \t [-1.8012058   0.68545903]. \t  -662.6351334070664 \t -6.867717811955245\n",
            "2      \t [-0.43724669  0.32410651]. \t  \u001b[92m-3.8324996046651183\u001b[0m \t -3.8324996046651183\n",
            "3      \t [-0.74037338  0.72496404]. \t  -6.155122682350399 \t -3.8324996046651183\n",
            "4      \t [1.25594009 1.69598967]. \t  \u001b[92m-1.4722002702680579\u001b[0m \t -1.4722002702680579\n",
            "5      \t [0.99642726 0.16862642]. \t  -67.93731254820314 \t -1.4722002702680579\n",
            "6      \t [-0.32748232 -0.09152068]. \t  -5.712975739964262 \t -1.4722002702680579\n",
            "7      \t [-1.23315299  1.42836845]. \t  -5.838861619488394 \t -1.4722002702680579\n",
            "8      \t [1.11976051 1.97554298]. \t  -52.096456025954026 \t -1.4722002702680579\n",
            "9      \t [1.52955484 1.24989993]. \t  -119.01154052053846 \t -1.4722002702680579\n",
            "10     \t [0.29154696 0.11699657]. \t  \u001b[92m-0.6042860952742548\u001b[0m \t -0.6042860952742548\n",
            "11     \t [0.95020009 1.39892014]. \t  -24.608040773431387 \t -0.6042860952742548\n",
            "12     \t [-1.08444934  1.19282974]. \t  -4.373150917248846 \t -0.6042860952742548\n",
            "13     \t [-1.25985261  1.76845899]. \t  -8.391379564428181 \t -0.6042860952742548\n",
            "14     \t [-0.56419161  0.17175273]. \t  -4.594662333999015 \t -0.6042860952742548\n",
            "15     \t [ 0.17746211 -0.10349001]. \t  -2.4986043734294694 \t -0.6042860952742548\n",
            "16     \t [1.75978278 2.00396544]. \t  -120.01375610069695 \t -0.6042860952742548\n",
            "17     \t [ 1.74534324 -2.01574388]. \t  -2562.906427816463 \t -0.6042860952742548\n",
            "18     \t [0.1467519  1.24213598]. \t  -149.714434235153 \t -0.6042860952742548\n",
            "19     \t [0.87307096 1.35901039]. \t  -35.6280596638206 \t -0.6042860952742548\n",
            "20     \t [-0.10790502  0.01403524]. \t  -1.2280255716226158 \t -0.6042860952742548\n",
            "21     \t [0.18702334 0.02579743]. \t  -0.6693588430825558 \t -0.6042860952742548\n",
            "22     \t [0.25881451 0.0782059 ]. \t  \u001b[92m-0.5619469014730655\u001b[0m \t -0.5619469014730655\n",
            "23     \t [0.49790348 1.06233932]. \t  -66.5819590662643 \t -0.5619469014730655\n",
            "24     \t [0.1389005  0.06926979]. \t  -0.9912567810151823 \t -0.5619469014730655\n",
            "25     \t [0.26957856 0.10505275]. \t  -0.6383629076158982 \t -0.5619469014730655\n",
            "26     \t [-0.79377824 -1.52647203]. \t  -468.29098537736076 \t -0.5619469014730655\n",
            "27     \t [0.68320998 0.47494266]. \t  \u001b[92m-0.10702555738314529\u001b[0m \t -0.10702555738314529\n",
            "28     \t [0.45468556 0.37947763]. \t  -3.2812325866320244 \t -0.10702555738314529\n",
            "29     \t [-0.0198923  -0.92816516]. \t  -87.26270799406798 \t -0.10702555738314529\n",
            "30     \t [-0.29231659 -0.27329828]. \t  -14.54004238267034 \t -0.10702555738314529\n",
            "31     \t [ 0.82312635 -1.53627469]. \t  -490.1274956390867 \t -0.10702555738314529\n",
            "32     \t [ 0.18192633 -2.0209277 ]. \t  -422.5710679637814 \t -0.10702555738314529\n",
            "33     \t [0.48689264 0.29705517]. \t  -0.6231679491346888 \t -0.10702555738314529\n",
            "34     \t [-0.89207253  1.00619275]. \t  -8.006727594948323 \t -0.10702555738314529\n",
            "35     \t [0.6276886  0.41282151]. \t  -0.17406713455960235 \t -0.10702555738314529\n",
            "36     \t [-1.28838542  1.59686896]. \t  -5.634465412808513 \t -0.10702555738314529\n",
            "37     \t [1.32553206 0.05199988]. \t  -290.8205294941111 \t -0.10702555738314529\n",
            "38     \t [ 0.96702542 -1.29902543]. \t  -499.1497819678622 \t -0.10702555738314529\n",
            "39     \t [ 1.52335898 -1.74116497]. \t  -1650.0857154121345 \t -0.10702555738314529\n",
            "40     \t [0.48981704 0.28786431]. \t  -0.4901452940195897 \t -0.10702555738314529\n",
            "41     \t [ 0.60876009 -0.43284615]. \t  -64.70384880407995 \t -0.10702555738314529\n",
            "42     \t [-1.48294538  0.26723435]. \t  -379.38593650876345 \t -0.10702555738314529\n",
            "43     \t [0.58584159 0.33329162]. \t  -0.18136534652147004 \t -0.10702555738314529\n",
            "44     \t [0.54550469 0.29804652]. \t  -0.20658818071952822 \t -0.10702555738314529\n",
            "45     \t [ 1.94414784 -1.33544602]. \t  -2617.3743477510557 \t -0.10702555738314529\n",
            "46     \t [ 0.13817213 -0.44650176]. \t  -22.420459174192946 \t -0.10702555738314529\n",
            "47     \t [1.38415117 1.44425517]. \t  -22.39004806086102 \t -0.10702555738314529\n",
            "48     \t [1.17079819 1.38768064]. \t  \u001b[92m-0.05777435420106396\u001b[0m \t -0.05777435420106396\n",
            "49     \t [-0.00440638 -1.59305034]. \t  -254.79595807217444 \t -0.05777435420106396\n",
            "50     \t [-1.29320616 -1.48855631]. \t  -1004.4120042144065 \t -0.05777435420106396\n",
            "51     \t [0.48167906 1.58793747]. \t  -184.12130734420694 \t -0.05777435420106396\n",
            "52     \t [-1.9011107  -0.01322906]. \t  -1324.2564808383454 \t -0.05777435420106396\n",
            "53     \t [-1.15579233  0.22534611]. \t  -127.97064146641327 \t -0.05777435420106396\n",
            "54     \t [ 0.12801058 -1.83120073]. \t  -342.11829813971707 \t -0.05777435420106396\n",
            "55     \t [0.22798276 0.8511745 ]. \t  -64.46781299692562 \t -0.05777435420106396\n",
            "56     \t [-1.13629085 -1.93066034]. \t  -1042.5743678349131 \t -0.05777435420106396\n",
            "57     \t [-1.44601532 -0.39884523]. \t  -625.8961522131331 \t -0.05777435420106396\n",
            "58     \t [ 1.4866585  -0.20730164]. \t  -584.6457703960374 \t -0.05777435420106396\n",
            "59     \t [-1.68502746  0.51322119]. \t  -548.281791286128 \t -0.05777435420106396\n",
            "60     \t [0.54717588 0.30048743]. \t  -0.2051676183969559 \t -0.05777435420106396\n",
            "61     \t [1.25639194 1.45320796]. \t  -1.6360656210100721 \t -0.05777435420106396\n",
            "62     \t [1.28661493 1.62086164]. \t  -0.20128587323878006 \t -0.05777435420106396\n",
            "63     \t [1.2098789  1.46535097]. \t  \u001b[92m-0.04428755224829542\u001b[0m \t -0.04428755224829542\n",
            "64     \t [-0.57808571 -0.72258276]. \t  -114.16576044861316 \t -0.04428755224829542\n",
            "65     \t [ 1.95035338 -1.96655006]. \t  -3330.687513865893 \t -0.04428755224829542\n",
            "66     \t [1.76872942 0.34832836]. \t  -773.4728649615515 \t -0.04428755224829542\n",
            "67     \t [-0.05724343  1.9533385 ]. \t  -381.3918215074497 \t -0.04428755224829542\n",
            "68     \t [1.26647715 1.62377216]. \t  -0.11024491926290325 \t -0.04428755224829542\n",
            "69     \t [0.91692307 0.23882775]. \t  -36.23769107463086 \t -0.04428755224829542\n",
            "70     \t [1.0535951  0.55616467]. \t  -30.683166783002658 \t -0.04428755224829542\n",
            "71     \t [-0.01961047  1.89374533]. \t  -359.5211002445944 \t -0.04428755224829542\n",
            "72     \t [0.64286561 0.41001696]. \t  -0.1286072330775103 \t -0.04428755224829542\n",
            "73     \t [-0.96139332 -1.51235982]. \t  -597.5670219233991 \t -0.04428755224829542\n",
            "74     \t [-1.24483887  0.07042865]. \t  -223.8411389965985 \t -0.04428755224829542\n",
            "75     \t [-1.86428692  1.96795304]. \t  -235.49373628057674 \t -0.04428755224829542\n",
            "76     \t [0.74634834 0.52368188]. \t  -0.17558790752856746 \t -0.04428755224829542\n",
            "77     \t [-0.63261107  1.97065911]. \t  -249.30061465535087 \t -0.04428755224829542\n",
            "78     \t [1.90437555 0.06965906]. \t  -1266.0336628787975 \t -0.04428755224829542\n",
            "79     \t [-1.10184809  0.84917803]. \t  -17.732323399067432 \t -0.04428755224829542\n",
            "80     \t [-0.88245473  0.73950085]. \t  -3.69749981268287 \t -0.04428755224829542\n",
            "81     \t [0.64497093 0.41191547]. \t  -0.12770378357700504 \t -0.04428755224829542\n",
            "82     \t [-1.45764011  1.92065382]. \t  -10.204079062223498 \t -0.04428755224829542\n",
            "83     \t [1.24328263 1.53774161]. \t  -0.06560259500737861 \t -0.04428755224829542\n",
            "84     \t [-0.86777235 -1.25494479]. \t  -406.6843850848933 \t -0.04428755224829542\n",
            "85     \t [-0.85232843  1.35452091]. \t  -42.87669922124042 \t -0.04428755224829542\n",
            "86     \t [-0.16650858 -0.2351556 ]. \t  -8.271368864830501 \t -0.04428755224829542\n",
            "87     \t [-0.18601672 -0.25940638]. \t  -10.05074152806717 \t -0.04428755224829542\n",
            "88     \t [0.78873311 0.60254248]. \t  -0.08288301770115694 \t -0.04428755224829542\n",
            "89     \t [ 1.66125533 -1.00365242]. \t  -1416.7715446889222 \t -0.04428755224829542\n",
            "90     \t [ 0.60800516 -1.57700372]. \t  -379.107622928594 \t -0.04428755224829542\n",
            "91     \t [ 0.45080911 -1.64806577]. \t  -343.0307894880755 \t -0.04428755224829542\n",
            "92     \t [0.94661852 0.91241153]. \t  \u001b[92m-0.029499860745498308\u001b[0m \t -0.029499860745498308\n",
            "93     \t [-1.77464031  0.63760555]. \t  -638.5837589794946 \t -0.029499860745498308\n",
            "94     \t [-1.23939631 -0.96735506]. \t  -631.7452304759217 \t -0.029499860745498308\n",
            "95     \t [0.93798283 0.84226238]. \t  -0.1448420089182058 \t -0.029499860745498308\n",
            "96     \t [0.02856468 1.71387643]. \t  -294.40131162784513 \t -0.029499860745498308\n",
            "97     \t [0.94650351 0.91131675]. \t  \u001b[92m-0.0267254679106745\u001b[0m \t -0.0267254679106745\n",
            "98     \t [0.74135726 0.53852833]. \t  -0.07917769844144658 \t -0.0267254679106745\n",
            "99     \t [-2.03919919  1.0231488 ]. \t  -992.1749368582529 \t -0.0267254679106745\n",
            "100    \t [-1.07258036 -0.09198245]. \t  -158.65411831160364 \t -0.0267254679106745\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tJ-ta9Gtd7H",
        "outputId": "7a451567-3446-489c-c548-1b965ff14c6a"
      },
      "source": [
        "\r\n",
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_loser_16 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_16 = dGPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.49073237 -0.75216486]. \t  -884.9748202573794 \t -21.690996320546372\n",
            "init   \t [0.70512959 0.03240619]. \t  -21.690996320546372 \t -21.690996320546372\n",
            "init   \t [ 1.15368111 -0.88603985]. \t  -491.54136063574043 \t -21.690996320546372\n",
            "init   \t [-1.09072882  0.26132352]. \t  -90.5574614167091 \t -21.690996320546372\n",
            "init   \t [1.5360998 0.8967902]. \t  -214.26941031258198 \t -21.690996320546372\n",
            "1      \t [ 0.0221755  -1.04001206]. \t  -109.2209581414894 \t -21.690996320546372\n",
            "2      \t [0.22984789 0.62935581]. \t  -33.831328641897436 \t -21.690996320546372\n",
            "3      \t [0.27847476 0.05249583]. \t  \u001b[92m-0.5833607298068536\u001b[0m \t -0.5833607298068536\n",
            "4      \t [-1.89666678  0.44479792]. \t  -1002.2459176523861 \t -0.5833607298068536\n",
            "5      \t [-0.64908274  0.24042698]. \t  -5.9912828906732205 \t -0.5833607298068536\n",
            "6      \t [-0.90568542 -0.13633405]. \t  -95.14001721251631 \t -0.5833607298068536\n",
            "7      \t [-0.70879595  0.54496077]. \t  -3.10119619289842 \t -0.5833607298068536\n",
            "8      \t [ 0.40133467 -1.98033381]. \t  -458.9192215256066 \t -0.5833607298068536\n",
            "9      \t [ 0.39960744 -0.44253111]. \t  -36.627028547741325 \t -0.5833607298068536\n",
            "10     \t [0.50526239 0.34749532]. \t  -1.0949458446627538 \t -0.5833607298068536\n",
            "11     \t [0.35325267 0.22098225]. \t  -1.343626186243945 \t -0.5833607298068536\n",
            "12     \t [0.36058282 2.01233949]. \t  -354.72153426859774 \t -0.5833607298068536\n",
            "13     \t [ 0.45142949 -0.02723036]. \t  -5.637904947369633 \t -0.5833607298068536\n",
            "14     \t [0.44858559 0.14463035]. \t  -0.6243989373758893 \t -0.5833607298068536\n",
            "15     \t [ 0.23034354 -0.58857712]. \t  -41.761952530407115 \t -0.5833607298068536\n",
            "16     \t [-0.64879193  0.39829601]. \t  -2.7697489339403494 \t -0.5833607298068536\n",
            "17     \t [0.40356791 0.12336103]. \t  \u001b[92m-0.5118038424321075\u001b[0m \t -0.5118038424321075\n",
            "18     \t [0.36164892 0.10164547]. \t  \u001b[92m-0.49243209476833494\u001b[0m \t -0.49243209476833494\n",
            "19     \t [0.67686122 1.11927547]. \t  -43.81428231245156 \t -0.49243209476833494\n",
            "20     \t [0.49224278 0.28582549]. \t  \u001b[92m-0.44723847994298704\u001b[0m \t -0.44723847994298704\n",
            "21     \t [-1.64505925  0.84496156]. \t  -353.4246094702246 \t -0.44723847994298704\n",
            "22     \t [-1.45163433  1.73142691]. \t  -20.134225770346212 \t -0.44723847994298704\n",
            "23     \t [-1.09990965  0.96276837]. \t  -10.512144080158453 \t -0.44723847994298704\n",
            "24     \t [-1.65818853  1.93516594]. \t  -73.39448969348811 \t -0.44723847994298704\n",
            "25     \t [-0.78195468  0.53364097]. \t  -3.7808355450109574 \t -0.44723847994298704\n",
            "26     \t [-1.21768292  1.40012239]. \t  -5.600877967326659 \t -0.44723847994298704\n",
            "27     \t [0.76892328 0.51479858]. \t  -0.6377716476949535 \t -0.44723847994298704\n",
            "28     \t [0.38485341 0.76341975]. \t  -38.23874944180172 \t -0.44723847994298704\n",
            "29     \t [-0.01455274  1.61486514]. \t  -261.7398640546982 \t -0.44723847994298704\n",
            "30     \t [0.60631394 0.35566924]. \t  \u001b[92m-0.16926264601925312\u001b[0m \t -0.16926264601925312\n",
            "31     \t [0.49402913 0.22594906]. \t  -0.28882444719442163 \t -0.16926264601925312\n",
            "32     \t [0.87602124 1.10065017]. \t  -11.120058119862527 \t -0.16926264601925312\n",
            "33     \t [-1.97445608 -2.00286589]. \t  -3491.4319350150254 \t -0.16926264601925312\n",
            "34     \t [ 1.55749306 -1.20712058]. \t  -1320.1108231409005 \t -0.16926264601925312\n",
            "35     \t [0.23713683 1.18773581]. \t  -128.61162224204764 \t -0.16926264601925312\n",
            "36     \t [0.88022081 0.8572566 ]. \t  -0.6944429296422251 \t -0.16926264601925312\n",
            "37     \t [1.04560963 1.57958228]. \t  -23.64917436500178 \t -0.16926264601925312\n",
            "38     \t [1.92845623 2.03002895]. \t  -286.1052464867677 \t -0.16926264601925312\n",
            "39     \t [ 0.61478593 -0.26500832]. \t  -41.48944069562792 \t -0.16926264601925312\n",
            "40     \t [0.58037137 0.3187701 ]. \t  -0.20870751829868253 \t -0.16926264601925312\n",
            "41     \t [-0.86845363 -0.43874869]. \t  -145.80657003785683 \t -0.16926264601925312\n",
            "42     \t [0.65970292 0.3962484 ]. \t  -0.2675867340403693 \t -0.16926264601925312\n",
            "43     \t [0.68687634 0.47560627]. \t  \u001b[92m-0.09949588079885996\u001b[0m \t -0.09949588079885996\n",
            "44     \t [-1.94792302  0.21433422]. \t  -1290.3802851118294 \t -0.09949588079885996\n",
            "45     \t [-0.16806043  0.00923074]. \t  -1.4005167514366625 \t -0.09949588079885996\n",
            "46     \t [-1.24862228  1.74790749]. \t  -8.622730190813058 \t -0.09949588079885996\n",
            "47     \t [1.16704652 0.59980248]. \t  -58.122041039349625 \t -0.09949588079885996\n",
            "48     \t [-2.00507972  1.89669292]. \t  -460.0201820484579 \t -0.09949588079885996\n",
            "49     \t [-1.89921475 -1.4414918 ]. \t  -2557.1492100176924 \t -0.09949588079885996\n",
            "50     \t [-1.44755681  2.03501648]. \t  -6.355401732521033 \t -0.09949588079885996\n",
            "51     \t [ 0.35378044 -0.84419428]. \t  -94.38248797014575 \t -0.09949588079885996\n",
            "52     \t [ 0.9481272  -1.78426622]. \t  -719.9650394382073 \t -0.09949588079885996\n",
            "53     \t [ 1.65406445 -1.40107662]. \t  -1711.9095241778048 \t -0.09949588079885996\n",
            "54     \t [-1.11632408 -0.88810956]. \t  -459.9977901252435 \t -0.09949588079885996\n",
            "55     \t [0.6976035  0.45289812]. \t  -0.2053668778332181 \t -0.09949588079885996\n",
            "56     \t [0.73083496 0.50010988]. \t  -0.18811688846718624 \t -0.09949588079885996\n",
            "57     \t [-0.61019301 -1.3025744 ]. \t  -283.12504324506335 \t -0.09949588079885996\n",
            "58     \t [-1.96939784  0.54070658]. \t  -1122.9224080108465 \t -0.09949588079885996\n",
            "59     \t [ 0.75819958 -0.10407861]. \t  -46.15512888106376 \t -0.09949588079885996\n",
            "60     \t [-0.85982678 -0.39708684]. \t  -132.59693612405053 \t -0.09949588079885996\n",
            "61     \t [0.62939101 0.3950653 ]. \t  -0.13746503155297732 \t -0.09949588079885996\n",
            "62     \t [ 2.01318748 -1.93732052]. \t  -3589.329304979957 \t -0.09949588079885996\n",
            "63     \t [-0.05408798  0.06684726]. \t  -1.519700545075625 \t -0.09949588079885996\n",
            "64     \t [-1.50676836  0.61391828]. \t  -280.6607894481304 \t -0.09949588079885996\n",
            "65     \t [-0.83705712  0.88073911]. \t  -6.61746108713509 \t -0.09949588079885996\n",
            "66     \t [0.76691191 0.57108803]. \t  \u001b[92m-0.08345436494850517\u001b[0m \t -0.08345436494850517\n",
            "67     \t [ 0.57272774 -0.95069889]. \t  -163.69401005435313 \t -0.08345436494850517\n",
            "68     \t [-1.5895948   0.68742701]. \t  -345.0395757885496 \t -0.08345436494850517\n",
            "69     \t [0.26877118 0.47381962]. \t  -16.661479673194858 \t -0.08345436494850517\n",
            "70     \t [-1.52791129  2.01102875]. \t  -16.854535730329797 \t -0.08345436494850517\n",
            "71     \t [ 0.59946234 -0.79637604]. \t  -133.73187665435353 \t -0.08345436494850517\n",
            "72     \t [-0.43541378  0.47213973]. \t  -10.044121049607899 \t -0.08345436494850517\n",
            "73     \t [-1.7514003  -0.23603889]. \t  -1098.8430494818413 \t -0.08345436494850517\n",
            "74     \t [-0.41332787 -0.35282988]. \t  -29.420502585851374 \t -0.08345436494850517\n",
            "75     \t [-0.94227343 -0.16669875]. \t  -114.9858968322364 \t -0.08345436494850517\n",
            "76     \t [-1.04843339  0.61214391]. \t  -27.91966742391449 \t -0.08345436494850517\n",
            "77     \t [1.33962623 0.84995024]. \t  -89.35136915823698 \t -0.08345436494850517\n",
            "78     \t [0.8428471  0.71415851]. \t  \u001b[92m-0.026116265463732566\u001b[0m \t -0.026116265463732566\n",
            "79     \t [-1.82204876  0.4768997 ]. \t  -816.2072388153531 \t -0.026116265463732566\n",
            "80     \t [0.96502104 0.93573892]. \t  \u001b[92m-0.0032245849660961575\u001b[0m \t -0.0032245849660961575\n",
            "81     \t [ 0.53500734 -1.22816197]. \t  -229.55538676540743 \t -0.0032245849660961575\n",
            "82     \t [ 1.95780327 -0.0344688 ]. \t  -1496.6439525931778 \t -0.0032245849660961575\n",
            "83     \t [-0.74594318  1.73776012]. \t  -142.60211472973953 \t -0.0032245849660961575\n",
            "84     \t [0.40482454 0.38518204]. \t  -5.251564672638031 \t -0.0032245849660961575\n",
            "85     \t [0.80431251 0.633683  ]. \t  -0.05581173473667576 \t -0.0032245849660961575\n",
            "86     \t [1.02784239 1.1126412 ]. \t  -0.31640808521841157 \t -0.0032245849660961575\n",
            "87     \t [1.05697618 1.09381078]. \t  -0.057945483197729006 \t -0.0032245849660961575\n",
            "88     \t [0.96120873 0.93188314]. \t  -0.00784238103777529 \t -0.0032245849660961575\n",
            "89     \t [-0.49930821 -1.30813117]. \t  -244.80981656957928 \t -0.0032245849660961575\n",
            "90     \t [-0.63042838  1.71535492]. \t  -176.3482881164944 \t -0.0032245849660961575\n",
            "91     \t [ 0.49039704 -1.13699842]. \t  -190.0069262493083 \t -0.0032245849660961575\n",
            "92     \t [1.00045097 0.98805537]. \t  -0.016504144327074127 \t -0.0032245849660961575\n",
            "93     \t [ 0.21938768 -0.35648909]. \t  -16.981093934902173 \t -0.0032245849660961575\n",
            "94     \t [ 0.40091597 -0.29888962]. \t  -21.4842532107915 \t -0.0032245849660961575\n",
            "95     \t [-0.02629612 -0.737631  ]. \t  -55.56529365654726 \t -0.0032245849660961575\n",
            "96     \t [1.09132668 2.0162445 ]. \t  -68.11219215117217 \t -0.0032245849660961575\n",
            "97     \t [0.98188868 0.90391976]. \t  -0.36255880336726304 \t -0.0032245849660961575\n",
            "98     \t [1.03224282 1.06896304]. \t  \u001b[92m-0.002221450870597632\u001b[0m \t -0.002221450870597632\n",
            "99     \t [-1.03049915  1.23553349]. \t  -7.13679627376443 \t -0.002221450870597632\n",
            "100    \t [-1.01481    1.5943567]. \t  -35.92744461630901 \t -0.002221450870597632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqjDLo0otd9-",
        "outputId": "e577567c-5119-4037-ec05-babb51395046"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_loser_17 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_17 = dGPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.62187122 0.94420838]. \t  -31.22188590191926 \t -31.22188590191926\n",
            "init   \t [-1.25596728  0.08059797]. \t  -229.14713198618372 \t -31.22188590191926\n",
            "init   \t [0.72246586 1.2470062 ]. \t  -52.64667111223406 \t -31.22188590191926\n",
            "init   \t [0.13726869 1.671308  ]. \t  -273.80846401311214 \t -31.22188590191926\n",
            "init   \t [-0.2315013  -0.99953783]. \t  -112.42501801714917 \t -31.22188590191926\n",
            "1      \t [1.59286487 0.71471414]. \t  -332.50370461464615 \t -31.22188590191926\n",
            "2      \t [ 1.90278748 -2.02756811]. \t  -3190.995555019365 \t -31.22188590191926\n",
            "3      \t [-1.34540721 -1.87107521]. \t  -1360.6211641082973 \t -31.22188590191926\n",
            "4      \t [-0.05612606 -0.17716441]. \t  \u001b[92m-4.366735792685078\u001b[0m \t -4.366735792685078\n",
            "5      \t [-0.14818959 -0.3550115 ]. \t  -15.52910245168684 \t -4.366735792685078\n",
            "6      \t [ 0.06392531 -0.23881863]. \t  -6.776523181613979 \t -4.366735792685078\n",
            "7      \t [0.07863961 0.06826898]. \t  \u001b[92m-1.2343570551701712\u001b[0m \t -1.2343570551701712\n",
            "8      \t [0.04751241 0.075279  ]. \t  -1.4404475611620164 \t -1.2343570551701712\n",
            "9      \t [-0.37850307 -0.75061539]. \t  -81.80241034086906 \t -1.2343570551701712\n",
            "10     \t [-0.83386556 -0.14471424]. \t  -73.9307927272724 \t -1.2343570551701712\n",
            "11     \t [0.13651715 0.03341295]. \t  \u001b[92m-0.767435687989711\u001b[0m \t -0.767435687989711\n",
            "12     \t [-0.15252992  1.50680842]. \t  -221.41832287515706 \t -0.767435687989711\n",
            "13     \t [ 0.09914949 -0.01195521]. \t  -0.8589938639648796 \t -0.767435687989711\n",
            "14     \t [-1.83933026  1.86959459]. \t  -237.1425038369888 \t -0.767435687989711\n",
            "15     \t [-0.63748997 -1.38065512]. \t  -322.03563613815305 \t -0.767435687989711\n",
            "16     \t [0.01783411 0.39098217]. \t  -16.226494790123382 \t -0.767435687989711\n",
            "17     \t [-0.53458742 -0.72927028]. \t  -105.38841833205716 \t -0.767435687989711\n",
            "18     \t [0.04802418 0.20630082]. \t  -5.067633597172671 \t -0.767435687989711\n",
            "19     \t [1.74151857 1.86840679]. \t  -136.15124722922013 \t -0.767435687989711\n",
            "20     \t [-0.35289534 -1.2149306 ]. \t  -181.247167573373 \t -0.767435687989711\n",
            "21     \t [1.73420509 1.46014498]. \t  -239.95969685251026 \t -0.767435687989711\n",
            "22     \t [0.17391461 1.21263513]. \t  -140.486753581382 \t -0.767435687989711\n",
            "23     \t [ 0.3935953  -1.31480281]. \t  -216.3754355014879 \t -0.767435687989711\n",
            "24     \t [-0.40145841  0.11863616]. \t  -2.1449887368294287 \t -0.767435687989711\n",
            "25     \t [1.7954402  0.87939316]. \t  -550.1658746391516 \t -0.767435687989711\n",
            "26     \t [0.78979885 0.20528743]. \t  -17.557972867201215 \t -0.767435687989711\n",
            "27     \t [1.12371334 1.22165208]. \t  \u001b[92m-0.1840581385236697\u001b[0m \t -0.1840581385236697\n",
            "28     \t [0.78560143 0.61215899]. \t  \u001b[92m-0.04847736861870728\u001b[0m \t -0.04847736861870728\n",
            "29     \t [0.25774702 1.44265543]. \t  -189.94961304576694 \t -0.04847736861870728\n",
            "30     \t [1.20429826 1.49952164]. \t  -0.2836771911602363 \t -0.04847736861870728\n",
            "31     \t [1.17272494 1.43730764]. \t  -0.41452969596738526 \t -0.04847736861870728\n",
            "32     \t [-1.49651819 -2.02411352]. \t  -1824.1294927044426 \t -0.04847736861870728\n",
            "33     \t [0.3186804  1.09882346]. \t  -99.9181963257552 \t -0.04847736861870728\n",
            "34     \t [-1.38459603  1.2141535 ]. \t  -55.100543620283794 \t -0.04847736861870728\n",
            "35     \t [-0.74083596 -1.65416337]. \t  -488.351976952918 \t -0.04847736861870728\n",
            "36     \t [-0.88311058  0.81469966]. \t  -3.6673163804431126 \t -0.04847736861870728\n",
            "37     \t [ 0.6677092  -1.23963205]. \t  -284.19052973424647 \t -0.04847736861870728\n",
            "38     \t [0.84508445 0.74732108]. \t  -0.1339132568038867 \t -0.04847736861870728\n",
            "39     \t [-1.91187252 -1.00609437]. \t  -2181.2982218772822 \t -0.04847736861870728\n",
            "40     \t [-0.35318094  1.10103328]. \t  -97.14658577025038 \t -0.04847736861870728\n",
            "41     \t [ 0.54088612 -1.76136025]. \t  -422.0687201794085 \t -0.04847736861870728\n",
            "42     \t [1.15930595 1.37799479]. \t  -0.14100900958555315 \t -0.04847736861870728\n",
            "43     \t [0.9389775  0.96886211]. \t  -0.7638175853516199 \t -0.04847736861870728\n",
            "44     \t [0.93362488 0.85191999]. \t  \u001b[92m-0.04335440223053822\u001b[0m \t -0.04335440223053822\n",
            "45     \t [-1.3093457   1.90911608]. \t  -9.12505201197023 \t -0.04335440223053822\n",
            "46     \t [0.42498651 1.46456472]. \t  -165.1837035566068 \t -0.04335440223053822\n",
            "47     \t [-0.41592949 -0.74377796]. \t  -86.05255252223562 \t -0.04335440223053822\n",
            "48     \t [-1.52637107  1.3341993 ]. \t  -105.50634993417196 \t -0.04335440223053822\n",
            "49     \t [-1.63676595  1.16647122]. \t  -235.72770352656477 \t -0.04335440223053822\n",
            "50     \t [-1.05600458  1.36478073]. \t  -10.458921203572837 \t -0.04335440223053822\n",
            "51     \t [0.94171099 0.85942387]. \t  -0.07845015341681784 \t -0.04335440223053822\n",
            "52     \t [ 1.16414869 -0.27786964]. \t  -266.7323671760979 \t -0.04335440223053822\n",
            "53     \t [ 0.67582072 -0.86045542]. \t  -173.60379613220516 \t -0.04335440223053822\n",
            "54     \t [ 1.96307728 -1.52272095]. \t  -2891.4880782547907 \t -0.04335440223053822\n",
            "55     \t [-1.63769437  1.23503965]. \t  -216.33925826148388 \t -0.04335440223053822\n",
            "56     \t [-2.03099333  0.76013649]. \t  -1141.3730869217545 \t -0.04335440223053822\n",
            "57     \t [0.92122461 0.5995163 ]. \t  -6.213204097598539 \t -0.04335440223053822\n",
            "58     \t [-0.79301495  0.56919613]. \t  -3.5710320327526817 \t -0.04335440223053822\n",
            "59     \t [1.13983853 1.35630082]. \t  -0.34524121457321233 \t -0.04335440223053822\n",
            "60     \t [-1.7722413  -1.23475196]. \t  -1922.2651483292495 \t -0.04335440223053822\n",
            "61     \t [ 0.53234742 -1.40459953]. \t  -285.15083875605234 \t -0.04335440223053822\n",
            "62     \t [0.52314246 1.00172271]. \t  -53.23229777535611 \t -0.04335440223053822\n",
            "63     \t [1.3560085  0.25684078]. \t  -250.37328181217796 \t -0.04335440223053822\n",
            "64     \t [0.95065482 0.90554263]. \t  \u001b[92m-0.002758242625587753\u001b[0m \t -0.002758242625587753\n",
            "65     \t [ 1.24926445 -1.44904194]. \t  -905.8937177577728 \t -0.002758242625587753\n",
            "66     \t [-0.25376903  0.75078484]. \t  -48.68452741058197 \t -0.002758242625587753\n",
            "67     \t [-0.31251138  0.16572607]. \t  -2.185939276476964 \t -0.002758242625587753\n",
            "68     \t [0.54072251 1.04809334]. \t  -57.321074583400176 \t -0.002758242625587753\n",
            "69     \t [ 0.8301388  -1.30815169]. \t  -398.9424420447321 \t -0.002758242625587753\n",
            "70     \t [-1.93510578  0.10608985]. \t  -1332.515481306013 \t -0.002758242625587753\n",
            "71     \t [0.40650213 0.08189062]. \t  -1.0470180309555928 \t -0.002758242625587753\n",
            "72     \t [-0.04385934  0.51270369]. \t  -27.179268024709717 \t -0.002758242625587753\n",
            "73     \t [-1.24530025  1.56947825]. \t  -5.076362904596787 \t -0.002758242625587753\n",
            "74     \t [1.13509276 1.30522457]. \t  -0.04643710583144919 \t -0.002758242625587753\n",
            "75     \t [-0.06305095 -0.65857327]. \t  -45.027153579346496 \t -0.002758242625587753\n",
            "76     \t [ 1.12189078 -1.62498668]. \t  -831.5445178179002 \t -0.002758242625587753\n",
            "77     \t [-1.37516928 -1.8899802 ]. \t  -1435.2910264935185 \t -0.002758242625587753\n",
            "78     \t [ 1.25378386 -1.87414395]. \t  -1187.6372739167462 \t -0.002758242625587753\n",
            "79     \t [1.01220441 1.03486883]. \t  -0.01078073154289555 \t -0.002758242625587753\n",
            "80     \t [-1.22027478 -1.5265442 ]. \t  -914.3228437697458 \t -0.002758242625587753\n",
            "81     \t [0.65221889 1.46184715]. \t  -107.54540135343447 \t -0.002758242625587753\n",
            "82     \t [ 1.55054754 -1.96262208]. \t  -1907.2145839346979 \t -0.002758242625587753\n",
            "83     \t [1.09561815 1.21014797]. \t  -0.01868586857163028 \t -0.002758242625587753\n",
            "84     \t [1.99429542 1.99971413]. \t  -392.0392903776973 \t -0.002758242625587753\n",
            "85     \t [1.21552989 1.40522623]. \t  -0.56898970885753 \t -0.002758242625587753\n",
            "86     \t [1.34299937 1.70234647]. \t  -1.1438344722033924 \t -0.002758242625587753\n",
            "87     \t [ 1.53227294 -2.04175182]. \t  -1927.1528159404543 \t -0.002758242625587753\n",
            "88     \t [1.2747173  1.60120578]. \t  -0.13163112839381907 \t -0.002758242625587753\n",
            "89     \t [-0.45368952 -0.42290734]. \t  -41.644803839985755 \t -0.002758242625587753\n",
            "90     \t [-0.06955067  0.76414131]. \t  -58.79819787744729 \t -0.002758242625587753\n",
            "91     \t [0.98077986 0.96992581]. \t  -0.006764101125278395 \t -0.002758242625587753\n",
            "92     \t [-0.88903176  0.80760815]. \t  -3.5981306052969417 \t -0.002758242625587753\n",
            "93     \t [-1.73497169  0.81764991]. \t  -488.17555231351713 \t -0.002758242625587753\n",
            "94     \t [-0.20004687  1.61504794]. \t  -249.51180708440614 \t -0.002758242625587753\n",
            "95     \t [1.00030486 0.94983827]. \t  -0.2577749979055035 \t -0.002758242625587753\n",
            "96     \t [-1.63142951  1.44204324]. \t  -155.64708326487352 \t -0.002758242625587753\n",
            "97     \t [1.06144614 1.11495861]. \t  -0.01748637430562179 \t -0.002758242625587753\n",
            "98     \t [1.54415053 0.93054645]. \t  -211.66536041100017 \t -0.002758242625587753\n",
            "99     \t [0.99939249 0.63883855]. \t  -12.956169399285843 \t -0.002758242625587753\n",
            "100    \t [1.8395479 1.5593048]. \t  -333.6329134872885 \t -0.002758242625587753\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOqNmgSIteAw",
        "outputId": "b59c9b97-5de4-4ae4-cc88-a0ddd3c993db"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_loser_18 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_18 = dGPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.52956683 1.91914255]. \t  -17.95675982372887 \t -1.7663579664225912\n",
            "init   \t [1.51222084 0.12638491]. \t  -467.0068292530542 \t -1.7663579664225912\n",
            "init   \t [-1.09474477 -2.0013105 ]. \t  -1028.244988519411 \t -1.7663579664225912\n",
            "init   \t [-0.28479972 -0.39996883]. \t  -24.79447905426233 \t -1.7663579664225912\n",
            "init   \t [ 0.09287545 -0.0885072 ]. \t  -1.7663579664225912 \t -1.7663579664225912\n",
            "1      \t [-0.56813572  0.40153442]. \t  -3.0793038960278705 \t -1.7663579664225912\n",
            "2      \t [-0.15462805 -0.07652601]. \t  -2.341901869752791 \t -1.7663579664225912\n",
            "3      \t [-1.20242026  1.32756507]. \t  -6.248947237067588 \t -1.7663579664225912\n",
            "4      \t [ 0.34076202 -0.76279921]. \t  -77.68427478499966 \t -1.7663579664225912\n",
            "5      \t [-1.68770408  0.39178462]. \t  -610.6926759353652 \t -1.7663579664225912\n",
            "6      \t [-0.03540629  1.33846942]. \t  -179.88668010529636 \t -1.7663579664225912\n",
            "7      \t [-0.8823456   1.11145957]. \t  -14.627184329389557 \t -1.7663579664225912\n",
            "8      \t [-1.54842812  2.04257632]. \t  -19.100772316288136 \t -1.7663579664225912\n",
            "9      \t [-1.48102522  1.62501465]. \t  -38.4657364786583 \t -1.7663579664225912\n",
            "10     \t [-1.17658605  1.43183107]. \t  -4.9629269899146085 \t -1.7663579664225912\n",
            "11     \t [1.25007914 1.99328278]. \t  -18.60287771607783 \t -1.7663579664225912\n",
            "12     \t [0.18748892 0.28236639]. \t  -6.771665231097794 \t -1.7663579664225912\n",
            "13     \t [2.048 2.048]. \t  -461.7603900415999 \t -1.7663579664225912\n",
            "14     \t [1.13611964 1.32949764]. \t  \u001b[92m-0.16852826950798047\u001b[0m \t -0.16852826950798047\n",
            "15     \t [1.32792035 1.61792688]. \t  -2.2229733597680563 \t -0.16852826950798047\n",
            "16     \t [0.66848703 0.83939105]. \t  -15.516792861305607 \t -0.16852826950798047\n",
            "17     \t [-1.28543464  2.04464581]. \t  -20.613423791400496 \t -0.16852826950798047\n",
            "18     \t [-1.03226585 -1.05883981]. \t  -455.44299340967336 \t -0.16852826950798047\n",
            "19     \t [ 0.93458502 -0.79773117]. \t  -279.28864832088374 \t -0.16852826950798047\n",
            "20     \t [1.39236348 1.95887501]. \t  -0.19474890023561603 \t -0.16852826950798047\n",
            "21     \t [1.40458745 1.42902346]. \t  -29.740151978250694 \t -0.16852826950798047\n",
            "22     \t [0.05016724 0.06478462]. \t  -1.2899109775693323 \t -0.16852826950798047\n",
            "23     \t [-0.9724487  -0.53938542]. \t  -224.42549533519733 \t -0.16852826950798047\n",
            "24     \t [-1.62039324  1.92577765]. \t  -55.851985861387206 \t -0.16852826950798047\n",
            "25     \t [-0.27086497  0.23223916]. \t  -4.139107739749431 \t -0.16852826950798047\n",
            "26     \t [-1.92924409 -0.31165391]. \t  -1635.6029441475737 \t -0.16852826950798047\n",
            "27     \t [-0.45260348  0.19406349]. \t  -2.1216915859921293 \t -0.16852826950798047\n",
            "28     \t [ 0.11048423 -0.18739773]. \t  -4.775433930768888 \t -0.16852826950798047\n",
            "29     \t [-1.66391243 -0.01085516]. \t  -779.6360682164172 \t -0.16852826950798047\n",
            "30     \t [ 0.16093448 -1.44886951]. \t  -218.1985133803662 \t -0.16852826950798047\n",
            "31     \t [1.6702811  1.70236723]. \t  -118.70875420772408 \t -0.16852826950798047\n",
            "32     \t [ 0.0462226  -1.42077002]. \t  -203.3759959397796 \t -0.16852826950798047\n",
            "33     \t [-0.1208149   1.48729322]. \t  -218.1398643921702 \t -0.16852826950798047\n",
            "34     \t [1.14429545 1.27412903]. \t  \u001b[92m-0.14531061234750747\u001b[0m \t -0.14531061234750747\n",
            "35     \t [ 0.61123068 -0.5270834 ]. \t  -81.27473123256354 \t -0.14531061234750747\n",
            "36     \t [1.38648903 1.95227109]. \t  -0.23888990096381357 \t -0.14531061234750747\n",
            "37     \t [1.12024044 1.27597448]. \t  \u001b[92m-0.058708410729165106\u001b[0m \t -0.058708410729165106\n",
            "38     \t [ 1.88724723 -0.1642529 ]. \t  -1389.0612815590353 \t -0.058708410729165106\n",
            "39     \t [0.89588206 0.44978386]. \t  -12.459092029628481 \t -0.058708410729165106\n",
            "40     \t [ 0.01331491 -0.54313407]. \t  -30.49227055750872 \t -0.058708410729165106\n",
            "41     \t [1.01842587 1.09550373]. \t  -0.3403739417468834 \t -0.058708410729165106\n",
            "42     \t [-1.57117056 -1.41562841]. \t  -1515.3160328313832 \t -0.058708410729165106\n",
            "43     \t [-1.14378271  1.53790107]. \t  -9.870275462570547 \t -0.058708410729165106\n",
            "44     \t [ 0.30717576 -1.52429166]. \t  -262.48233492832753 \t -0.058708410729165106\n",
            "45     \t [-1.56696708  2.0126539 ]. \t  -26.19047522490724 \t -0.058708410729165106\n",
            "46     \t [1.88525821 1.26502271]. \t  -524.8162642938845 \t -0.058708410729165106\n",
            "47     \t [ 0.62318168 -1.80127686]. \t  -479.5909388679256 \t -0.058708410729165106\n",
            "48     \t [1.04833034 1.13063514]. \t  -0.1024361471659078 \t -0.058708410729165106\n",
            "49     \t [-1.84079572 -0.74649412]. \t  -1717.9116308337475 \t -0.058708410729165106\n",
            "50     \t [-0.88092941  0.62727091]. \t  -5.751019127351177 \t -0.058708410729165106\n",
            "51     \t [1.83553193 0.33688002]. \t  -920.1808958748647 \t -0.058708410729165106\n",
            "52     \t [-1.52331428 -1.84815522]. \t  -1744.1244075997931 \t -0.058708410729165106\n",
            "53     \t [-0.73564415 -0.06838934]. \t  -40.169002790667555 \t -0.058708410729165106\n",
            "54     \t [0.66462833 0.28171671]. \t  -2.6729253866224685 \t -0.058708410729165106\n",
            "55     \t [1.39556159 0.18113757]. \t  -312.1926517882368 \t -0.058708410729165106\n",
            "56     \t [0.72385481 0.5282031 ]. \t  -0.0780516430958971 \t -0.058708410729165106\n",
            "57     \t [-2.04673134  0.73703322]. \t  -1200.965413094416 \t -0.058708410729165106\n",
            "58     \t [-0.36103456  1.36325542]. \t  -153.85898978774878 \t -0.058708410729165106\n",
            "59     \t [-0.90866029 -2.02634107]. \t  -817.0360048393575 \t -0.058708410729165106\n",
            "60     \t [1.22290955 0.92966209]. \t  -32.06782246041859 \t -0.058708410729165106\n",
            "61     \t [-0.79383284  0.19903415]. \t  -21.80569745199871 \t -0.058708410729165106\n",
            "62     \t [ 1.92822791 -0.76753784]. \t  -2012.9229642283742 \t -0.058708410729165106\n",
            "63     \t [ 1.57226794 -1.26678748]. \t  -1398.200459272032 \t -0.058708410729165106\n",
            "64     \t [0.9881643  0.92755245]. \t  -0.23941983709691012 \t -0.058708410729165106\n",
            "65     \t [0.80554936 0.70715048]. \t  -0.3770091531700728 \t -0.058708410729165106\n",
            "66     \t [0.83257272 0.65495279]. \t  -0.17414348972741428 \t -0.058708410729165106\n",
            "67     \t [0.8505218  0.72786466]. \t  \u001b[92m-0.0243483749920794\u001b[0m \t -0.0243483749920794\n",
            "68     \t [1.0212155  0.97096363]. \t  -0.5176622726142415 \t -0.0243483749920794\n",
            "69     \t [0.91253656 0.79462043]. \t  -0.15283022315571587 \t -0.0243483749920794\n",
            "70     \t [0.92354311 0.81902767]. \t  -0.12079525787715831 \t -0.0243483749920794\n",
            "71     \t [-1.45974772 -1.46417609]. \t  -1298.4812654067393 \t -0.0243483749920794\n",
            "72     \t [-1.00611908 -1.01242955]. \t  -413.96760905821293 \t -0.0243483749920794\n",
            "73     \t [0.91719488 0.81693209]. \t  -0.06597551841845316 \t -0.0243483749920794\n",
            "74     \t [-1.88198369  0.9001927 ]. \t  -706.147826850092 \t -0.0243483749920794\n",
            "75     \t [1.34733018 1.80186027]. \t  -0.13869718257733363 \t -0.0243483749920794\n",
            "76     \t [0.94375639 0.86248619]. \t  -0.08263062507113914 \t -0.0243483749920794\n",
            "77     \t [-1.38585433  1.52392818]. \t  -21.42653644467692 \t -0.0243483749920794\n",
            "78     \t [ 0.21168654 -2.04525519]. \t  -437.45918748948486 \t -0.0243483749920794\n",
            "79     \t [-0.26562798 -0.10382778]. \t  -4.642861829112339 \t -0.0243483749920794\n",
            "80     \t [ 1.07400472 -0.48560451]. \t  -268.6672967922276 \t -0.0243483749920794\n",
            "81     \t [0.00470101 1.36244757]. \t  -186.6109362309 \t -0.0243483749920794\n",
            "82     \t [0.94328388 0.85009212]. \t  -0.1607649780410142 \t -0.0243483749920794\n",
            "83     \t [ 0.15233627 -0.3486703 ]. \t  -14.547756874936322 \t -0.0243483749920794\n",
            "84     \t [0.58467639 0.35330111]. \t  -0.18561456194862833 \t -0.0243483749920794\n",
            "85     \t [ 1.07502315 -1.54739593]. \t  -730.6647565439483 \t -0.0243483749920794\n",
            "86     \t [-0.66106048 -1.2168329 ]. \t  -276.275766282547 \t -0.0243483749920794\n",
            "87     \t [0.81351869 0.65559561]. \t  -0.038640448719723115 \t -0.0243483749920794\n",
            "88     \t [-0.4204121   0.06342461]. \t  -3.3017519346355586 \t -0.0243483749920794\n",
            "89     \t [0.83657766 0.72191524]. \t  -0.07534062005519546 \t -0.0243483749920794\n",
            "90     \t [-1.42651633 -0.83545454]. \t  -829.8095373075021 \t -0.0243483749920794\n",
            "91     \t [0.56601284 0.31318939]. \t  -0.19350174366378786 \t -0.0243483749920794\n",
            "92     \t [1.01822452 1.0009088 ]. \t  -0.12901492330007186 \t -0.0243483749920794\n",
            "93     \t [ 0.73320313 -2.01624691]. \t  -652.2778548155615 \t -0.0243483749920794\n",
            "94     \t [1.3049085 0.6343088]. \t  -114.25736576089821 \t -0.0243483749920794\n",
            "95     \t [1.08881537 1.18329627]. \t  \u001b[92m-0.00838218409886892\u001b[0m \t -0.00838218409886892\n",
            "96     \t [-0.84241255  1.53069914]. \t  -70.80519133967313 \t -0.00838218409886892\n",
            "97     \t [0.96485522 0.89783782]. \t  -0.11084757031898443 \t -0.00838218409886892\n",
            "98     \t [-0.84467376 -1.08464724]. \t  -326.7267386814881 \t -0.00838218409886892\n",
            "99     \t [-0.169081   1.8286972]. \t  -325.40592360796836 \t -0.00838218409886892\n",
            "100    \t [1.0736673  1.15605205]. \t  \u001b[92m-0.0065096651679119495\u001b[0m \t -0.0065096651679119495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4HnKuoqteDo",
        "outputId": "a5ccbe50-975b-4cd8-9dd3-6e94228436e0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_loser_19 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_19 = dGPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.24348908 1.07107119]. \t  -102.94304782477323 \t -4.219752052396591\n",
            "init   \t [-1.04639875  1.11284007]. \t  -4.219752052396591 \t -4.219752052396591\n",
            "init   \t [-0.59608235  1.21527255]. \t  -76.50031976976825 \t -4.219752052396591\n",
            "init   \t [-0.63078391 -0.22989031]. \t  -42.07005958957044 \t -4.219752052396591\n",
            "init   \t [-0.8056685  -0.23710138]. \t  -81.79603430667063 \t -4.219752052396591\n",
            "1      \t [ 0.45286922 -0.60514617]. \t  -65.94770306515855 \t -4.219752052396591\n",
            "2      \t [-1.9115265   1.53808142]. \t  -456.16001072637476 \t -4.219752052396591\n",
            "3      \t [-0.76878948  0.5483007 ]. \t  \u001b[92m-3.311257684438914\u001b[0m \t -3.311257684438914\n",
            "4      \t [1.50989435 1.30714346]. \t  -94.86236329029082 \t -3.311257684438914\n",
            "5      \t [-0.06855973 -0.20779091]. \t  -5.6570771189820555 \t -3.311257684438914\n",
            "6      \t [-0.21718482 -1.25152481]. \t  -170.14216241918132 \t -3.311257684438914\n",
            "7      \t [-1.12098267  0.71437221]. \t  -33.89989778483975 \t -3.311257684438914\n",
            "8      \t [0.86718777 0.38448612]. \t  -13.525359581074524 \t -3.311257684438914\n",
            "9      \t [-0.3698933   0.13024091]. \t  \u001b[92m-1.8809374880110328\u001b[0m \t -1.8809374880110328\n",
            "10     \t [-0.85352564  0.82928547]. \t  -4.451207116317159 \t -1.8809374880110328\n",
            "11     \t [0.83889826 0.78763991]. \t  \u001b[92m-0.7297005765149895\u001b[0m \t -0.7297005765149895\n",
            "12     \t [0.8226043  0.86534855]. \t  -3.5911331260262944 \t -0.7297005765149895\n",
            "13     \t [-1.53636148  0.48205596]. \t  -359.2532426805832 \t -0.7297005765149895\n",
            "14     \t [ 2.02523343 -2.0352233 ]. \t  -3767.074858043112 \t -0.7297005765149895\n",
            "15     \t [0.88284465 0.72015273]. \t  \u001b[92m-0.36492308373772725\u001b[0m \t -0.36492308373772725\n",
            "16     \t [0.759251   0.60408869]. \t  \u001b[92m-0.13428305632045917\u001b[0m \t -0.13428305632045917\n",
            "17     \t [-0.59840815  0.22619528]. \t  -4.294591524836828 \t -0.13428305632045917\n",
            "18     \t [-0.7745717   1.93339717]. \t  -180.95422369574854 \t -0.13428305632045917\n",
            "19     \t [0.21469522 0.04673812]. \t  -0.616745081528505 \t -0.13428305632045917\n",
            "20     \t [-1.75212771  1.1013791 ]. \t  -395.1019391827967 \t -0.13428305632045917\n",
            "21     \t [1.4252979  0.46022507]. \t  -247.06323074062726 \t -0.13428305632045917\n",
            "22     \t [0.37273513 0.03011682]. \t  -1.5775241694602857 \t -0.13428305632045917\n",
            "23     \t [1.06437523 1.70505825]. \t  -32.74126330082493 \t -0.13428305632045917\n",
            "24     \t [0.76101456 0.58943797]. \t  \u001b[92m-0.06771235104415185\u001b[0m \t -0.06771235104415185\n",
            "25     \t [ 0.60120811 -0.72900613]. \t  -119.0687515408353 \t -0.06771235104415185\n",
            "26     \t [0.6981431  0.44858296]. \t  -0.2418232592397735 \t -0.06771235104415185\n",
            "27     \t [-0.02586991 -1.69586234]. \t  -288.8743529540324 \t -0.06771235104415185\n",
            "28     \t [0.7284284  0.49569926]. \t  -0.19561273230144782 \t -0.06771235104415185\n",
            "29     \t [0.11330097 1.90764284]. \t  -359.81511194786157 \t -0.06771235104415185\n",
            "30     \t [1.41028662 1.82126185]. \t  -2.9788701110792903 \t -0.06771235104415185\n",
            "31     \t [1.22026773 1.09116792]. \t  -15.879797912924316 \t -0.06771235104415185\n",
            "32     \t [0.92173749 0.9008191 ]. \t  -0.26846459008552587 \t -0.06771235104415185\n",
            "33     \t [1.17559437 1.46732948]. \t  -0.7585676841325021 \t -0.06771235104415185\n",
            "34     \t [1.36646387 1.84186363]. \t  -0.19860803646555897 \t -0.06771235104415185\n",
            "35     \t [0.97145593 0.99725933]. \t  -0.2873898375386886 \t -0.06771235104415185\n",
            "36     \t [ 0.54668634 -0.75384016]. \t  -111.02450826383281 \t -0.06771235104415185\n",
            "37     \t [0.72896395 0.54696825]. \t  -0.09773360590918789 \t -0.06771235104415185\n",
            "38     \t [1.14191476 1.27311495]. \t  -0.11533894445110274 \t -0.06771235104415185\n",
            "39     \t [0.99785923 1.02203016]. \t  -0.06921107633250193 \t -0.06771235104415185\n",
            "40     \t [0.3122556  1.58620122]. \t  -222.09506467666787 \t -0.06771235104415185\n",
            "41     \t [-1.64096815  1.06322663]. \t  -272.51797697246604 \t -0.06771235104415185\n",
            "42     \t [-1.37063836  1.96020876]. \t  -6.285117237218309 \t -0.06771235104415185\n",
            "43     \t [-1.48643837  1.6254754 ]. \t  -40.29073407872109 \t -0.06771235104415185\n",
            "44     \t [0.95687951 0.91392703]. \t  \u001b[92m-0.00214544917233134\u001b[0m \t -0.00214544917233134\n",
            "45     \t [0.96673899 0.86586977]. \t  -0.4732744870171971 \t -0.00214544917233134\n",
            "46     \t [ 0.04979269 -1.63019338]. \t  -267.46490688217597 \t -0.00214544917233134\n",
            "47     \t [0.92442281 0.82308284]. \t  -0.10477754316675009 \t -0.00214544917233134\n",
            "48     \t [1.09287213 1.18906238]. \t  -0.01144177199045211 \t -0.00214544917233134\n",
            "49     \t [-1.55336905 -0.94182605]. \t  -1131.975543495534 \t -0.00214544917233134\n",
            "50     \t [0.55456785 0.27760079]. \t  -0.28807838200332336 \t -0.00214544917233134\n",
            "51     \t [-0.12291179  1.65516704]. \t  -270.24052394820245 \t -0.00214544917233134\n",
            "52     \t [-0.8913757  -1.21622906]. \t  -407.9008014958416 \t -0.00214544917233134\n",
            "53     \t [1.40039471 2.01317744]. \t  -0.4314663470058887 \t -0.00214544917233134\n",
            "54     \t [1.78058467 0.35588287]. \t  -792.8060006187319 \t -0.00214544917233134\n",
            "55     \t [0.22960627 0.10039259]. \t  -0.8207832359447311 \t -0.00214544917233134\n",
            "56     \t [ 0.77667859 -1.45008068]. \t  -421.65819600229224 \t -0.00214544917233134\n",
            "57     \t [1.16552328 0.63863447]. \t  -51.840050038487895 \t -0.00214544917233134\n",
            "58     \t [-1.88003489  1.2894185 ]. \t  -512.3476918814229 \t -0.00214544917233134\n",
            "59     \t [0.07486711 0.19051247]. \t  -4.274945064997554 \t -0.00214544917233134\n",
            "60     \t [0.88993571 0.79143583]. \t  -0.01214436931057295 \t -0.00214544917233134\n",
            "61     \t [-0.73526457  1.37349327]. \t  -72.37993316314449 \t -0.00214544917233134\n",
            "62     \t [1.02234657 1.04675329]. \t  \u001b[92m-0.0007429711644439714\u001b[0m \t -0.0007429711644439714\n",
            "63     \t [-1.26788648  1.07391939]. \t  -33.61799191629573 \t -0.0007429711644439714\n",
            "64     \t [1.08412302 1.15192111]. \t  -0.061840240111214015 \t -0.0007429711644439714\n",
            "65     \t [0.00666815 0.1064489 ]. \t  -2.1188986542501347 \t -0.0007429711644439714\n",
            "66     \t [-0.99005096 -1.41917455]. \t  -579.6605564382465 \t -0.0007429711644439714\n",
            "67     \t [-0.12782104 -0.34805222]. \t  -14.550019663292455 \t -0.0007429711644439714\n",
            "68     \t [-0.21021943 -1.84038436]. \t  -356.6275139476734 \t -0.0007429711644439714\n",
            "69     \t [ 1.25787611 -1.03913335]. \t  -687.2327766306505 \t -0.0007429711644439714\n",
            "70     \t [1.45679267 2.03870159]. \t  -0.9066076663296834 \t -0.0007429711644439714\n",
            "71     \t [-1.88464203  0.89496835]. \t  -714.236756096389 \t -0.0007429711644439714\n",
            "72     \t [-1.48129145  1.4231625 ]. \t  -65.61044659048798 \t -0.0007429711644439714\n",
            "73     \t [0.70714689 0.47395272]. \t  -0.1539048247424365 \t -0.0007429711644439714\n",
            "74     \t [0.91878218 1.19058995]. \t  -12.00791942197261 \t -0.0007429711644439714\n",
            "75     \t [1.34073137 1.92806476]. \t  -1.8192313711411956 \t -0.0007429711644439714\n",
            "76     \t [-0.44059091  0.50493421]. \t  -11.73582806019964 \t -0.0007429711644439714\n",
            "77     \t [-1.63805227  1.99291998]. \t  -54.61007531448305 \t -0.0007429711644439714\n",
            "78     \t [-1.96120296 -0.54796173]. \t  -1939.737311709064 \t -0.0007429711644439714\n",
            "79     \t [-0.25470364  0.21967616]. \t  -3.9706539656228124 \t -0.0007429711644439714\n",
            "80     \t [1.51229804 0.73918524]. \t  -239.8495473713162 \t -0.0007429711644439714\n",
            "81     \t [-0.80986668 -0.00261677]. \t  -46.637947996074075 \t -0.0007429711644439714\n",
            "82     \t [ 0.48940055 -1.91505832]. \t  -464.4784249439519 \t -0.0007429711644439714\n",
            "83     \t [1.00462471 0.49002277]. \t  -26.961873973475413 \t -0.0007429711644439714\n",
            "84     \t [-0.22239426 -1.4187918 ]. \t  -217.07035042605008 \t -0.0007429711644439714\n",
            "85     \t [ 0.23122171 -1.58196944]. \t  -268.05510264287017 \t -0.0007429711644439714\n",
            "86     \t [0.26118112 1.94675998]. \t  -353.43875976637065 \t -0.0007429711644439714\n",
            "87     \t [-1.6533929  -1.47474582]. \t  -1778.148931412991 \t -0.0007429711644439714\n",
            "88     \t [-1.73150366 -1.92966205]. \t  -2435.7498493694384 \t -0.0007429711644439714\n",
            "89     \t [0.46346717 0.22266945]. \t  -0.29405745378780723 \t -0.0007429711644439714\n",
            "90     \t [ 1.09161653 -1.74010343]. \t  -859.5125247473624 \t -0.0007429711644439714\n",
            "91     \t [0.57770317 0.37451944]. \t  -0.34462310723257716 \t -0.0007429711644439714\n",
            "92     \t [ 1.86123761 -1.17692962]. \t  -2154.755215436421 \t -0.0007429711644439714\n",
            "93     \t [-0.48850096 -0.58947631]. \t  -70.79216904289397 \t -0.0007429711644439714\n",
            "94     \t [0.48560572 0.26495299]. \t  -0.34951582377183243 \t -0.0007429711644439714\n",
            "95     \t [ 0.07153458 -0.88529216]. \t  -80.14492959171838 \t -0.0007429711644439714\n",
            "96     \t [0.26290806 1.8942863 ]. \t  -333.66626838296577 \t -0.0007429711644439714\n",
            "97     \t [0.98284828 0.81183232]. \t  -2.376776374037633 \t -0.0007429711644439714\n",
            "98     \t [ 0.17833914 -1.71005162]. \t  -304.0815231773848 \t -0.0007429711644439714\n",
            "99     \t [ 1.29958474 -1.45677609]. \t  -989.630449668447 \t -0.0007429711644439714\n",
            "100    \t [-0.01542798  2.0425412 ]. \t  -418.1313208144742 \t -0.0007429711644439714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfiDmH_bteGg",
        "outputId": "96c63270-e9d5-4be2-c76e-7373137c45d0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_loser_20 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_20 = dGPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.2888388  0.26430978]. \t  -3.777577453542735 \t -3.777577453542735\n",
            "init   \t [-0.04734225 -0.66978712]. \t  -46.25914429040042 \t -3.777577453542735\n",
            "init   \t [-0.50844394  0.13121892]. \t  -3.895838569726608 \t -3.777577453542735\n",
            "init   \t [-1.76903664  0.34623103]. \t  -782.3209705533411 \t -3.777577453542735\n",
            "init   \t [-1.07357076 -1.38954104]. \t  -650.524506936468 \t -3.777577453542735\n",
            "1      \t [-0.12445779  0.09875914]. \t  \u001b[92m-1.9577846182863219\u001b[0m \t -1.9577846182863219\n",
            "2      \t [1.53218166 0.73906821]. \t  -259.01444417510237 \t -1.9577846182863219\n",
            "3      \t [ 0.92617356 -1.26728579]. \t  -451.6033360820788 \t -1.9577846182863219\n",
            "4      \t [0.04098166 1.4092975 ]. \t  -199.05856175553572 \t -1.9577846182863219\n",
            "5      \t [-0.35605494  0.10030413]. \t  \u001b[92m-1.908956308825034\u001b[0m \t -1.908956308825034\n",
            "6      \t [ 0.44408175 -0.04477678]. \t  -6.1647374965964925 \t -1.908956308825034\n",
            "7      \t [-0.50597028  0.52152132]. \t  -9.31778908557592 \t -1.908956308825034\n",
            "8      \t [ 0.07418699 -0.06313874]. \t  \u001b[92m-1.3283082955160073\u001b[0m \t -1.3283082955160073\n",
            "9      \t [-0.48316149  0.04533448]. \t  -5.738325805710129 \t -1.3283082955160073\n",
            "10     \t [0.19938642 0.09381441]. \t  \u001b[92m-0.9332246150755861\u001b[0m \t -0.9332246150755861\n",
            "11     \t [-0.4132618   0.24390081]. \t  -2.531896475294343 \t -0.9332246150755861\n",
            "12     \t [ 1.61316771 -0.64057739]. \t  -1052.0078787746854 \t -0.9332246150755861\n",
            "13     \t [ 0.1922373  -0.04692773]. \t  -1.356114779508586 \t -0.9332246150755861\n",
            "14     \t [-0.09641529 -0.55990481]. \t  -33.60107280746357 \t -0.9332246150755861\n",
            "15     \t [0.30449669 0.04409213]. \t  \u001b[92m-0.720174622641462\u001b[0m \t -0.720174622641462\n",
            "16     \t [-0.53802008  1.68551668]. \t  -197.26136577179662 \t -0.720174622641462\n",
            "17     \t [ 0.53483739 -1.11396349]. \t  -196.22044177322152 \t -0.720174622641462\n",
            "18     \t [ 1.08585101 -0.14887929]. \t  -176.35294415771122 \t -0.720174622641462\n",
            "19     \t [0.28720071 0.03015814]. \t  -0.7818849215335612 \t -0.720174622641462\n",
            "20     \t [0.38644135 0.13973336]. \t  \u001b[92m-0.3856770475051456\u001b[0m \t -0.3856770475051456\n",
            "21     \t [0.59758858 1.12328572]. \t  -58.864134396647785 \t -0.3856770475051456\n",
            "22     \t [-1.20433052  1.77068799]. \t  -15.11674392603221 \t -0.3856770475051456\n",
            "23     \t [0.39304684 0.1292966 ]. \t  -0.43184182513176 \t -0.3856770475051456\n",
            "24     \t [1.98335063 1.77854771]. \t  -465.4263786808205 \t -0.3856770475051456\n",
            "25     \t [ 1.9065061  -0.75279122]. \t  -1925.8871723897323 \t -0.3856770475051456\n",
            "26     \t [-1.82681554  1.9426022 ]. \t  -202.49653787335362 \t -0.3856770475051456\n",
            "27     \t [-1.28963972  1.53715749]. \t  -6.830380602629708 \t -0.3856770475051456\n",
            "28     \t [-0.97310767  1.07730258]. \t  -5.59263230344469 \t -0.3856770475051456\n",
            "29     \t [-1.12981645  1.43216933]. \t  -6.959872717729103 \t -0.3856770475051456\n",
            "30     \t [-1.82429495  2.02042425]. \t  -178.9656971094364 \t -0.3856770475051456\n",
            "31     \t [0.32647522 0.09581771]. \t  -0.4652313823830797 \t -0.3856770475051456\n",
            "32     \t [0.71743311 1.86409402]. \t  -182.16349544847446 \t -0.3856770475051456\n",
            "33     \t [-1.75119222 -1.53266927]. \t  -2122.965094391892 \t -0.3856770475051456\n",
            "34     \t [-0.34315729 -1.82150672]. \t  -377.8784220837419 \t -0.3856770475051456\n",
            "35     \t [0.71344874 0.57377747]. \t  -0.5016056477204732 \t -0.3856770475051456\n",
            "36     \t [ 0.23209137 -0.67313579]. \t  -53.44290326077746 \t -0.3856770475051456\n",
            "37     \t [0.71680633 0.63813981]. \t  -1.6259561502064939 \t -0.3856770475051456\n",
            "38     \t [ 0.23765545 -1.4905528 ]. \t  -239.91225349165356 \t -0.3856770475051456\n",
            "39     \t [1.5768618  1.87960322]. \t  -37.1643059079143 \t -0.3856770475051456\n",
            "40     \t [1.2227872 1.9454104]. \t  -20.317806259435226 \t -0.3856770475051456\n",
            "41     \t [1.23911497 1.5713019 ]. \t  \u001b[92m-0.18602820046103113\u001b[0m \t -0.18602820046103113\n",
            "42     \t [1.33976262 1.80300852]. \t  \u001b[92m-0.1219102711382461\u001b[0m \t -0.1219102711382461\n",
            "43     \t [1.15796021 1.42087308]. \t  -0.664971072607035 \t -0.1219102711382461\n",
            "44     \t [1.37576884 1.90701543]. \t  -0.16158125926322597 \t -0.1219102711382461\n",
            "45     \t [1.26091935 1.37815125]. \t  -4.552578064003302 \t -0.1219102711382461\n",
            "46     \t [1.42376987 1.95616896]. \t  -0.6829951413682424 \t -0.1219102711382461\n",
            "47     \t [-1.61358248 -0.27306354]. \t  -834.3779756965681 \t -0.1219102711382461\n",
            "48     \t [-0.73357865 -0.93107915]. \t  -218.86508952773173 \t -0.1219102711382461\n",
            "49     \t [-1.88685634  0.14283044]. \t  -1176.193766234887 \t -0.1219102711382461\n",
            "50     \t [1.18393232 1.36293762]. \t  -0.18405026548519626 \t -0.1219102711382461\n",
            "51     \t [-0.27884781 -1.65221693]. \t  -300.91612072205027 \t -0.1219102711382461\n",
            "52     \t [0.95958742 0.4540251 ]. \t  -21.79026178952775 \t -0.1219102711382461\n",
            "53     \t [-1.23526845  1.31858593]. \t  -9.293846013936001 \t -0.1219102711382461\n",
            "54     \t [1.16727424 1.37071159]. \t  \u001b[92m-0.03467588777220765\u001b[0m \t -0.03467588777220765\n",
            "55     \t [0.84650621 0.67813071]. \t  -0.1713395092054355 \t -0.03467588777220765\n",
            "56     \t [0.68192035 0.41370143]. \t  -0.3644867267060248 \t -0.03467588777220765\n",
            "57     \t [1.38024534 1.90179781]. \t  -0.1456619528361899 \t -0.03467588777220765\n",
            "58     \t [0.86138726 0.78131544]. \t  -0.17387812398445762 \t -0.03467588777220765\n",
            "59     \t [1.01939187 1.03552353]. \t  \u001b[92m-0.001698275401478381\u001b[0m \t -0.001698275401478381\n",
            "60     \t [-0.79141436  0.56135413]. \t  -3.631438818286104 \t -0.001698275401478381\n",
            "61     \t [ 0.05867034 -1.4849026 ]. \t  -222.40312704335412 \t -0.001698275401478381\n",
            "62     \t [1.17938961 0.52339748]. \t  -75.29862545450827 \t -0.001698275401478381\n",
            "63     \t [ 0.9797235  -1.05616155]. \t  -406.4339491431488 \t -0.001698275401478381\n",
            "64     \t [0.89953123 0.79230413]. \t  -0.038494015891723674 \t -0.001698275401478381\n",
            "65     \t [0.75258149 0.56861868]. \t  -0.06171757795640556 \t -0.001698275401478381\n",
            "66     \t [-0.31349505 -0.18238048]. \t  -9.602251893713131 \t -0.001698275401478381\n",
            "67     \t [1.29570111 1.68430532]. \t  -0.09042461646734903 \t -0.001698275401478381\n",
            "68     \t [1.22010126 1.48256734]. \t  -0.052140900376403884 \t -0.001698275401478381\n",
            "69     \t [1.26030215 0.3072364 ]. \t  -164.19590973551527 \t -0.001698275401478381\n",
            "70     \t [1.76491521 1.48479772]. \t  -266.3168113776189 \t -0.001698275401478381\n",
            "71     \t [0.9378632  0.83764791]. \t  -0.17975293198643705 \t -0.001698275401478381\n",
            "72     \t [-1.66686773 -1.97826056]. \t  -2269.73983224685 \t -0.001698275401478381\n",
            "73     \t [0.91273974 0.83358512]. \t  -0.007638489299499125 \t -0.001698275401478381\n",
            "74     \t [1.22371457 1.48714945]. \t  -0.06071476267657547 \t -0.001698275401478381\n",
            "75     \t [0.58494512 0.5863745 ]. \t  -6.1363039550597644 \t -0.001698275401478381\n",
            "76     \t [1.27947381 1.62033334]. \t  -0.10606103609796559 \t -0.001698275401478381\n",
            "77     \t [0.79135292 1.58866863]. \t  -92.67052567735936 \t -0.001698275401478381\n",
            "78     \t [-0.45584474 -1.99917753]. \t  -489.1920031520368 \t -0.001698275401478381\n",
            "79     \t [0.74078949 0.54122632]. \t  -0.07287939025297656 \t -0.001698275401478381\n",
            "80     \t [1.34227767 1.77403493]. \t  -0.1937414006596084 \t -0.001698275401478381\n",
            "81     \t [ 1.21183187 -0.47090083]. \t  -376.1865774429796 \t -0.001698275401478381\n",
            "82     \t [ 0.53417678 -0.92816523]. \t  -147.47765806935433 \t -0.001698275401478381\n",
            "83     \t [0.66456458 0.44058435]. \t  -0.11262964677660582 \t -0.001698275401478381\n",
            "84     \t [-0.16315989 -0.73093746]. \t  -58.74244509940196 \t -0.001698275401478381\n",
            "85     \t [ 1.30373351 -0.58779085]. \t  -523.3633306799418 \t -0.001698275401478381\n",
            "86     \t [0.57389326 0.34698358]. \t  -0.21264901190978835 \t -0.001698275401478381\n",
            "87     \t [0.62451653 0.39346195]. \t  -0.14217192306761597 \t -0.001698275401478381\n",
            "88     \t [0.62742699 0.38542423]. \t  -0.14560107322342758 \t -0.001698275401478381\n",
            "89     \t [0.06839996 1.52341975]. \t  -231.52535975324432 \t -0.001698275401478381\n",
            "90     \t [0.62018364 0.37872747]. \t  -0.1477417902693745 \t -0.001698275401478381\n",
            "91     \t [0.970399   0.92872497]. \t  -0.01764455556492652 \t -0.001698275401478381\n",
            "92     \t [1.04503976 1.08390155]. \t  -0.00876330905533472 \t -0.001698275401478381\n",
            "93     \t [-0.18733043  1.95873802]. \t  -371.45089028383757 \t -0.001698275401478381\n",
            "94     \t [ 1.18442453 -0.54371545]. \t  -378.9501784625779 \t -0.001698275401478381\n",
            "95     \t [-0.8874626 -0.222455 ]. \t  -105.58157972927293 \t -0.001698275401478381\n",
            "96     \t [0.99870085 1.02741027]. \t  -0.0900429631432507 \t -0.001698275401478381\n",
            "97     \t [0.87780046 0.76482178]. \t  -0.018195278625614486 \t -0.001698275401478381\n",
            "98     \t [0.04630707 1.35580958]. \t  -184.1504868396169 \t -0.001698275401478381\n",
            "99     \t [ 1.46831754 -0.18825834]. \t  -549.7536009552517 \t -0.001698275401478381\n",
            "100    \t [1.04147996 1.09470167]. \t  -0.011762966578556024 \t -0.001698275401478381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kM4bcwSteJO",
        "outputId": "404abf9b-48c3-4a1d-8fb8-7e9d87989a2e"
      },
      "source": [
        "end_lose = time.time()\r\n",
        "end_lose\r\n",
        "\r\n",
        "time_lose = end_lose - start_lose\r\n",
        "time_lose\r\n",
        "\r\n",
        "start_win = time.time()\r\n",
        "start_win"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1615133255.903231"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NMzLM7nteQH",
        "outputId": "312e3481-6b45-49d5-ac5c-4b3a5eae1922"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_winner_1 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_1 = dGPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.45944904 -1.35549029]. \t  -245.71064611316496 \t -108.5713485785257\n",
            "init   \t [-0.26190226  1.10289909]. \t  -108.5713485785257 \t -108.5713485785257\n",
            "init   \t [-0.83834755 -1.43702853]. \t  -461.2775269355244 \t -108.5713485785257\n",
            "init   \t [-1.95592878 -0.32676048]. \t  -1732.9949421003257 \t -108.5713485785257\n",
            "init   \t [-1.07035795 -0.66496024]. \t  -332.12317010404104 \t -108.5713485785257\n",
            "1      \t [-0.27863117 -0.25378189]. \t  \u001b[92m-12.618635198822698\u001b[0m \t -12.618635198822698\n",
            "2      \t [ 2.01512356 -1.72012678]. \t  -3342.8528372665846 \t -12.618635198822698\n",
            "3      \t [-0.18076915 -0.89040318]. \t  -86.60200720604004 \t -12.618635198822698\n",
            "4      \t [-0.47497278 -0.25393809]. \t  -25.171139860134154 \t -12.618635198822698\n",
            "5      \t [1.6733781  1.94437529]. \t  -73.69604806058263 \t -12.618635198822698\n",
            "6      \t [0.19585914 1.35285458]. \t  -173.43603037699663 \t -12.618635198822698\n",
            "7      \t [-0.82321339  1.2832978 ]. \t  -40.00136365302549 \t -12.618635198822698\n",
            "8      \t [-1.2970488   2.03988602]. \t  -18.060664280264422 \t -12.618635198822698\n",
            "9      \t [-0.81741877  1.79679039]. \t  -130.6806327010777 \t -12.618635198822698\n",
            "10     \t [-1.62320868  1.91504656]. \t  -58.686647613096916 \t -12.618635198822698\n",
            "11     \t [1.10334076 0.82077753]. \t  -15.738509863923502 \t -12.618635198822698\n",
            "12     \t [-0.73826796  0.85450126]. \t  \u001b[92m-12.598228932592956\u001b[0m \t -12.598228932592956\n",
            "13     \t [-1.9487096   1.65113376]. \t  -469.37043159536927 \t -12.598228932592956\n",
            "14     \t [ 0.22229418 -0.27838367]. \t  \u001b[92m-11.350003958952993\u001b[0m \t -11.350003958952993\n",
            "15     \t [-0.52027153  0.35508588]. \t  \u001b[92m-3.0236191451316823\u001b[0m \t -3.0236191451316823\n",
            "16     \t [2.02252935 0.61865063]. \t  -1206.5061585137946 \t -3.0236191451316823\n",
            "17     \t [0.7607245  0.70422255]. \t  \u001b[92m-1.6327993665675586\u001b[0m \t -1.6327993665675586\n",
            "18     \t [1.05726862 1.5716986 ]. \t  -20.604135767958493 \t -1.6327993665675586\n",
            "19     \t [0.96028506 1.14111525]. \t  -4.796269464835756 \t -1.6327993665675586\n",
            "20     \t [1.28925969 2.03361921]. \t  -13.87959612767731 \t -1.6327993665675586\n",
            "21     \t [0.85669731 0.67608802]. \t  \u001b[92m-0.3551083693386862\u001b[0m \t -0.3551083693386862\n",
            "22     \t [1.12438706 1.0847952 ]. \t  -3.2357407426825646 \t -0.3551083693386862\n",
            "23     \t [-0.95070309  1.48236624]. \t  -37.27492459488175 \t -0.3551083693386862\n",
            "24     \t [0.96564048 1.00967148]. \t  -0.5973181372039185 \t -0.3551083693386862\n",
            "25     \t [0.4496127  0.17899278]. \t  -0.35655918982835205 \t -0.3551083693386862\n",
            "26     \t [1.05556557 1.03764811]. \t  -0.5893927390118394 \t -0.3551083693386862\n",
            "27     \t [0.97254944 0.91457515]. \t  \u001b[92m-0.0985801857821006\u001b[0m \t -0.0985801857821006\n",
            "28     \t [0.61277162 0.34978262]. \t  -0.2160279219698361 \t -0.0985801857821006\n",
            "29     \t [0.79694529 0.65416029]. \t  \u001b[92m-0.07747763434602004\u001b[0m \t -0.07747763434602004\n",
            "30     \t [0.5994469  0.37866452]. \t  -0.19779968761968295 \t -0.07747763434602004\n",
            "31     \t [0.91143508 0.81911236]. \t  \u001b[92m-0.02130331248597161\u001b[0m \t -0.02130331248597161\n",
            "32     \t [1.0573334  1.06879388]. \t  -0.2449581118713727 \t -0.02130331248597161\n",
            "33     \t [ 0.66521893 -0.23347934]. \t  -45.80907842624932 \t -0.02130331248597161\n",
            "34     \t [-0.41736404  0.42717354]. \t  -8.408849557644185 \t -0.02130331248597161\n",
            "35     \t [ 0.74271154 -1.83825245]. \t  -571.2154314512803 \t -0.02130331248597161\n",
            "36     \t [0.03658015 0.7696289 ]. \t  -59.95525251309522 \t -0.02130331248597161\n",
            "37     \t [0.94712867 0.91261806]. \t  -0.027023372759658458 \t -0.02130331248597161\n",
            "38     \t [0.98562259 0.92921117]. \t  -0.17863457512455758 \t -0.02130331248597161\n",
            "39     \t [1.05498416 1.12829335]. \t  -0.026437674680498402 \t -0.02130331248597161\n",
            "40     \t [-0.47088606 -0.46262524]. \t  -48.99821853353074 \t -0.02130331248597161\n",
            "41     \t [-0.3125598   0.45186613]. \t  -14.266629745783442 \t -0.02130331248597161\n",
            "42     \t [0.38598463 0.51471049]. \t  -13.752591818790338 \t -0.02130331248597161\n",
            "43     \t [0.81162664 0.63207845]. \t  -0.10655660504243622 \t -0.02130331248597161\n",
            "44     \t [ 1.73016164 -1.2227373 ]. \t  -1778.1645165098817 \t -0.02130331248597161\n",
            "45     \t [0.79959064 0.59623303]. \t  -0.22602969164074396 \t -0.02130331248597161\n",
            "46     \t [0.27098297 1.72492339]. \t  -273.27392385300084 \t -0.02130331248597161\n",
            "47     \t [-1.96679809  0.3621967 ]. \t  -1238.0742281602288 \t -0.02130331248597161\n",
            "48     \t [-0.04083381  0.04372802]. \t  -1.2602445579439834 \t -0.02130331248597161\n",
            "49     \t [1.23422419 1.1855391 ]. \t  -11.463734691460713 \t -0.02130331248597161\n",
            "50     \t [ 1.07759365 -0.39877846]. \t  -243.36182294818892 \t -0.02130331248597161\n",
            "51     \t [-1.03360426 -0.86578155]. \t  -378.21729846994225 \t -0.02130331248597161\n",
            "52     \t [-0.08642683  1.66645759]. \t  -276.40443835653934 \t -0.02130331248597161\n",
            "53     \t [1.03601251 1.06682825]. \t  \u001b[92m-0.005513691327294288\u001b[0m \t -0.005513691327294288\n",
            "54     \t [0.87674812 0.76641735]. \t  -0.015706279746331837 \t -0.005513691327294288\n",
            "55     \t [0.98389891 0.91629935]. \t  -0.26814526768509883 \t -0.005513691327294288\n",
            "56     \t [ 0.72322477 -1.94321176]. \t  -608.323317047283 \t -0.005513691327294288\n",
            "57     \t [0.91431373 0.84356974]. \t  -0.013118342511655605 \t -0.005513691327294288\n",
            "58     \t [-1.54262211 -0.70711967]. \t  -959.2999939976576 \t -0.005513691327294288\n",
            "59     \t [ 0.79100073 -0.80632854]. \t  -205.1091446202374 \t -0.005513691327294288\n",
            "60     \t [1.01557556 0.98236521]. \t  -0.24062213890521664 \t -0.005513691327294288\n",
            "61     \t [-1.94556662 -0.06092498]. \t  -1487.966766887634 \t -0.005513691327294288\n",
            "62     \t [-1.24554527 -0.53065282]. \t  -438.5297977056196 \t -0.005513691327294288\n",
            "63     \t [1.18403973 1.37069953]. \t  -0.13153039670748173 \t -0.005513691327294288\n",
            "64     \t [1.7002555  0.27668319]. \t  -683.8869845466562 \t -0.005513691327294288\n",
            "65     \t [1.99383884 1.10766416]. \t  -823.3747770666051 \t -0.005513691327294288\n",
            "66     \t [-0.35057316  0.69200381]. \t  -34.21178769527856 \t -0.005513691327294288\n",
            "67     \t [-1.39008788  0.14732385]. \t  -324.3423311777545 \t -0.005513691327294288\n",
            "68     \t [1.15695747 1.3347715 ]. \t  -0.026063798731862424 \t -0.005513691327294288\n",
            "69     \t [-1.50630826 -0.16398703]. \t  -598.206928062259 \t -0.005513691327294288\n",
            "70     \t [ 1.68333128 -1.97788986]. \t  -2315.5144414799224 \t -0.005513691327294288\n",
            "71     \t [0.49000013 0.20987825]. \t  -0.3514360196045616 \t -0.005513691327294288\n",
            "72     \t [0.97431717 0.92740352]. \t  -0.04857866950663945 \t -0.005513691327294288\n",
            "73     \t [ 0.74592129 -0.2301352 ]. \t  -61.928093339755414 \t -0.005513691327294288\n",
            "74     \t [-1.87109385  1.76343759]. \t  -310.15278416471114 \t -0.005513691327294288\n",
            "75     \t [-0.93266155 -1.55003675]. \t  -589.324028504928 \t -0.005513691327294288\n",
            "76     \t [-1.68024478  0.26353533]. \t  -662.383562847581 \t -0.005513691327294288\n",
            "77     \t [ 1.85179849 -1.57495903]. \t  -2504.843938692811 \t -0.005513691327294288\n",
            "78     \t [ 0.40210176 -0.89842239]. \t  -112.74042528726775 \t -0.005513691327294288\n",
            "79     \t [-0.05339432 -1.69392365]. \t  -289.01404418626623 \t -0.005513691327294288\n",
            "80     \t [-0.94283968 -1.63797583]. \t  -642.3083516225248 \t -0.005513691327294288\n",
            "81     \t [ 1.05683909 -0.15691659]. \t  -162.26635865176712 \t -0.005513691327294288\n",
            "82     \t [1.15671826 1.36276671]. \t  -0.0859137561541217 \t -0.005513691327294288\n",
            "83     \t [1.3296437  0.28016115]. \t  -221.46093307441774 \t -0.005513691327294288\n",
            "84     \t [-2.02364883 -1.77656229]. \t  -3456.8483534725187 \t -0.005513691327294288\n",
            "85     \t [-1.27770636  1.26153307]. \t  -18.95208179070812 \t -0.005513691327294288\n",
            "86     \t [-0.66326769 -1.69950119]. \t  -460.4804877218692 \t -0.005513691327294288\n",
            "87     \t [ 0.50612368 -1.16357471]. \t  -201.8089132793395 \t -0.005513691327294288\n",
            "88     \t [-0.88460821  0.61773558]. \t  -6.267523692053022 \t -0.005513691327294288\n",
            "89     \t [-1.99166903 -0.11334568]. \t  -1673.6644939831742 \t -0.005513691327294288\n",
            "90     \t [1.23532159 1.5378446 ]. \t  -0.06935969493415647 \t -0.005513691327294288\n",
            "91     \t [-0.63994283  0.08808947]. \t  -13.021610016776167 \t -0.005513691327294288\n",
            "92     \t [1.17175395 1.39572663]. \t  -0.08111610080418832 \t -0.005513691327294288\n",
            "93     \t [1.06284503 1.13140222]. \t  \u001b[92m-0.004260196964201298\u001b[0m \t -0.004260196964201298\n",
            "94     \t [-1.15969297  0.44127987]. \t  -86.31500129402497 \t -0.004260196964201298\n",
            "95     \t [-0.60324118  1.28584672]. \t  -87.56897214586364 \t -0.004260196964201298\n",
            "96     \t [-1.07175646 -1.97376558]. \t  -979.2475128021114 \t -0.004260196964201298\n",
            "97     \t [1.74495754 0.21290267]. \t  -802.5627060826796 \t -0.004260196964201298\n",
            "98     \t [1.35693448 1.64384049]. \t  -4.0252899496503955 \t -0.004260196964201298\n",
            "99     \t [0.88133044 0.79074803]. \t  -0.03369559239437238 \t -0.004260196964201298\n",
            "100    \t [ 0.33124127 -0.51873772]. \t  -39.943247223297455 \t -0.004260196964201298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBLcW6tlteS2",
        "outputId": "5c66d747-4410-43cd-e12a-0521feaf9f14"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_winner_2 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_2 = dGPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.44173258 -1.74529086]. \t  -1462.4037722675753 \t -1.3013277264983028\n",
            "init   \t [ 1.6176405  -0.26012243]. \t  -828.0271838688934 \t -1.3013277264983028\n",
            "init   \t [-1.525032    0.31071385]. \t  -412.40181251788414 \t -1.3013277264983028\n",
            "init   \t [ 1.39456889 -0.26574622]. \t  -488.8170397507174 \t -1.3013277264983028\n",
            "init   \t [0.80244966 0.75627765]. \t  -1.3013277264983028 \t -1.3013277264983028\n",
            "1      \t [ 0.33902339 -0.26495037]. \t  -14.868320436819797 \t -1.3013277264983028\n",
            "2      \t [ 0.63723347 -0.00495259]. \t  -17.025268728735657 \t -1.3013277264983028\n",
            "3      \t [0.97082692 1.75663077]. \t  -66.28094057138775 \t -1.3013277264983028\n",
            "4      \t [ 0.2824696  -0.71012934]. \t  -62.91195966330248 \t -1.3013277264983028\n",
            "5      \t [0.83001916 1.2011707 ]. \t  -26.267760827047802 \t -1.3013277264983028\n",
            "6      \t [1.15913032 1.12667404]. \t  -4.730276512247014 \t -1.3013277264983028\n",
            "7      \t [-0.74569877 -0.4126468 ]. \t  -96.88804112091125 \t -1.3013277264983028\n",
            "8      \t [0.97459716 0.82879227]. \t  -1.4658913551775805 \t -1.3013277264983028\n",
            "9      \t [1.61837733 2.00595213]. \t  -37.98296228972681 \t -1.3013277264983028\n",
            "10     \t [-0.26031814 -1.37812248]. \t  -210.64761671696127 \t -1.3013277264983028\n",
            "11     \t [0.94399219 0.95375463]. \t  \u001b[92m-0.39543082639989185\u001b[0m \t -0.39543082639989185\n",
            "12     \t [-0.24003767  0.63442817]. \t  -34.808681479786905 \t -0.39543082639989185\n",
            "13     \t [ 0.99179266 -0.2276715 ]. \t  -146.7306964172385 \t -0.39543082639989185\n",
            "14     \t [-0.17257696 -0.17236333]. \t  -5.461242854981611 \t -0.39543082639989185\n",
            "15     \t [-0.99871037  1.86643011]. \t  -79.5122841638443 \t -0.39543082639989185\n",
            "16     \t [-0.1103374 -0.2189951]. \t  -6.576779990310856 \t -0.39543082639989185\n",
            "17     \t [-0.0687056   0.13412913]. \t  -2.8167921253496013 \t -0.39543082639989185\n",
            "18     \t [0.25609951 1.14170362]. \t  -116.35609426092552 \t -0.39543082639989185\n",
            "19     \t [0.79806246 0.64724868]. \t  \u001b[92m-0.051480636842797745\u001b[0m \t -0.051480636842797745\n",
            "20     \t [-0.74325245 -0.41707328]. \t  -97.03146624563828 \t -0.051480636842797745\n",
            "21     \t [-0.47121853  0.22209073]. \t  -2.164484154975887 \t -0.051480636842797745\n",
            "22     \t [0.81522103 0.6769212 ]. \t  \u001b[92m-0.04936063201465491\u001b[0m \t -0.04936063201465491\n",
            "23     \t [0.72451938 0.50584602]. \t  -0.11230301877260293 \t -0.04936063201465491\n",
            "24     \t [ 0.56866235 -0.90474152]. \t  -151.01353122800091 \t -0.04936063201465491\n",
            "25     \t [-0.70658052  0.80086568]. \t  -12.009255227230367 \t -0.04936063201465491\n",
            "26     \t [0.88264132 0.79574464]. \t  \u001b[92m-0.04162511363239053\u001b[0m \t -0.04162511363239053\n",
            "27     \t [ 0.00087105 -0.02824439]. \t  -1.0780375219307974 \t -0.04162511363239053\n",
            "28     \t [0.51158158 0.29053007]. \t  -0.3215792826944577 \t -0.04162511363239053\n",
            "29     \t [0.74365306 0.54086756]. \t  -0.08048163691127136 \t -0.04162511363239053\n",
            "30     \t [0.88155552 0.77067489]. \t  \u001b[92m-0.018209025228376006\u001b[0m \t -0.018209025228376006\n",
            "31     \t [1.76927873 1.12586904]. \t  -402.38507314741815 \t -0.018209025228376006\n",
            "32     \t [1.11699157 1.28003647]. \t  -0.11844473253612928 \t -0.018209025228376006\n",
            "33     \t [1.00064555 1.06371229]. \t  -0.3896356004505905 \t -0.018209025228376006\n",
            "34     \t [-1.64723225  2.04749812]. \t  -51.34691866317241 \t -0.018209025228376006\n",
            "35     \t [0.68762104 0.41937062]. \t  -0.38329296543179847 \t -0.018209025228376006\n",
            "36     \t [0.93821415 0.86251887]. \t  -0.035241894498354374 \t -0.018209025228376006\n",
            "37     \t [0.62877442 0.36371166]. \t  -0.23795293688178648 \t -0.018209025228376006\n",
            "38     \t [ 1.32130641 -1.83018897]. \t  -1278.909164298691 \t -0.018209025228376006\n",
            "39     \t [1.17085781 0.86407361]. \t  -25.717303713322334 \t -0.018209025228376006\n",
            "40     \t [-0.13088412  1.64935483]. \t  -267.6944770334617 \t -0.018209025228376006\n",
            "41     \t [0.82857804 0.6756641 ]. \t  -0.04121741828727636 \t -0.018209025228376006\n",
            "42     \t [-1.0567372  -0.45304023]. \t  -250.63656771094728 \t -0.018209025228376006\n",
            "43     \t [ 2.02301052 -1.55702126]. \t  -3192.836445995665 \t -0.018209025228376006\n",
            "44     \t [1.00667453 1.04123289]. \t  -0.0775470933850176 \t -0.018209025228376006\n",
            "45     \t [0.3266981  0.14496727]. \t  -0.5995316965153333 \t -0.018209025228376006\n",
            "46     \t [-1.31511555  1.46180082]. \t  -12.527592687691108 \t -0.018209025228376006\n",
            "47     \t [1.50263101 1.01997912]. \t  -153.4974360773491 \t -0.018209025228376006\n",
            "48     \t [1.39418891 1.89122823]. \t  -0.4313722070295265 \t -0.018209025228376006\n",
            "49     \t [-0.79787615 -1.07553245]. \t  -296.3742858915344 \t -0.018209025228376006\n",
            "50     \t [-1.02178457  1.77133917]. \t  -56.98348258669194 \t -0.018209025228376006\n",
            "51     \t [1.09068139 1.18676366]. \t  \u001b[92m-0.009019614431296175\u001b[0m \t -0.009019614431296175\n",
            "52     \t [-0.75825752  1.91406987]. \t  -182.4144761163101 \t -0.009019614431296175\n",
            "53     \t [-0.80497207  1.85963067]. \t  -150.06764795856756 \t -0.009019614431296175\n",
            "54     \t [1.45858248 2.00412266]. \t  -1.7315783674379341 \t -0.009019614431296175\n",
            "55     \t [-1.90268391  0.96809939]. \t  -711.7925471201771 \t -0.009019614431296175\n",
            "56     \t [ 0.11728508 -0.63640837]. \t  -43.05052897459257 \t -0.009019614431296175\n",
            "57     \t [0.3879084  1.76656536]. \t  -261.55013351865534 \t -0.009019614431296175\n",
            "58     \t [1.14915145 1.31422172]. \t  -0.02624966171987911 \t -0.009019614431296175\n",
            "59     \t [1.42595225 2.03319581]. \t  -0.1814373959673249 \t -0.009019614431296175\n",
            "60     \t [1.36698151 1.89563033]. \t  -0.20753153402032193 \t -0.009019614431296175\n",
            "61     \t [-1.24963833 -1.51014022]. \t  -948.6171836162908 \t -0.009019614431296175\n",
            "62     \t [-1.69061122 -0.69050267]. \t  -1266.5445430312402 \t -0.009019614431296175\n",
            "63     \t [1.12425267 1.2847814 ]. \t  -0.05885817996697626 \t -0.009019614431296175\n",
            "64     \t [1.05882539 1.15049556]. \t  -0.08980443793740926 \t -0.009019614431296175\n",
            "65     \t [ 0.12167434 -1.732526  ]. \t  -306.08789281014197 \t -0.009019614431296175\n",
            "66     \t [ 0.90384204 -0.75436427]. \t  -246.90595110770494 \t -0.009019614431296175\n",
            "67     \t [ 0.32900738 -0.38466485]. \t  -24.746327383062937 \t -0.009019614431296175\n",
            "68     \t [ 0.88967876 -1.29655175]. \t  -436.0199975541458 \t -0.009019614431296175\n",
            "69     \t [-0.73423947 -1.10368982]. \t  -272.8859221612377 \t -0.009019614431296175\n",
            "70     \t [-1.46224411  0.12652474]. \t  -410.7294206442398 \t -0.009019614431296175\n",
            "71     \t [-1.84477672  1.67970458]. \t  -305.1367924344605 \t -0.009019614431296175\n",
            "72     \t [1.14317236 1.33546406]. \t  -0.10241459586758428 \t -0.009019614431296175\n",
            "73     \t [1.16520998 1.37270041]. \t  -0.049752698478014445 \t -0.009019614431296175\n",
            "74     \t [0.75483372 0.56410819]. \t  -0.06331658876375948 \t -0.009019614431296175\n",
            "75     \t [-1.57882819 -0.776232  ]. \t  -1075.2409896985714 \t -0.009019614431296175\n",
            "76     \t [ 1.34076278 -1.96365453]. \t  -1414.853401573814 \t -0.009019614431296175\n",
            "77     \t [-1.06561444  1.17830657]. \t  -4.449711194964767 \t -0.009019614431296175\n",
            "78     \t [-0.29102032  1.95558644]. \t  -351.6910250679199 \t -0.009019614431296175\n",
            "79     \t [-2.0382743   0.10376635]. \t  -1650.1257476316432 \t -0.009019614431296175\n",
            "80     \t [0.65656556 0.43960001]. \t  -0.1252090995013737 \t -0.009019614431296175\n",
            "81     \t [0.02940831 1.96199674]. \t  -385.54587692391846 \t -0.009019614431296175\n",
            "82     \t [0.63456521 0.01139685]. \t  -15.443245414218985 \t -0.009019614431296175\n",
            "83     \t [-0.35186844  1.22744145]. \t  -123.62747676309935 \t -0.009019614431296175\n",
            "84     \t [0.02188484 1.06933684]. \t  -115.20242968324177 \t -0.009019614431296175\n",
            "85     \t [-1.41357471 -1.37068528]. \t  -1140.7597357766265 \t -0.009019614431296175\n",
            "86     \t [1.07109144 1.16399651]. \t  -0.033142507071458195 \t -0.009019614431296175\n",
            "87     \t [-0.06220783  1.90175057]. \t  -361.3234217223068 \t -0.009019614431296175\n",
            "88     \t [ 0.78679977 -0.75523225]. \t  -188.91169055958073 \t -0.009019614431296175\n",
            "89     \t [ 2.04698854 -0.95916468]. \t  -2652.652812167113 \t -0.009019614431296175\n",
            "90     \t [0.6762704  0.45083108]. \t  -0.10903961275380887 \t -0.009019614431296175\n",
            "91     \t [-0.31919473 -0.90157516]. \t  -102.43355936094454 \t -0.009019614431296175\n",
            "92     \t [0.58415127 0.33190171]. \t  -0.18163690667767013 \t -0.009019614431296175\n",
            "93     \t [1.11428792 1.24962405]. \t  -0.019440124210038993 \t -0.009019614431296175\n",
            "94     \t [0.02694048 1.70314608]. \t  -290.7703290523399 \t -0.009019614431296175\n",
            "95     \t [-0.20726081  0.57599578]. \t  -29.87050810289775 \t -0.009019614431296175\n",
            "96     \t [-1.81775869 -1.24819742]. \t  -2080.414455294042 \t -0.009019614431296175\n",
            "97     \t [0.41936547 0.26065522]. \t  -1.056033835945708 \t -0.009019614431296175\n",
            "98     \t [-1.70982569  0.65088814]. \t  -523.821384062963 \t -0.009019614431296175\n",
            "99     \t [0.59389047 0.36878622]. \t  -0.1907826396952075 \t -0.009019614431296175\n",
            "100    \t [0.65717454 1.15221083]. \t  -52.00541436633597 \t -0.009019614431296175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYCkhR-td2t8",
        "outputId": "ff4f16d8-86a3-4359-e8f2-b55cf4388678"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_winner_3 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_3 = dGPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.03045369 -1.60050678]. \t  -708.8071948417274 \t -1.118465165857483\n",
            "init   \t [-0.05684724 -0.0006915 ]. \t  -1.118465165857483 \t -1.118465165857483\n",
            "init   \t [ 0.64352256 -1.08181622]. \t  -223.90997740226942 \t -1.118465165857483\n",
            "init   \t [ 0.46200787 -1.55790376]. \t  -314.05929962166397 \t -1.118465165857483\n",
            "init   \t [ 0.86918061 -0.52199202]. \t  -163.20929387450548 \t -1.118465165857483\n",
            "1      \t [-0.56212963 -1.0124551 ]. \t  -178.9168151053094 \t -1.118465165857483\n",
            "2      \t [ 0.19510124 -0.39571949]. \t  -19.464716969441064 \t -1.118465165857483\n",
            "3      \t [-0.49748272  1.20436342]. \t  -93.80331056602205 \t -1.118465165857483\n",
            "4      \t [-0.03748552  0.27949138]. \t  -8.809570650521628 \t -1.118465165857483\n",
            "5      \t [-0.7505487   0.37715744]. \t  -6.530195027947112 \t -1.118465165857483\n",
            "6      \t [-0.37659969  0.1941843 ]. \t  -2.169151970106639 \t -1.118465165857483\n",
            "7      \t [-2.00247703  0.74814527]. \t  -1072.928562346176 \t -1.118465165857483\n",
            "8      \t [0.26428985 0.00057534]. \t  \u001b[92m-1.0211551767045783\u001b[0m \t -1.0211551767045783\n",
            "9      \t [-1.93971307  0.59809255]. \t  -1009.9810140546972 \t -1.0211551767045783\n",
            "10     \t [-0.48234215  0.34288097]. \t  -3.4123377264274266 \t -1.0211551767045783\n",
            "11     \t [0.11487674 0.05912955]. \t  \u001b[92m-0.9944261838126743\u001b[0m \t -0.9944261838126743\n",
            "12     \t [1.20192189 0.99338095]. \t  -20.402100015337346 \t -0.9944261838126743\n",
            "13     \t [0.74262342 0.6380968 ]. \t  \u001b[92m-0.8163243440674126\u001b[0m \t -0.8163243440674126\n",
            "14     \t [2.048 2.048]. \t  -461.76039004149084 \t -0.8163243440674126\n",
            "15     \t [0.87090426 0.58403709]. \t  -3.059497423275766 \t -0.8163243440674126\n",
            "16     \t [-0.52862651  0.14906406]. \t  -4.036643944535179 \t -0.8163243440674126\n",
            "17     \t [ 0.78733669 -2.04448324]. \t  -709.9385288558885 \t -0.8163243440674126\n",
            "18     \t [-1.65285435 -1.03867129]. \t  -1428.779171439316 \t -0.8163243440674126\n",
            "19     \t [0.3891863  0.20822208]. \t  \u001b[92m-0.6952189034137927\u001b[0m \t -0.6952189034137927\n",
            "20     \t [-0.03644805  0.93272059]. \t  -87.82335433855155 \t -0.6952189034137927\n",
            "21     \t [-1.54677484 -1.42960587]. \t  -1467.344875545267 \t -0.6952189034137927\n",
            "22     \t [0.85238635 0.74968644]. \t  \u001b[92m-0.07526150509776647\u001b[0m \t -0.07526150509776647\n",
            "23     \t [-0.93135105 -0.28698016]. \t  -136.99288509973206 \t -0.07526150509776647\n",
            "24     \t [-0.73480037  0.62186792]. \t  -3.6808886074715783 \t -0.07526150509776647\n",
            "25     \t [-0.53104766 -1.92376744]. \t  -488.89022912611455 \t -0.07526150509776647\n",
            "26     \t [-1.91893083  1.42636393]. \t  -517.4428957647219 \t -0.07526150509776647\n",
            "27     \t [-1.50388476  1.97864226]. \t  -14.279872700331438 \t -0.07526150509776647\n",
            "28     \t [ 0.31077384 -1.25349688]. \t  -182.7458916218189 \t -0.07526150509776647\n",
            "29     \t [-0.33250964 -0.39777081]. \t  -27.615873630731052 \t -0.07526150509776647\n",
            "30     \t [0.00105395 0.42343274]. \t  -18.927327629439564 \t -0.07526150509776647\n",
            "31     \t [ 0.65582879 -0.27149012]. \t  -49.342923630423144 \t -0.07526150509776647\n",
            "32     \t [ 0.52575715 -0.44393616]. \t  -52.11629023706623 \t -0.07526150509776647\n",
            "33     \t [-0.28444708 -0.11423451]. \t  -5.457947591478899 \t -0.07526150509776647\n",
            "34     \t [-1.28321751  0.98886312]. \t  -48.48106860740266 \t -0.07526150509776647\n",
            "35     \t [-1.24396408  1.61208391]. \t  -5.4531725633735615 \t -0.07526150509776647\n",
            "36     \t [-1.18592699  1.30026721]. \t  -5.905178406132372 \t -0.07526150509776647\n",
            "37     \t [0.98859182 1.03202878]. \t  -0.29950323691060665 \t -0.07526150509776647\n",
            "38     \t [-1.19700754  1.98096872]. \t  -34.872772054924496 \t -0.07526150509776647\n",
            "39     \t [ 1.7242307  -0.10450261]. \t  -947.6092010408778 \t -0.07526150509776647\n",
            "40     \t [0.67512689 0.42792531]. \t  -0.18322182443533316 \t -0.07526150509776647\n",
            "41     \t [ 0.69436079 -1.97932458]. \t  -605.9726815722216 \t -0.07526150509776647\n",
            "42     \t [1.02597366 1.02183014]. \t  -0.09548818264151808 \t -0.07526150509776647\n",
            "43     \t [-1.13469519 -0.78596058]. \t  -434.4945583852397 \t -0.07526150509776647\n",
            "44     \t [1.18179087 1.6422259 ]. \t  -6.064799197105735 \t -0.07526150509776647\n",
            "45     \t [0.81755276 0.63478545]. \t  -0.1462304654578011 \t -0.07526150509776647\n",
            "46     \t [0.91229568 2.03548005]. \t  -144.77590713612958 \t -0.07526150509776647\n",
            "47     \t [1.14907196 1.24566204]. \t  -0.5802961480496691 \t -0.07526150509776647\n",
            "48     \t [-0.14167054 -1.84125797]. \t  -347.75779274568123 \t -0.07526150509776647\n",
            "49     \t [0.78356446 0.60454962]. \t  \u001b[92m-0.05572485243131822\u001b[0m \t -0.05572485243131822\n",
            "50     \t [0.08799344 0.14240424]. \t  -2.6451251959418776 \t -0.05572485243131822\n",
            "51     \t [-1.86423292  1.96849273]. \t  -235.27004747386428 \t -0.05572485243131822\n",
            "52     \t [-1.14761215 -0.61437996]. \t  -377.64036514287966 \t -0.05572485243131822\n",
            "53     \t [1.08165771 1.16910576]. \t  \u001b[92m-0.006745005607482398\u001b[0m \t -0.006745005607482398\n",
            "54     \t [-0.87758037 -0.48692504]. \t  -161.5483951751627 \t -0.006745005607482398\n",
            "55     \t [-1.17155945  1.30088295]. \t  -5.229309276185716 \t -0.006745005607482398\n",
            "56     \t [0.79195311 0.59402262]. \t  -0.153289244599389 \t -0.006745005607482398\n",
            "57     \t [0.94133188 0.84400333]. \t  -0.18070291189999024 \t -0.006745005607482398\n",
            "58     \t [0.71543278 0.49697379]. \t  -0.10309101568058185 \t -0.006745005607482398\n",
            "59     \t [0.81671288 0.67273469]. \t  -0.036860009169423474 \t -0.006745005607482398\n",
            "60     \t [-1.62102871 -0.55884008]. \t  -1022.2952884966214 \t -0.006745005607482398\n",
            "61     \t [ 0.26570157 -1.88139766]. \t  -381.56763794999836 \t -0.006745005607482398\n",
            "62     \t [-1.43811614  1.44980927]. \t  -44.18240357398234 \t -0.006745005607482398\n",
            "63     \t [-1.89418234 -1.62116101]. \t  -2721.8358303256878 \t -0.006745005607482398\n",
            "64     \t [0.2233846  1.53186692]. \t  -220.2255244937937 \t -0.006745005607482398\n",
            "65     \t [-0.33670335  1.87999363]. \t  -313.88298237180817 \t -0.006745005607482398\n",
            "66     \t [0.9427676  0.89343169]. \t  \u001b[92m-0.0054108541640425275\u001b[0m \t -0.0054108541640425275\n",
            "67     \t [0.20204697 0.38969872]. \t  -12.80815708669491 \t -0.0054108541640425275\n",
            "68     \t [1.50003658 0.3734927 ]. \t  -352.4191905238935 \t -0.0054108541640425275\n",
            "69     \t [0.76956633 0.58995518]. \t  -0.05361822201319874 \t -0.0054108541640425275\n",
            "70     \t [1.06728518 1.1441875 ]. \t  -0.00711795424153261 \t -0.0054108541640425275\n",
            "71     \t [-2.00348166  0.78190776]. \t  -1053.623340234436 \t -0.0054108541640425275\n",
            "72     \t [0.66974218 0.45957582]. \t  -0.12121698002851679 \t -0.0054108541640425275\n",
            "73     \t [0.17664647 0.85788352]. \t  -69.01781709494013 \t -0.0054108541640425275\n",
            "74     \t [-1.62209618  0.89567602]. \t  -308.0783502449619 \t -0.0054108541640425275\n",
            "75     \t [0.84979767 0.74523492]. \t  -0.07582403903418544 \t -0.0054108541640425275\n",
            "76     \t [-0.56482304 -0.9845483 ]. \t  -172.37902397038314 \t -0.0054108541640425275\n",
            "77     \t [-0.76123896 -0.88227645]. \t  -216.7765445007591 \t -0.0054108541640425275\n",
            "78     \t [0.34422813 0.4797669 ]. \t  -13.481919620506176 \t -0.0054108541640425275\n",
            "79     \t [-0.3496611   1.99775184]. \t  -353.5674665544358 \t -0.0054108541640425275\n",
            "80     \t [ 1.96497114 -1.29178412]. \t  -2656.164566043114 \t -0.0054108541640425275\n",
            "81     \t [ 0.39394077 -1.05235458]. \t  -146.1835367236264 \t -0.0054108541640425275\n",
            "82     \t [-0.22920604 -0.05496795]. \t  -2.6666446324635102 \t -0.0054108541640425275\n",
            "83     \t [0.30981187 0.87976317]. \t  -61.907432340113104 \t -0.0054108541640425275\n",
            "84     \t [0.73450574 0.5647427 ]. \t  -0.13421322165378918 \t -0.0054108541640425275\n",
            "85     \t [1.16257432 1.35383555]. \t  -0.02693958739093613 \t -0.0054108541640425275\n",
            "86     \t [0.85705668 0.75128495]. \t  -0.04845152496022658 \t -0.0054108541640425275\n",
            "87     \t [ 2.00030005 -1.1263103 ]. \t  -2630.137104728218 \t -0.0054108541640425275\n",
            "88     \t [1.42267845 0.33450296]. \t  -285.62340308224816 \t -0.0054108541640425275\n",
            "89     \t [0.99560463 0.98250413]. \t  -0.007630923643092643 \t -0.0054108541640425275\n",
            "90     \t [ 0.85116426 -0.33166719]. \t  -111.56696821035466 \t -0.0054108541640425275\n",
            "91     \t [1.16964768 1.39835204]. \t  -0.12044612349207662 \t -0.0054108541640425275\n",
            "92     \t [0.84259152 0.68930194]. \t  -0.06745493301139165 \t -0.0054108541640425275\n",
            "93     \t [ 0.34866918 -0.0873866 ]. \t  -4.7905262893301686 \t -0.0054108541640425275\n",
            "94     \t [0.88414022 0.7804986 ]. \t  -0.013568770554377583 \t -0.0054108541640425275\n",
            "95     \t [-1.0219889  -0.74463384]. \t  -324.17458727864135 \t -0.0054108541640425275\n",
            "96     \t [-1.97904207 -1.1990717 ]. \t  -2625.8920644414125 \t -0.0054108541640425275\n",
            "97     \t [0.88678747 0.76523517]. \t  -0.05757829370212176 \t -0.0054108541640425275\n",
            "98     \t [-0.84088724  1.7432666 ]. \t  -110.75478073212808 \t -0.0054108541640425275\n",
            "99     \t [1.15066738 1.3436792 ]. \t  -0.061288447658918714 \t -0.0054108541640425275\n",
            "100    \t [-1.33438605  0.77689646]. \t  -106.18865265597901 \t -0.0054108541640425275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8riJpBBKuvT4",
        "outputId": "d0590349-6bfe-420b-d5a4-60e194cf4a64"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_winner_4 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_4 = dGPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.39002285 1.31602418]. \t  -38.11488737154776 \t -12.122423820878506\n",
            "init   \t [0.58248055 0.68494384]. \t  -12.122423820878506 \t -12.122423820878506\n",
            "init   \t [-1.88768295 -1.7790059 ]. \t  -2862.4120613667637 \t -12.122423820878506\n",
            "init   \t [-0.91545338 -0.75218738]. \t  -256.5560079635026 \t -12.122423820878506\n",
            "init   \t [-1.25504548 -1.24070756]. \t  -797.98450090518 \t -12.122423820878506\n",
            "1      \t [-0.07700328 -1.60236599]. \t  -259.8213749889663 \t -12.122423820878506\n",
            "2      \t [0.59737347 1.19847784]. \t  -70.9949983342334 \t -12.122423820878506\n",
            "3      \t [1.37794346 0.38842274]. \t  -228.24509739867707 \t -12.122423820878506\n",
            "4      \t [1.97887267 1.83898365]. \t  -432.3317389383738 \t -12.122423820878506\n",
            "5      \t [1.08363028 0.84122816]. \t  \u001b[92m-11.097654127987353\u001b[0m \t -11.097654127987353\n",
            "6      \t [0.97677209 1.00055463]. \t  \u001b[92m-0.21649406009956415\u001b[0m \t -0.21649406009956415\n",
            "7      \t [1.14263635 1.00850813]. \t  -8.847762431690139 \t -0.21649406009956415\n",
            "8      \t [-1.86706833  1.80379066]. \t  -291.18412402872536 \t -0.21649406009956415\n",
            "9      \t [0.82601877 0.64998071]. \t  \u001b[92m-0.13476841094465802\u001b[0m \t -0.13476841094465802\n",
            "10     \t [0.80336351 0.8402846 ]. \t  -3.8369421649119575 \t -0.13476841094465802\n",
            "11     \t [0.7964193 2.0268285]. \t  -193.95954519238225 \t -0.13476841094465802\n",
            "12     \t [-1.78443008  1.4136824 ]. \t  -321.22301968113425 \t -0.13476841094465802\n",
            "13     \t [1.10690661 1.22160063]. \t  \u001b[92m-0.012755165666884116\u001b[0m \t -0.012755165666884116\n",
            "14     \t [ 1.31136086 -1.87520142]. \t  -1292.4050536155364 \t -0.012755165666884116\n",
            "15     \t [-1.37723669  1.28158256]. \t  -43.498152342951585 \t -0.012755165666884116\n",
            "16     \t [-0.86098291  1.3681576 ]. \t  -42.75935983266734 \t -0.012755165666884116\n",
            "17     \t [-1.13817539  0.77600354]. \t  -31.553550867213094 \t -0.012755165666884116\n",
            "18     \t [-1.07220925  1.12475824]. \t  -4.355924940463668 \t -0.012755165666884116\n",
            "19     \t [-1.23317524  1.70747383]. \t  -8.474727225081644 \t -0.012755165666884116\n",
            "20     \t [1.43289088 1.93136973]. \t  -1.6710779543216412 \t -0.012755165666884116\n",
            "21     \t [1.32622291 1.98321263]. \t  -5.1395076732227984 \t -0.012755165666884116\n",
            "22     \t [1.29509    1.64768161]. \t  -0.1745550209891446 \t -0.012755165666884116\n",
            "23     \t [-1.32313472  1.93226442]. \t  -8.694046079110318 \t -0.012755165666884116\n",
            "24     \t [ 0.58124914 -1.68658277]. \t  -410.0083811563644 \t -0.012755165666884116\n",
            "25     \t [ 1.58903548 -0.02424579]. \t  -650.2295841149264 \t -0.012755165666884116\n",
            "26     \t [1.23119677 1.49215426]. \t  -0.10957928879357076 \t -0.012755165666884116\n",
            "27     \t [0.71197017 0.50526466]. \t  -0.083229114579198 \t -0.012755165666884116\n",
            "28     \t [0.9395445  1.81776527]. \t  -87.43015582923303 \t -0.012755165666884116\n",
            "29     \t [-0.60496585  0.88407464]. \t  -29.41773966484951 \t -0.012755165666884116\n",
            "30     \t [0.81149774 0.67475577]. \t  -0.06186525973497762 \t -0.012755165666884116\n",
            "31     \t [0.33941695 0.41635123]. \t  -9.50534310900239 \t -0.012755165666884116\n",
            "32     \t [ 2.02404427 -0.30309566]. \t  -1936.9174397968495 \t -0.012755165666884116\n",
            "33     \t [ 0.79369966 -0.64283847]. \t  -162.04393906599825 \t -0.012755165666884116\n",
            "34     \t [-0.42174442  0.35286944]. \t  -5.0838953819707555 \t -0.012755165666884116\n",
            "35     \t [0.83573851 0.66254748]. \t  -0.15594456368702336 \t -0.012755165666884116\n",
            "36     \t [-1.68502299  0.781346  ]. \t  -430.7278318942348 \t -0.012755165666884116\n",
            "37     \t [2.04206484 1.39503674]. \t  -771.1439943589678 \t -0.012755165666884116\n",
            "38     \t [ 1.84826548 -0.78314523]. \t  -1764.073248849323 \t -0.012755165666884116\n",
            "39     \t [-0.71342764  0.58310577]. \t  -3.4853120594453757 \t -0.012755165666884116\n",
            "40     \t [0.74620719 0.50507761]. \t  -0.33219178114267806 \t -0.012755165666884116\n",
            "41     \t [ 0.05100065 -1.14075   ]. \t  -131.6257648471527 \t -0.012755165666884116\n",
            "42     \t [-0.48263409 -0.17790873]. \t  -19.077516014712117 \t -0.012755165666884116\n",
            "43     \t [ 0.02600817 -0.21887524]. \t  -5.7689534922929 \t -0.012755165666884116\n",
            "44     \t [-0.18806388 -0.00610804]. \t  -1.5835221397490897 \t -0.012755165666884116\n",
            "45     \t [-0.63915313 -1.52523522]. \t  -376.62647934729335 \t -0.012755165666884116\n",
            "46     \t [-0.93050001 -0.81481761]. \t  -286.1845595033058 \t -0.012755165666884116\n",
            "47     \t [ 0.16957906 -1.30817191]. \t  -179.42750437290832 \t -0.012755165666884116\n",
            "48     \t [-0.02410094 -0.51242169]. \t  -27.365943875563282 \t -0.012755165666884116\n",
            "49     \t [1.14365637 1.26419863]. \t  -0.2120545304560897 \t -0.012755165666884116\n",
            "50     \t [ 0.50426436 -1.43702746]. \t  -286.2987090020451 \t -0.012755165666884116\n",
            "51     \t [1.25718045 1.58434559]. \t  -0.06761857063208612 \t -0.012755165666884116\n",
            "52     \t [ 1.30973476 -1.95863185]. \t  -1349.9507199683198 \t -0.012755165666884116\n",
            "53     \t [1.26600886 1.59203891]. \t  -0.08229443664479079 \t -0.012755165666884116\n",
            "54     \t [ 1.04104279 -0.9580443 ]. \t  -416.90228422396586 \t -0.012755165666884116\n",
            "55     \t [1.23726192 1.51706772]. \t  -0.07519767202618159 \t -0.012755165666884116\n",
            "56     \t [1.22366848 1.49645547]. \t  -0.05011022807994524 \t -0.012755165666884116\n",
            "57     \t [1.9173578  0.61140306]. \t  -940.1769340714045 \t -0.012755165666884116\n",
            "58     \t [ 1.94491488 -1.47738315]. \t  -2767.733922681714 \t -0.012755165666884116\n",
            "59     \t [0.61405767 2.03367284]. \t  -274.5833037339654 \t -0.012755165666884116\n",
            "60     \t [-0.11891331  0.13662719]. \t  -2.752269057151917 \t -0.012755165666884116\n",
            "61     \t [-0.10786281  1.00686775]. \t  -100.27630475842861 \t -0.012755165666884116\n",
            "62     \t [1.97698163 1.70213882]. \t  -487.7381985083474 \t -0.012755165666884116\n",
            "63     \t [0.53734859 1.65356498]. \t  -186.48780951709446 \t -0.012755165666884116\n",
            "64     \t [1.37132842 1.92224828]. \t  -0.31182923921532274 \t -0.012755165666884116\n",
            "65     \t [0.57654663 0.33809908]. \t  -0.1825538601652444 \t -0.012755165666884116\n",
            "66     \t [1.25126118 0.0571559 ]. \t  -227.6199426633258 \t -0.012755165666884116\n",
            "67     \t [-0.6937863  -1.40106436]. \t  -357.2133150989895 \t -0.012755165666884116\n",
            "68     \t [-1.36682657  1.43870971]. \t  -24.049337170023556 \t -0.012755165666884116\n",
            "69     \t [1.28225655 1.6353635 ]. \t  -0.08744509852916786 \t -0.012755165666884116\n",
            "70     \t [1.29279332 1.69465946]. \t  -0.14022634828409308 \t -0.012755165666884116\n",
            "71     \t [ 0.30088331 -1.44707546]. \t  -236.9120555068589 \t -0.012755165666884116\n",
            "72     \t [-0.04492776  0.72938472]. \t  -53.998035904037216 \t -0.012755165666884116\n",
            "73     \t [0.61443815 1.81916878]. \t  -207.97967276656087 \t -0.012755165666884116\n",
            "74     \t [-0.63982464  0.29052785]. \t  -4.101503073935859 \t -0.012755165666884116\n",
            "75     \t [ 1.11552486 -0.4361074 ]. \t  -282.4224218613643 \t -0.012755165666884116\n",
            "76     \t [1.33431584 1.83202348]. \t  -0.37827825449139396 \t -0.012755165666884116\n",
            "77     \t [1.07946458 1.15642783]. \t  -0.014086695969303124 \t -0.012755165666884116\n",
            "78     \t [-1.23013135  0.9694735 ]. \t  -34.53985251506643 \t -0.012755165666884116\n",
            "79     \t [ 1.6256887 -0.6896703]. \t  -1110.969801925948 \t -0.012755165666884116\n",
            "80     \t [0.98147796 0.9510975 ]. \t  -0.015230670575747661 \t -0.012755165666884116\n",
            "81     \t [1.36100848 1.80696245]. \t  -0.33627647368339797 \t -0.012755165666884116\n",
            "82     \t [ 0.3824406  -0.99422429]. \t  -130.45200646480208 \t -0.012755165666884116\n",
            "83     \t [-0.82565706 -0.22225113]. \t  -85.04752133209 \t -0.012755165666884116\n",
            "84     \t [1.24169556 0.05195789]. \t  -222.02371025462764 \t -0.012755165666884116\n",
            "85     \t [1.13161725 1.30122789]. \t  -0.06004915639605651 \t -0.012755165666884116\n",
            "86     \t [0.98453716 0.98003788]. \t  \u001b[92m-0.011740524325372582\u001b[0m \t -0.011740524325372582\n",
            "87     \t [ 0.30211268 -0.94085493]. \t  -107.01566089528842 \t -0.011740524325372582\n",
            "88     \t [0.53278786 0.30454755]. \t  -0.2610726473186308 \t -0.011740524325372582\n",
            "89     \t [ 1.9476981  -1.06044231]. \t  -2357.0008076212116 \t -0.011740524325372582\n",
            "90     \t [1.17513285 0.41271901]. \t  -93.77532296883037 \t -0.011740524325372582\n",
            "91     \t [-1.98501009 -0.04967342]. \t  -1600.8712085231891 \t -0.011740524325372582\n",
            "92     \t [-0.17324745 -0.76910253]. \t  -65.23534049934628 \t -0.011740524325372582\n",
            "93     \t [0.58963501 0.34054937]. \t  -0.17346897888556995 \t -0.011740524325372582\n",
            "94     \t [-0.00638332  0.30425086]. \t  -10.267186585876454 \t -0.011740524325372582\n",
            "95     \t [-0.83907006  0.07181086]. \t  -43.35336521664036 \t -0.011740524325372582\n",
            "96     \t [-1.54050903 -1.51038405]. \t  -1514.6518971866183 \t -0.011740524325372582\n",
            "97     \t [ 0.17692078 -0.02372435]. \t  -0.9802378801340391 \t -0.011740524325372582\n",
            "98     \t [-0.63542945  1.47879097]. \t  -118.24151322059333 \t -0.011740524325372582\n",
            "99     \t [0.31591526 0.09364236]. \t  -0.47176660139426496 \t -0.011740524325372582\n",
            "100    \t [1.02881989 1.08108782]. \t  -0.05198549630782605 \t -0.011740524325372582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLa1m1HPd8Fd",
        "outputId": "1a469fc5-2132-4d37-a8c1-95e693d131e4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_winner_5 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_5 = dGPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.23491901 0.17105153]. \t  -1.9278091788796494 \t -1.9278091788796494\n",
            "init   \t [ 0.24749582 -0.48313101]. \t  -30.201785940713883 \t -1.9278091788796494\n",
            "init   \t [-1.56019024  1.41721238]. \t  -109.97965348801837 \t -1.9278091788796494\n",
            "init   \t [0.98198164 1.77093658]. \t  -65.06852657321838 \t -1.9278091788796494\n",
            "init   \t [1.47180752 0.85239655]. \t  -172.83511538815276 \t -1.9278091788796494\n",
            "1      \t [-0.22001069  0.22846888]. \t  -4.730736857945067 \t -1.9278091788796494\n",
            "2      \t [-0.0497627   0.34058552]. \t  -12.533784555540677 \t -1.9278091788796494\n",
            "3      \t [-1.48105507 -0.4847623 ]. \t  -723.4774422442588 \t -1.9278091788796494\n",
            "4      \t [0.03340674 0.00532193]. \t  \u001b[92m-0.936071503532055\u001b[0m \t -0.936071503532055\n",
            "5      \t [ 1.14140857 -0.510327  ]. \t  -328.76784953323136 \t -0.936071503532055\n",
            "6      \t [-0.90124818  0.69681294]. \t  -4.947276273380818 \t -0.936071503532055\n",
            "7      \t [ 0.18401997 -1.76174727]. \t  -323.0875723555078 \t -0.936071503532055\n",
            "8      \t [0.20932893 0.07469341]. \t  \u001b[92m-0.7204861468950398\u001b[0m \t -0.7204861468950398\n",
            "9      \t [-0.75311808  0.56041242]. \t  -3.0780122787983073 \t -0.7204861468950398\n",
            "10     \t [ 1.05958506 -1.26592864]. \t  -570.5680208492064 \t -0.7204861468950398\n",
            "11     \t [-1.29511157  1.01406091]. \t  -49.258002513003866 \t -0.7204861468950398\n",
            "12     \t [-0.7362782   0.69862617]. \t  -5.4645313392049 \t -0.7204861468950398\n",
            "13     \t [ 0.4327339  -1.35970691]. \t  -239.6320259130426 \t -0.7204861468950398\n",
            "14     \t [-0.60173605  1.09331657]. \t  -56.0353319781476 \t -0.7204861468950398\n",
            "15     \t [-0.5033492  -1.35122097]. \t  -259.7282014742476 \t -0.7204861468950398\n",
            "16     \t [1.80992487 0.13819377]. \t  -985.1308628648286 \t -0.7204861468950398\n",
            "17     \t [1.35520032 1.46174194]. \t  -14.175617138018643 \t -0.7204861468950398\n",
            "18     \t [0.85298208 0.93336493]. \t  -4.256422467787531 \t -0.7204861468950398\n",
            "19     \t [1.02072805 1.06095785]. \t  \u001b[92m-0.03680415011202419\u001b[0m \t -0.03680415011202419\n",
            "20     \t [-1.56484179  1.13832926]. \t  -178.2933814488036 \t -0.03680415011202419\n",
            "21     \t [-1.11573484  1.59768522]. \t  -16.924599308818898 \t -0.03680415011202419\n",
            "22     \t [-1.10941858  1.13646652]. \t  -5.339707976297703 \t -0.03680415011202419\n",
            "23     \t [-1.406551  2.038767]. \t  -6.1560776031807025 \t -0.03680415011202419\n",
            "24     \t [1.10451588 1.18002579]. \t  -0.17036045928748234 \t -0.03680415011202419\n",
            "25     \t [-0.81535936  0.58043406]. \t  -4.007474413946899 \t -0.03680415011202419\n",
            "26     \t [-0.78857247 -1.59982509]. \t  -496.78147157734134 \t -0.03680415011202419\n",
            "27     \t [0.35255914 0.09657011]. \t  -0.4960629799934465 \t -0.03680415011202419\n",
            "28     \t [1.10271346 1.1649134 ]. \t  -0.27129901041038773 \t -0.03680415011202419\n",
            "29     \t [ 1.47354817 -0.45096802]. \t  -687.8763878924361 \t -0.03680415011202419\n",
            "30     \t [-0.11339345 -0.71322107]. \t  -53.95873790462344 \t -0.03680415011202419\n",
            "31     \t [ 0.25685491 -1.73171831]. \t  -323.72218821205206 \t -0.03680415011202419\n",
            "32     \t [-1.94162085  1.78946102]. \t  -400.8636249015539 \t -0.03680415011202419\n",
            "33     \t [ 0.92531161 -0.60711948]. \t  -214.1364263200481 \t -0.03680415011202419\n",
            "34     \t [1.05682908 1.16355347]. \t  -0.2209989883589829 \t -0.03680415011202419\n",
            "35     \t [0.525904   0.88949121]. \t  -37.79139283682128 \t -0.03680415011202419\n",
            "36     \t [-1.40731081 -1.74373723]. \t  -1392.8070946925925 \t -0.03680415011202419\n",
            "37     \t [0.65885387 0.42154776]. \t  -0.13210749682527967 \t -0.03680415011202419\n",
            "38     \t [1.36081877 0.32800021]. \t  -232.33521879013418 \t -0.03680415011202419\n",
            "39     \t [0.83701046 0.52468944]. \t  -3.120543561137695 \t -0.03680415011202419\n",
            "40     \t [-1.28295362  1.64136206]. \t  -5.214000562010194 \t -0.03680415011202419\n",
            "41     \t [0.51728153 0.19417005]. \t  -0.7719218470078391 \t -0.03680415011202419\n",
            "42     \t [-0.95981259  1.55133123]. \t  -43.54233524581199 \t -0.03680415011202419\n",
            "43     \t [1.06676256 1.11248317]. \t  -0.06947813248497035 \t -0.03680415011202419\n",
            "44     \t [0.58863239 1.87354764]. \t  -233.360311243499 \t -0.03680415011202419\n",
            "45     \t [1.51498139 1.95273984]. \t  -11.990952738836587 \t -0.03680415011202419\n",
            "46     \t [ 1.2578951  -0.78115751]. \t  -558.6596858541424 \t -0.03680415011202419\n",
            "47     \t [-0.70498112  1.35567505]. \t  -76.63952263722884 \t -0.03680415011202419\n",
            "48     \t [ 1.59433827 -1.56934331]. \t  -1690.597343715668 \t -0.03680415011202419\n",
            "49     \t [1.51693909 0.89691712]. \t  -197.4413596981743 \t -0.03680415011202419\n",
            "50     \t [1.31429158 1.72232306]. \t  -0.10131864804788308 \t -0.03680415011202419\n",
            "51     \t [-1.42388075  0.1146593 ]. \t  -371.7468143613834 \t -0.03680415011202419\n",
            "52     \t [1.14531378 1.29555435]. \t  -0.047325417169440026 \t -0.03680415011202419\n",
            "53     \t [1.23098948 1.51874122]. \t  -0.05451630174164665 \t -0.03680415011202419\n",
            "54     \t [1.01931537 1.03345034]. \t  \u001b[92m-0.0034571917858046007\u001b[0m \t -0.0034571917858046007\n",
            "55     \t [0.97757198 0.93685042]. \t  -0.03583406251464394 \t -0.0034571917858046007\n",
            "56     \t [-1.45956907 -0.7097167 ]. \t  -812.6427464783716 \t -0.0034571917858046007\n",
            "57     \t [1.2782382  1.64148487]. \t  -0.08318030477171093 \t -0.0034571917858046007\n",
            "58     \t [ 0.98175078 -1.36234828]. \t  -541.1130116542306 \t -0.0034571917858046007\n",
            "59     \t [-0.52593876 -0.33033281]. \t  -39.16663742186751 \t -0.0034571917858046007\n",
            "60     \t [1.0504432  1.11387854]. \t  -0.013459781296602602 \t -0.0034571917858046007\n",
            "61     \t [1.35617493 0.55961046]. \t  -163.86447197354195 \t -0.0034571917858046007\n",
            "62     \t [ 1.22728454 -0.04072529]. \t  -239.35790028920837 \t -0.0034571917858046007\n",
            "63     \t [-0.16196487 -1.7443089 ]. \t  -314.83188763669534 \t -0.0034571917858046007\n",
            "64     \t [1.37328284 1.6831954 ]. \t  -4.248488633261746 \t -0.0034571917858046007\n",
            "65     \t [1.35039131 0.21889215]. \t  -257.617603280128 \t -0.0034571917858046007\n",
            "66     \t [-0.62266337  1.87911137]. \t  -225.06093923446917 \t -0.0034571917858046007\n",
            "67     \t [-0.25270945  1.38131143]. \t  -175.13656335377453 \t -0.0034571917858046007\n",
            "68     \t [0.19064963 0.18377161]. \t  -2.8284411978450037 \t -0.0034571917858046007\n",
            "69     \t [-1.33577761  0.72523685]. \t  -117.61771581899634 \t -0.0034571917858046007\n",
            "70     \t [0.42332497 1.62320704]. \t  -208.8470227456658 \t -0.0034571917858046007\n",
            "71     \t [-2.02702739 -0.26349559]. \t  -1920.894768467468 \t -0.0034571917858046007\n",
            "72     \t [0.39183501 0.50543384]. \t  -12.753166403029297 \t -0.0034571917858046007\n",
            "73     \t [0.91289768 0.81275029]. \t  -0.05015427109711077 \t -0.0034571917858046007\n",
            "74     \t [ 0.76614337 -1.31814465]. \t  -363.00302733841585 \t -0.0034571917858046007\n",
            "75     \t [1.17075796 0.97059246]. \t  -16.035698709335858 \t -0.0034571917858046007\n",
            "76     \t [0.54796312 0.52820353]. \t  -5.399999618793693 \t -0.0034571917858046007\n",
            "77     \t [0.79176731 1.25677505]. \t  -39.71818790076598 \t -0.0034571917858046007\n",
            "78     \t [-0.0121439  -0.02680703]. \t  -1.0970898146320398 \t -0.0034571917858046007\n",
            "79     \t [0.85512775 0.72113603]. \t  -0.031203980078682414 \t -0.0034571917858046007\n",
            "80     \t [-0.59155645 -0.47791717]. \t  -71.06764146316972 \t -0.0034571917858046007\n",
            "81     \t [ 1.53401241 -1.94544776]. \t  -1848.117337600054 \t -0.0034571917858046007\n",
            "82     \t [-1.26090414 -1.76207818]. \t  -1128.6735459557462 \t -0.0034571917858046007\n",
            "83     \t [-1.68651501 -1.61570474]. \t  -1996.4109351925408 \t -0.0034571917858046007\n",
            "84     \t [1.46877044 0.82530088]. \t  -177.6383431187912 \t -0.0034571917858046007\n",
            "85     \t [-1.16358746 -0.31670177]. \t  -283.784094866158 \t -0.0034571917858046007\n",
            "86     \t [1.03028438 1.04660832]. \t  -0.023051409914086236 \t -0.0034571917858046007\n",
            "87     \t [0.3867384  0.02623288]. \t  -1.897210205768112 \t -0.0034571917858046007\n",
            "88     \t [-1.48794505  1.79929581]. \t  -23.386208377647066 \t -0.0034571917858046007\n",
            "89     \t [1.24054662 1.52465116]. \t  -0.07832531223517429 \t -0.0034571917858046007\n",
            "90     \t [ 0.26647502 -0.99000205]. \t  -113.11248998845508 \t -0.0034571917858046007\n",
            "91     \t [ 1.14662124 -1.3075032 ]. \t  -687.6375860656689 \t -0.0034571917858046007\n",
            "92     \t [-1.42508924 -1.98741794]. \t  -1620.5523613714354 \t -0.0034571917858046007\n",
            "93     \t [-1.25859021  1.86885188]. \t  -13.212480352712507 \t -0.0034571917858046007\n",
            "94     \t [-1.65480693  0.49966047]. \t  -508.2371864994538 \t -0.0034571917858046007\n",
            "95     \t [-0.00691463  0.37379988]. \t  -14.9829378826118 \t -0.0034571917858046007\n",
            "96     \t [1.24404156 1.55830734]. \t  -0.07093675417944352 \t -0.0034571917858046007\n",
            "97     \t [-0.73715664 -1.36234614]. \t  -366.2045166787597 \t -0.0034571917858046007\n",
            "98     \t [ 0.72868273 -0.32781321]. \t  -73.82593673875684 \t -0.0034571917858046007\n",
            "99     \t [ 0.77014904 -0.30280835]. \t  -80.32330169562528 \t -0.0034571917858046007\n",
            "100    \t [-0.40242189  0.70870106]. \t  -31.861182802358318 \t -0.0034571917858046007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO3I_9cbuvY2",
        "outputId": "e848f949-9320-44ca-a542-df588008420d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_winner_6 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_6 = dGPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.60915518 -0.68821072]. \t  -1074.6314195925434 \t -3.0269049669752817\n",
            "init   \t [ 1.31575449 -1.87721062]. \t  -1302.169546932896 \t -3.0269049669752817\n",
            "init   \t [-1.60703824  0.38933325]. \t  -487.8262244570432 \t -3.0269049669752817\n",
            "init   \t [ 0.12213192 -0.33256477]. \t  -12.844955340992902 \t -3.0269049669752817\n",
            "init   \t [-0.67416945  0.50183959]. \t  -3.0269049669752817 \t -3.0269049669752817\n",
            "1      \t [-0.06756934  0.5044222 ]. \t  -26.125364719936726 \t -3.0269049669752817\n",
            "2      \t [-0.46957794 -0.51546013]. \t  -56.3238972552609 \t -3.0269049669752817\n",
            "3      \t [-0.27427447  0.04468444]. \t  \u001b[92m-1.7170570344409537\u001b[0m \t -1.7170570344409537\n",
            "4      \t [-0.70979472  1.4764578 ]. \t  -97.52805611837617 \t -1.7170570344409537\n",
            "5      \t [-0.46620343  0.64766462]. \t  -20.6671949456617 \t -1.7170570344409537\n",
            "6      \t [1.14170796 1.00707173]. \t  -8.806878951259257 \t -1.7170570344409537\n",
            "7      \t [0.26282348 0.02154315]. \t  \u001b[92m-0.7693680794170482\u001b[0m \t -0.7693680794170482\n",
            "8      \t [0.03161304 0.1353918 ]. \t  -2.743905335004257 \t -0.7693680794170482\n",
            "9      \t [0.7160177 0.9020446]. \t  -15.241019914013586 \t -0.7693680794170482\n",
            "10     \t [1.96124111 2.02640691]. \t  -332.1857399492993 \t -0.7693680794170482\n",
            "11     \t [0.68690642 0.54172498]. \t  \u001b[92m-0.5864127142055116\u001b[0m \t -0.5864127142055116\n",
            "12     \t [-0.57562481  0.09105933]. \t  -8.256262187490892 \t -0.5864127142055116\n",
            "13     \t [-1.2633983   2.01059122]. \t  -22.297030000740335 \t -0.5864127142055116\n",
            "14     \t [-0.53412161  0.32453067]. \t  -2.5075444155732045 \t -0.5864127142055116\n",
            "15     \t [1.65026453 1.56159772]. \t  -135.3950316719991 \t -0.5864127142055116\n",
            "16     \t [ 0.06808342 -0.05494013]. \t  -1.2233923201044103 \t -0.5864127142055116\n",
            "17     \t [-1.32276393  1.52291463]. \t  -10.538593051725298 \t -0.5864127142055116\n",
            "18     \t [0.40206003 0.32074291]. \t  -2.8885155923797963 \t -0.5864127142055116\n",
            "19     \t [ 1.48293339 -1.80569008]. \t  -1604.0607230964827 \t -0.5864127142055116\n",
            "20     \t [ 0.02023307 -1.96033525]. \t  -385.4118927620545 \t -0.5864127142055116\n",
            "21     \t [1.09049223 0.78083127]. \t  -16.682510072198944 \t -0.5864127142055116\n",
            "22     \t [-0.95134931  0.99163421]. \t  -4.557178158376554 \t -0.5864127142055116\n",
            "23     \t [-1.31227773  2.03619975]. \t  -15.214200315819033 \t -0.5864127142055116\n",
            "24     \t [ 0.80570692 -1.02979778]. \t  -281.92889771006395 \t -0.5864127142055116\n",
            "25     \t [1.41971308 1.58634581]. \t  -18.60080807169307 \t -0.5864127142055116\n",
            "26     \t [1.08502341 1.84932264]. \t  -45.171923678289374 \t -0.5864127142055116\n",
            "27     \t [1.2121726  1.47672163]. \t  \u001b[92m-0.05043301495345337\u001b[0m \t -0.05043301495345337\n",
            "28     \t [-1.53106049  2.04697568]. \t  -15.23730093299708 \t -0.05043301495345337\n",
            "29     \t [1.29342365 1.89973972]. \t  -5.229693472405031 \t -0.05043301495345337\n",
            "30     \t [1.29249346 1.65065966]. \t  -0.12507261239548606 \t -0.05043301495345337\n",
            "31     \t [1.59467551 0.79854923]. \t  -304.6609988132639 \t -0.05043301495345337\n",
            "32     \t [ 1.21994205 -0.46641591]. \t  -382.1236178357452 \t -0.05043301495345337\n",
            "33     \t [1.16813263 0.97525035]. \t  -15.182431900590537 \t -0.05043301495345337\n",
            "34     \t [0.85061931 0.72472308]. \t  \u001b[92m-0.022451450323285087\u001b[0m \t -0.022451450323285087\n",
            "35     \t [0.8763174  0.77042807]. \t  \u001b[92m-0.015920326692339105\u001b[0m \t -0.015920326692339105\n",
            "36     \t [0.80696297 0.621519  ]. \t  -0.12529561581640905 \t -0.015920326692339105\n",
            "37     \t [-1.68156932 -1.93210104]. \t  -2272.737961281629 \t -0.015920326692339105\n",
            "38     \t [-1.74521501  0.40006713]. \t  -707.513450898005 \t -0.015920326692339105\n",
            "39     \t [0.36582236 0.10767114]. \t  -0.4705889761811356 \t -0.015920326692339105\n",
            "40     \t [0.68825983 0.4507629 ]. \t  -0.14980027203826463 \t -0.015920326692339105\n",
            "41     \t [1.1697857  1.30610234]. \t  -0.4169093805073787 \t -0.015920326692339105\n",
            "42     \t [0.67097999 0.38534677]. \t  -0.5290319358220437 \t -0.015920326692339105\n",
            "43     \t [1.03157812 1.04914732]. \t  -0.02351549960770606 \t -0.015920326692339105\n",
            "44     \t [ 1.22316586 -1.80538469]. \t  -1090.0528432604024 \t -0.015920326692339105\n",
            "45     \t [0.88999024 0.82047694]. \t  -0.09272585256731537 \t -0.015920326692339105\n",
            "46     \t [-1.2323997  -1.29908191]. \t  -799.0345393152076 \t -0.015920326692339105\n",
            "47     \t [-1.97354965 -0.5127121 ]. \t  -1951.544885096209 \t -0.015920326692339105\n",
            "48     \t [0.65243029 1.06414155]. \t  -40.88599954011427 \t -0.015920326692339105\n",
            "49     \t [-1.44990784  0.03540469]. \t  -433.17987312320264 \t -0.015920326692339105\n",
            "50     \t [-0.03454204 -0.1886721 ]. \t  -4.675158818828568 \t -0.015920326692339105\n",
            "51     \t [ 0.32389493 -0.78436483]. \t  -79.5377213413769 \t -0.015920326692339105\n",
            "52     \t [1.39524932 1.49131097]. \t  -20.896020069571428 \t -0.015920326692339105\n",
            "53     \t [0.71897545 0.47363279]. \t  -0.2664023941976621 \t -0.015920326692339105\n",
            "54     \t [1.98642507 0.24927477]. \t  -1367.4654192944565 \t -0.015920326692339105\n",
            "55     \t [ 1.0966438  -1.78210956]. \t  -890.8749525146312 \t -0.015920326692339105\n",
            "56     \t [0.81645276 0.67938207]. \t  -0.05004022079805938 \t -0.015920326692339105\n",
            "57     \t [ 0.49486868 -0.24228212]. \t  -23.989313356066123 \t -0.015920326692339105\n",
            "58     \t [ 0.05426691 -2.01085777]. \t  -406.43452858749674 \t -0.015920326692339105\n",
            "59     \t [-2.04478475  1.56061005]. \t  -695.9908816884517 \t -0.015920326692339105\n",
            "60     \t [-1.35586485  1.82774524]. \t  -5.561386652997457 \t -0.015920326692339105\n",
            "61     \t [1.28394422 1.65510503]. \t  -0.08497011651164961 \t -0.015920326692339105\n",
            "62     \t [ 1.23043616 -1.71564361]. \t  -1043.095545084003 \t -0.015920326692339105\n",
            "63     \t [-1.15405526  1.40809765]. \t  -5.221423059658525 \t -0.015920326692339105\n",
            "64     \t [1.98349663 0.05164635]. \t  -1508.4352567550156 \t -0.015920326692339105\n",
            "65     \t [-1.90296653 -0.52848348]. \t  -1730.4822462565644 \t -0.015920326692339105\n",
            "66     \t [ 0.15070243 -0.35797581]. \t  -15.213567966194207 \t -0.015920326692339105\n",
            "67     \t [-0.84196325 -0.03742189]. \t  -59.09278066675081 \t -0.015920326692339105\n",
            "68     \t [0.73029918 1.53562134]. \t  -100.53015091470867 \t -0.015920326692339105\n",
            "69     \t [-0.93643438 -1.58272987]. \t  -608.7322847330377 \t -0.015920326692339105\n",
            "70     \t [1.3120196  2.00453818]. \t  -8.114337895616032 \t -0.015920326692339105\n",
            "71     \t [-0.57799437  0.67734756]. \t  -14.273499949981206 \t -0.015920326692339105\n",
            "72     \t [-1.64153317 -0.05783136]. \t  -764.5826832498624 \t -0.015920326692339105\n",
            "73     \t [1.2761842  1.63256769]. \t  -0.07781558502933011 \t -0.015920326692339105\n",
            "74     \t [-0.47475672 -0.9289252 ]. \t  -135.4201751947115 \t -0.015920326692339105\n",
            "75     \t [-0.67246471 -0.39538125]. \t  -74.63802490155976 \t -0.015920326692339105\n",
            "76     \t [-0.54687729 -1.66262826]. \t  -387.2207058781848 \t -0.015920326692339105\n",
            "77     \t [0.18670875 0.77461356]. \t  -55.38495183637282 \t -0.015920326692339105\n",
            "78     \t [0.57932525 0.30548482]. \t  -0.2677665466265738 \t -0.015920326692339105\n",
            "79     \t [1.82501194 1.48588699]. \t  -341.0025557765546 \t -0.015920326692339105\n",
            "80     \t [ 0.05802756 -1.76728631]. \t  -314.4086978048547 \t -0.015920326692339105\n",
            "81     \t [-1.38037896 -1.92281164]. \t  -1471.2219228399538 \t -0.015920326692339105\n",
            "82     \t [ 0.94766669 -1.82111946]. \t  -739.4030453416033 \t -0.015920326692339105\n",
            "83     \t [0.60642899 0.36686834]. \t  -0.154976955738953 \t -0.015920326692339105\n",
            "84     \t [0.68289888 0.45603552]. \t  -0.11119377830962912 \t -0.015920326692339105\n",
            "85     \t [0.63536852 0.42462548]. \t  -0.17677232428280445 \t -0.015920326692339105\n",
            "86     \t [1.82389133 1.28038113]. \t  -419.37161019000627 \t -0.015920326692339105\n",
            "87     \t [ 0.88625922 -0.3468704 ]. \t  -128.2291097907271 \t -0.015920326692339105\n",
            "88     \t [0.85864023 0.04165531]. \t  -48.406994899042076 \t -0.015920326692339105\n",
            "89     \t [0.46181679 0.9575149 ]. \t  -55.678982222462196 \t -0.015920326692339105\n",
            "90     \t [-0.7935673   1.42080154]. \t  -65.79328558574495 \t -0.015920326692339105\n",
            "91     \t [ 2.04234106 -0.12966452]. \t  -1850.7930646007785 \t -0.015920326692339105\n",
            "92     \t [-1.74562883 -0.36097039]. \t  -1169.1146703632246 \t -0.015920326692339105\n",
            "93     \t [-0.04485983 -0.57316159]. \t  -34.17424461595006 \t -0.015920326692339105\n",
            "94     \t [ 2.02538509 -0.76140625]. \t  -2366.5031536246265 \t -0.015920326692339105\n",
            "95     \t [-1.13633512  0.98973497]. \t  -13.655511713834432 \t -0.015920326692339105\n",
            "96     \t [-1.95689662 -1.63522269]. \t  -2995.0018570387247 \t -0.015920326692339105\n",
            "97     \t [0.53297873 0.2985904 ]. \t  -0.23920372920358662 \t -0.015920326692339105\n",
            "98     \t [-2.01222674  0.53070006]. \t  -1246.9566845983406 \t -0.015920326692339105\n",
            "99     \t [-1.57699106  0.31006092]. \t  -480.5040737267052 \t -0.015920326692339105\n",
            "100    \t [-0.90499114  1.91485776]. \t  -123.71745106321148 \t -0.015920326692339105\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rQbLZD8uvbI",
        "outputId": "ae1e3a77-59eb-41fe-f444-43d960a7fe32"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_winner_7 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_7 = dGPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.42268934 -0.80954733]. \t  -808.7939502616841 \t -2.0077595729598063\n",
            "init   \t [-1.79389885 -0.16441204]. \t  -1151.9264213711547 \t -2.0077595729598063\n",
            "init   \t [1.37319786 1.74897991]. \t  -2.0077595729598063 \t -2.0077595729598063\n",
            "init   \t [0.92974688 1.09976052]. \t  -5.543015908052858 \t -2.0077595729598063\n",
            "init   \t [-0.94533605  0.58994398]. \t  -13.008689168416776 \t -2.0077595729598063\n",
            "1      \t [-0.37627455  1.06262548]. \t  -86.7261431628382 \t -2.0077595729598063\n",
            "2      \t [ 0.03487276 -0.06746438]. \t  \u001b[92m-1.4031714851152948\u001b[0m \t -1.4031714851152948\n",
            "3      \t [1.7096368  0.62539905]. \t  -528.3353333844661 \t -1.4031714851152948\n",
            "4      \t [1.00339051 1.66581221]. \t  -43.43070787489704 \t -1.4031714851152948\n",
            "5      \t [-0.28032686  0.40111693]. \t  -12.042040934665737 \t -1.4031714851152948\n",
            "6      \t [-1.54656294  1.52869334]. \t  -80.9901226712704 \t -1.4031714851152948\n",
            "7      \t [ 0.1042354  -1.46368615]. \t  -218.2325091875684 \t -1.4031714851152948\n",
            "8      \t [-0.95817415  0.95084608]. \t  -3.9416916290172903 \t -1.4031714851152948\n",
            "9      \t [1.92585728 1.93941244]. \t  -313.9751288778981 \t -1.4031714851152948\n",
            "10     \t [-0.7289408   0.54875501]. \t  -3.019513411687114 \t -1.4031714851152948\n",
            "11     \t [1.19809967 1.57889091]. \t  -2.0969788156427867 \t -1.4031714851152948\n",
            "12     \t [0.46213398 0.26553063]. \t  \u001b[92m-0.5593132411126935\u001b[0m \t -0.5593132411126935\n",
            "13     \t [1.31752137 1.87878269]. \t  -2.1434364336099545 \t -0.5593132411126935\n",
            "14     \t [0.37108277 0.52137574]. \t  -15.116058880763775 \t -0.5593132411126935\n",
            "15     \t [-0.26825319  0.06307885]. \t  -1.6163532359681827 \t -0.5593132411126935\n",
            "16     \t [0.71528941 0.59139852]. \t  -0.7172191667192169 \t -0.5593132411126935\n",
            "17     \t [-1.82254402  1.35506658]. \t  -394.71835903777065 \t -0.5593132411126935\n",
            "18     \t [-0.87005758  1.959549  ]. \t  -148.10947868722712 \t -0.5593132411126935\n",
            "19     \t [-1.57880905  2.01599139]. \t  -29.369457071796123 \t -0.5593132411126935\n",
            "20     \t [-1.21396727  1.42979585]. \t  -5.0945538095494864 \t -0.5593132411126935\n",
            "21     \t [0.36952465 0.09014023]. \t  -0.6128716060279112 \t -0.5593132411126935\n",
            "22     \t [-0.82332945  0.69260763]. \t  -3.3462459779542706 \t -0.5593132411126935\n",
            "23     \t [-0.16242764  0.10868984]. \t  -2.0286839774770082 \t -0.5593132411126935\n",
            "24     \t [-1.97561109 -1.33580864]. \t  -2753.4069170851544 \t -0.5593132411126935\n",
            "25     \t [1.28512457 1.67638423]. \t  \u001b[92m-0.14299389640982096\u001b[0m \t -0.14299389640982096\n",
            "26     \t [-0.38732012 -0.42046345]. \t  -34.46943774123645 \t -0.14299389640982096\n",
            "27     \t [-1.50631286 -0.4668566 ]. \t  -754.760939408136 \t -0.14299389640982096\n",
            "28     \t [0.23958518 0.02401745]. \t  -0.6896772001263887 \t -0.14299389640982096\n",
            "29     \t [1.55375794 0.04422647]. \t  -561.9669176444024 \t -0.14299389640982096\n",
            "30     \t [1.3276727  1.75830368]. \t  \u001b[92m-0.10931519096714074\u001b[0m \t -0.10931519096714074\n",
            "31     \t [ 1.84142169 -0.68915833]. \t  -1665.3415997891302 \t -0.10931519096714074\n",
            "32     \t [0.70587209 0.46014561]. \t  -0.23174687792878337 \t -0.10931519096714074\n",
            "33     \t [1.29468303 1.72357606]. \t  -0.31124794977871817 \t -0.10931519096714074\n",
            "34     \t [0.284278   0.11054426]. \t  -0.6006469070125858 \t -0.10931519096714074\n",
            "35     \t [-1.15538548 -0.32404392]. \t  -279.8603611900302 \t -0.10931519096714074\n",
            "36     \t [-0.43360139  1.78682773]. \t  -257.67697327928425 \t -0.10931519096714074\n",
            "37     \t [0.61813169 0.35402239]. \t  -0.22458442920762428 \t -0.10931519096714074\n",
            "38     \t [0.63363781 0.41420535]. \t  -0.15037179969733092 \t -0.10931519096714074\n",
            "39     \t [-1.20940169 -1.15883084]. \t  -692.0989167941194 \t -0.10931519096714074\n",
            "40     \t [-1.75043739 -0.96288583]. \t  -1629.1708668895085 \t -0.10931519096714074\n",
            "41     \t [0.62464319 0.36056858]. \t  -0.2285711090652039 \t -0.10931519096714074\n",
            "42     \t [-1.57895555  1.65991668]. \t  -76.07055806729942 \t -0.10931519096714074\n",
            "43     \t [ 1.22473655 -0.02664716]. \t  -233.10943775648116 \t -0.10931519096714074\n",
            "44     \t [ 0.35130358 -0.51042541]. \t  -40.596073343757254 \t -0.10931519096714074\n",
            "45     \t [1.31169037 1.71205742]. \t  \u001b[92m-0.10433210379994093\u001b[0m \t -0.10433210379994093\n",
            "46     \t [-1.59405765 -0.72663747]. \t  -1074.487542263908 \t -0.10433210379994093\n",
            "47     \t [0.58829335 0.33251456]. \t  -0.18792907961849245 \t -0.10433210379994093\n",
            "48     \t [ 1.17786294 -1.63653741]. \t  -914.4278621791691 \t -0.10433210379994093\n",
            "49     \t [-1.73635555  0.31082404]. \t  -738.7068672809747 \t -0.10433210379994093\n",
            "50     \t [-1.59047368  0.71679317]. \t  -335.3397783815587 \t -0.10433210379994093\n",
            "51     \t [0.52905304 0.18515364]. \t  -1.119423726815701 \t -0.10433210379994093\n",
            "52     \t [-0.74435884  0.34039411]. \t  -7.6085296637269195 \t -0.10433210379994093\n",
            "53     \t [ 1.2372061  -0.68713198]. \t  -491.92478572930867 \t -0.10433210379994093\n",
            "54     \t [ 1.40208804 -1.51978808]. \t  -1215.1295635480267 \t -0.10433210379994093\n",
            "55     \t [1.12644203 1.83016155]. \t  -31.520623750753167 \t -0.10433210379994093\n",
            "56     \t [-1.9442035  -0.50696483]. \t  -1846.4127120946107 \t -0.10433210379994093\n",
            "57     \t [-1.24954666  1.78525415]. \t  -10.073012518900768 \t -0.10433210379994093\n",
            "58     \t [-1.27943158 -1.39720914]. \t  -925.8050486834616 \t -0.10433210379994093\n",
            "59     \t [-1.14405725 -0.52535609]. \t  -341.0344123154294 \t -0.10433210379994093\n",
            "60     \t [-2.03755605 -0.5649929 ]. \t  -2233.8842901072085 \t -0.10433210379994093\n",
            "61     \t [-1.37789349 -0.22158635]. \t  -455.1693559042023 \t -0.10433210379994093\n",
            "62     \t [0.85341712 0.71702901]. \t  \u001b[92m-0.034236963606674095\u001b[0m \t -0.034236963606674095\n",
            "63     \t [ 1.59228325 -1.13145804]. \t  -1344.9106239100822 \t -0.034236963606674095\n",
            "64     \t [-0.53403902 -0.71698408]. \t  -102.79010100693604 \t -0.034236963606674095\n",
            "65     \t [0.71131079 0.51730102]. \t  -0.09619642912615073 \t -0.034236963606674095\n",
            "66     \t [-1.98057804  0.15332739]. \t  -1429.6928273167414 \t -0.034236963606674095\n",
            "67     \t [0.98843431 1.12276857]. \t  -2.124911635520699 \t -0.034236963606674095\n",
            "68     \t [ 0.37605334 -0.813153  ]. \t  -91.50952887424619 \t -0.034236963606674095\n",
            "69     \t [-0.99219442  1.58641447]. \t  -40.204989906806524 \t -0.034236963606674095\n",
            "70     \t [-0.86162263 -0.11474692]. \t  -76.93461829347547 \t -0.034236963606674095\n",
            "71     \t [0.83359929 0.73895517]. \t  -0.2218826829362415 \t -0.034236963606674095\n",
            "72     \t [-0.75778259 -0.25072129]. \t  -71.14499647851899 \t -0.034236963606674095\n",
            "73     \t [0.79066819 0.65555778]. \t  -0.13624550572954233 \t -0.034236963606674095\n",
            "74     \t [0.76422544 0.59494797]. \t  -0.06748687884423658 \t -0.034236963606674095\n",
            "75     \t [1.65163192 0.53075901]. \t  -483.1621969903884 \t -0.034236963606674095\n",
            "76     \t [-1.57455518 -0.76378093]. \t  -1058.3364396475633 \t -0.034236963606674095\n",
            "77     \t [-1.26065506  0.69556431]. \t  -84.97818350638204 \t -0.034236963606674095\n",
            "78     \t [0.75157005 0.54179966]. \t  -0.11488399313826303 \t -0.034236963606674095\n",
            "79     \t [-1.02331335 -1.98831652]. \t  -925.511763560693 \t -0.034236963606674095\n",
            "80     \t [-1.76585423  0.51846661]. \t  -683.5327188082996 \t -0.034236963606674095\n",
            "81     \t [ 0.52098817 -0.27048761]. \t  -29.596777899655326 \t -0.034236963606674095\n",
            "82     \t [-0.34050634 -1.25170618]. \t  -188.8438156755429 \t -0.034236963606674095\n",
            "83     \t [-0.06717539 -0.55957888]. \t  -32.958775452655466 \t -0.034236963606674095\n",
            "84     \t [-1.14547294  0.88767978]. \t  -22.617006778073353 \t -0.034236963606674095\n",
            "85     \t [-1.0565167   0.00792408]. \t  -127.06291560199584 \t -0.034236963606674095\n",
            "86     \t [-0.50462327  1.43140067]. \t  -140.7393657780711 \t -0.034236963606674095\n",
            "87     \t [-1.93157144 -1.29004162]. \t  -2529.648099431939 \t -0.034236963606674095\n",
            "88     \t [-0.56364059  1.63132091]. \t  -175.00740333734777 \t -0.034236963606674095\n",
            "89     \t [1.30276769 1.72131991]. \t  -0.14982766937968078 \t -0.034236963606674095\n",
            "90     \t [-0.80306651 -1.303614  ]. \t  -382.92789483486376 \t -0.034236963606674095\n",
            "91     \t [ 1.98490407 -1.38419481]. \t  -2835.5091279274216 \t -0.034236963606674095\n",
            "92     \t [ 0.24002072 -1.48468418]. \t  -238.44468467100532 \t -0.034236963606674095\n",
            "93     \t [1.30623684 1.71433512]. \t  -0.1003103558526618 \t -0.034236963606674095\n",
            "94     \t [0.71401783 0.47146019]. \t  -0.22894446618932762 \t -0.034236963606674095\n",
            "95     \t [1.38696035 1.33125903]. \t  -35.24351314917995 \t -0.034236963606674095\n",
            "96     \t [-0.37698796  1.69647981]. \t  -243.49956308551828 \t -0.034236963606674095\n",
            "97     \t [-0.33165955  0.04899537]. \t  -2.1454500004474717 \t -0.034236963606674095\n",
            "98     \t [-0.33682144 -0.00609563]. \t  -3.2161758712036335 \t -0.034236963606674095\n",
            "99     \t [0.03997086 1.39923583]. \t  -196.26089887888634 \t -0.034236963606674095\n",
            "100    \t [ 0.85383476 -0.95814509]. \t  -284.67862138554796 \t -0.034236963606674095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YRio_skuvd2",
        "outputId": "84d9df1c-dc61-4e8c-9de8-c3fbfd087114"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_winner_8 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_8 = dGPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.59948851 -1.8750873 ]. \t  -1972.3059423071145 \t -7.238275380208551\n",
            "init   \t [ 0.40834083 -0.81972959]. \t  -97.66272647293047 \t -7.238275380208551\n",
            "init   \t [ 1.21492186 -0.54806067]. \t  -409.7425700197761 \t -7.238275380208551\n",
            "init   \t [ 1.55897178 -0.82804067]. \t  -1062.051446884513 \t -7.238275380208551\n",
            "init   \t [-0.69999303  0.28146449]. \t  -7.238275380208551 \t -7.238275380208551\n",
            "1      \t [0.61358515 0.32770247]. \t  \u001b[92m-0.38730686645997536\u001b[0m \t -0.38730686645997536\n",
            "2      \t [ 0.44929913 -0.14971827]. \t  -12.664682151883305 \t -0.38730686645997536\n",
            "3      \t [-1.39201605  0.8797452 ]. \t  -117.65041497657442 \t -0.38730686645997536\n",
            "4      \t [1.41566408 0.84930114]. \t  -133.52992403681057 \t -0.38730686645997536\n",
            "5      \t [-0.69235604  0.64255972]. \t  -5.527585581303217 \t -0.38730686645997536\n",
            "6      \t [0.75907281 0.16844054]. \t  -16.68413317414213 \t -0.38730686645997536\n",
            "7      \t [0.35649834 1.43339895]. \t  -171.05812419025793 \t -0.38730686645997536\n",
            "8      \t [-0.01558989  0.50781742]. \t  -26.79459700088855 \t -0.38730686645997536\n",
            "9      \t [-0.89658541  0.44711936]. \t  -16.32381007370692 \t -0.38730686645997536\n",
            "10     \t [0.71945107 0.70960929]. \t  -3.765086437214085 \t -0.38730686645997536\n",
            "11     \t [-1.45841171  2.02404169]. \t  -7.103102879159884 \t -0.38730686645997536\n",
            "12     \t [0.5643111  0.40904053]. \t  -1.0105432891995718 \t -0.38730686645997536\n",
            "13     \t [-0.56718583  0.41803615]. \t  -3.3841414778098042 \t -0.38730686645997536\n",
            "14     \t [-1.01385586  1.62704583]. \t  -39.952744159584476 \t -0.38730686645997536\n",
            "15     \t [-1.80825474  0.3658329 ]. \t  -851.1801983666131 \t -0.38730686645997536\n",
            "16     \t [-1.3357996   1.52482109]. \t  -12.192033971666145 \t -0.38730686645997536\n",
            "17     \t [2.02060323 1.4606836 ]. \t  -688.6106859946037 \t -0.38730686645997536\n",
            "18     \t [0.67703737 0.52165016]. \t  -0.5046212177954222 \t -0.38730686645997536\n",
            "19     \t [-1.07582552  1.90269457]. \t  -59.855369374280805 \t -0.38730686645997536\n",
            "20     \t [-1.02909572  0.99046119]. \t  -4.587507344236973 \t -0.38730686645997536\n",
            "21     \t [1.03799728 1.22229839]. \t  -2.0998868809733406 \t -0.38730686645997536\n",
            "22     \t [-0.3584911  -0.79284176]. \t  -86.7354853518259 \t -0.38730686645997536\n",
            "23     \t [-1.67153479  1.92394855]. \t  -82.84102054681857 \t -0.38730686645997536\n",
            "24     \t [-1.90600432  0.6011805 ]. \t  -927.5483607203025 \t -0.38730686645997536\n",
            "25     \t [-0.26935638 -0.24773242]. \t  -11.86953142243642 \t -0.38730686645997536\n",
            "26     \t [1.00759156 1.0147943 ]. \t  \u001b[92m-7.756460297975939e-05\u001b[0m \t -7.756460297975939e-05\n",
            "27     \t [-2.02223264 -0.51849968]. \t  -2132.4307428489506 \t -7.756460297975939e-05\n",
            "28     \t [1.92951462 1.09425467]. \t  -691.9082143721105 \t -7.756460297975939e-05\n",
            "29     \t [1.1114454  1.94770308]. \t  -50.762684150808944 \t -7.756460297975939e-05\n",
            "30     \t [1.26408211 1.66227   ]. \t  -0.4840431001946601 \t -7.756460297975939e-05\n",
            "31     \t [ 1.81540232 -0.88185794]. \t  -1745.8518633501558 \t -7.756460297975939e-05\n",
            "32     \t [0.69790708 0.5133581 ]. \t  -0.16034392327694275 \t -7.756460297975939e-05\n",
            "33     \t [0.99284275 1.0056339 ]. \t  -0.03964099558735307 \t -7.756460297975939e-05\n",
            "34     \t [ 0.42324543 -2.01100952]. \t  -480.00669093860853 \t -7.756460297975939e-05\n",
            "35     \t [ 0.190941   -0.04878897]. \t  -1.3812889957609409 \t -7.756460297975939e-05\n",
            "36     \t [0.71994864 0.48905238]. \t  -0.1641235227410736 \t -7.756460297975939e-05\n",
            "37     \t [0.78235227 0.57600637]. \t  -0.17746569379737367 \t -7.756460297975939e-05\n",
            "38     \t [1.28676527 0.96379415]. \t  -47.96458146455062 \t -7.756460297975939e-05\n",
            "39     \t [0.84739661 0.66496656]. \t  -0.30540227712623935 \t -7.756460297975939e-05\n",
            "40     \t [0.98682247 0.88198503]. \t  -0.8435139811338765 \t -7.756460297975939e-05\n",
            "41     \t [1.13497217 1.3479745 ]. \t  -0.37597313041685787 \t -7.756460297975939e-05\n",
            "42     \t [ 2.01538058 -0.33779095]. \t  -1936.6348722404737 \t -7.756460297975939e-05\n",
            "43     \t [-0.13928095 -0.00812953]. \t  -1.3737441002928956 \t -7.756460297975939e-05\n",
            "44     \t [0.76894288 0.25341776]. \t  -11.468014348629087 \t -7.756460297975939e-05\n",
            "45     \t [0.53270337 0.28957486]. \t  -0.22173243181297383 \t -7.756460297975939e-05\n",
            "46     \t [1.14426583 1.3305682 ]. \t  -0.06585805921383167 \t -7.756460297975939e-05\n",
            "47     \t [ 1.04555863 -0.33677927]. \t  -204.48410237553725 \t -7.756460297975939e-05\n",
            "48     \t [1.00290499 1.02540005]. \t  -0.03835248962786031 \t -7.756460297975939e-05\n",
            "49     \t [1.05743598 1.10373731]. \t  -0.024131610439733883 \t -7.756460297975939e-05\n",
            "50     \t [-1.17614757  1.22149043]. \t  -7.354599934031947 \t -7.756460297975939e-05\n",
            "51     \t [-0.27721953  1.68451558]. \t  -260.0899357890036 \t -7.756460297975939e-05\n",
            "52     \t [-0.68574931 -1.57082399]. \t  -419.44092006851207 \t -7.756460297975939e-05\n",
            "53     \t [-0.61627617 -1.56073142]. \t  -379.17713985251373 \t -7.756460297975939e-05\n",
            "54     \t [ 0.21553377 -1.99468377]. \t  -417.2400550622069 \t -7.756460297975939e-05\n",
            "55     \t [1.91542278 1.13366555]. \t  -643.5511920107601 \t -7.756460297975939e-05\n",
            "56     \t [1.07033615 1.1296849 ]. \t  -0.03033826753624711 \t -7.756460297975939e-05\n",
            "57     \t [1.2844243  1.65102556]. \t  -0.0810609651575504 \t -7.756460297975939e-05\n",
            "58     \t [1.29958651 1.62648537]. \t  -0.4796239177587806 \t -7.756460297975939e-05\n",
            "59     \t [-1.8061815  -0.50959029]. \t  -1430.583972383175 \t -7.756460297975939e-05\n",
            "60     \t [1.21494496 1.43701005]. \t  -0.19893536555795016 \t -7.756460297975939e-05\n",
            "61     \t [ 0.82569906 -0.12695731]. \t  -65.43581387594298 \t -7.756460297975939e-05\n",
            "62     \t [ 0.64167029 -0.50889445]. \t  -84.88531997530704 \t -7.756460297975939e-05\n",
            "63     \t [1.01977412 1.05765385]. \t  -0.03177169933083389 \t -7.756460297975939e-05\n",
            "64     \t [0.21991589 1.36604332]. \t  -174.23667298663855 \t -7.756460297975939e-05\n",
            "65     \t [1.09010628 1.15955649]. \t  -0.09092041293698495 \t -7.756460297975939e-05\n",
            "66     \t [0.93405663 0.88546978]. \t  -0.02126932659159693 \t -7.756460297975939e-05\n",
            "67     \t [-0.34263104 -1.21563687]. \t  -179.50032890794742 \t -7.756460297975939e-05\n",
            "68     \t [1.25184265 1.54759236]. \t  -0.10151859433259561 \t -7.756460297975939e-05\n",
            "69     \t [ 1.55417844 -1.95204842]. \t  -1907.8293870643543 \t -7.756460297975939e-05\n",
            "70     \t [ 0.45966197 -0.08303173]. \t  -8.954441874760606 \t -7.756460297975939e-05\n",
            "71     \t [1.23879348 1.52835138]. \t  -0.06093845547303213 \t -7.756460297975939e-05\n",
            "72     \t [1.24469319 1.5425329 ]. \t  -0.0644016599843763 \t -7.756460297975939e-05\n",
            "73     \t [1.23365977 1.46773491]. \t  -0.3481605292878141 \t -7.756460297975939e-05\n",
            "74     \t [0.82828724 0.69188992]. \t  -0.0328843578777028 \t -7.756460297975939e-05\n",
            "75     \t [-1.44041432  1.4358205 ]. \t  -46.7842596086021 \t -7.756460297975939e-05\n",
            "76     \t [-1.33185859  0.89627932]. \t  -82.45012243198344 \t -7.756460297975939e-05\n",
            "77     \t [1.69438806 1.87624578]. \t  -99.42600253272379 \t -7.756460297975939e-05\n",
            "78     \t [1.39078969 0.55250285]. \t  -191.08793337017656 \t -7.756460297975939e-05\n",
            "79     \t [-0.51996186  0.82887034]. \t  -33.50362549718966 \t -7.756460297975939e-05\n",
            "80     \t [1.42107585 1.9476602 ]. \t  -0.6927766847983966 \t -7.756460297975939e-05\n",
            "81     \t [-1.238806    0.24087278]. \t  -172.39569306997012 \t -7.756460297975939e-05\n",
            "82     \t [ 1.77431915 -0.13828966]. \t  -1080.7065423179451 \t -7.756460297975939e-05\n",
            "83     \t [1.12188108 1.19800288]. \t  -0.38226392461321157 \t -7.756460297975939e-05\n",
            "84     \t [1.48877818 0.71148583]. \t  -226.73376870939367 \t -7.756460297975939e-05\n",
            "85     \t [-1.37601372  0.86599628]. \t  -111.20410801626119 \t -7.756460297975939e-05\n",
            "86     \t [1.08717925 1.18457261]. \t  -0.008283461506304414 \t -7.756460297975939e-05\n",
            "87     \t [1.76035076 1.83989797]. \t  -159.07032400881388 \t -7.756460297975939e-05\n",
            "88     \t [0.80900436 0.69177448]. \t  -0.17550713681364954 \t -7.756460297975939e-05\n",
            "89     \t [-0.1206613  -0.72036862]. \t  -55.26776406103588 \t -7.756460297975939e-05\n",
            "90     \t [-1.71028388  0.04698636]. \t  -835.6827274435514 \t -7.756460297975939e-05\n",
            "91     \t [0.33503966 1.87471175]. \t  -311.0687599752305 \t -7.756460297975939e-05\n",
            "92     \t [0.72281174 0.5480056 ]. \t  -0.14210734060667937 \t -7.756460297975939e-05\n",
            "93     \t [-0.11151438 -0.26141094]. \t  -8.734648863820025 \t -7.756460297975939e-05\n",
            "94     \t [0.93555506 0.47855305]. \t  -15.742052906702769 \t -7.756460297975939e-05\n",
            "95     \t [0.6228158  0.32330682]. \t  -0.5594896262848823 \t -7.756460297975939e-05\n",
            "96     \t [-1.58165864  1.13644603]. \t  -193.0415290939626 \t -7.756460297975939e-05\n",
            "97     \t [ 0.73390107 -1.9012    ]. \t  -595.3384730621333 \t -7.756460297975939e-05\n",
            "98     \t [ 0.54241415 -0.67884865]. \t  -94.89430414020869 \t -7.756460297975939e-05\n",
            "99     \t [ 0.87049883 -0.36050997]. \t  -125.07138009111134 \t -7.756460297975939e-05\n",
            "100    \t [0.72641085 0.51056939]. \t  -0.1041034302496647 \t -7.756460297975939e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejw6v-Ihuvf_",
        "outputId": "5c5769f3-b9fb-4135-db7e-673a3a642657"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_winner_9 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_9 = dGPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.24284125 0.11273132]. \t  -205.0993354093405 \t -3.486729021084656\n",
            "init   \t [-1.56011944  0.5721352 ]. \t  -353.19808767458017 \t -3.486729021084656\n",
            "init   \t [-1.67557012 -0.68720361]. \t  -1228.4786390381805 \t -3.486729021084656\n",
            "init   \t [-0.29744764  0.22276429]. \t  -3.486729021084656 \t -3.486729021084656\n",
            "init   \t [0.52480624 0.8085215 ]. \t  -28.645360944397154 \t -3.486729021084656\n",
            "1      \t [-0.42042294  1.10485081]. \t  -88.1537008084973 \t -3.486729021084656\n",
            "2      \t [0.09141118 0.27724351]. \t  -8.055583032200332 \t -3.486729021084656\n",
            "3      \t [1.34027586 1.76760892]. \t  \u001b[92m-0.19833152756144756\u001b[0m \t -0.19833152756144756\n",
            "4      \t [-0.18773603  0.42557002]. \t  -16.646093525515155 \t -0.19833152756144756\n",
            "5      \t [0.88242309 1.50447934]. \t  -52.69367137402105 \t -0.19833152756144756\n",
            "6      \t [2.048 2.048]. \t  -461.7603900415999 \t -0.19833152756144756\n",
            "7      \t [1.27982732 1.09230691]. \t  -29.851811719281347 \t -0.19833152756144756\n",
            "8      \t [-1.44807573  1.90157698]. \t  -9.809094279930342 \t -0.19833152756144756\n",
            "9      \t [1.25353455 2.01222292]. \t  -19.501272900917254 \t -0.19833152756144756\n",
            "10     \t [-0.08309162 -0.58835563]. \t  -36.606516095015934 \t -0.19833152756144756\n",
            "11     \t [1.1706029  1.45791562]. \t  -0.7965597682308627 \t -0.19833152756144756\n",
            "12     \t [-0.28673025  0.01206275]. \t  -2.147797915596625 \t -0.19833152756144756\n",
            "13     \t [-1.05613481  1.95822733]. \t  -75.25998743816882 \t -0.19833152756144756\n",
            "14     \t [-2.02019481  1.97518937]. \t  -452.6442141264078 \t -0.19833152756144756\n",
            "15     \t [-1.10078946  1.10156659]. \t  -5.627078020735778 \t -0.19833152756144756\n",
            "16     \t [-1.23064667  1.36221466]. \t  -7.294599583177861 \t -0.19833152756144756\n",
            "17     \t [1.26353523 1.63114134]. \t  \u001b[92m-0.18930565683810374\u001b[0m \t -0.18930565683810374\n",
            "18     \t [-0.09043234 -0.14656179]. \t  -3.5834832225588453 \t -0.18930565683810374\n",
            "19     \t [-0.02653405 -1.8935541 ]. \t  -359.8751683618359 \t -0.18930565683810374\n",
            "20     \t [ 1.15001054 -0.44460152]. \t  -312.29585033327953 \t -0.18930565683810374\n",
            "21     \t [-0.11820137  0.00938438]. \t  -1.252478536491523 \t -0.18930565683810374\n",
            "22     \t [1.05813244 0.2848522 ]. \t  -69.69115789460176 \t -0.18930565683810374\n",
            "23     \t [0.92047418 0.75594267]. \t  -0.8404422108627415 \t -0.18930565683810374\n",
            "24     \t [0.54315476 0.38594919]. \t  -1.0355720404930138 \t -0.18930565683810374\n",
            "25     \t [0.55445804 0.31745805]. \t  -0.20857641342136113 \t -0.18930565683810374\n",
            "26     \t [1.28248897 1.73353593]. \t  -0.8675976483108774 \t -0.18930565683810374\n",
            "27     \t [-1.30688062  1.78197909]. \t  -5.869921939157478 \t -0.18930565683810374\n",
            "28     \t [0.68682063 0.42164498]. \t  -0.3488578571294868 \t -0.18930565683810374\n",
            "29     \t [-1.8643726   0.33477749]. \t  -994.8603984179497 \t -0.18930565683810374\n",
            "30     \t [0.18541668 2.02241846]. \t  -395.8934976899818 \t -0.18930565683810374\n",
            "31     \t [ 0.25671492 -0.77944939]. \t  -72.01446283418205 \t -0.18930565683810374\n",
            "32     \t [0.75375726 1.95812448]. \t  -193.26353764808724 \t -0.18930565683810374\n",
            "33     \t [-1.33865525  1.99377279]. \t  -9.540619700456945 \t -0.18930565683810374\n",
            "34     \t [-0.83790152  0.54076328]. \t  -5.980156786950536 \t -0.18930565683810374\n",
            "35     \t [ 1.97804235 -1.42167784]. \t  -2846.463564514072 \t -0.18930565683810374\n",
            "36     \t [0.56146413 1.02155538]. \t  -50.080178024431504 \t -0.18930565683810374\n",
            "37     \t [-0.08990894 -0.27126149]. \t  -8.991270284363749 \t -0.18930565683810374\n",
            "38     \t [0.56864991 0.28227656]. \t  -0.35487011841052396 \t -0.18930565683810374\n",
            "39     \t [1.26642229 1.5848537 ]. \t  \u001b[92m-0.10697346879458314\u001b[0m \t -0.10697346879458314\n",
            "40     \t [-0.33311469  0.41650083]. \t  -11.11238496663284 \t -0.10697346879458314\n",
            "41     \t [1.26354807 1.59274078]. \t  \u001b[92m-0.07091144896219523\u001b[0m \t -0.07091144896219523\n",
            "42     \t [-0.0081328  -1.64996496]. \t  -273.2765953041409 \t -0.07091144896219523\n",
            "43     \t [0.24031165 0.7048205 ]. \t  -42.4471901057517 \t -0.07091144896219523\n",
            "44     \t [ 1.61631112 -1.48418493]. \t  -1678.6311613573498 \t -0.07091144896219523\n",
            "45     \t [0.80186834 0.66991416]. \t  -0.11173191969560997 \t -0.07091144896219523\n",
            "46     \t [0.83432269 0.70029004]. \t  \u001b[92m-0.029209353426092856\u001b[0m \t -0.029209353426092856\n",
            "47     \t [-0.84626561  0.11723745]. \t  -39.28017645872388 \t -0.029209353426092856\n",
            "48     \t [0.85218835 0.78618476]. \t  -0.38136572961918597 \t -0.029209353426092856\n",
            "49     \t [1.29295007 1.69839462]. \t  -0.15697385171704595 \t -0.029209353426092856\n",
            "50     \t [0.46107921 0.14660056]. \t  -0.7259494902267962 \t -0.029209353426092856\n",
            "51     \t [ 0.62494351 -1.00468031]. \t  -194.8086538956211 \t -0.029209353426092856\n",
            "52     \t [1.91380379 0.04697279]. \t  -1308.1435369146268 \t -0.029209353426092856\n",
            "53     \t [1.42305022 0.68026062]. \t  -181.03071718711308 \t -0.029209353426092856\n",
            "54     \t [-0.48066461 -1.48866884]. \t  -297.9316893556971 \t -0.029209353426092856\n",
            "55     \t [-1.76315685  0.68887059]. \t  -593.2031575612671 \t -0.029209353426092856\n",
            "56     \t [-1.04413933  0.99343303]. \t  -5.115411718133063 \t -0.029209353426092856\n",
            "57     \t [ 0.22782441 -0.67752742]. \t  -53.80326992530991 \t -0.029209353426092856\n",
            "58     \t [-1.73618415 -0.26471173]. \t  -1082.701722128939 \t -0.029209353426092856\n",
            "59     \t [1.00231572 1.01679636]. \t  \u001b[92m-0.014790859610477337\u001b[0m \t -0.014790859610477337\n",
            "60     \t [-0.28329683  1.87040894]. \t  -322.11121448069946 \t -0.014790859610477337\n",
            "61     \t [1.17500449 0.46233213]. \t  -84.35874181021748 \t -0.014790859610477337\n",
            "62     \t [-2.03594066 -1.14187835]. \t  -2804.382699699828 \t -0.014790859610477337\n",
            "63     \t [1.61722616 0.67038105]. \t  -378.698788822706 \t -0.014790859610477337\n",
            "64     \t [-1.24991045  1.5834155 ]. \t  -5.106784346627112 \t -0.014790859610477337\n",
            "65     \t [0.97661826 0.91847687]. \t  -0.1252005677515105 \t -0.014790859610477337\n",
            "66     \t [-0.22080734  1.68418939]. \t  -268.9546463000948 \t -0.014790859610477337\n",
            "67     \t [0.86509372 0.72051701]. \t  -0.09587416181968236 \t -0.014790859610477337\n",
            "68     \t [-0.19707086  1.02216595]. \t  -98.1265751377523 \t -0.014790859610477337\n",
            "69     \t [-0.2785283  -1.66794408]. \t  -306.31937322007997 \t -0.014790859610477337\n",
            "70     \t [0.94736035 0.88334862]. \t  -0.022773402489447783 \t -0.014790859610477337\n",
            "71     \t [1.28082171 1.63700312]. \t  -0.08008663068279206 \t -0.014790859610477337\n",
            "72     \t [ 0.78084473 -1.0355336 ]. \t  -270.73347291450534 \t -0.014790859610477337\n",
            "73     \t [1.24548825 1.57593378]. \t  -0.12123790820979732 \t -0.014790859610477337\n",
            "74     \t [1.57236249 1.21634884]. \t  -158.0749092992291 \t -0.014790859610477337\n",
            "75     \t [0.94808783 0.88381763]. \t  -0.025353876616298517 \t -0.014790859610477337\n",
            "76     \t [-0.53765381 -0.87006796]. \t  -136.72483573616964 \t -0.014790859610477337\n",
            "77     \t [ 1.71771102 -1.06836806]. \t  -1615.6701892856786 \t -0.014790859610477337\n",
            "78     \t [1.99507809 0.19555372]. \t  -1433.4483271429306 \t -0.014790859610477337\n",
            "79     \t [-0.75290941  1.52508029]. \t  -94.88889269840215 \t -0.014790859610477337\n",
            "80     \t [-0.54369605 -0.22622495]. \t  -29.61368851760308 \t -0.014790859610477337\n",
            "81     \t [0.39197243 0.75268142]. \t  -36.25447406670638 \t -0.014790859610477337\n",
            "82     \t [-1.39694674  1.90175093]. \t  -5.99245488403269 \t -0.014790859610477337\n",
            "83     \t [0.46528531 0.22968001]. \t  -0.3033163068388056 \t -0.014790859610477337\n",
            "84     \t [-0.03631004 -0.17967916]. \t  -4.349950937713276 \t -0.014790859610477337\n",
            "85     \t [0.83131104 0.68531339]. \t  -0.031779088848027395 \t -0.014790859610477337\n",
            "86     \t [ 0.74943733 -1.54421299]. \t  -443.5313342993164 \t -0.014790859610477337\n",
            "87     \t [0.69344788 0.49325605]. \t  -0.10931574852959045 \t -0.014790859610477337\n",
            "88     \t [-1.45703678  0.68005051]. \t  -214.23470547517417 \t -0.014790859610477337\n",
            "89     \t [0.15820226 1.50186129]. \t  -218.81229318625364 \t -0.014790859610477337\n",
            "90     \t [ 0.23336998 -0.50785557]. \t  -32.207775776733314 \t -0.014790859610477337\n",
            "91     \t [ 0.0974665  -0.92170077]. \t  -87.52800091675904 \t -0.014790859610477337\n",
            "92     \t [ 1.5748337  -0.48260344]. \t  -878.0923130673467 \t -0.014790859610477337\n",
            "93     \t [-0.38641532 -1.61581306]. \t  -313.4904893996541 \t -0.014790859610477337\n",
            "94     \t [-0.00623463  0.74020108]. \t  -55.796518282112864 \t -0.014790859610477337\n",
            "95     \t [-0.99623645 -0.80853576]. \t  -328.3532776023888 \t -0.014790859610477337\n",
            "96     \t [-0.92802601  0.40819344]. \t  -24.241702126559957 \t -0.014790859610477337\n",
            "97     \t [-0.17331047  1.9859159 ]. \t  -383.9230711749065 \t -0.014790859610477337\n",
            "98     \t [-0.21997478  1.33713612]. \t  -167.57527888398772 \t -0.014790859610477337\n",
            "99     \t [-1.85034181  0.43629014]. \t  -900.6249422614046 \t -0.014790859610477337\n",
            "100    \t [0.32081319 1.77697988]. \t  -280.7085761516899 \t -0.014790859610477337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4xG5dbuuvig",
        "outputId": "c510dcc2-1e67-4e64-cac5-b4cb8f49c9f5"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_winner_10 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_10 = dGPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.62910294 -1.57693156]. \t  -389.29291138113445 \t -8.580376531587937\n",
            "init   \t [ 1.84435861 -0.07294402]. \t  -1207.999341247548 \t -8.580376531587937\n",
            "init   \t [ 1.5256557  -1.17828534]. \t  -1229.4172568797705 \t -8.580376531587937\n",
            "init   \t [-1.88125338 -0.42109149]. \t  -1576.6245828972787 \t -8.580376531587937\n",
            "init   \t [-1.09309052  1.39977001]. \t  -8.580376531587937 \t -8.580376531587937\n",
            "1      \t [-0.86019812  2.03750805]. \t  -171.82841453589126 \t -8.580376531587937\n",
            "2      \t [-0.02273896 -1.98197652]. \t  -394.07407427702145 \t -8.580376531587937\n",
            "3      \t [-0.69769706  1.31213822]. \t  -71.00359845335805 \t -8.580376531587937\n",
            "4      \t [-1.66873493  1.66586017]. \t  -132.29708986028288 \t -8.580376531587937\n",
            "5      \t [-0.21590304  1.35197276]. \t  -171.8745365573776 \t -8.580376531587937\n",
            "6      \t [ 0.55073144 -1.79055537]. \t  -438.6270149029219 \t -8.580376531587937\n",
            "7      \t [0.98226372 1.2697215 ]. \t  -9.295464012565606 \t -8.580376531587937\n",
            "8      \t [1.31261675 1.94600611]. \t  \u001b[92m-5.072563300257509\u001b[0m \t -5.072563300257509\n",
            "9      \t [1.22322875 1.53592989]. \t  \u001b[92m-0.20697449521825664\u001b[0m \t -0.20697449521825664\n",
            "10     \t [ 0.04185613 -0.79432263]. \t  -64.29151160117509 \t -0.20697449521825664\n",
            "11     \t [1.11386998 1.55927118]. \t  -10.161322885418127 \t -0.20697449521825664\n",
            "12     \t [1.9137685  1.95871587]. \t  -291.1263791228549 \t -0.20697449521825664\n",
            "13     \t [1.14260469 2.04792664]. \t  -55.13331365889127 \t -0.20697449521825664\n",
            "14     \t [0.33945859 0.69106219]. \t  -33.59434005278298 \t -0.20697449521825664\n",
            "15     \t [-1.08562463 -1.84139658]. \t  -916.3761885599844 \t -0.20697449521825664\n",
            "16     \t [-0.13223803 -0.52404935]. \t  -30.608113526862876 \t -0.20697449521825664\n",
            "17     \t [-0.08877716 -0.10000637]. \t  -2.349412367258534 \t -0.20697449521825664\n",
            "18     \t [-0.92740451 -1.66284511]. \t  -640.2295593766216 \t -0.20697449521825664\n",
            "19     \t [-0.25858329  0.17411188]. \t  -2.7342145371422886 \t -0.20697449521825664\n",
            "20     \t [-0.10846019  0.14036144]. \t  -2.8824240158515875 \t -0.20697449521825664\n",
            "21     \t [ 0.75641574 -1.40332656]. \t  -390.315934888768 \t -0.20697449521825664\n",
            "22     \t [-1.19287258 -0.37119095]. \t  -326.70106981370185 \t -0.20697449521825664\n",
            "23     \t [-0.46621267 -0.12445515]. \t  -13.833146298878923 \t -0.20697449521825664\n",
            "24     \t [-0.96299088  0.87769036]. \t  -4.0999554446210045 \t -0.20697449521825664\n",
            "25     \t [-1.66270093 -1.55400635]. \t  -1872.1039474825202 \t -0.20697449521825664\n",
            "26     \t [-0.16764884  1.54445651]. \t  -231.2952497295111 \t -0.20697449521825664\n",
            "27     \t [-1.4064444  -0.22099504]. \t  -489.3866516394009 \t -0.20697449521825664\n",
            "28     \t [-0.9855934   0.55922248]. \t  -20.931145718957623 \t -0.20697449521825664\n",
            "29     \t [-1.18950253  1.15004471]. \t  -11.809616007935968 \t -0.20697449521825664\n",
            "30     \t [-1.81413059 -1.87982595]. \t  -2681.735608840056 \t -0.20697449521825664\n",
            "31     \t [ 0.66620021 -1.06959045]. \t  -229.15336766252585 \t -0.20697449521825664\n",
            "32     \t [ 1.6640782  -1.91679959]. \t  -2196.259224926139 \t -0.20697449521825664\n",
            "33     \t [-0.27940807  0.03190978]. \t  -1.8499510973057176 \t -0.20697449521825664\n",
            "34     \t [-0.92809415  0.14507641]. \t  -55.02358501900352 \t -0.20697449521825664\n",
            "35     \t [ 0.22527611 -1.7089182 ]. \t  -310.24317612133376 \t -0.20697449521825664\n",
            "36     \t [-1.50893354  0.0046614 ]. \t  -522.5926764247796 \t -0.20697449521825664\n",
            "37     \t [ 0.46183315 -0.73357907]. \t  -89.94570007824322 \t -0.20697449521825664\n",
            "38     \t [0.72553714 0.50464965]. \t  \u001b[92m-0.12265564496669791\u001b[0m \t -0.12265564496669791\n",
            "39     \t [0.70079929 0.47927986]. \t  \u001b[92m-0.10353912784283456\u001b[0m \t -0.10353912784283456\n",
            "40     \t [0.80630157 0.74840835]. \t  -1.003535321845259 \t -0.10353912784283456\n",
            "41     \t [0.559457   0.31031897]. \t  -0.19479271673298587 \t -0.10353912784283456\n",
            "42     \t [1.17305896 0.13752364]. \t  -153.42899481728517 \t -0.10353912784283456\n",
            "43     \t [0.71116503 0.39003773]. \t  -1.4224903976080303 \t -0.10353912784283456\n",
            "44     \t [1.11324246 1.26419724]. \t  \u001b[92m-0.07476741223952807\u001b[0m \t -0.07476741223952807\n",
            "45     \t [1.94437636 0.81987649]. \t  -877.4798877068349 \t -0.07476741223952807\n",
            "46     \t [-1.83870579 -0.43580196]. \t  -1464.7330592657775 \t -0.07476741223952807\n",
            "47     \t [ 2.00666098 -0.03070804]. \t  -1647.2598439717965 \t -0.07476741223952807\n",
            "48     \t [-0.41532174 -0.47551646]. \t  -43.99465085536307 \t -0.07476741223952807\n",
            "49     \t [0.547969  0.3392053]. \t  -0.3559275347487126 \t -0.07476741223952807\n",
            "50     \t [ 0.57893847 -1.48140375]. \t  -330.1712234659606 \t -0.07476741223952807\n",
            "51     \t [1.04561825 0.3972521 ]. \t  -48.45278872762367 \t -0.07476741223952807\n",
            "52     \t [-0.01548409  0.04620613]. \t  -1.242498702837128 \t -0.07476741223952807\n",
            "53     \t [0.97042774 0.85663006]. \t  -0.7250743906233528 \t -0.07476741223952807\n",
            "54     \t [1.09110897 1.20658126]. \t  \u001b[92m-0.03410118322642207\u001b[0m \t -0.03410118322642207\n",
            "55     \t [1.06788392 1.1589494 ]. \t  -0.03910510426612971 \t -0.03410118322642207\n",
            "56     \t [0.55084584 1.55763767]. \t  -157.50514222288103 \t -0.03410118322642207\n",
            "57     \t [-1.98002668 -1.70712373]. \t  -3175.901794188777 \t -0.03410118322642207\n",
            "58     \t [0.47224193 0.18646869]. \t  -0.4120731418366379 \t -0.03410118322642207\n",
            "59     \t [1.93029386 0.8663766 ]. \t  -818.6297032102251 \t -0.03410118322642207\n",
            "60     \t [-0.58186118  0.32100713]. \t  -2.533103648209388 \t -0.03410118322642207\n",
            "61     \t [0.79352934 0.62681458]. \t  -0.043456255772006194 \t -0.03410118322642207\n",
            "62     \t [-1.53711804  0.59339743]. \t  -319.4914002029559 \t -0.03410118322642207\n",
            "63     \t [1.74125534 1.43053369]. \t  -257.00934025861875 \t -0.03410118322642207\n",
            "64     \t [ 0.34578262 -0.99307802]. \t  -124.22558745727085 \t -0.03410118322642207\n",
            "65     \t [ 1.22527161 -0.36433001]. \t  -348.1047442233006 \t -0.03410118322642207\n",
            "66     \t [0.9582054  0.88846632]. \t  -0.08990390927979725 \t -0.03410118322642207\n",
            "67     \t [0.97253808 0.96107658]. \t  \u001b[92m-0.023999031551614876\u001b[0m \t -0.023999031551614876\n",
            "68     \t [-0.37329274  1.59657522]. \t  -214.23720569315782 \t -0.023999031551614876\n",
            "69     \t [0.97148866 0.93859269]. \t  \u001b[92m-0.0035143294462421076\u001b[0m \t -0.0035143294462421076\n",
            "70     \t [0.87729541 0.79974483]. \t  -0.10564292944820287 \t -0.0035143294462421076\n",
            "71     \t [0.84160655 0.70851516]. \t  -0.025093047031433585 \t -0.0035143294462421076\n",
            "72     \t [-1.72421319 -0.65417702]. \t  -1322.9981744186289 \t -0.0035143294462421076\n",
            "73     \t [ 1.24898057 -1.24694444]. \t  -787.9290060785235 \t -0.0035143294462421076\n",
            "74     \t [1.24007303 1.54893508]. \t  -0.07007612892122705 \t -0.0035143294462421076\n",
            "75     \t [-1.2773907  -0.23007098]. \t  -351.8156843349986 \t -0.0035143294462421076\n",
            "76     \t [0.56752619 0.36663967]. \t  -0.3855367578864273 \t -0.0035143294462421076\n",
            "77     \t [0.93846077 0.86695317]. \t  -0.0227083060154697 \t -0.0035143294462421076\n",
            "78     \t [0.30802441 0.11345996]. \t  -0.5133552833907497 \t -0.0035143294462421076\n",
            "79     \t [-1.17896272  0.67580822]. \t  -55.74816867150558 \t -0.0035143294462421076\n",
            "80     \t [1.01591254 1.71707928]. \t  -46.92288984327722 \t -0.0035143294462421076\n",
            "81     \t [-0.09053564 -1.38041038]. \t  -194.0122314118666 \t -0.0035143294462421076\n",
            "82     \t [-1.44375349  0.00707512]. \t  -437.5098304110983 \t -0.0035143294462421076\n",
            "83     \t [0.88420505 0.81212013]. \t  -0.10522692211466184 \t -0.0035143294462421076\n",
            "84     \t [0.8096961  0.64029575]. \t  -0.05966137472774141 \t -0.0035143294462421076\n",
            "85     \t [0.65351936 1.23699786]. \t  -65.71551927001417 \t -0.0035143294462421076\n",
            "86     \t [-0.796771    1.73565395]. \t  -124.40663419788791 \t -0.0035143294462421076\n",
            "87     \t [-1.03130049  0.56275634]. \t  -29.20868491804581 \t -0.0035143294462421076\n",
            "88     \t [-0.32412935 -0.05427506]. \t  -4.292079571359494 \t -0.0035143294462421076\n",
            "89     \t [-1.45123758  0.69030709]. \t  -206.45283409422657 \t -0.0035143294462421076\n",
            "90     \t [-1.81082405 -1.93748806]. \t  -2729.162850805099 \t -0.0035143294462421076\n",
            "91     \t [1.45357638 0.68649884]. \t  -203.6632813385702 \t -0.0035143294462421076\n",
            "92     \t [ 1.11334257 -1.4045816 ]. \t  -699.1463474533937 \t -0.0035143294462421076\n",
            "93     \t [-1.84946302 -1.03142848]. \t  -1990.0981513159281 \t -0.0035143294462421076\n",
            "94     \t [1.54089267 1.39196953]. \t  -96.79974380386078 \t -0.0035143294462421076\n",
            "95     \t [1.15390446 1.28777343]. \t  -0.21484850903310604 \t -0.0035143294462421076\n",
            "96     \t [1.18441455 1.4374353 ]. \t  -0.15370723930503633 \t -0.0035143294462421076\n",
            "97     \t [1.15884451 1.32613401]. \t  -0.05341049491815951 \t -0.0035143294462421076\n",
            "98     \t [1.42308353 0.96051045]. \t  -113.5282996267748 \t -0.0035143294462421076\n",
            "99     \t [0.67810219 1.92564012]. \t  -214.96572407533358 \t -0.0035143294462421076\n",
            "100    \t [1.43707224 0.96877898]. \t  -120.39981096582514 \t -0.0035143294462421076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSj_CQIAuvk3",
        "outputId": "c6b9776d-6ebd-48aa-b22e-526749352894"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_winner_11 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_11 = dGPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.97032207 1.31583119]. \t  -659.5505379977991 \t -45.00854175284071\n",
            "init   \t [ 0.59758024 -0.3125739 ]. \t  -45.00854175284071 \t -45.00854175284071\n",
            "init   \t [-1.21933423 -0.03314987]. \t  -235.94289844157058 \t -45.00854175284071\n",
            "init   \t [-1.48046507 -0.19447384]. \t  -575.5719351651762 \t -45.00854175284071\n",
            "init   \t [-1.58214297 -2.0360213 ]. \t  -2067.0989998073865 \t -45.00854175284071\n",
            "1      \t [-0.59782787  0.66030788]. \t  \u001b[92m-11.728483422473923\u001b[0m \t -11.728483422473923\n",
            "2      \t [-0.43844995  0.04578832]. \t  \u001b[92m-4.213899636921916\u001b[0m \t -4.213899636921916\n",
            "3      \t [-1.13505201  0.60692079]. \t  -50.992078170283 \t -4.213899636921916\n",
            "4      \t [ 1.30792565 -1.05782521]. \t  -766.5511117868322 \t -4.213899636921916\n",
            "5      \t [0.48464231 0.19162575]. \t  \u001b[92m-0.4526706910196805\u001b[0m \t -0.4526706910196805\n",
            "6      \t [-0.8004777   0.38377433]. \t  -9.846117180705239 \t -0.4526706910196805\n",
            "7      \t [-0.36159767  1.94598938]. \t  -331.36230478725497 \t -0.4526706910196805\n",
            "8      \t [ 0.14952152 -0.24307731]. \t  -7.768834516534318 \t -0.4526706910196805\n",
            "9      \t [ 0.49963856 -0.02647195]. \t  -7.874069924128357 \t -0.4526706910196805\n",
            "10     \t [0.41134755 0.68978487]. \t  -27.446663511024585 \t -0.4526706910196805\n",
            "11     \t [-0.14187471 -0.47588322]. \t  -25.90663422489753 \t -0.4526706910196805\n",
            "12     \t [-0.12785978  1.53599347]. \t  -232.2042641976356 \t -0.4526706910196805\n",
            "13     \t [-1.92815604  1.88370959]. \t  -344.95762087649587 \t -0.4526706910196805\n",
            "14     \t [ 1.8690072  -0.66010978]. \t  -1725.7433317989564 \t -0.4526706910196805\n",
            "15     \t [0.43993639 0.29285539]. \t  -1.2999459024067366 \t -0.4526706910196805\n",
            "16     \t [0.49605266 0.22660521]. \t  \u001b[92m-0.2918438673432136\u001b[0m \t -0.2918438673432136\n",
            "17     \t [0.53458034 0.30928026]. \t  \u001b[92m-0.2718598252129951\u001b[0m \t -0.2718598252129951\n",
            "18     \t [-0.79356863  0.7646965 ]. \t  -5.037912481201847 \t -0.2718598252129951\n",
            "19     \t [-0.0069477   0.10223989]. \t  -2.058256476849774 \t -0.2718598252129951\n",
            "20     \t [ 0.40042207 -1.80383233]. \t  -386.15593588440373 \t -0.2718598252129951\n",
            "21     \t [0.61794618 1.65841838]. \t  -163.10673822296363 \t -0.2718598252129951\n",
            "22     \t [ 0.0099395  -1.73764558]. \t  -302.9557719589955 \t -0.2718598252129951\n",
            "23     \t [-0.04710502 -1.72545809]. \t  -299.5832021814439 \t -0.2718598252129951\n",
            "24     \t [ 1.52887166 -0.62121014]. \t  -875.6458228419564 \t -0.2718598252129951\n",
            "25     \t [-0.09206683 -0.10865247]. \t  -2.5645249983111684 \t -0.2718598252129951\n",
            "26     \t [-1.33775942 -1.84923898]. \t  -1329.5802276803001 \t -0.2718598252129951\n",
            "27     \t [0.40434128 0.5194397 ]. \t  -13.024695036665513 \t -0.2718598252129951\n",
            "28     \t [ 1.75863482 -1.59610798]. \t  -2199.157970132603 \t -0.2718598252129951\n",
            "29     \t [ 0.25493755 -0.03496926]. \t  -1.5543664998219522 \t -0.2718598252129951\n",
            "30     \t [-1.67066864 -1.91418671]. \t  -2221.136489962148 \t -0.2718598252129951\n",
            "31     \t [-1.30501148  0.16778885]. \t  -241.01728153385264 \t -0.2718598252129951\n",
            "32     \t [-0.96901408 -0.41602442]. \t  -187.4829578926612 \t -0.2718598252129951\n",
            "33     \t [ 0.28557859 -0.87338187]. \t  -91.7008658277363 \t -0.2718598252129951\n",
            "34     \t [1.694842   1.68393527]. \t  -141.74889602888092 \t -0.2718598252129951\n",
            "35     \t [1.33505181 1.86729779]. \t  -0.8336459762839883 \t -0.2718598252129951\n",
            "36     \t [ 0.94980219 -0.49364355]. \t  -194.81928355041182 \t -0.2718598252129951\n",
            "37     \t [1.4656905  2.04725089]. \t  -1.2369219481070255 \t -0.2718598252129951\n",
            "38     \t [-0.99380261  1.01320559]. \t  -4.040590246457218 \t -0.2718598252129951\n",
            "39     \t [0.11221832 0.17690602]. \t  -3.488034753685005 \t -0.2718598252129951\n",
            "40     \t [ 1.99868364 -1.1725469 ]. \t  -2671.0789436399677 \t -0.2718598252129951\n",
            "41     \t [-0.84178006  1.97879725]. \t  -164.73386512539443 \t -0.2718598252129951\n",
            "42     \t [1.43074419 1.96773165]. \t  -0.8143464958172819 \t -0.2718598252129951\n",
            "43     \t [1.20713569 1.64185634]. \t  -3.453567178607419 \t -0.2718598252129951\n",
            "44     \t [-0.56907761 -0.55733849]. \t  -80.11120082104294 \t -0.2718598252129951\n",
            "45     \t [ 0.13646305 -0.627146  ]. \t  -42.4473482539538 \t -0.2718598252129951\n",
            "46     \t [-1.26944774  1.33133604]. \t  -12.999440423476049 \t -0.2718598252129951\n",
            "47     \t [-0.61562099 -0.90126674]. \t  -166.51575971908775 \t -0.2718598252129951\n",
            "48     \t [1.41503045 1.20426995]. \t  -63.859231229124 \t -0.2718598252129951\n",
            "49     \t [1.00383832 0.91343973]. \t  -0.8883516799742099 \t -0.2718598252129951\n",
            "50     \t [1.20922083 1.39513856]. \t  -0.4936983156482533 \t -0.2718598252129951\n",
            "51     \t [0.92603761 0.68282728]. \t  -3.0581216448770676 \t -0.2718598252129951\n",
            "52     \t [-2.00016163  1.27511611]. \t  -751.8525811840013 \t -0.2718598252129951\n",
            "53     \t [-1.39269204  1.89928462]. \t  -5.887436572264647 \t -0.2718598252129951\n",
            "54     \t [-1.34635848  0.99460828]. \t  -72.42972137593217 \t -0.2718598252129951\n",
            "55     \t [0.80917833 1.08217512]. \t  -18.303963539891566 \t -0.2718598252129951\n",
            "56     \t [-1.38736273  2.03308957]. \t  -6.872697932281808 \t -0.2718598252129951\n",
            "57     \t [1.28581776 1.63321757]. \t  \u001b[92m-0.12213195001499574\u001b[0m \t -0.12213195001499574\n",
            "58     \t [1.07329218 1.0926405 ]. \t  -0.3572057257490675 \t -0.12213195001499574\n",
            "59     \t [1.40377509 1.90581211]. \t  -0.5825807991488328 \t -0.12213195001499574\n",
            "60     \t [1.13911062 1.31650468]. \t  \u001b[92m-0.05519260533361432\u001b[0m \t -0.05519260533361432\n",
            "61     \t [0.30952834 0.29868557]. \t  -4.592690227278553 \t -0.05519260533361432\n",
            "62     \t [-0.02256099 -1.87738528]. \t  -353.69432248362006 \t -0.05519260533361432\n",
            "63     \t [-0.52472305  1.61408781]. \t  -181.55088101018254 \t -0.05519260533361432\n",
            "64     \t [ 1.4679008  -0.51628712]. \t  -713.6536443801239 \t -0.05519260533361432\n",
            "65     \t [1.26870097 1.6287887 ]. \t  -0.10901259926320231 \t -0.05519260533361432\n",
            "66     \t [-1.82360152 -0.2660919 ]. \t  -1297.942128509376 \t -0.05519260533361432\n",
            "67     \t [-1.13713511 -0.21115239]. \t  -230.83773110929698 \t -0.05519260533361432\n",
            "68     \t [1.01340528 1.0224479 ]. \t  \u001b[92m-0.002242998075737683\u001b[0m \t -0.002242998075737683\n",
            "69     \t [-0.52678452  0.87257878]. \t  -37.74271649707644 \t -0.002242998075737683\n",
            "70     \t [0.93964742 0.88625533]. \t  -0.004743378025302945 \t -0.002242998075737683\n",
            "71     \t [1.33769194 1.78257324]. \t  -0.11872329993619223 \t -0.002242998075737683\n",
            "72     \t [-1.74346609 -1.86063148]. \t  -2408.8259938304755 \t -0.002242998075737683\n",
            "73     \t [0.82208687 1.0993916 ]. \t  -17.972365520672504 \t -0.002242998075737683\n",
            "74     \t [1.19651072 1.42768979]. \t  -0.04017521801404709 \t -0.002242998075737683\n",
            "75     \t [0.77319092 0.92358938]. \t  -10.663737077866921 \t -0.002242998075737683\n",
            "76     \t [ 0.72625056 -1.8458145 ]. \t  -563.308570745773 \t -0.002242998075737683\n",
            "77     \t [1.0775763 1.1350031]. \t  -0.07449226219560307 \t -0.002242998075737683\n",
            "78     \t [ 0.28985867 -1.50123496]. \t  -251.80701272330782 \t -0.002242998075737683\n",
            "79     \t [ 1.59307307 -1.7643583 ]. \t  -1851.2787304984117 \t -0.002242998075737683\n",
            "80     \t [-2.03817801  1.59554601]. \t  -663.8859957234097 \t -0.002242998075737683\n",
            "81     \t [ 0.50290089 -0.10287362]. \t  -12.90525671117939 \t -0.002242998075737683\n",
            "82     \t [0.97752441 0.9187306 ]. \t  -0.13610124171914031 \t -0.002242998075737683\n",
            "83     \t [0.25956166 1.4313478 ]. \t  -186.59117783448178 \t -0.002242998075737683\n",
            "84     \t [1.00089468 0.96779202]. \t  -0.1155881498955011 \t -0.002242998075737683\n",
            "85     \t [-1.53789396 -0.57477465]. \t  -870.7376845511368 \t -0.002242998075737683\n",
            "86     \t [0.97555693 0.98664936]. \t  -0.12266405706898867 \t -0.002242998075737683\n",
            "87     \t [1.00446244 0.97181362]. \t  -0.13789227554855052 \t -0.002242998075737683\n",
            "88     \t [1.29140887 1.67957801]. \t  -0.09894036512572665 \t -0.002242998075737683\n",
            "89     \t [0.99411229 1.01643687]. \t  -0.07943248471330619 \t -0.002242998075737683\n",
            "90     \t [-1.0695059   0.02320873]. \t  -129.8649439623663 \t -0.002242998075737683\n",
            "91     \t [1.6383552  0.39314319]. \t  -525.3051833926082 \t -0.002242998075737683\n",
            "92     \t [-1.79594354  1.39508404]. \t  -342.82778427199776 \t -0.002242998075737683\n",
            "93     \t [0.44476235 0.20435305]. \t  -0.31256535285506576 \t -0.002242998075737683\n",
            "94     \t [1.2645492  1.61263704]. \t  -0.08835294101537394 \t -0.002242998075737683\n",
            "95     \t [1.33324603 1.76561308]. \t  -0.12528990659868072 \t -0.002242998075737683\n",
            "96     \t [-1.55204177 -1.32402239]. \t  -1399.9343445006775 \t -0.002242998075737683\n",
            "97     \t [-1.81455988 -0.57823758]. \t  -1506.2814452342047 \t -0.002242998075737683\n",
            "98     \t [0.6486892  1.18761898]. \t  -58.92490993311827 \t -0.002242998075737683\n",
            "99     \t [-1.09863269  0.1074321 ]. \t  -125.3078511226666 \t -0.002242998075737683\n",
            "100    \t [-0.61278378 -0.73685194]. \t  -126.33463761962125 \t -0.002242998075737683\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l98Nt7Tguvna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f71b31e4-4d27-4ec0-dbfa-32ec8e451543"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_winner_12 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_12 = dGPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.26353633  0.50015753]. \t  -125.32555445527122 \t -19.52113145175031\n",
            "init   \t [-0.25506718  1.16882876]. \t  -123.40590278793995 \t -19.52113145175031\n",
            "init   \t [ 1.14678091 -0.93146069]. \t  -504.72793807789463 \t -19.52113145175031\n",
            "init   \t [-0.91560241  1.23646844]. \t  -19.52113145175031 \t -19.52113145175031\n",
            "init   \t [1.87653879 1.53982007]. \t  -393.43336554061835 \t -19.52113145175031\n",
            "1      \t [-1.50623477  1.94347507]. \t  \u001b[92m-16.861146488467053\u001b[0m \t -16.861146488467053\n",
            "2      \t [-1.20192208  1.57470601]. \t  \u001b[92m-6.540783929609845\u001b[0m \t -6.540783929609845\n",
            "3      \t [-1.91012218 -2.00311393]. \t  -3202.61823928594 \t -6.540783929609845\n",
            "4      \t [-1.77962776  1.29233421]. \t  -359.1916226402331 \t -6.540783929609845\n",
            "5      \t [-0.66466132  2.03080142]. \t  -255.2716958040872 \t -6.540783929609845\n",
            "6      \t [-0.47063642  0.25132545]. \t  \u001b[92m-2.2517353516123206\u001b[0m \t -2.2517353516123206\n",
            "7      \t [-0.65475451  0.65572418]. \t  -7.892052597058424 \t -2.2517353516123206\n",
            "8      \t [-0.57425146  0.29737449]. \t  -2.5831804167687262 \t -2.2517353516123206\n",
            "9      \t [-0.45255332  0.29666251]. \t  -2.953700413808556 \t -2.2517353516123206\n",
            "10     \t [-1.17642383  1.90948463]. \t  -32.353064285422846 \t -2.2517353516123206\n",
            "11     \t [-1.62219888 -0.9583691 ]. \t  -1295.6129145410832 \t -2.2517353516123206\n",
            "12     \t [-0.13126529  1.19972162]. \t  -141.10826922453052 \t -2.2517353516123206\n",
            "13     \t [-0.25784332 -1.26042859]. \t  -177.651652670226 \t -2.2517353516123206\n",
            "14     \t [-1.5467193   1.60571348]. \t  -68.36400000188472 \t -2.2517353516123206\n",
            "15     \t [0.39123103 0.48807647]. \t  -11.594088132188192 \t -2.2517353516123206\n",
            "16     \t [-0.20786631 -0.13287834]. \t  -4.559595216779751 \t -2.2517353516123206\n",
            "17     \t [-1.60659348  0.49208595]. \t  -443.21010900556564 \t -2.2517353516123206\n",
            "18     \t [-0.01882974  0.27381299]. \t  -8.5159655949322 \t -2.2517353516123206\n",
            "19     \t [ 0.03298517 -0.06661061]. \t  \u001b[92m-1.3934281580334764\u001b[0m \t -1.3934281580334764\n",
            "20     \t [1.21991899 0.54005023]. \t  -89.9476082754366 \t -1.3934281580334764\n",
            "21     \t [ 0.20679112 -1.91326379]. \t  -383.23308876674696 \t -1.3934281580334764\n",
            "22     \t [-0.46813722  0.56682228]. \t  -14.242857785593285 \t -1.3934281580334764\n",
            "23     \t [ 1.81666571 -1.39798178]. \t  -2208.027975628221 \t -1.3934281580334764\n",
            "24     \t [ 0.41343102 -0.61317307]. \t  -61.825074089908 \t -1.3934281580334764\n",
            "25     \t [ 0.98904278 -0.1530125 ]. \t  -127.9655611785147 \t -1.3934281580334764\n",
            "26     \t [0.76633848 0.85344066]. \t  -7.1390316777838505 \t -1.3934281580334764\n",
            "27     \t [ 0.62192199 -1.03710214]. \t  -202.88896043583838 \t -1.3934281580334764\n",
            "28     \t [-0.78944214  1.03138771]. \t  -19.862281627288276 \t -1.3934281580334764\n",
            "29     \t [-1.47018171  0.40299074]. \t  -315.3141561232518 \t -1.3934281580334764\n",
            "30     \t [-0.19222577  0.17077657]. \t  -3.212337453204319 \t -1.3934281580334764\n",
            "31     \t [1.35840484 0.76388173]. \t  -117.06715361980729 \t -1.3934281580334764\n",
            "32     \t [-0.5013865  -1.12124902]. \t  -190.66751573937825 \t -1.3934281580334764\n",
            "33     \t [-0.05827082 -1.51731853]. \t  -232.3770506578901 \t -1.3934281580334764\n",
            "34     \t [1.84199247 1.9401659 ]. \t  -211.76312826514445 \t -1.3934281580334764\n",
            "35     \t [1.89917358 1.78193519]. \t  -333.84367620804505 \t -1.3934281580334764\n",
            "36     \t [ 2.04163797 -0.03123183]. \t  -1764.679667609521 \t -1.3934281580334764\n",
            "37     \t [1.3234135  2.01028765]. \t  -6.805671963859563 \t -1.3934281580334764\n",
            "38     \t [0.83830468 0.59173248]. \t  \u001b[92m-1.2587393236434223\u001b[0m \t -1.2587393236434223\n",
            "39     \t [ 0.21458814 -1.62712584]. \t  -280.5679665819224 \t -1.2587393236434223\n",
            "40     \t [-0.18489852  0.72966304]. \t  -49.77261283862049 \t -1.2587393236434223\n",
            "41     \t [0.81858808 0.63245375]. \t  \u001b[92m-0.17453225259032354\u001b[0m \t -0.17453225259032354\n",
            "42     \t [-0.11661194 -1.42082376]. \t  -207.00350025199268 \t -0.17453225259032354\n",
            "43     \t [0.82182184 0.70719717]. \t  \u001b[92m-0.13290976106887126\u001b[0m \t -0.13290976106887126\n",
            "44     \t [-1.5448088  -0.08848165]. \t  -618.996911885718 \t -0.13290976106887126\n",
            "45     \t [0.67551616 0.98433008]. \t  -27.984534946266628 \t -0.13290976106887126\n",
            "46     \t [1.63519227 0.98986453]. \t  -283.98544470693525 \t -0.13290976106887126\n",
            "47     \t [ 1.49332523 -0.11835829]. \t  -551.7315501101823 \t -0.13290976106887126\n",
            "48     \t [-1.16862764  0.18512278]. \t  -144.0769750622515 \t -0.13290976106887126\n",
            "49     \t [1.83379956 0.83301368]. \t  -640.6876431922506 \t -0.13290976106887126\n",
            "50     \t [1.53198305 1.9715412 ]. \t  -14.377840539326167 \t -0.13290976106887126\n",
            "51     \t [-0.80761456  1.1829677 ]. \t  -31.434523438301706 \t -0.13290976106887126\n",
            "52     \t [-1.41560164  1.36861148]. \t  -46.19783924735541 \t -0.13290976106887126\n",
            "53     \t [ 0.54265383 -1.56585271]. \t  -346.290406391232 \t -0.13290976106887126\n",
            "54     \t [1.26424016 1.75224292]. \t  -2.4395666748961307 \t -0.13290976106887126\n",
            "55     \t [1.40324134 2.0256033 ]. \t  -0.48202108431406976 \t -0.13290976106887126\n",
            "56     \t [-1.94776034  1.91400905]. \t  -362.03954637881907 \t -0.13290976106887126\n",
            "57     \t [-0.81217444  0.77265863]. \t  -4.561583921449694 \t -0.13290976106887126\n",
            "58     \t [-1.16058688 -1.8209149 ]. \t  -1008.212481886966 \t -0.13290976106887126\n",
            "59     \t [-0.56975816 -0.92238716]. \t  -157.96791415481863 \t -0.13290976106887126\n",
            "60     \t [-0.10827484  1.21484224]. \t  -145.97775879481577 \t -0.13290976106887126\n",
            "61     \t [0.92135239 0.85127538]. \t  \u001b[92m-0.006754344787098047\u001b[0m \t -0.006754344787098047\n",
            "62     \t [1.31322222 1.6949437 ]. \t  -0.1857768509223507 \t -0.006754344787098047\n",
            "63     \t [-1.89142655 -0.44566288]. \t  -1626.9397817292536 \t -0.006754344787098047\n",
            "64     \t [ 1.19781481 -0.23367227]. \t  -278.40586509620704 \t -0.006754344787098047\n",
            "65     \t [0.54939769 0.31910598]. \t  -0.23286136183857084 \t -0.006754344787098047\n",
            "66     \t [0.65550532 0.41506088]. \t  -0.14006957228822822 \t -0.006754344787098047\n",
            "67     \t [1.3119778 1.663592 ]. \t  -0.4301869996805588 \t -0.006754344787098047\n",
            "68     \t [1.42815851 2.0058245 ]. \t  -0.29764636239295106 \t -0.006754344787098047\n",
            "69     \t [0.58790828 0.74704427]. \t  -16.28266764067562 \t -0.006754344787098047\n",
            "70     \t [-0.15007015 -1.21747465]. \t  -155.08159606040635 \t -0.006754344787098047\n",
            "71     \t [1.09669711 1.19545764]. \t  -0.014660256667027569 \t -0.006754344787098047\n",
            "72     \t [1.19891441 1.3948363 ]. \t  -0.22069774735029318 \t -0.006754344787098047\n",
            "73     \t [0.59368062 0.84720693]. \t  -24.64287629699933 \t -0.006754344787098047\n",
            "74     \t [-0.00327105  1.70628234]. \t  -292.14284529343075 \t -0.006754344787098047\n",
            "75     \t [0.71416772 0.49790557]. \t  -0.09641370319097037 \t -0.006754344787098047\n",
            "76     \t [-1.59583951 -1.82759243]. \t  -1920.1850770306987 \t -0.006754344787098047\n",
            "77     \t [0.20300028 1.63855854]. \t  -255.7877270315223 \t -0.006754344787098047\n",
            "78     \t [ 0.44960628 -0.21123476]. \t  -17.39128263076904 \t -0.006754344787098047\n",
            "79     \t [1.44236381 2.03739056]. \t  -0.3807819756057391 \t -0.006754344787098047\n",
            "80     \t [-0.19349692 -1.86509339]. \t  -363.38816596376915 \t -0.006754344787098047\n",
            "81     \t [-0.99180545  1.79309836]. \t  -69.48341305380626 \t -0.006754344787098047\n",
            "82     \t [1.42966273 1.46328414]. \t  -33.90021318514245 \t -0.006754344787098047\n",
            "83     \t [1.24865303 1.5349974 ]. \t  -0.12008774002270477 \t -0.006754344787098047\n",
            "84     \t [-1.08094176  1.62134052]. \t  -24.84265147854368 \t -0.006754344787098047\n",
            "85     \t [1.34303957 1.78515743]. \t  -0.15226423357130164 \t -0.006754344787098047\n",
            "86     \t [1.16523437 1.33844182]. \t  -0.064664642663035 \t -0.006754344787098047\n",
            "87     \t [ 0.52288954 -1.0201586 ]. \t  -167.56050269590096 \t -0.006754344787098047\n",
            "88     \t [0.13530442 0.06395526]. \t  -0.9560722181291014 \t -0.006754344787098047\n",
            "89     \t [ 0.7760306  -1.36997905]. \t  -389.0084461128361 \t -0.006754344787098047\n",
            "90     \t [-1.22173971  1.70618719]. \t  -9.49602906924913 \t -0.006754344787098047\n",
            "91     \t [0.6752779  0.46080042]. \t  -0.10774861915384297 \t -0.006754344787098047\n",
            "92     \t [-0.39760952  0.73843906]. \t  -35.633428818966976 \t -0.006754344787098047\n",
            "93     \t [-0.76961064  1.86473579]. \t  -165.04066698652198 \t -0.006754344787098047\n",
            "94     \t [0.2779988  0.15041822]. \t  -1.0561568923118965 \t -0.006754344787098047\n",
            "95     \t [-1.25939561 -0.5085482 ]. \t  -443.8504713348479 \t -0.006754344787098047\n",
            "96     \t [-1.85875106  1.24298154]. \t  -497.45533978245345 \t -0.006754344787098047\n",
            "97     \t [-1.13354993  2.00624636]. \t  -56.58097840260251 \t -0.006754344787098047\n",
            "98     \t [1.13592812 1.28181978]. \t  -0.02572342235004445 \t -0.006754344787098047\n",
            "99     \t [-1.88399263  0.56730864]. \t  -897.6211402026098 \t -0.006754344787098047\n",
            "100    \t [2.00461881 1.6776865 ]. \t  -548.9484404727244 \t -0.006754344787098047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bpn-kmNuvqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570cd681-401a-41e9-dd3a-7ec6231952d4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_winner_13 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_13 = dGPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.65799909 -0.35389173]. \t  -62.0309701572776 \t -62.0309701572776\n",
            "init   \t [ 0.68854808 -1.10615174]. \t  -249.81607110978882 \t -62.0309701572776\n",
            "init   \t [1.26025046 0.56040843]. \t  -105.7097019264073 \t -62.0309701572776\n",
            "init   \t [-1.34269398 -0.98145948]. \t  -780.713409392333 \t -62.0309701572776\n",
            "init   \t [ 1.70115068 -0.15230767]. \t  -928.4380405275942 \t -62.0309701572776\n",
            "1      \t [0.39666085 0.75181144]. \t  \u001b[92m-35.703667546077746\u001b[0m \t -35.703667546077746\n",
            "2      \t [1.55724713 1.61136978]. \t  -66.51296774721507 \t -35.703667546077746\n",
            "3      \t [0.45795266 0.09637848]. \t  \u001b[92m-1.5784598330083528\u001b[0m \t -1.5784598330083528\n",
            "4      \t [0.96213087 0.89897489]. \t  \u001b[92m-0.07283487658287202\u001b[0m \t -0.07283487658287202\n",
            "5      \t [-1.05613246  1.97955428]. \t  -78.90121630773777 \t -0.07283487658287202\n",
            "6      \t [0.68770831 0.60171718]. \t  -1.7558123270072936 \t -0.07283487658287202\n",
            "7      \t [ 0.85956863 -0.72063444]. \t  -213.0316067957737 \t -0.07283487658287202\n",
            "8      \t [1.19067678 1.85217997]. \t  -18.91266804836645 \t -0.07283487658287202\n",
            "9      \t [0.05215744 1.95553245]. \t  -382.2458952191842 \t -0.07283487658287202\n",
            "10     \t [-1.69722168  1.47164335]. \t  -205.78002284770417 \t -0.07283487658287202\n",
            "11     \t [0.52442303 0.31757893]. \t  -0.4073037931602729 \t -0.07283487658287202\n",
            "12     \t [-1.21312934 -1.84984178]. \t  -1108.1504863914267 \t -0.07283487658287202\n",
            "13     \t [0.95820847 0.85110252]. \t  -0.45146374868511036 \t -0.07283487658287202\n",
            "14     \t [-1.33206651 -0.77282734]. \t  -654.2758580168237 \t -0.07283487658287202\n",
            "15     \t [-0.14916624 -1.03404296]. \t  -112.89618517418847 \t -0.07283487658287202\n",
            "16     \t [ 0.20599813 -0.40424233]. \t  -20.582523760820685 \t -0.07283487658287202\n",
            "17     \t [-1.36586415  2.04363529]. \t  -8.767508002243801 \t -0.07283487658287202\n",
            "18     \t [-1.09768704 -0.14853859]. \t  -187.5844485263904 \t -0.07283487658287202\n",
            "19     \t [-0.62223451  0.20942131]. \t  -5.791309994814112 \t -0.07283487658287202\n",
            "20     \t [0.08258876 1.12986524]. \t  -126.96450152380928 \t -0.07283487658287202\n",
            "21     \t [-0.62056197 -1.76656905]. \t  -465.5929716936383 \t -0.07283487658287202\n",
            "22     \t [-0.64375027  0.39811163]. \t  -2.7284930151817117 \t -0.07283487658287202\n",
            "23     \t [0.8318862  1.35426881]. \t  -43.88367000284221 \t -0.07283487658287202\n",
            "24     \t [ 0.53430913 -1.11507241]. \t  -196.37332171209164 \t -0.07283487658287202\n",
            "25     \t [-2.04602583 -0.00310691]. \t  -1764.3256972698305 \t -0.07283487658287202\n",
            "26     \t [-1.07589953  1.37685728]. \t  -9.118497357270465 \t -0.07283487658287202\n",
            "27     \t [-0.76126337  0.73511133]. \t  -5.5228550559244205 \t -0.07283487658287202\n",
            "28     \t [-0.61573676  0.34630772]. \t  -2.718347043616581 \t -0.07283487658287202\n",
            "29     \t [-1.30505595  1.77505941]. \t  -5.830076710294114 \t -0.07283487658287202\n",
            "30     \t [-0.41162842  0.99998671]. \t  -70.97381704737913 \t -0.07283487658287202\n",
            "31     \t [ 1.10650968 -0.76968109]. \t  -397.6327937907719 \t -0.07283487658287202\n",
            "32     \t [0.02168843 1.09583277]. \t  -120.93896809365916 \t -0.07283487658287202\n",
            "33     \t [ 1.16142564 -0.78263878]. \t  -454.37587569895425 \t -0.07283487658287202\n",
            "34     \t [ 1.19751534 -0.68056059]. \t  -447.1938388605762 \t -0.07283487658287202\n",
            "35     \t [ 0.1191211  -0.08553735]. \t  -1.770498882758178 \t -0.07283487658287202\n",
            "36     \t [0.90442299 0.82706784]. \t  \u001b[92m-0.017392143187383573\u001b[0m \t -0.017392143187383573\n",
            "37     \t [1.26013997 2.01043038]. \t  -17.91640790402721 \t -0.017392143187383573\n",
            "38     \t [0.9268223  0.77512887]. \t  -0.7087844629576677 \t -0.017392143187383573\n",
            "39     \t [-1.44592539  0.65227005]. \t  -212.89069487972304 \t -0.017392143187383573\n",
            "40     \t [0.24003888 0.08689773]. \t  -0.6632673063188136 \t -0.017392143187383573\n",
            "41     \t [ 2.02097812 -0.56767553]. \t  -2165.1789393183 \t -0.017392143187383573\n",
            "42     \t [1.06708043 1.1464565 ]. \t  \u001b[92m-0.010577331352798142\u001b[0m \t -0.010577331352798142\n",
            "43     \t [-0.88585976  0.6904623 ]. \t  -4.445437063170702 \t -0.010577331352798142\n",
            "44     \t [1.0461974  1.46347809]. \t  -13.614477131329513 \t -0.010577331352798142\n",
            "45     \t [ 0.23220122 -0.07927605]. \t  -2.363564553793089 \t -0.010577331352798142\n",
            "46     \t [0.95939181 0.96411156]. \t  -0.19243378934690822 \t -0.010577331352798142\n",
            "47     \t [-1.10795254  1.14002369]. \t  -5.209704069626256 \t -0.010577331352798142\n",
            "48     \t [0.65805946 0.42830142]. \t  -0.11917088429513312 \t -0.010577331352798142\n",
            "49     \t [0.35094864 0.10836632]. \t  -0.44316761714796343 \t -0.010577331352798142\n",
            "50     \t [1.23184518 1.19995382]. \t  -10.133660895515124 \t -0.010577331352798142\n",
            "51     \t [0.83743151 0.68253509]. \t  -0.061608931186862545 \t -0.010577331352798142\n",
            "52     \t [-1.05415735 -1.03439792]. \t  -464.5990871560298 \t -0.010577331352798142\n",
            "53     \t [0.92359552 0.57127259]. \t  -7.944488047024072 \t -0.010577331352798142\n",
            "54     \t [-0.3029216   0.63837222]. \t  -31.575932970358767 \t -0.010577331352798142\n",
            "55     \t [0.88439215 0.80153632]. \t  -0.050950171115834816 \t -0.010577331352798142\n",
            "56     \t [ 0.11572348 -0.02973209]. \t  -0.9679130526096114 \t -0.010577331352798142\n",
            "57     \t [-0.54087444 -1.81978742]. \t  -448.5691858428467 \t -0.010577331352798142\n",
            "58     \t [0.98889748 0.97856586]. \t  \u001b[92m-0.00016520705189970985\u001b[0m \t -0.00016520705189970985\n",
            "59     \t [1.49541888 1.13688769]. \t  -121.1112632568946 \t -0.00016520705189970985\n",
            "60     \t [0.8221016  0.66652715]. \t  -0.04034132185288897 \t -0.00016520705189970985\n",
            "61     \t [ 1.18729071 -1.45128176]. \t  -818.5334177883941 \t -0.00016520705189970985\n",
            "62     \t [1.9067089  1.83720731]. \t  -324.2217457125814 \t -0.00016520705189970985\n",
            "63     \t [1.25334794 1.54798428]. \t  -0.11661142255741444 \t -0.00016520705189970985\n",
            "64     \t [-0.63342095  0.81723983]. \t  -19.975139704498243 \t -0.00016520705189970985\n",
            "65     \t [-1.07156387  1.22114234]. \t  -4.8227190361964345 \t -0.00016520705189970985\n",
            "66     \t [0.92379814 0.87338444]. \t  -0.045732531894914434 \t -0.00016520705189970985\n",
            "67     \t [ 0.51757545 -1.3693464 ]. \t  -268.28518421901623 \t -0.00016520705189970985\n",
            "68     \t [0.83278569 0.69348729]. \t  -0.02796082631567785 \t -0.00016520705189970985\n",
            "69     \t [1.18378803 1.39776365]. \t  -0.0350671779172992 \t -0.00016520705189970985\n",
            "70     \t [0.94674636 0.93795967]. \t  -0.17615003491377926 \t -0.00016520705189970985\n",
            "71     \t [1.18279996 1.33011823]. \t  -0.5081026299431267 \t -0.00016520705189970985\n",
            "72     \t [-1.27769429  0.11797437]. \t  -234.5674956047715 \t -0.00016520705189970985\n",
            "73     \t [0.75740133 0.5676955 ]. \t  -0.06240780411504479 \t -0.00016520705189970985\n",
            "74     \t [ 0.79547643 -1.96309743]. \t  -673.9012232044952 \t -0.00016520705189970985\n",
            "75     \t [1.06935728 1.1365664 ]. \t  -0.009652645072618482 \t -0.00016520705189970985\n",
            "76     \t [-1.50435297 -0.48764829]. \t  -762.9212151557622 \t -0.00016520705189970985\n",
            "77     \t [0.98352975 1.00209038]. \t  -0.12109429422198517 \t -0.00016520705189970985\n",
            "78     \t [ 1.93059903 -0.51219088]. \t  -1798.120198187391 \t -0.00016520705189970985\n",
            "79     \t [-0.48917173  0.11073679]. \t  -3.8701990251313996 \t -0.00016520705189970985\n",
            "80     \t [0.8268194  0.66066641]. \t  -0.08272558750586026 \t -0.00016520705189970985\n",
            "81     \t [-1.02363533  1.24361565]. \t  -7.928329935595599 \t -0.00016520705189970985\n",
            "82     \t [-0.6357308  -1.23168039]. \t  -270.2709147453694 \t -0.00016520705189970985\n",
            "83     \t [-0.19146048 -0.65550324]. \t  -49.32817336411976 \t -0.00016520705189970985\n",
            "84     \t [1.34792586 0.78290787]. \t  -107.03587823357041 \t -0.00016520705189970985\n",
            "85     \t [0.97552344 0.9380648 ]. \t  -0.019043964394079054 \t -0.00016520705189970985\n",
            "86     \t [ 1.22373925 -0.18190517]. \t  -282.102907398439 \t -0.00016520705189970985\n",
            "87     \t [-1.95040851 -1.82662026]. \t  -3179.1984984099763 \t -0.00016520705189970985\n",
            "88     \t [-1.08662028  1.66416685]. \t  -27.723784719500078 \t -0.00016520705189970985\n",
            "89     \t [-0.46386249 -0.76884304]. \t  -98.9707474145879 \t -0.00016520705189970985\n",
            "90     \t [0.05516837 0.74965998]. \t  -56.63631662900497 \t -0.00016520705189970985\n",
            "91     \t [-2.03381803 -0.29522442]. \t  -1973.147534557621 \t -0.00016520705189970985\n",
            "92     \t [0.88204445 0.54686205]. \t  -5.35650045213236 \t -0.00016520705189970985\n",
            "93     \t [-0.73415056 -0.96868031]. \t  -230.31034784815657 \t -0.00016520705189970985\n",
            "94     \t [0.89875558 1.62848863]. \t  -67.36953814857557 \t -0.00016520705189970985\n",
            "95     \t [1.19167287 1.40612286]. \t  -0.0562304965395464 \t -0.00016520705189970985\n",
            "96     \t [-0.07486746 -1.21481139]. \t  -150.0969901918518 \t -0.00016520705189970985\n",
            "97     \t [-0.10064113 -0.19189334]. \t  -5.292698725182635 \t -0.00016520705189970985\n",
            "98     \t [-0.40566945  0.12810339]. \t  -2.1088712429628327 \t -0.00016520705189970985\n",
            "99     \t [-1.95537468 -1.02301032]. \t  -2357.5909153464518 \t -0.00016520705189970985\n",
            "100    \t [ 1.8715921  -0.93847778]. \t  -1973.3051177606942 \t -0.00016520705189970985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NdFRXtPuvsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f318e66-e46a-4745-cfe6-c8d39909b2d4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_winner_14 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_14 = dGPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.6043691  0.02928512]. \t  -11.444570460525577 \t -4.306489127802793\n",
            "init   \t [0.11608629 1.6231842 ]. \t  -259.89734211112767 \t -4.306489127802793\n",
            "init   \t [0.81916392 0.87776093]. \t  -4.306489127802793 \t -4.306489127802793\n",
            "init   \t [ 0.89021801 -1.13533148]. \t  -371.66089065122344 \t -4.306489127802793\n",
            "init   \t [-1.33056707 -0.17677726]. \t  -384.58487399689074 \t -4.306489127802793\n",
            "1      \t [1.22616673 0.4906171 ]. \t  -102.64126056482887 \t -4.306489127802793\n",
            "2      \t [0.39768831 0.43261479]. \t  -7.8955425297502035 \t -4.306489127802793\n",
            "3      \t [1.48251177 1.88453283]. \t  -10.049028113352751 \t -4.306489127802793\n",
            "4      \t [0.44231651 0.37003955]. \t  \u001b[92m-3.3523954353876624\u001b[0m \t -3.3523954353876624\n",
            "5      \t [1.11969363 1.26054942]. \t  \u001b[92m-0.018999105716874715\u001b[0m \t -0.018999105716874715\n",
            "6      \t [1.02729802 1.2165308 ]. \t  -2.5989529034397827 \t -0.018999105716874715\n",
            "7      \t [1.47751334 1.37887618]. \t  -64.89687718089974 \t -0.018999105716874715\n",
            "8      \t [1.28291166 1.72330405]. \t  -0.6797608842812664 \t -0.018999105716874715\n",
            "9      \t [-1.25912759  1.61914487]. \t  -5.217513608665121 \t -0.018999105716874715\n",
            "10     \t [0.56958754 0.36912861]. \t  -0.3850517103361976 \t -0.018999105716874715\n",
            "11     \t [1.35949135 1.98432447]. \t  -1.981765697419812 \t -0.018999105716874715\n",
            "12     \t [-1.40611301  1.95482855]. \t  -5.839221403425711 \t -0.018999105716874715\n",
            "13     \t [-1.08945402 -0.16533092]. \t  -187.22138818008403 \t -0.018999105716874715\n",
            "14     \t [-0.52566317 -0.03337751]. \t  -11.919012727855804 \t -0.018999105716874715\n",
            "15     \t [-0.77026479 -1.054689  ]. \t  -274.72319931886915 \t -0.018999105716874715\n",
            "16     \t [-1.16945133  1.81513045]. \t  -24.733400220273843 \t -0.018999105716874715\n",
            "17     \t [-1.84569403  2.03869948]. \t  -195.2094504412627 \t -0.018999105716874715\n",
            "18     \t [-1.33388722  1.50714295]. \t  -12.851533279590381 \t -0.018999105716874715\n",
            "19     \t [-1.9703253   1.59067058]. \t  -533.9251830288297 \t -0.018999105716874715\n",
            "20     \t [-0.98868031  0.93263785]. \t  -4.156009749076503 \t -0.018999105716874715\n",
            "21     \t [-1.16735302  1.31908145]. \t  -4.887790964172026 \t -0.018999105716874715\n",
            "22     \t [-0.59011784  0.31061766]. \t  -2.670011793977624 \t -0.018999105716874715\n",
            "23     \t [ 0.33061555 -1.7446117 ]. \t  -344.14939823471684 \t -0.018999105716874715\n",
            "24     \t [-1.04781805  1.77185419]. \t  -49.611927791670425 \t -0.018999105716874715\n",
            "25     \t [-0.98387027 -1.93324359]. \t  -845.6575912879017 \t -0.018999105716874715\n",
            "26     \t [-0.15521305  1.6408169 ]. \t  -262.7147527326001 \t -0.018999105716874715\n",
            "27     \t [-1.38319376 -0.62574996]. \t  -650.3189806290884 \t -0.018999105716874715\n",
            "28     \t [-0.86717191  0.60866227]. \t  -5.540532207507602 \t -0.018999105716874715\n",
            "29     \t [-0.55951318  0.93751851]. \t  -41.427548758187484 \t -0.018999105716874715\n",
            "30     \t [1.05253757 1.0987825 ]. \t  \u001b[92m-0.01095559731997343\u001b[0m \t -0.01095559731997343\n",
            "31     \t [ 1.68856139 -1.69422831]. \t  -2066.601950465006 \t -0.01095559731997343\n",
            "32     \t [ 0.16409842 -0.85719638]. \t  -78.86637559798619 \t -0.01095559731997343\n",
            "33     \t [-1.49436433  0.52273651]. \t  -298.7646462028464 \t -0.01095559731997343\n",
            "34     \t [0.76860612 0.64796429]. \t  -0.3808290981593947 \t -0.01095559731997343\n",
            "35     \t [0.67926703 0.52328557]. \t  -0.4858063498402786 \t -0.01095559731997343\n",
            "36     \t [-1.71144866 -0.58798446]. \t  -1244.3096860073397 \t -0.01095559731997343\n",
            "37     \t [ 0.12631292 -0.00319858]. \t  -0.8000148881249695 \t -0.01095559731997343\n",
            "38     \t [-0.33038334  0.21984588]. \t  -2.9952078181700217 \t -0.01095559731997343\n",
            "39     \t [ 1.19759156 -0.75635892]. \t  -479.90507167496264 \t -0.01095559731997343\n",
            "40     \t [1.30756829 1.08991906]. \t  -38.51175756710936 \t -0.01095559731997343\n",
            "41     \t [-1.80520355  1.80372094]. \t  -219.58299236338456 \t -0.01095559731997343\n",
            "42     \t [0.93605721 0.85563527]. \t  -0.046392233932515146 \t -0.01095559731997343\n",
            "43     \t [1.38748228 0.59142923]. \t  -178.01980050446724 \t -0.01095559731997343\n",
            "44     \t [1.01854305 1.01762241]. \t  -0.03957770466913952 \t -0.01095559731997343\n",
            "45     \t [0.73522446 0.54528247]. \t  -0.07234097869880866 \t -0.01095559731997343\n",
            "46     \t [1.01767863 1.05705422]. \t  -0.04604193359000726 \t -0.01095559731997343\n",
            "47     \t [-1.12935594 -1.9840474 ]. \t  -1066.9631255625009 \t -0.01095559731997343\n",
            "48     \t [-1.60059024 -0.73668979]. \t  -1094.8253393396633 \t -0.01095559731997343\n",
            "49     \t [ 1.85063588 -0.97466119]. \t  -1936.2962367024556 \t -0.01095559731997343\n",
            "50     \t [-1.94939435  1.05330205]. \t  -763.2098910205044 \t -0.01095559731997343\n",
            "51     \t [1.16947067 1.3918476 ]. \t  -0.08721636919719067 \t -0.01095559731997343\n",
            "52     \t [1.11870545 1.26178763]. \t  -0.02467066284644326 \t -0.01095559731997343\n",
            "53     \t [0.88335428 0.76682488]. \t  -0.03180399498064435 \t -0.01095559731997343\n",
            "54     \t [-0.11440045 -0.01028719]. \t  -1.2965258039116783 \t -0.01095559731997343\n",
            "55     \t [-0.78117471 -1.60706685]. \t  -494.8148560291321 \t -0.01095559731997343\n",
            "56     \t [ 1.71130173 -0.6721661 ]. \t  -1297.0242076040583 \t -0.01095559731997343\n",
            "57     \t [-0.29868956 -1.07522585]. \t  -137.27895007309056 \t -0.01095559731997343\n",
            "58     \t [-0.45305845 -1.33390007]. \t  -239.01335496671797 \t -0.01095559731997343\n",
            "59     \t [1.08314669 1.17839741]. \t  \u001b[92m-0.0096076690503152\u001b[0m \t -0.0096076690503152\n",
            "60     \t [0.74820506 0.55446216]. \t  -0.06626149664293017 \t -0.0096076690503152\n",
            "61     \t [0.77950451 0.5939453 ]. \t  -0.06733791204210669 \t -0.0096076690503152\n",
            "62     \t [-1.03233629 -1.32807551]. \t  -577.1552290395983 \t -0.0096076690503152\n",
            "63     \t [-0.59947585 -1.43610485]. \t  -324.9317829816583 \t -0.0096076690503152\n",
            "64     \t [0.51171294 0.69297598]. \t  -18.825373352716998 \t -0.0096076690503152\n",
            "65     \t [ 0.01102164 -0.63187888]. \t  -40.92052333757982 \t -0.0096076690503152\n",
            "66     \t [ 1.08362544 -1.74324689]. \t  -851.1823555090484 \t -0.0096076690503152\n",
            "67     \t [1.08742693 1.20392833]. \t  -0.05357229922007646 \t -0.0096076690503152\n",
            "68     \t [-0.41089759  0.62934004]. \t  -23.196952287907333 \t -0.0096076690503152\n",
            "69     \t [ 0.39130643 -1.49916247]. \t  -273.37448175824903 \t -0.0096076690503152\n",
            "70     \t [-1.07184414 -0.90691365]. \t  -426.9089002026663 \t -0.0096076690503152\n",
            "71     \t [-1.07872013  1.14281955]. \t  -4.364414492511502 \t -0.0096076690503152\n",
            "72     \t [-0.84180355 -0.03393875]. \t  -58.53355289837961 \t -0.0096076690503152\n",
            "73     \t [0.97531884 0.92621786]. \t  -0.0632541150404522 \t -0.0096076690503152\n",
            "74     \t [-1.3150643  -0.37131301]. \t  -446.6565677912945 \t -0.0096076690503152\n",
            "75     \t [-0.71785847  0.56052251]. \t  -3.1553573574022105 \t -0.0096076690503152\n",
            "76     \t [-1.50847021 -0.01868099]. \t  -532.6109801298252 \t -0.0096076690503152\n",
            "77     \t [ 0.42264889 -0.6119495 ]. \t  -62.83525877699036 \t -0.0096076690503152\n",
            "78     \t [-0.74381035 -0.01336308]. \t  -35.14634796034531 \t -0.0096076690503152\n",
            "79     \t [1.01339524 1.03386855]. \t  \u001b[92m-0.004938559968799545\u001b[0m \t -0.004938559968799545\n",
            "80     \t [-1.01837067 -1.92221297]. \t  -879.8146046778985 \t -0.004938559968799545\n",
            "81     \t [-1.50431446 -1.46030715]. \t  -1392.5449028268836 \t -0.004938559968799545\n",
            "82     \t [0.27956274 0.79907521]. \t  -52.49157805656037 \t -0.004938559968799545\n",
            "83     \t [ 0.52086063 -1.67252572]. \t  -378.0737835953263 \t -0.004938559968799545\n",
            "84     \t [-0.94978618  0.4780828 ]. \t  -21.78019844628485 \t -0.004938559968799545\n",
            "85     \t [ 0.12877742 -1.95684838]. \t  -390.20241691311753 \t -0.004938559968799545\n",
            "86     \t [-2.00204856 -1.87930626]. \t  -3475.283431726853 \t -0.004938559968799545\n",
            "87     \t [1.01418849 1.04282493]. \t  -0.020497946347800135 \t -0.004938559968799545\n",
            "88     \t [-0.44596638  1.84690402]. \t  -273.6871541784703 \t -0.004938559968799545\n",
            "89     \t [1.04508429 1.09138733]. \t  \u001b[92m-0.002098826448990657\u001b[0m \t -0.002098826448990657\n",
            "90     \t [0.90197033 0.79343018]. \t  -0.050092473861474356 \t -0.002098826448990657\n",
            "91     \t [0.98566747 0.99043632]. \t  -0.03591112795888479 \t -0.002098826448990657\n",
            "92     \t [0.7633255 0.5644944]. \t  -0.08903487997010442 \t -0.002098826448990657\n",
            "93     \t [ 1.3943274  -1.43945206]. \t  -1145.0310449248936 \t -0.002098826448990657\n",
            "94     \t [-1.98353862  1.55387886]. \t  -575.6017084221132 \t -0.002098826448990657\n",
            "95     \t [-1.14234013  0.82687416]. \t  -27.444408404006722 \t -0.002098826448990657\n",
            "96     \t [ 0.1251758  -1.38381926]. \t  -196.62205066614248 \t -0.002098826448990657\n",
            "97     \t [-0.29259507  0.5531349 ]. \t  -23.528579551977003 \t -0.002098826448990657\n",
            "98     \t [-1.35419313 -0.7584376 ]. \t  -677.5320326314333 \t -0.002098826448990657\n",
            "99     \t [0.88404381 0.75411643]. \t  -0.08861519901642444 \t -0.002098826448990657\n",
            "100    \t [1.07944551 1.14732029]. \t  -0.03828935860376582 \t -0.002098826448990657\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d86panpOuvum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00290468-95e8-439b-f1d4-3130f53cb9a4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_winner_15 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_15 = dGPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.31900409 -1.00146655]. \t  -122.17543449532224 \t -6.867717811955245\n",
            "init   \t [-1.01364994  0.85976824]. \t  -6.867717811955245 \t -6.867717811955245\n",
            "init   \t [-0.21482552 -1.11844164]. \t  -137.1031718367486 \t -6.867717811955245\n",
            "init   \t [-0.40259762  1.56572768]. \t  -198.98860193376754 \t -6.867717811955245\n",
            "init   \t [-0.25717364  1.55002954]. \t  -221.77381285664336 \t -6.867717811955245\n",
            "1      \t [-1.80428823  0.65131853]. \t  -686.0172298947007 \t -6.867717811955245\n",
            "2      \t [-0.39431841  0.38732566]. \t  -7.319039630086051 \t -6.867717811955245\n",
            "3      \t [-0.74233496  0.72080431]. \t  \u001b[92m-5.917003643344755\u001b[0m \t -5.917003643344755\n",
            "4      \t [1.39526915 1.04631859]. \t  -81.23859217725087 \t -5.917003643344755\n",
            "5      \t [-1.0740283   1.19307772]. \t  \u001b[92m-4.4579419412449655\u001b[0m \t -4.4579419412449655\n",
            "6      \t [ 0.12931444 -0.16944971]. \t  \u001b[92m-4.22409241735088\u001b[0m \t -4.22409241735088\n",
            "7      \t [-0.31654783 -0.16856539]. \t  -8.956917574982707 \t -4.22409241735088\n",
            "8      \t [-0.52184702  0.27850729]. \t  \u001b[92m-2.3198412787821603\u001b[0m \t -2.3198412787821603\n",
            "9      \t [-0.25194547 -0.0580387 ]. \t  -3.043962474276573 \t -2.3198412787821603\n",
            "10     \t [0.7742702 0.2689334]. \t  -10.978007405939358 \t -2.3198412787821603\n",
            "11     \t [0.23807974 0.12713905]. \t  \u001b[92m-1.0769425219891444\u001b[0m \t -1.0769425219891444\n",
            "12     \t [ 1.71435201 -0.58267442]. \t  -1240.7313562989934 \t -1.0769425219891444\n",
            "13     \t [0.71358031 0.66144399]. \t  -2.3999548677455564 \t -1.0769425219891444\n",
            "14     \t [0.76487366 0.39837213]. \t  -3.5394645894137136 \t -1.0769425219891444\n",
            "15     \t [0.53432323 0.24611884]. \t  \u001b[92m-0.3719527308274797\u001b[0m \t -0.3719527308274797\n",
            "16     \t [0.42983392 0.09180358]. \t  -1.1891268508733448 \t -0.3719527308274797\n",
            "17     \t [-1.33649111 -0.36559919]. \t  -468.48682209390284 \t -0.3719527308274797\n",
            "18     \t [0.54913683 0.31632196]. \t  \u001b[92m-0.22509494033860566\u001b[0m \t -0.22509494033860566\n",
            "19     \t [0.87307096 1.35901039]. \t  -35.6280596638206 \t -0.22509494033860566\n",
            "20     \t [-0.63901759 -0.21895831]. \t  -42.037132990791996 \t -0.22509494033860566\n",
            "21     \t [1.82117592 0.02005112]. \t  -1087.4516611715662 \t -0.22509494033860566\n",
            "22     \t [-1.96792425 -0.34645112]. \t  -1788.9540170373616 \t -0.22509494033860566\n",
            "23     \t [-1.58736997  1.87935818]. \t  -47.703810739714605 \t -0.22509494033860566\n",
            "24     \t [-0.0448911   0.01350086]. \t  -1.1049894141350702 \t -0.22509494033860566\n",
            "25     \t [0.49676181 0.3065502 ]. \t  -0.6105884926302754 \t -0.22509494033860566\n",
            "26     \t [-0.79377824 -1.52647203]. \t  -468.29098537736076 \t -0.22509494033860566\n",
            "27     \t [0.68320998 0.47494266]. \t  \u001b[92m-0.10702555738314529\u001b[0m \t -0.10702555738314529\n",
            "28     \t [0.95070665 0.96148041]. \t  -0.33463531909074684 \t -0.10702555738314529\n",
            "29     \t [-0.0198923  -0.92816516]. \t  -87.26270799406798 \t -0.10702555738314529\n",
            "30     \t [ 0.42686272 -2.01168711]. \t  -481.6477224077527 \t -0.10702555738314529\n",
            "31     \t [ 0.82312635 -1.53627469]. \t  -490.1274956390867 \t -0.10702555738314529\n",
            "32     \t [0.6547521  0.44081045]. \t  -0.1338616449430302 \t -0.10702555738314529\n",
            "33     \t [1.58678507 2.0096641 ]. \t  -26.173355388676914 \t -0.10702555738314529\n",
            "34     \t [-0.89207253  1.00619275]. \t  -8.006727594948323 \t -0.10702555738314529\n",
            "35     \t [-1.00569849  0.65747462]. \t  -16.551228310140736 \t -0.10702555738314529\n",
            "36     \t [-1.28838542  1.59686896]. \t  -5.634465412808513 \t -0.10702555738314529\n",
            "37     \t [1.32553206 0.05199988]. \t  -290.8205294941111 \t -0.10702555738314529\n",
            "38     \t [ 0.96702542 -1.29902543]. \t  -499.1497819678622 \t -0.10702555738314529\n",
            "39     \t [ 1.52335898 -1.74116497]. \t  -1650.0857154121345 \t -0.10702555738314529\n",
            "40     \t [-0.34042685 -0.57306855]. \t  -49.26319289547917 \t -0.10702555738314529\n",
            "41     \t [1.23935767 1.50616157]. \t  -0.1463696742522261 \t -0.10702555738314529\n",
            "42     \t [-1.48294538  0.26723435]. \t  -379.38593650876345 \t -0.10702555738314529\n",
            "43     \t [0.6363883  0.07558283]. \t  -10.983125860228954 \t -0.10702555738314529\n",
            "44     \t [-1.14012804  0.26105516]. \t  -112.49833618581917 \t -0.10702555738314529\n",
            "45     \t [-1.50494119  2.00371159]. \t  -13.09395237778391 \t -0.10702555738314529\n",
            "46     \t [1.02870186 1.0726905 ]. \t  \u001b[92m-0.021741606479710504\u001b[0m \t -0.021741606479710504\n",
            "47     \t [1.38415117 1.44425517]. \t  -22.39004806086102 \t -0.021741606479710504\n",
            "48     \t [1.33253552 1.891761  ]. \t  -1.45873515495705 \t -0.021741606479710504\n",
            "49     \t [-0.00440638 -1.59305034]. \t  -254.79595807217444 \t -0.021741606479710504\n",
            "50     \t [-1.29320616 -1.48855631]. \t  -1004.4120042144065 \t -0.021741606479710504\n",
            "51     \t [0.48167906 1.58793747]. \t  -184.12130734420694 \t -0.021741606479710504\n",
            "52     \t [-1.9011107  -0.01322906]. \t  -1324.2564808383454 \t -0.021741606479710504\n",
            "53     \t [-1.15579233  0.22534611]. \t  -127.97064146641327 \t -0.021741606479710504\n",
            "54     \t [1.08103866 1.18389973]. \t  -0.029839196367572228 \t -0.021741606479710504\n",
            "55     \t [0.22798276 0.8511745 ]. \t  -64.46781299692562 \t -0.021741606479710504\n",
            "56     \t [1.08812254 1.18097088]. \t  \u001b[92m-0.00868961702740477\u001b[0m \t -0.00868961702740477\n",
            "57     \t [-1.44601532 -0.39884523]. \t  -625.8961522131331 \t -0.00868961702740477\n",
            "58     \t [ 1.4866585  -0.20730164]. \t  -584.6457703960374 \t -0.00868961702740477\n",
            "59     \t [-1.68502746  0.51322119]. \t  -548.281791286128 \t -0.00868961702740477\n",
            "60     \t [-1.7808276   0.71049707]. \t  -613.3112022193153 \t -0.00868961702740477\n",
            "61     \t [1.06635942 1.1426233 ]. \t  \u001b[92m-0.007429558894027766\u001b[0m \t -0.007429558894027766\n",
            "62     \t [-1.43468012  1.37644102]. \t  -52.42179583826388 \t -0.007429558894027766\n",
            "63     \t [-0.51408943 -0.61129804]. \t  -78.95754780898235 \t -0.007429558894027766\n",
            "64     \t [-0.57808571 -0.72258276]. \t  -114.16576044861316 \t -0.007429558894027766\n",
            "65     \t [ 1.95035338 -1.96655006]. \t  -3330.687513865893 \t -0.007429558894027766\n",
            "66     \t [1.76872942 0.34832836]. \t  -773.4728649615515 \t -0.007429558894027766\n",
            "67     \t [1.17452269 1.4107134 ]. \t  -0.12786361740676255 \t -0.007429558894027766\n",
            "68     \t [0.82793846 0.70688289]. \t  -0.07540457182963817 \t -0.007429558894027766\n",
            "69     \t [1.12302386 1.24958856]. \t  -0.028577029830406248 \t -0.007429558894027766\n",
            "70     \t [1.0535951  0.55616467]. \t  -30.683166783002658 \t -0.007429558894027766\n",
            "71     \t [-0.01961047  1.89374533]. \t  -359.5211002445944 \t -0.007429558894027766\n",
            "72     \t [-1.54126376  0.22992038]. \t  -466.8066333460439 \t -0.007429558894027766\n",
            "73     \t [1.14405175 1.31460883]. \t  -0.02406223724883102 \t -0.007429558894027766\n",
            "74     \t [-1.24483887  0.07042865]. \t  -223.8411389965985 \t -0.007429558894027766\n",
            "75     \t [-1.86428692  1.96795304]. \t  -235.49373628057674 \t -0.007429558894027766\n",
            "76     \t [ 1.60229713 -1.03341015]. \t  -1296.9145215136468 \t -0.007429558894027766\n",
            "77     \t [-0.63261107  1.97065911]. \t  -249.30061465535087 \t -0.007429558894027766\n",
            "78     \t [1.90437555 0.06965906]. \t  -1266.0336628787975 \t -0.007429558894027766\n",
            "79     \t [-1.10184809  0.84917803]. \t  -17.732323399067432 \t -0.007429558894027766\n",
            "80     \t [-0.88245473  0.73950085]. \t  -3.69749981268287 \t -0.007429558894027766\n",
            "81     \t [0.64497093 0.41191547]. \t  -0.12770378357700504 \t -0.007429558894027766\n",
            "82     \t [-1.45764011  1.92065382]. \t  -10.204079062223498 \t -0.007429558894027766\n",
            "83     \t [1.24328263 1.53774161]. \t  -0.06560259500737861 \t -0.007429558894027766\n",
            "84     \t [-0.86777235 -1.25494479]. \t  -406.6843850848933 \t -0.007429558894027766\n",
            "85     \t [0.47629076 0.21536102]. \t  -0.2874776863246792 \t -0.007429558894027766\n",
            "86     \t [0.61043715 0.37887087]. \t  -0.1556496693316382 \t -0.007429558894027766\n",
            "87     \t [-0.18601672 -0.25940638]. \t  -10.05074152806717 \t -0.007429558894027766\n",
            "88     \t [-1.87286753 -0.47244772]. \t  -1592.357454169097 \t -0.007429558894027766\n",
            "89     \t [1.00484527 1.00373154]. \t  \u001b[92m-0.0036024735380178535\u001b[0m \t -0.0036024735380178535\n",
            "90     \t [1.27058356 1.66122968]. \t  -0.2926804384293351 \t -0.0036024735380178535\n",
            "91     \t [ 0.45080911 -1.64806577]. \t  -343.0307894880755 \t -0.0036024735380178535\n",
            "92     \t [ 1.57676031 -1.6779391 ]. \t  -1734.3156717142688 \t -0.0036024735380178535\n",
            "93     \t [-1.77464031  0.63760555]. \t  -638.5837589794946 \t -0.0036024735380178535\n",
            "94     \t [0.4789251  0.22827377]. \t  -0.2716390602848913 \t -0.0036024735380178535\n",
            "95     \t [1.88439225 1.81926553]. \t  -300.6497658529011 \t -0.0036024735380178535\n",
            "96     \t [1.36231493 1.84096703]. \t  -0.15357733087337394 \t -0.0036024735380178535\n",
            "97     \t [0.02253337 1.07324372]. \t  -116.0316868516606 \t -0.0036024735380178535\n",
            "98     \t [1.3669428  1.88888345]. \t  -0.1760626649422016 \t -0.0036024735380178535\n",
            "99     \t [-2.03919919  1.0231488 ]. \t  -992.1749368582529 \t -0.0036024735380178535\n",
            "100    \t [-1.07258036 -0.09198245]. \t  -158.65411831160364 \t -0.0036024735380178535\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "any0xrgYuvxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb5d8525-efd0-4f34-d2c3-4bfeb5afe547"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_winner_16 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_16 = dGPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.49073237 -0.75216486]. \t  -884.9748202573794 \t -21.690996320546372\n",
            "init   \t [0.70512959 0.03240619]. \t  -21.690996320546372 \t -21.690996320546372\n",
            "init   \t [ 1.15368111 -0.88603985]. \t  -491.54136063574043 \t -21.690996320546372\n",
            "init   \t [-1.09072882  0.26132352]. \t  -90.5574614167091 \t -21.690996320546372\n",
            "init   \t [1.5360998 0.8967902]. \t  -214.26941031258198 \t -21.690996320546372\n",
            "1      \t [ 0.0206574  -1.04141013]. \t  -109.5015164534733 \t -21.690996320546372\n",
            "2      \t [0.22660787 0.62554752]. \t  -33.568285031310054 \t -21.690996320546372\n",
            "3      \t [0.27886845 0.05285128]. \t  \u001b[92m-0.5821130603088832\u001b[0m \t -0.5821130603088832\n",
            "4      \t [-1.89666678  0.44479792]. \t  -1002.2459176523861 \t -0.5821130603088832\n",
            "5      \t [-0.64960232  0.23957703]. \t  -6.048388281834793 \t -0.5821130603088832\n",
            "6      \t [-0.90627997 -0.13732903]. \t  -95.53918160157806 \t -0.5821130603088832\n",
            "7      \t [-0.70879595  0.54496077]. \t  -3.10119619289842 \t -0.5821130603088832\n",
            "8      \t [ 0.40133467 -1.98033381]. \t  -458.9192215256066 \t -0.5821130603088832\n",
            "9      \t [ 0.39953549 -0.4425783 ]. \t  -36.625873858993714 \t -0.5821130603088832\n",
            "10     \t [0.50526239 0.34749532]. \t  -1.0949458446627538 \t -0.5821130603088832\n",
            "11     \t [0.35325267 0.22098225]. \t  -1.343626186243945 \t -0.5821130603088832\n",
            "12     \t [0.36058282 2.01233949]. \t  -354.72153426859774 \t -0.5821130603088832\n",
            "13     \t [ 0.45129004 -0.02734311]. \t  -5.637450859672818 \t -0.5821130603088832\n",
            "14     \t [0.44858559 0.14463035]. \t  -0.6243989373758893 \t -0.5821130603088832\n",
            "15     \t [ 0.23034354 -0.58857712]. \t  -41.761952530407115 \t -0.5821130603088832\n",
            "16     \t [-0.6553      0.39431484]. \t  -2.86324193725272 \t -0.5821130603088832\n",
            "17     \t [-0.33921788  2.02645588]. \t  -367.13357261498413 \t -0.5821130603088832\n",
            "18     \t [ 0.95291635 -1.64333792]. \t  -650.9600326480871 \t -0.5821130603088832\n",
            "19     \t [0.67686122 1.11927547]. \t  -43.81428231245156 \t -0.5821130603088832\n",
            "20     \t [-0.75234336 -0.73660468]. \t  -172.75395356546744 \t -0.5821130603088832\n",
            "21     \t [-1.64505925  0.84496156]. \t  -353.4246094702246 \t -0.5821130603088832\n",
            "22     \t [-1.77316542  1.8669476 ]. \t  -170.80626387099454 \t -0.5821130603088832\n",
            "23     \t [-1.0791796   0.99834649]. \t  -7.087961920745174 \t -0.5821130603088832\n",
            "24     \t [-0.93991592  0.77228725]. \t  -4.998810234724818 \t -0.5821130603088832\n",
            "25     \t [-0.78139034  0.53479172]. \t  -3.747599573318423 \t -0.5821130603088832\n",
            "26     \t [ 1.22114632 -0.29246096]. \t  -318.1929516497988 \t -0.5821130603088832\n",
            "27     \t [-1.5604096  -1.72930828]. \t  -1740.6005414601545 \t -0.5821130603088832\n",
            "28     \t [0.38485341 0.76341975]. \t  -38.23874944180172 \t -0.5821130603088832\n",
            "29     \t [-0.01455274  1.61486514]. \t  -261.7398640546982 \t -0.5821130603088832\n",
            "30     \t [1.46240679 1.15644189]. \t  -96.68388199077845 \t -0.5821130603088832\n",
            "31     \t [1.67593111 1.87316161]. \t  -87.98852906947357 \t -0.5821130603088832\n",
            "32     \t [-0.09734527 -0.03328792]. \t  -1.3870428362051446 \t -0.5821130603088832\n",
            "33     \t [-1.97445608 -2.00286589]. \t  -3491.4319350150254 \t -0.5821130603088832\n",
            "34     \t [ 1.55749306 -1.20712058]. \t  -1320.1108231409005 \t -0.5821130603088832\n",
            "35     \t [0.23713683 1.18773581]. \t  -128.61162224204764 \t -0.5821130603088832\n",
            "36     \t [ 1.09913973 -1.21545676]. \t  -587.37651999131 \t -0.5821130603088832\n",
            "37     \t [1.04560963 1.57958228]. \t  -23.64917436500178 \t -0.5821130603088832\n",
            "38     \t [ 1.25898832 -1.22705758]. \t  -790.8628694426475 \t -0.5821130603088832\n",
            "39     \t [-0.53688679 -2.00842178]. \t  -529.8309662396748 \t -0.5821130603088832\n",
            "40     \t [ 1.91185813 -0.38241844]. \t  -1631.0689727858066 \t -0.5821130603088832\n",
            "41     \t [0.94345729 0.89635503]. \t  \u001b[92m-0.007095061914909485\u001b[0m \t -0.007095061914909485\n",
            "42     \t [0.65970292 0.3962484 ]. \t  -0.2675867340403693 \t -0.007095061914909485\n",
            "43     \t [0.09912836 0.03557281]. \t  -0.8778573388999975 \t -0.007095061914909485\n",
            "44     \t [0.66163737 0.49598763]. \t  -0.4534882246624157 \t -0.007095061914909485\n",
            "45     \t [ 0.60579902 -1.05045006]. \t  -201.06972286530902 \t -0.007095061914909485\n",
            "46     \t [1.32637219 0.08657232]. \t  -279.89599061247236 \t -0.007095061914909485\n",
            "47     \t [0.92373229 0.82536722]. \t  -0.08373661363758395 \t -0.007095061914909485\n",
            "48     \t [0.64637592 0.37020792]. \t  -0.35156802573864754 \t -0.007095061914909485\n",
            "49     \t [1.11842872 1.34595768]. \t  -0.917948377892014 \t -0.007095061914909485\n",
            "50     \t [0.50367717 1.99031047]. \t  -301.83116171813464 \t -0.007095061914909485\n",
            "51     \t [0.86369503 0.69511638]. \t  -0.27717903202854494 \t -0.007095061914909485\n",
            "52     \t [ 0.9481272  -1.78426622]. \t  -719.9650394382073 \t -0.007095061914909485\n",
            "53     \t [0.13143502 0.00849313]. \t  -0.76211754048191 \t -0.007095061914909485\n",
            "54     \t [-1.11632408 -0.88810956]. \t  -459.9977901252435 \t -0.007095061914909485\n",
            "55     \t [0.6976035  0.45289812]. \t  -0.2053668778332181 \t -0.007095061914909485\n",
            "56     \t [0.73083496 0.50010988]. \t  -0.18811688846718624 \t -0.007095061914909485\n",
            "57     \t [-0.61019301 -1.3025744 ]. \t  -283.12504324506335 \t -0.007095061914909485\n",
            "58     \t [-1.96939784  0.54070658]. \t  -1122.9224080108465 \t -0.007095061914909485\n",
            "59     \t [ 0.75819958 -0.10407861]. \t  -46.15512888106376 \t -0.007095061914909485\n",
            "60     \t [-0.85982678 -0.39708684]. \t  -132.59693612405053 \t -0.007095061914909485\n",
            "61     \t [1.5445402  0.83279579]. \t  -241.41798946325693 \t -0.007095061914909485\n",
            "62     \t [ 2.01318748 -1.93732052]. \t  -3589.329304979957 \t -0.007095061914909485\n",
            "63     \t [-0.05408798  0.06684726]. \t  -1.519700545075625 \t -0.007095061914909485\n",
            "64     \t [-1.50676836  0.61391828]. \t  -280.6607894481304 \t -0.007095061914909485\n",
            "65     \t [0.97679753 1.09796785]. \t  -2.069372853004308 \t -0.007095061914909485\n",
            "66     \t [0.76691191 0.57108803]. \t  -0.08345436494850517 \t -0.007095061914909485\n",
            "67     \t [ 0.57272774 -0.95069889]. \t  -163.69401005435313 \t -0.007095061914909485\n",
            "68     \t [-1.5895948   0.68742701]. \t  -345.0395757885496 \t -0.007095061914909485\n",
            "69     \t [0.26877118 0.47381962]. \t  -16.661479673194858 \t -0.007095061914909485\n",
            "70     \t [-1.52791129  2.01102875]. \t  -16.854535730329797 \t -0.007095061914909485\n",
            "71     \t [ 0.59946234 -0.79637604]. \t  -133.73187665435353 \t -0.007095061914909485\n",
            "72     \t [-0.43541378  0.47213973]. \t  -10.044121049607899 \t -0.007095061914909485\n",
            "73     \t [-1.7514003  -0.23603889]. \t  -1098.8430494818413 \t -0.007095061914909485\n",
            "74     \t [-0.41332787 -0.35282988]. \t  -29.420502585851374 \t -0.007095061914909485\n",
            "75     \t [-0.94227343 -0.16669875]. \t  -114.9858968322364 \t -0.007095061914909485\n",
            "76     \t [-1.04843339  0.61214391]. \t  -27.91966742391449 \t -0.007095061914909485\n",
            "77     \t [1.33962623 0.84995024]. \t  -89.35136915823698 \t -0.007095061914909485\n",
            "78     \t [0.8428471  0.71415851]. \t  -0.026116265463732566 \t -0.007095061914909485\n",
            "79     \t [-1.82204876  0.4768997 ]. \t  -816.2072388153531 \t -0.007095061914909485\n",
            "80     \t [0.94642172 0.8935862 ]. \t  \u001b[92m-0.00332341206940825\u001b[0m \t -0.00332341206940825\n",
            "81     \t [ 0.53500734 -1.22816197]. \t  -229.55538676540743 \t -0.00332341206940825\n",
            "82     \t [ 1.95780327 -0.0344688 ]. \t  -1496.6439525931778 \t -0.00332341206940825\n",
            "83     \t [-0.74594318  1.73776012]. \t  -142.60211472973953 \t -0.00332341206940825\n",
            "84     \t [0.40482454 0.38518204]. \t  -5.251564672638031 \t -0.00332341206940825\n",
            "85     \t [0.80431251 0.633683  ]. \t  -0.05581173473667576 \t -0.00332341206940825\n",
            "86     \t [-0.27536364  1.91745466]. \t  -340.7864828566773 \t -0.00332341206940825\n",
            "87     \t [-0.54314009 -1.1545853 ]. \t  -212.51137175167688 \t -0.00332341206940825\n",
            "88     \t [0.96120873 0.93188314]. \t  -0.00784238103777529 \t -0.00332341206940825\n",
            "89     \t [-0.49930821 -1.30813117]. \t  -244.80981656957928 \t -0.00332341206940825\n",
            "90     \t [-0.63042838  1.71535492]. \t  -176.3482881164944 \t -0.00332341206940825\n",
            "91     \t [ 0.49039704 -1.13699842]. \t  -190.0069262493083 \t -0.00332341206940825\n",
            "92     \t [0.32516725 0.05902681]. \t  -0.6735529430497147 \t -0.00332341206940825\n",
            "93     \t [ 0.21938768 -0.35648909]. \t  -16.981093934902173 \t -0.00332341206940825\n",
            "94     \t [ 0.40091597 -0.29888962]. \t  -21.4842532107915 \t -0.00332341206940825\n",
            "95     \t [-0.02629612 -0.737631  ]. \t  -55.56529365654726 \t -0.00332341206940825\n",
            "96     \t [1.09132668 2.0162445 ]. \t  -68.11219215117217 \t -0.00332341206940825\n",
            "97     \t [-1.05913244  0.13441469]. \t  -101.72540366419422 \t -0.00332341206940825\n",
            "98     \t [1.03224282 1.06896304]. \t  \u001b[92m-0.002221450870597632\u001b[0m \t -0.002221450870597632\n",
            "99     \t [-1.03049915  1.23553349]. \t  -7.13679627376443 \t -0.002221450870597632\n",
            "100    \t [-1.01481    1.5943567]. \t  -35.92744461630901 \t -0.002221450870597632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reLyKt6Quvzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "362107f0-9a84-41ac-d514-6cf3465d5490"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_winner_17 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_17 = dGPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.62187122 0.94420838]. \t  -31.22188590191926 \t -31.22188590191926\n",
            "init   \t [-1.25596728  0.08059797]. \t  -229.14713198618372 \t -31.22188590191926\n",
            "init   \t [0.72246586 1.2470062 ]. \t  -52.64667111223406 \t -31.22188590191926\n",
            "init   \t [0.13726869 1.671308  ]. \t  -273.80846401311214 \t -31.22188590191926\n",
            "init   \t [-0.2315013  -0.99953783]. \t  -112.42501801714917 \t -31.22188590191926\n",
            "1      \t [1.59286487 0.71471414]. \t  -332.5037046146484 \t -31.22188590191926\n",
            "2      \t [ 1.90278748 -2.02756811]. \t  -3190.995555019365 \t -31.22188590191926\n",
            "3      \t [-1.33871017 -1.86938878]. \t  -1346.1524660840382 \t -31.22188590191926\n",
            "4      \t [-0.05951272 -0.18034528]. \t  \u001b[92m-4.504011857609685\u001b[0m \t -4.504011857609685\n",
            "5      \t [-0.14818959 -0.3550115 ]. \t  -15.52910245168684 \t -4.504011857609685\n",
            "6      \t [ 0.06733315 -0.25229126]. \t  -7.465776236662366 \t -4.504011857609685\n",
            "7      \t [0.19958071 0.18862031]. \t  \u001b[92m-2.8544533831880186\u001b[0m \t -2.8544533831880186\n",
            "8      \t [0.04751241 0.075279  ]. \t  \u001b[92m-1.4404475611620164\u001b[0m \t -1.4404475611620164\n",
            "9      \t [-0.37850307 -0.75061539]. \t  -81.80241034086906 \t -1.4404475611620164\n",
            "10     \t [-0.83386556 -0.14471424]. \t  -73.9307927272724 \t -1.4404475611620164\n",
            "11     \t [-1.0962071   0.44784178]. \t  -61.219782983337005 \t -1.4404475611620164\n",
            "12     \t [-0.67561069  0.4765076 ]. \t  -2.8479027132976142 \t -1.4404475611620164\n",
            "13     \t [-1.91334159  1.69907721]. \t  -393.3530220734682 \t -1.4404475611620164\n",
            "14     \t [-0.58018335  0.23368311]. \t  -3.5564298502433713 \t -1.4404475611620164\n",
            "15     \t [-0.63748997 -1.38065512]. \t  -322.03563613815305 \t -1.4404475611620164\n",
            "16     \t [0.01783411 0.39098217]. \t  -16.226494790123382 \t -1.4404475611620164\n",
            "17     \t [-0.53458742 -0.72927028]. \t  -105.38841833205716 \t -1.4404475611620164\n",
            "18     \t [0.04802418 0.20630082]. \t  -5.067633597172671 \t -1.4404475611620164\n",
            "19     \t [1.75456992 1.83954575]. \t  -154.07400851032006 \t -1.4404475611620164\n",
            "20     \t [-0.35289534 -1.2149306 ]. \t  -181.247167573373 \t -1.4404475611620164\n",
            "21     \t [1.73420509 1.46014498]. \t  -239.95969685251026 \t -1.4404475611620164\n",
            "22     \t [0.17391461 1.21263513]. \t  -140.486753581382 \t -1.4404475611620164\n",
            "23     \t [ 0.3935953  -1.31480281]. \t  -216.3754355014879 \t -1.4404475611620164\n",
            "24     \t [ 1.08209914 -0.81131882]. \t  -392.9411641639197 \t -1.4404475611620164\n",
            "25     \t [-0.93224769  0.85501003]. \t  -3.7533937152100187 \t -1.4404475611620164\n",
            "26     \t [-0.48898107  1.48909735]. \t  -158.46578146552847 \t -1.4404475611620164\n",
            "27     \t [1.13036921 1.22851349]. \t  \u001b[92m-0.259267473751217\u001b[0m \t -0.259267473751217\n",
            "28     \t [1.20501534 1.65905418]. \t  -4.326608570499859 \t -0.259267473751217\n",
            "29     \t [1.10517263 1.30764206]. \t  -0.7547178275161652 \t -0.259267473751217\n",
            "30     \t [1.20429826 1.49952164]. \t  -0.2836771911602363 \t -0.259267473751217\n",
            "31     \t [-1.88671546  1.04551085]. \t  -640.4454340967029 \t -0.259267473751217\n",
            "32     \t [-1.49651819 -2.02411352]. \t  -1824.1294927044426 \t -0.259267473751217\n",
            "33     \t [0.3186804  1.09882346]. \t  -99.9181963257552 \t -0.259267473751217\n",
            "34     \t [-1.38459603  1.2141535 ]. \t  -55.100543620283794 \t -0.259267473751217\n",
            "35     \t [-1.34604237  1.89372284]. \t  -6.1745574584551886 \t -0.259267473751217\n",
            "36     \t [-1.13848632  1.38874649]. \t  -5.430514501573927 \t -0.259267473751217\n",
            "37     \t [ 0.6677092  -1.23963205]. \t  -284.19052973424647 \t -0.259267473751217\n",
            "38     \t [ 1.04764411 -1.63437212]. \t  -746.3465868051511 \t -0.259267473751217\n",
            "39     \t [-1.91187252 -1.00609437]. \t  -2181.2982218772822 \t -0.259267473751217\n",
            "40     \t [0.54634779 0.20628551]. \t  -1.0560761739032594 \t -0.259267473751217\n",
            "41     \t [ 0.17979682 -0.02733992]. \t  -1.0287461379714624 \t -0.259267473751217\n",
            "42     \t [-1.32097863  2.02275197]. \t  -13.102415917738863 \t -0.259267473751217\n",
            "43     \t [1.07927518 1.25537747]. \t  -0.8260802386650273 \t -0.259267473751217\n",
            "44     \t [ 1.33883419 -2.03632038]. \t  -1466.0837406611788 \t -0.259267473751217\n",
            "45     \t [0.36471814 0.10927891]. \t  -0.4599437304337089 \t -0.259267473751217\n",
            "46     \t [0.42498651 1.46456472]. \t  -165.1837035566068 \t -0.259267473751217\n",
            "47     \t [-0.41592949 -0.74377796]. \t  -86.05255252223562 \t -0.259267473751217\n",
            "48     \t [-1.52637107  1.3341993 ]. \t  -105.50634993417196 \t -0.259267473751217\n",
            "49     \t [-1.63676595  1.16647122]. \t  -235.72770352656477 \t -0.259267473751217\n",
            "50     \t [-0.91170464  0.9884602 ]. \t  -6.127523363733994 \t -0.259267473751217\n",
            "51     \t [-1.03822171  0.99968946]. \t  -4.766104158188168 \t -0.259267473751217\n",
            "52     \t [ 1.16414869 -0.27786964]. \t  -266.7323671760979 \t -0.259267473751217\n",
            "53     \t [ 0.67582072 -0.86045542]. \t  -173.60379613220516 \t -0.259267473751217\n",
            "54     \t [ 1.96307728 -1.52272095]. \t  -2891.4880782547907 \t -0.259267473751217\n",
            "55     \t [-1.63769437  1.23503965]. \t  -216.33925826148388 \t -0.259267473751217\n",
            "56     \t [-2.03099333  0.76013649]. \t  -1141.3730869217545 \t -0.259267473751217\n",
            "57     \t [0.92122461 0.5995163 ]. \t  -6.213204097598539 \t -0.259267473751217\n",
            "58     \t [0.87071153 0.7500134 ]. \t  \u001b[92m-0.023317333264273654\u001b[0m \t -0.023317333264273654\n",
            "59     \t [0.9309996  0.80929142]. \t  -0.33502771710390133 \t -0.023317333264273654\n",
            "60     \t [-1.7722413  -1.23475196]. \t  -1922.2651483292495 \t -0.023317333264273654\n",
            "61     \t [ 0.53234742 -1.40459953]. \t  -285.15083875605234 \t -0.023317333264273654\n",
            "62     \t [0.52314246 1.00172271]. \t  -53.23229777535611 \t -0.023317333264273654\n",
            "63     \t [0.80167363 0.69328928]. \t  -0.295457141950412 \t -0.023317333264273654\n",
            "64     \t [0.95065482 0.90554263]. \t  \u001b[92m-0.002758242625587753\u001b[0m \t -0.002758242625587753\n",
            "65     \t [0.71406684 0.50184679]. \t  -0.08822943450371674 \t -0.002758242625587753\n",
            "66     \t [-0.25376903  0.75078484]. \t  -48.68452741058197 \t -0.002758242625587753\n",
            "67     \t [-0.31251138  0.16572607]. \t  -2.185939276476964 \t -0.002758242625587753\n",
            "68     \t [0.54072251 1.04809334]. \t  -57.321074583400176 \t -0.002758242625587753\n",
            "69     \t [0.79843735 0.61928528]. \t  -0.07381313054833452 \t -0.002758242625587753\n",
            "70     \t [-1.93510578  0.10608985]. \t  -1332.515481306013 \t -0.002758242625587753\n",
            "71     \t [ 0.7001982  -1.08373974]. \t  -247.84291406674387 \t -0.002758242625587753\n",
            "72     \t [-0.04385934  0.51270369]. \t  -27.179268024709717 \t -0.002758242625587753\n",
            "73     \t [-1.24530025  1.56947825]. \t  -5.076362904596787 \t -0.002758242625587753\n",
            "74     \t [-0.96214766  0.6314955 ]. \t  -12.507306242094252 \t -0.002758242625587753\n",
            "75     \t [-0.06305095 -0.65857327]. \t  -45.027153579346496 \t -0.002758242625587753\n",
            "76     \t [0.72607021 0.52856884]. \t  -0.07523098831933318 \t -0.002758242625587753\n",
            "77     \t [-1.37516928 -1.8899802 ]. \t  -1435.2910264935185 \t -0.002758242625587753\n",
            "78     \t [ 1.25378386 -1.87414395]. \t  -1187.6372739167462 \t -0.002758242625587753\n",
            "79     \t [1.01220441 1.03486883]. \t  -0.01078073154289555 \t -0.002758242625587753\n",
            "80     \t [-1.22027478 -1.5265442 ]. \t  -914.3228437697458 \t -0.002758242625587753\n",
            "81     \t [0.65221889 1.46184715]. \t  -107.54540135343447 \t -0.002758242625587753\n",
            "82     \t [ 1.55054754 -1.96262208]. \t  -1907.2145839346979 \t -0.002758242625587753\n",
            "83     \t [0.91774114 1.02736134]. \t  -3.4334318264022183 \t -0.002758242625587753\n",
            "84     \t [1.99429542 1.99971413]. \t  -392.0392903776973 \t -0.002758242625587753\n",
            "85     \t [-0.43402524 -0.54886121]. \t  -56.40858051018757 \t -0.002758242625587753\n",
            "86     \t [1.34299937 1.70234647]. \t  -1.1438344722033924 \t -0.002758242625587753\n",
            "87     \t [1.32110257 1.67860577]. \t  -0.5480790294497937 \t -0.002758242625587753\n",
            "88     \t [1.2747173  1.60120578]. \t  -0.13163112839381907 \t -0.002758242625587753\n",
            "89     \t [-0.45368952 -0.42290734]. \t  -41.644803839985755 \t -0.002758242625587753\n",
            "90     \t [0.92502429 0.90509421]. \t  -0.2498973287628308 \t -0.002758242625587753\n",
            "91     \t [0.98077986 0.96992581]. \t  -0.006764101125278395 \t -0.002758242625587753\n",
            "92     \t [-0.88903176  0.80760815]. \t  -3.5981306052969417 \t -0.002758242625587753\n",
            "93     \t [-1.73497169  0.81764991]. \t  -488.17555231351713 \t -0.002758242625587753\n",
            "94     \t [-0.20004687  1.61504794]. \t  -249.51180708440614 \t -0.002758242625587753\n",
            "95     \t [-0.6326007   0.27785922]. \t  -4.161711723950305 \t -0.002758242625587753\n",
            "96     \t [-1.63142951  1.44204324]. \t  -155.64708326487352 \t -0.002758242625587753\n",
            "97     \t [ 2.02066043 -0.01952128]. \t  -1684.1661048395354 \t -0.002758242625587753\n",
            "98     \t [1.54415053 0.93054645]. \t  -211.66536041100017 \t -0.002758242625587753\n",
            "99     \t [0.99939249 0.63883855]. \t  -12.956169399285843 \t -0.002758242625587753\n",
            "100    \t [1.8395479 1.5593048]. \t  -333.6329134872885 \t -0.002758242625587753\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI3FtfYSuv2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d48c3dc-42ce-403a-8c6d-fbb49a11c079"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_winner_18 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_18 = dGPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.52956683 1.91914255]. \t  -17.95675982372887 \t -1.7663579664225912\n",
            "init   \t [1.51222084 0.12638491]. \t  -467.0068292530542 \t -1.7663579664225912\n",
            "init   \t [-1.09474477 -2.0013105 ]. \t  -1028.244988519411 \t -1.7663579664225912\n",
            "init   \t [-0.28479972 -0.39996883]. \t  -24.79447905426233 \t -1.7663579664225912\n",
            "init   \t [ 0.09287545 -0.0885072 ]. \t  -1.7663579664225912 \t -1.7663579664225912\n",
            "1      \t [-0.56407431  0.40221499]. \t  -3.152519236956738 \t -1.7663579664225912\n",
            "2      \t [-0.15472488 -0.07662684]. \t  -2.3447541166733443 \t -1.7663579664225912\n",
            "3      \t [-1.25150403  1.2743779 ]. \t  -13.588923486938203 \t -1.7663579664225912\n",
            "4      \t [ 0.3397827  -0.76332611]. \t  -77.66103325486918 \t -1.7663579664225912\n",
            "5      \t [-0.02047237  1.53024584]. \t  -235.07834459947998 \t -1.7663579664225912\n",
            "6      \t [-1.56737154  0.39039077]. \t  -433.53558235091447 \t -1.7663579664225912\n",
            "7      \t [-0.8823456   1.11145957]. \t  -14.627184329389557 \t -1.7663579664225912\n",
            "8      \t [-1.95789654  1.85765592]. \t  -399.08936370091635 \t -1.7663579664225912\n",
            "9      \t [0.15475859 0.27349949]. \t  -6.941916690848477 \t -1.7663579664225912\n",
            "10     \t [-1.01589236  1.01236841]. \t  -4.102508464377995 \t -1.7663579664225912\n",
            "11     \t [1.25007914 1.99328278]. \t  -18.60287771607783 \t -1.7663579664225912\n",
            "12     \t [-1.80176246 -0.98723994]. \t  -1800.1765291487204 \t -1.7663579664225912\n",
            "13     \t [1.74296737 1.99955686]. \t  -108.37496791925314 \t -1.7663579664225912\n",
            "14     \t [1.13611964 1.32949764]. \t  \u001b[92m-0.16852826950798047\u001b[0m \t -0.16852826950798047\n",
            "15     \t [1.32792035 1.61792688]. \t  -2.2229733597680563 \t -0.16852826950798047\n",
            "16     \t [0.58374347 0.55747679]. \t  -4.870040290106109 \t -0.16852826950798047\n",
            "17     \t [0.77854644 0.94279953]. \t  -11.383371291769235 \t -0.16852826950798047\n",
            "18     \t [1.10113523 1.10185109]. \t  -1.2345200208772742 \t -0.16852826950798047\n",
            "19     \t [1.09059446 1.13807512]. \t  -0.2715933930879733 \t -0.16852826950798047\n",
            "20     \t [1.39236348 1.95887501]. \t  -0.19474890023561603 \t -0.16852826950798047\n",
            "21     \t [ 0.22431893 -0.01304156]. \t  -1.0031369957016563 \t -0.16852826950798047\n",
            "22     \t [0.00385851 0.24199403]. \t  -6.847688499682927 \t -0.16852826950798047\n",
            "23     \t [1.20434455 1.33203822]. \t  -1.443792258568382 \t -0.16852826950798047\n",
            "24     \t [0.32940244 0.11433151]. \t  -0.4530947858990708 \t -0.16852826950798047\n",
            "25     \t [ 1.91918506 -0.9411309 ]. \t  -2139.354445757398 \t -0.16852826950798047\n",
            "26     \t [-1.92924409 -0.31165391]. \t  -1635.6029441475737 \t -0.16852826950798047\n",
            "27     \t [ 1.18926945 -0.81746395]. \t  -498.14045549179895 \t -0.16852826950798047\n",
            "28     \t [-0.87167063  2.02376863]. \t  -163.2623686749242 \t -0.16852826950798047\n",
            "29     \t [0.32883387 0.0871576 ]. \t  -0.49445529971649393 \t -0.16852826950798047\n",
            "30     \t [0.41200002 0.01280776]. \t  -2.8086427334069453 \t -0.16852826950798047\n",
            "31     \t [1.6702811  1.70236723]. \t  -118.70875420772408 \t -0.16852826950798047\n",
            "32     \t [ 0.0462226  -1.42077002]. \t  -203.3759959397796 \t -0.16852826950798047\n",
            "33     \t [1.05681225 1.10630803]. \t  \u001b[92m-0.014345462707764456\u001b[0m \t -0.014345462707764456\n",
            "34     \t [-0.27173737 -0.21123165]. \t  -9.743968643323559 \t -0.014345462707764456\n",
            "35     \t [ 0.61123068 -0.5270834 ]. \t  -81.27473123256354 \t -0.014345462707764456\n",
            "36     \t [ 0.66653116 -2.02961839]. \t  -612.1205091047414 \t -0.014345462707764456\n",
            "37     \t [1.12024044 1.27597448]. \t  -0.058708410729165106 \t -0.014345462707764456\n",
            "38     \t [0.52846599 0.29464055]. \t  -0.24595034572852495 \t -0.014345462707764456\n",
            "39     \t [1.03110902 1.07090098]. \t  \u001b[92m-0.0069201721416625875\u001b[0m \t -0.0069201721416625875\n",
            "40     \t [ 0.01331491 -0.54313407]. \t  -30.49227055750872 \t -0.0069201721416625875\n",
            "41     \t [ 1.41133283 -0.74573807]. \t  -749.6137111964568 \t -0.0069201721416625875\n",
            "42     \t [-1.57117056 -1.41562841]. \t  -1515.3160328313832 \t -0.0069201721416625875\n",
            "43     \t [-1.14378271  1.53790107]. \t  -9.870275462570547 \t -0.0069201721416625875\n",
            "44     \t [ 0.30717576 -1.52429166]. \t  -262.48233492832753 \t -0.0069201721416625875\n",
            "45     \t [0.99291052 0.99428759]. \t  -0.007133643108056444 \t -0.0069201721416625875\n",
            "46     \t [1.01737628 1.09373932]. \t  -0.34469266412947236 \t -0.0069201721416625875\n",
            "47     \t [ 0.62318168 -1.80127686]. \t  -479.5909388679256 \t -0.0069201721416625875\n",
            "48     \t [1.39580685 1.96360776]. \t  -0.1801669837960281 \t -0.0069201721416625875\n",
            "49     \t [0.3848562 0.17038  ]. \t  -0.4279780780530545 \t -0.0069201721416625875\n",
            "50     \t [ 1.73554344 -1.81805725]. \t  -2333.593578206008 \t -0.0069201721416625875\n",
            "51     \t [1.83553193 0.33688002]. \t  -920.1808958748647 \t -0.0069201721416625875\n",
            "52     \t [-1.52331428 -1.84815522]. \t  -1744.1244075997931 \t -0.0069201721416625875\n",
            "53     \t [-0.73564415 -0.06838934]. \t  -40.169002790667555 \t -0.0069201721416625875\n",
            "54     \t [0.66462833 0.28171671]. \t  -2.6729253866224685 \t -0.0069201721416625875\n",
            "55     \t [1.39556159 0.18113757]. \t  -312.1926517882368 \t -0.0069201721416625875\n",
            "56     \t [-0.5791225   0.29489406]. \t  -2.6575622324992167 \t -0.0069201721416625875\n",
            "57     \t [-2.04673134  0.73703322]. \t  -1200.965413094416 \t -0.0069201721416625875\n",
            "58     \t [-0.36103456  1.36325542]. \t  -153.85898978774878 \t -0.0069201721416625875\n",
            "59     \t [-0.90866029 -2.02634107]. \t  -817.0360048393575 \t -0.0069201721416625875\n",
            "60     \t [1.22290955 0.92966209]. \t  -32.06782246041859 \t -0.0069201721416625875\n",
            "61     \t [-0.79383284  0.19903415]. \t  -21.80569745199871 \t -0.0069201721416625875\n",
            "62     \t [ 1.92822791 -0.76753784]. \t  -2012.9229642283742 \t -0.0069201721416625875\n",
            "63     \t [ 1.57226794 -1.26678748]. \t  -1398.200459272032 \t -0.0069201721416625875\n",
            "64     \t [0.9881643  0.92755245]. \t  -0.23941983709691012 \t -0.0069201721416625875\n",
            "65     \t [-1.52145044  2.02984829]. \t  -14.478111269286732 \t -0.0069201721416625875\n",
            "66     \t [0.92085226 0.39632553]. \t  -20.404436941501736 \t -0.0069201721416625875\n",
            "67     \t [1.13507292 0.62139845]. \t  -44.50608969802986 \t -0.0069201721416625875\n",
            "68     \t [ 0.90297006 -0.3583112 ]. \t  -137.75863405171646 \t -0.0069201721416625875\n",
            "69     \t [-1.39612625 -0.66939262]. \t  -691.42765845403 \t -0.0069201721416625875\n",
            "70     \t [0.92354311 0.81902767]. \t  -0.12079525787715831 \t -0.0069201721416625875\n",
            "71     \t [-1.45974772 -1.46417609]. \t  -1298.4812654067393 \t -0.0069201721416625875\n",
            "72     \t [-1.00611908 -1.01242955]. \t  -413.96760905821293 \t -0.0069201721416625875\n",
            "73     \t [ 0.34525869 -0.27345225]. \t  -15.846544763902157 \t -0.0069201721416625875\n",
            "74     \t [-1.88198369  0.9001927 ]. \t  -706.147826850092 \t -0.0069201721416625875\n",
            "75     \t [-1.39046132  2.03472284]. \t  -6.741288318450928 \t -0.0069201721416625875\n",
            "76     \t [0.94375639 0.86248619]. \t  -0.08263062507113914 \t -0.0069201721416625875\n",
            "77     \t [-1.38585433  1.52392818]. \t  -21.42653644467692 \t -0.0069201721416625875\n",
            "78     \t [ 0.21168654 -2.04525519]. \t  -437.45918748948486 \t -0.0069201721416625875\n",
            "79     \t [1.33366641 1.76579868]. \t  -0.1278902845389587 \t -0.0069201721416625875\n",
            "80     \t [ 1.07400472 -0.48560451]. \t  -268.6672967922276 \t -0.0069201721416625875\n",
            "81     \t [0.00470101 1.36244757]. \t  -186.6109362309 \t -0.0069201721416625875\n",
            "82     \t [ 1.73435855 -0.9476757 ]. \t  -1565.2759861532109 \t -0.0069201721416625875\n",
            "83     \t [ 0.15233627 -0.3486703 ]. \t  -14.547756874936322 \t -0.0069201721416625875\n",
            "84     \t [-1.8808535  -1.48661555]. \t  -2532.583437800548 \t -0.0069201721416625875\n",
            "85     \t [ 1.07502315 -1.54739593]. \t  -730.6647565439483 \t -0.0069201721416625875\n",
            "86     \t [-0.66106048 -1.2168329 ]. \t  -276.275766282547 \t -0.0069201721416625875\n",
            "87     \t [-1.39539079 -0.90342888]. \t  -818.2981917420883 \t -0.0069201721416625875\n",
            "88     \t [-0.4204121   0.06342461]. \t  -3.3017519346355586 \t -0.0069201721416625875\n",
            "89     \t [0.66149648 1.53770358]. \t  -121.14230374289957 \t -0.0069201721416625875\n",
            "90     \t [-1.42651633 -0.83545454]. \t  -829.8095373075021 \t -0.0069201721416625875\n",
            "91     \t [ 0.33844729 -0.41988778]. \t  -28.999659494663035 \t -0.0069201721416625875\n",
            "92     \t [1.01822452 1.0009088 ]. \t  -0.12901492330007186 \t -0.0069201721416625875\n",
            "93     \t [ 0.73320313 -2.01624691]. \t  -652.2778548155615 \t -0.0069201721416625875\n",
            "94     \t [1.3049085 0.6343088]. \t  -114.25736576089821 \t -0.0069201721416625875\n",
            "95     \t [1.08881537 1.18329627]. \t  -0.00838218409886892 \t -0.0069201721416625875\n",
            "96     \t [-0.84241255  1.53069914]. \t  -70.80519133967313 \t -0.0069201721416625875\n",
            "97     \t [-0.48570409  1.39982826]. \t  -137.6782461015903 \t -0.0069201721416625875\n",
            "98     \t [-0.84467376 -1.08464724]. \t  -326.7267386814881 \t -0.0069201721416625875\n",
            "99     \t [-0.169081   1.8286972]. \t  -325.40592360796836 \t -0.0069201721416625875\n",
            "100    \t [1.0736673  1.15605205]. \t  \u001b[92m-0.0065096651679119495\u001b[0m \t -0.0065096651679119495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YT-CgKvuv4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae3f43c4-57ba-4310-dfb8-2867d87faa5f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_winner_19 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_19 = dGPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.24348908 1.07107119]. \t  -102.94304782477323 \t -4.219752052396591\n",
            "init   \t [-1.04639875  1.11284007]. \t  -4.219752052396591 \t -4.219752052396591\n",
            "init   \t [-0.59608235  1.21527255]. \t  -76.50031976976825 \t -4.219752052396591\n",
            "init   \t [-0.63078391 -0.22989031]. \t  -42.07005958957044 \t -4.219752052396591\n",
            "init   \t [-0.8056685  -0.23710138]. \t  -81.79603430667063 \t -4.219752052396591\n",
            "1      \t [ 0.44968884 -0.60729629]. \t  -65.83451268210109 \t -4.219752052396591\n",
            "2      \t [-1.91146227  1.53976904]. \t  -455.3419578559051 \t -4.219752052396591\n",
            "3      \t [-0.77078409  0.54425243]. \t  \u001b[92m-3.3842351790654344\u001b[0m \t -3.3842351790654344\n",
            "4      \t [1.5102531  1.30748176]. \t  -95.00774018067713 \t -3.3842351790654344\n",
            "5      \t [-0.05010178 -0.19063127]. \t  -4.833075892256382 \t -3.3842351790654344\n",
            "6      \t [-0.21883797 -0.94405562]. \t  -99.88118807940927 \t -3.3842351790654344\n",
            "7      \t [-1.12134615  0.71328909]. \t  -34.10764757777547 \t -3.3842351790654344\n",
            "8      \t [0.86718777 0.38448612]. \t  -13.525359581074524 \t -3.3842351790654344\n",
            "9      \t [ 1.908228   -1.76639366]. \t  -2925.176808602748 \t -3.3842351790654344\n",
            "10     \t [-0.87121242  0.78668101]. \t  -3.577998406110087 \t -3.3842351790654344\n",
            "11     \t [0.83889826 0.78763991]. \t  \u001b[92m-0.7297005765149895\u001b[0m \t -0.7297005765149895\n",
            "12     \t [0.8226043  0.86534855]. \t  -3.5911331260262944 \t -0.7297005765149895\n",
            "13     \t [-1.53636148  0.48205596]. \t  -359.2532426805832 \t -0.7297005765149895\n",
            "14     \t [ 0.10189807 -0.32339131]. \t  -11.947130367188748 \t -0.7297005765149895\n",
            "15     \t [-0.35441034  0.01257789]. \t  -3.111978405601779 \t -0.7297005765149895\n",
            "16     \t [-1.07346365  2.02037921]. \t  -79.6511994217322 \t -0.7297005765149895\n",
            "17     \t [0.43856967 0.09882511]. \t  -1.1897702693533614 \t -0.7297005765149895\n",
            "18     \t [-0.33469581  1.59631909]. \t  -222.0954117556684 \t -0.7297005765149895\n",
            "19     \t [0.73850107 0.59346067]. \t  \u001b[92m-0.29952000738086426\u001b[0m \t -0.29952000738086426\n",
            "20     \t [-1.75212771  1.1013791 ]. \t  -395.1019391827967 \t -0.29952000738086426\n",
            "21     \t [0.34393268 0.05930132]. \t  -0.778387156810874 \t -0.29952000738086426\n",
            "22     \t [ 1.46618443 -1.4380605 ]. \t  -1287.4175604866616 \t -0.29952000738086426\n",
            "23     \t [-0.13416585 -1.957137  ]. \t  -391.40313781338085 \t -0.29952000738086426\n",
            "24     \t [1.7389583  0.77709409]. \t  -505.39388630928266 \t -0.29952000738086426\n",
            "25     \t [1.5510659  2.03847483]. \t  -13.796851474855854 \t -0.29952000738086426\n",
            "26     \t [0.6981431  0.44858296]. \t  \u001b[92m-0.2418232592397735\u001b[0m \t -0.2418232592397735\n",
            "27     \t [-0.02586991 -1.69586234]. \t  -288.8743529540324 \t -0.2418232592397735\n",
            "28     \t [1.6670468  2.02431938]. \t  -57.40603075340769 \t -0.2418232592397735\n",
            "29     \t [1.28405436 1.81571248]. \t  -2.8668109406056677 \t -0.2418232592397735\n",
            "30     \t [-1.91633656 -1.2327526 ]. \t  -2414.5040714166985 \t -0.2418232592397735\n",
            "31     \t [1.1861969  1.38579799]. \t  \u001b[92m-0.07988975840639506\u001b[0m \t -0.07988975840639506\n",
            "32     \t [0.34617654 0.10474969]. \t  -0.45025141441944033 \t -0.07988975840639506\n",
            "33     \t [0.58747614 0.31057717]. \t  -0.28955336596200976 \t -0.07988975840639506\n",
            "34     \t [-0.09910605  0.67430144]. \t  -45.36132581601227 \t -0.07988975840639506\n",
            "35     \t [-1.22222237  1.39634206]. \t  -5.88861363497911 \t -0.07988975840639506\n",
            "36     \t [1.33964481 2.04209261]. \t  -6.2382313258674476 \t -0.07988975840639506\n",
            "37     \t [0.01824425 1.73847894]. \t  -303.07902757254914 \t -0.07988975840639506\n",
            "38     \t [-2.00747148 -1.12311436]. \t  -2664.44361781031 \t -0.07988975840639506\n",
            "39     \t [1.38475914 1.86108684]. \t  -0.4669372748868132 \t -0.07988975840639506\n",
            "40     \t [0.3122556  1.58620122]. \t  -222.09506467666787 \t -0.07988975840639506\n",
            "41     \t [1.25311638 1.54739459]. \t  -0.1165367478613851 \t -0.07988975840639506\n",
            "42     \t [ 0.99398845 -0.15613225]. \t  -130.90688137199103 \t -0.07988975840639506\n",
            "43     \t [1.23360293 1.5329146 ]. \t  \u001b[92m-0.0669767379040331\u001b[0m \t -0.0669767379040331\n",
            "44     \t [0.92509577 0.88089211]. \t  -0.06856105693177589 \t -0.0669767379040331\n",
            "45     \t [ 1.05783634 -1.11400089]. \t  -498.64055184503474 \t -0.0669767379040331\n",
            "46     \t [ 0.04979269 -1.63019338]. \t  -267.46490688217597 \t -0.0669767379040331\n",
            "47     \t [0.88677218 0.757011  ]. \t  -0.09898568049552457 \t -0.0669767379040331\n",
            "48     \t [ 1.33061395 -0.75013617]. \t  -635.4868590503045 \t -0.0669767379040331\n",
            "49     \t [-1.55336905 -0.94182605]. \t  -1131.975543495534 \t -0.0669767379040331\n",
            "50     \t [-0.47289011  2.02904991]. \t  -328.125297134358 \t -0.0669767379040331\n",
            "51     \t [1.24616975 1.55785379]. \t  \u001b[92m-0.06301501631758842\u001b[0m \t -0.06301501631758842\n",
            "52     \t [-0.8913757  -1.21622906]. \t  -407.9008014958416 \t -0.06301501631758842\n",
            "53     \t [1.01474924 1.05742917]. \t  -0.07701940804212183 \t -0.06301501631758842\n",
            "54     \t [-1.15937229  1.48385898]. \t  -6.614913273508305 \t -0.06301501631758842\n",
            "55     \t [1.79093948 0.91207222]. \t  -527.5080241904294 \t -0.06301501631758842\n",
            "56     \t [1.28405846 1.62093347]. \t  -0.1583776989636932 \t -0.06301501631758842\n",
            "57     \t [1.16552328 0.63863447]. \t  -51.840050038487895 \t -0.06301501631758842\n",
            "58     \t [-1.88003489  1.2894185 ]. \t  -512.3476918814229 \t -0.06301501631758842\n",
            "59     \t [0.07486711 0.19051247]. \t  -4.274945064997554 \t -0.06301501631758842\n",
            "60     \t [0.88993571 0.79143583]. \t  \u001b[92m-0.01214436931057295\u001b[0m \t -0.01214436931057295\n",
            "61     \t [0.94937661 0.86932405]. \t  -0.10491083821882738 \t -0.01214436931057295\n",
            "62     \t [1.02234657 1.04675329]. \t  \u001b[92m-0.0007429711644439714\u001b[0m \t -0.0007429711644439714\n",
            "63     \t [-1.26788648  1.07391939]. \t  -33.61799191629573 \t -0.0007429711644439714\n",
            "64     \t [1.08412302 1.15192111]. \t  -0.061840240111214015 \t -0.0007429711644439714\n",
            "65     \t [0.00666815 0.1064489 ]. \t  -2.1188986542501347 \t -0.0007429711644439714\n",
            "66     \t [-0.99005096 -1.41917455]. \t  -579.6605564382465 \t -0.0007429711644439714\n",
            "67     \t [1.36961344 1.87649173]. \t  -0.13665644271410946 \t -0.0007429711644439714\n",
            "68     \t [0.91970871 0.8304249 ]. \t  -0.03028359849395009 \t -0.0007429711644439714\n",
            "69     \t [ 1.25787611 -1.03913335]. \t  -687.2327766306505 \t -0.0007429711644439714\n",
            "70     \t [1.04273824 1.0489271 ]. \t  -0.14909777417499126 \t -0.0007429711644439714\n",
            "71     \t [-1.88464203  0.89496835]. \t  -714.236756096389 \t -0.0007429711644439714\n",
            "72     \t [1.09457603 1.16893772]. \t  -0.0939691660807154 \t -0.0007429711644439714\n",
            "73     \t [0.70714689 0.47395272]. \t  -0.1539048247424365 \t -0.0007429711644439714\n",
            "74     \t [0.91878218 1.19058995]. \t  -12.00791942197261 \t -0.0007429711644439714\n",
            "75     \t [0.93976389 0.87202185]. \t  -0.01602567764880745 \t -0.0007429711644439714\n",
            "76     \t [-0.44059091  0.50493421]. \t  -11.73582806019964 \t -0.0007429711644439714\n",
            "77     \t [-1.63805227  1.99291998]. \t  -54.61007531448305 \t -0.0007429711644439714\n",
            "78     \t [-1.96120296 -0.54796173]. \t  -1939.737311709064 \t -0.0007429711644439714\n",
            "79     \t [-0.25470364  0.21967616]. \t  -3.9706539656228124 \t -0.0007429711644439714\n",
            "80     \t [1.51229804 0.73918524]. \t  -239.8495473713162 \t -0.0007429711644439714\n",
            "81     \t [0.48937658 0.25823161]. \t  -0.29586320843693314 \t -0.0007429711644439714\n",
            "82     \t [ 0.48940055 -1.91505832]. \t  -464.4784249439519 \t -0.0007429711644439714\n",
            "83     \t [1.00462471 0.49002277]. \t  -26.961873973475413 \t -0.0007429711644439714\n",
            "84     \t [-0.22239426 -1.4187918 ]. \t  -217.07035042605008 \t -0.0007429711644439714\n",
            "85     \t [1.42515977 2.04528014]. \t  -0.20092416765883214 \t -0.0007429711644439714\n",
            "86     \t [1.19937156 1.44582364]. \t  -0.04512411116088473 \t -0.0007429711644439714\n",
            "87     \t [-1.6533929  -1.47474582]. \t  -1778.148931412991 \t -0.0007429711644439714\n",
            "88     \t [-1.73150366 -1.92966205]. \t  -2435.7498493694384 \t -0.0007429711644439714\n",
            "89     \t [0.46346717 0.22266945]. \t  -0.29405745378780723 \t -0.0007429711644439714\n",
            "90     \t [ 1.09161653 -1.74010343]. \t  -859.5125247473624 \t -0.0007429711644439714\n",
            "91     \t [0.57770317 0.37451944]. \t  -0.34462310723257716 \t -0.0007429711644439714\n",
            "92     \t [ 1.86123761 -1.17692962]. \t  -2154.755215436421 \t -0.0007429711644439714\n",
            "93     \t [-0.48850096 -0.58947631]. \t  -70.79216904289397 \t -0.0007429711644439714\n",
            "94     \t [0.48560572 0.26495299]. \t  -0.34951582377183243 \t -0.0007429711644439714\n",
            "95     \t [ 0.07153458 -0.88529216]. \t  -80.14492959171838 \t -0.0007429711644439714\n",
            "96     \t [0.26290806 1.8942863 ]. \t  -333.66626838296577 \t -0.0007429711644439714\n",
            "97     \t [0.98284828 0.81183232]. \t  -2.376776374037633 \t -0.0007429711644439714\n",
            "98     \t [ 0.17833914 -1.71005162]. \t  -304.0815231773848 \t -0.0007429711644439714\n",
            "99     \t [0.54082062 0.32209947]. \t  -0.29853584606526873 \t -0.0007429711644439714\n",
            "100    \t [-0.01542798  2.0425412 ]. \t  -418.1313208144742 \t -0.0007429711644439714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHz_Jg2_uv7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96073411-cc26-48b4-df78-413d1cfae895"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_winner_20 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_20 = dGPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.2888388  0.26430978]. \t  -3.777577453542735 \t -3.777577453542735\n",
            "init   \t [-0.04734225 -0.66978712]. \t  -46.25914429040042 \t -3.777577453542735\n",
            "init   \t [-0.50844394  0.13121892]. \t  -3.895838569726608 \t -3.777577453542735\n",
            "init   \t [-1.76903664  0.34623103]. \t  -782.3209705533411 \t -3.777577453542735\n",
            "init   \t [-1.07357076 -1.38954104]. \t  -650.524506936468 \t -3.777577453542735\n",
            "1      \t [-0.11974724  0.10401632]. \t  \u001b[92m-2.0580289031443395\u001b[0m \t -2.0580289031443395\n",
            "2      \t [1.57560185 0.62167074]. \t  -346.6077544697629 \t -2.0580289031443395\n",
            "3      \t [-0.17717637  1.32492342]. \t  -168.70823568062502 \t -2.0580289031443395\n",
            "4      \t [ 0.99329625 -1.29151482]. \t  -518.9978150564089 \t -2.0580289031443395\n",
            "5      \t [-0.30952852 -0.16999032]. \t  -8.779734388335175 \t -2.0580289031443395\n",
            "6      \t [-0.25756822  0.14444145]. \t  -2.191439889836391 \t -2.0580289031443395\n",
            "7      \t [ 0.35053821 -0.02231051]. \t  -2.5297430099047324 \t -2.0580289031443395\n",
            "8      \t [ 0.10421039 -0.0310265 ]. \t  \u001b[92m-0.9778853034766023\u001b[0m \t -0.9778853034766023\n",
            "9      \t [-0.48316149  0.04533448]. \t  -5.738325805710129 \t -0.9778853034766023\n",
            "10     \t [0.21359259 0.10883187]. \t  -1.0179879435964603 \t -0.9778853034766023\n",
            "11     \t [0.31166293 0.12442682]. \t  \u001b[92m-0.5482989182627084\u001b[0m \t -0.5482989182627084\n",
            "12     \t [ 1.61316771 -0.64057739]. \t  -1052.0078787746854 \t -0.5482989182627084\n",
            "13     \t [0.43348374 0.24789285]. \t  -0.6807571136465769 \t -0.5482989182627084\n",
            "14     \t [-0.09641529 -0.55990481]. \t  -33.60107280746357 \t -0.5482989182627084\n",
            "15     \t [-0.82508405  1.2406861 ]. \t  -34.68224222505055 \t -0.5482989182627084\n",
            "16     \t [-1.1928889   1.88219801]. \t  -25.896518021218792 \t -0.5482989182627084\n",
            "17     \t [ 0.53483739 -1.11396349]. \t  -196.22044177322152 \t -0.5482989182627084\n",
            "18     \t [ 1.08585101 -0.14887929]. \t  -176.35294415771122 \t -0.5482989182627084\n",
            "19     \t [-1.50941603 -1.10529415]. \t  -1151.1929674840562 \t -0.5482989182627084\n",
            "20     \t [0.38644135 0.13973336]. \t  \u001b[92m-0.3856770475051456\u001b[0m \t -0.3856770475051456\n",
            "21     \t [0.59758858 1.12328572]. \t  -58.864134396647785 \t -0.3856770475051456\n",
            "22     \t [-1.2151875   1.40706614]. \t  -5.391673848653323 \t -0.3856770475051456\n",
            "23     \t [1.10529464 0.8664695 ]. \t  -12.628270272037758 \t -0.3856770475051456\n",
            "24     \t [1.4387704  1.99626182]. \t  -0.7371404119503033 \t -0.3856770475051456\n",
            "25     \t [1.28228834 1.5791552 ]. \t  -0.503594388457324 \t -0.3856770475051456\n",
            "26     \t [0.91597724 0.94471673]. \t  -1.1243600447210667 \t -0.3856770475051456\n",
            "27     \t [0.84926969 0.70918189]. \t  \u001b[92m-0.03730530510363653\u001b[0m \t -0.03730530510363653\n",
            "28     \t [0.4856285  0.23864261]. \t  -0.2653662874851787 \t -0.03730530510363653\n",
            "29     \t [1.11036593 1.21933424]. \t  \u001b[92m-0.03061754712837163\u001b[0m \t -0.03061754712837163\n",
            "30     \t [1.39054661 1.75014118]. \t  -3.5189702900682422 \t -0.03061754712837163\n",
            "31     \t [-1.96944417 -0.5409523 ]. \t  -1962.1593894476853 \t -0.03061754712837163\n",
            "32     \t [-1.01920497  0.91902644]. \t  -5.511250627687939 \t -0.03061754712837163\n",
            "33     \t [1.31026362 1.78829515]. \t  -0.6075513136905792 \t -0.03061754712837163\n",
            "34     \t [0.97203986 0.93988444]. \t  \u001b[92m-0.0032588749019199564\u001b[0m \t -0.0032588749019199564\n",
            "35     \t [-0.85440648 -1.43697053]. \t  -473.019468218265 \t -0.0032588749019199564\n",
            "36     \t [0.99617373 1.02826434]. \t  -0.12891181844143038 \t -0.0032588749019199564\n",
            "37     \t [1.16588477 1.33200306]. \t  -0.10196074986438125 \t -0.0032588749019199564\n",
            "38     \t [ 0.23765545 -1.4905528 ]. \t  -239.91225349165356 \t -0.0032588749019199564\n",
            "39     \t [1.40438294 2.04300794]. \t  -0.6636078475932006 \t -0.0032588749019199564\n",
            "40     \t [ 1.48392645 -0.54976866]. \t  -757.4780105654045 \t -0.0032588749019199564\n",
            "41     \t [ 1.60135641 -0.5268925 ]. \t  -955.9349118832197 \t -0.0032588749019199564\n",
            "42     \t [-0.07030552  1.56656092]. \t  -245.01064869931335 \t -0.0032588749019199564\n",
            "43     \t [-1.5345509   1.86385239]. \t  -30.531466036238594 \t -0.0032588749019199564\n",
            "44     \t [1.10738795 1.23454028]. \t  -0.01830908801291715 \t -0.0032588749019199564\n",
            "45     \t [-1.26266081  0.44706352]. \t  -136.73761462981489 \t -0.0032588749019199564\n",
            "46     \t [0.95672611 0.89522026]. \t  -0.042292037124651904 \t -0.0032588749019199564\n",
            "47     \t [-1.61358248 -0.27306354]. \t  -834.3779756965681 \t -0.0032588749019199564\n",
            "48     \t [-0.73357865 -0.93107915]. \t  -218.86508952773173 \t -0.0032588749019199564\n",
            "49     \t [-1.88685634  0.14283044]. \t  -1176.193766234887 \t -0.0032588749019199564\n",
            "50     \t [-1.57004557 -1.58272228]. \t  -1645.0455727282526 \t -0.0032588749019199564\n",
            "51     \t [0.96374424 0.91683722]. \t  -0.015632378902909548 \t -0.0032588749019199564\n",
            "52     \t [0.95958742 0.4540251 ]. \t  -21.79026178952775 \t -0.0032588749019199564\n",
            "53     \t [1.27625873 0.58243956]. \t  -109.57094006309288 \t -0.0032588749019199564\n",
            "54     \t [1.16727424 1.37071159]. \t  -0.03467588777220765 \t -0.0032588749019199564\n",
            "55     \t [0.84650621 0.67813071]. \t  -0.1713395092054355 \t -0.0032588749019199564\n",
            "56     \t [1.16066343 1.40133363]. \t  -0.3195120506570094 \t -0.0032588749019199564\n",
            "57     \t [0.159372   1.26646341]. \t  -154.73063488532586 \t -0.0032588749019199564\n",
            "58     \t [1.22106511 1.47982986]. \t  -0.06134699780972379 \t -0.0032588749019199564\n",
            "59     \t [1.01939187 1.03552353]. \t  \u001b[92m-0.001698275401478381\u001b[0m \t -0.001698275401478381\n",
            "60     \t [1.09502184 1.23115969]. \t  -0.11198580225979698 \t -0.001698275401478381\n",
            "61     \t [ 0.05867034 -1.4849026 ]. \t  -222.40312704335412 \t -0.001698275401478381\n",
            "62     \t [1.17938961 0.52339748]. \t  -75.29862545450827 \t -0.001698275401478381\n",
            "63     \t [ 0.9797235  -1.05616155]. \t  -406.4339491431488 \t -0.001698275401478381\n",
            "64     \t [0.89953123 0.79230413]. \t  -0.038494015891723674 \t -0.001698275401478381\n",
            "65     \t [0.75258149 0.56861868]. \t  -0.06171757795640556 \t -0.001698275401478381\n",
            "66     \t [-0.31349505 -0.18238048]. \t  -9.602251893713131 \t -0.001698275401478381\n",
            "67     \t [1.70470046 0.46799914]. \t  -594.8832102096301 \t -0.001698275401478381\n",
            "68     \t [1.22010126 1.48256734]. \t  -0.052140900376403884 \t -0.001698275401478381\n",
            "69     \t [1.26030215 0.3072364 ]. \t  -164.19590973551527 \t -0.001698275401478381\n",
            "70     \t [1.76491521 1.48479772]. \t  -266.3168113776189 \t -0.001698275401478381\n",
            "71     \t [1.12461771 1.28626074]. \t  -0.061736293773171004 \t -0.001698275401478381\n",
            "72     \t [-1.66686773 -1.97826056]. \t  -2269.73983224685 \t -0.001698275401478381\n",
            "73     \t [-0.19238578  0.87592883]. \t  -71.7998798743416 \t -0.001698275401478381\n",
            "74     \t [1.22371457 1.48714945]. \t  -0.06071476267657547 \t -0.001698275401478381\n",
            "75     \t [0.58494512 0.5863745 ]. \t  -6.1363039550597644 \t -0.001698275401478381\n",
            "76     \t [1.15840049 1.34527213]. \t  -0.02623344085646881 \t -0.001698275401478381\n",
            "77     \t [0.79135292 1.58866863]. \t  -92.67052567735936 \t -0.001698275401478381\n",
            "78     \t [-0.45584474 -1.99917753]. \t  -489.1920031520368 \t -0.001698275401478381\n",
            "79     \t [1.28307294 1.67111241]. \t  -0.1418141946795645 \t -0.001698275401478381\n",
            "80     \t [ 1.05987951 -1.51600787]. \t  -696.6217148154801 \t -0.001698275401478381\n",
            "81     \t [ 1.21183187 -0.47090083]. \t  -376.1865774429796 \t -0.001698275401478381\n",
            "82     \t [ 0.53417678 -0.92816523]. \t  -147.47765806935433 \t -0.001698275401478381\n",
            "83     \t [0.66456458 0.44058435]. \t  -0.11262964677660582 \t -0.001698275401478381\n",
            "84     \t [-0.16315989 -0.73093746]. \t  -58.74244509940196 \t -0.001698275401478381\n",
            "85     \t [ 1.30373351 -0.58779085]. \t  -523.3633306799418 \t -0.001698275401478381\n",
            "86     \t [-0.81524242  1.15193323]. \t  -27.04250355779008 \t -0.001698275401478381\n",
            "87     \t [1.32893734 1.75724067]. \t  -0.11600335159479075 \t -0.001698275401478381\n",
            "88     \t [ 1.8398177  -1.57063984]. \t  -2456.47172122131 \t -0.001698275401478381\n",
            "89     \t [0.06839996 1.52341975]. \t  -231.52535975324432 \t -0.001698275401478381\n",
            "90     \t [-0.1630432   1.27105152]. \t  -156.22283907595943 \t -0.001698275401478381\n",
            "91     \t [-0.94617664  1.40007007]. \t  -29.27191044915864 \t -0.001698275401478381\n",
            "92     \t [1.04503976 1.08390155]. \t  -0.00876330905533472 \t -0.001698275401478381\n",
            "93     \t [-0.18733043  1.95873802]. \t  -371.45089028383757 \t -0.001698275401478381\n",
            "94     \t [ 1.18442453 -0.54371545]. \t  -378.9501784625779 \t -0.001698275401478381\n",
            "95     \t [-0.8874626 -0.222455 ]. \t  -105.58157972927293 \t -0.001698275401478381\n",
            "96     \t [-1.36158874  1.10716286]. \t  -61.342308474066506 \t -0.001698275401478381\n",
            "97     \t [ 1.02460023 -0.18049389]. \t  -151.36429556787274 \t -0.001698275401478381\n",
            "98     \t [0.04630707 1.35580958]. \t  -184.1504868396169 \t -0.001698275401478381\n",
            "99     \t [1.22683151 1.52399769]. \t  -0.08710607573335319 \t -0.001698275401478381\n",
            "100    \t [1.04147996 1.09470167]. \t  -0.011762966578556024 \t -0.001698275401478381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUnhsKpCuv9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6020066-58af-4625-8e95-0837e1b5402d"
      },
      "source": [
        "end_win = time.time()\r\n",
        "end_win\r\n",
        "\r\n",
        "time_win = end_win - start_win\r\n",
        "time_win"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3516.657824754715"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJG0SLpwuwAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1fc367c-b2c9-4698-c882-66acfcc5ffb8"
      },
      "source": [
        "### Training regret minimization: run number = 1\r\n",
        "\r\n",
        "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_1 = np.log(y_global_orig - loser_output_1)\r\n",
        "regret_winner_1 = np.log(y_global_orig - winner_output_1)\r\n",
        "\r\n",
        "train_regret_loser_1 = min_max_array(regret_loser_1)\r\n",
        "train_regret_winner_1 = min_max_array(regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1 = min(train_regret_loser_1)\r\n",
        "min_train_regret_winner_1 = min(train_regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1, min_train_regret_winner_1"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.458439884041693, -5.458439884041693)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lA9eZf0uwCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb6916b-be28-4149-fa7e-064094c07ffc"
      },
      "source": [
        "### Training regret minimization: run number = 2\r\n",
        "\r\n",
        "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_2 = np.log(y_global_orig - loser_output_2)\r\n",
        "regret_winner_2 = np.log(y_global_orig - winner_output_2)\r\n",
        "\r\n",
        "train_regret_loser_2 = min_max_array(regret_loser_2)\r\n",
        "train_regret_winner_2 = min_max_array(regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2 = min(train_regret_loser_2)\r\n",
        "min_train_regret_winner_2 = min(train_regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2, min_train_regret_winner_2"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.63379973658395, -4.7083536917972735)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glrTGcpAuwFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cb4ba40-f243-4939-abc9-bcc3dbf7d044"
      },
      "source": [
        "### Training regret minimization: run number = 3\r\n",
        "\r\n",
        "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_3 = np.log(y_global_orig - loser_output_3)\r\n",
        "regret_winner_3 = np.log(y_global_orig - winner_output_3)\r\n",
        "\r\n",
        "train_regret_loser_3 = min_max_array(regret_loser_3)\r\n",
        "train_regret_winner_3 = min_max_array(regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3 = min(train_regret_loser_3)\r\n",
        "min_train_regret_winner_3 = min(train_regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3, min_train_regret_winner_3"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.219348312441808, -5.219348312441808)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaRwfbxeuwIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa71eec-9ab1-4359-fff6-337f01c80b7a"
      },
      "source": [
        "### Training regret minimization: run number = 4\r\n",
        "\r\n",
        "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_4 = np.log(y_global_orig - loser_output_4)\r\n",
        "regret_winner_4 = np.log(y_global_orig - winner_output_4)\r\n",
        "\r\n",
        "train_regret_loser_4 = min_max_array(regret_loser_4)\r\n",
        "train_regret_winner_4 = min_max_array(regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4 = min(train_regret_loser_4)\r\n",
        "min_train_regret_winner_4 = min(train_regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4, min_train_regret_winner_4"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.581305235262819, -4.444708804133774)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nb5NkfyuwKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f484b3c2-bd71-429d-f863-6be65c3514c2"
      },
      "source": [
        "### Training regret minimization: run number = 5\r\n",
        "\r\n",
        "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_5 = np.log(y_global_orig - loser_output_5)\r\n",
        "regret_winner_5 = np.log(y_global_orig - winner_output_5)\r\n",
        "\r\n",
        "train_regret_loser_5 = min_max_array(regret_loser_5)\r\n",
        "train_regret_winner_5 = min_max_array(regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5 = min(train_regret_loser_5)\r\n",
        "min_train_regret_winner_5 = min(train_regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5, min_train_regret_winner_5"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.667298642056275, -5.667298642056275)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0Q-WfXbuwNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c00fb3da-838e-491c-df9a-2f902ad5a0a3"
      },
      "source": [
        "### Training regret minimization: run number = 6\r\n",
        "\r\n",
        "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_6 = np.log(y_global_orig - loser_output_6)\r\n",
        "regret_winner_6 = np.log(y_global_orig - winner_output_6)\r\n",
        "\r\n",
        "train_regret_loser_6 = min_max_array(regret_loser_6)\r\n",
        "train_regret_winner_6 = min_max_array(regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6 = min(train_regret_loser_6)\r\n",
        "min_train_regret_winner_6 = min(train_regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6, min_train_regret_winner_6"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-6.48362698855227, -4.140158577900879)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqqS7VLcuwPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf269fde-df3b-4f64-b6be-8a5ba71428a4"
      },
      "source": [
        "### Training regret minimization: run number = 7\r\n",
        "\r\n",
        "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_7 = np.log(y_global_orig - loser_output_7)\r\n",
        "regret_winner_7 = np.log(y_global_orig - winner_output_7)\r\n",
        "\r\n",
        "train_regret_loser_7 = min_max_array(regret_loser_7)\r\n",
        "train_regret_winner_7 = min_max_array(regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7 = min(train_regret_loser_7)\r\n",
        "min_train_regret_winner_7 = min(train_regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7, min_train_regret_winner_7"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.909225293887742, -3.374449411352495)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOi5iX8guwSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a9671df-d614-4385-fdb3-7e443fd4f10f"
      },
      "source": [
        "### Training regret minimization: run number = 8\r\n",
        "\r\n",
        "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_8 = np.log(y_global_orig - loser_output_8)\r\n",
        "regret_winner_8 = np.log(y_global_orig - winner_output_8)\r\n",
        "\r\n",
        "train_regret_loser_8 = min_max_array(regret_loser_8)\r\n",
        "train_regret_winner_8 = min_max_array(regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8 = min(train_regret_loser_8)\r\n",
        "min_train_regret_winner_8 = min(train_regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8, min_train_regret_winner_8"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.753182791297979, -9.464399382009917)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFVApeazuwU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286b25ab-fb42-4785-9b8c-b633ba619af1"
      },
      "source": [
        "### Training regret minimization: run number = 9\r\n",
        "\r\n",
        "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_9 = np.log(y_global_orig - loser_output_9)\r\n",
        "regret_winner_9 = np.log(y_global_orig - winner_output_9)\r\n",
        "\r\n",
        "train_regret_loser_9 = min_max_array(regret_loser_9)\r\n",
        "train_regret_winner_9 = min_max_array(regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9 = min(train_regret_loser_9)\r\n",
        "min_train_regret_winner_9 = min(train_regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9, min_train_regret_winner_9"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.363942292904479, -4.2137458828883965)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g92jk9WJuwXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926f4a5a-b287-4b41-aec0-58352aff44a4"
      },
      "source": [
        "### Training regret minimization: run number = 10\r\n",
        "\r\n",
        "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_10 = np.log(y_global_orig - loser_output_10)\r\n",
        "regret_winner_10 = np.log(y_global_orig - winner_output_10)\r\n",
        "\r\n",
        "train_regret_loser_10 = min_max_array(regret_loser_10)\r\n",
        "train_regret_winner_10 = min_max_array(regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10 = min(train_regret_loser_10)\r\n",
        "min_train_regret_winner_10 = min(train_regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10, min_train_regret_winner_10"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.650906541123928, -5.650906541123928)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmcF1x-NuwZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cad9b49-b906-4342-8d6e-3ece3617373e"
      },
      "source": [
        "### Training regret minimization: run number = 11\r\n",
        "\r\n",
        "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_11 = np.log(y_global_orig - loser_output_11)\r\n",
        "regret_winner_11 = np.log(y_global_orig - winner_output_11)\r\n",
        "\r\n",
        "train_regret_loser_11 = min_max_array(regret_loser_11)\r\n",
        "train_regret_winner_11 = min_max_array(regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11 = min(train_regret_loser_11)\r\n",
        "min_train_regret_winner_11 = min(train_regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11, min_train_regret_winner_11"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.174643386313633, -6.099941881341143)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8axVhb6uwcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "082a7d07-91ee-405a-af7d-1ab79d51c42a"
      },
      "source": [
        "### Training regret minimization: run number = 12\r\n",
        "\r\n",
        "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_12 = np.log(y_global_orig - loser_output_12)\r\n",
        "regret_winner_12 = np.log(y_global_orig - winner_output_12)\r\n",
        "\r\n",
        "train_regret_loser_12 = min_max_array(regret_loser_12)\r\n",
        "train_regret_winner_12 = min_max_array(regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12 = min(train_regret_loser_12)\r\n",
        "min_train_regret_winner_12 = min(train_regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12, min_train_regret_winner_12"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.9975693090031, -4.9975693090031)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzlrL8XFuwfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e7ef80-f9c4-4b11-cb6d-eb0a99edd9fc"
      },
      "source": [
        "### Training regret minimization: run number = 13\r\n",
        "\r\n",
        "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_13 = np.log(y_global_orig - loser_output_13)\r\n",
        "regret_winner_13 = np.log(y_global_orig - winner_output_13)\r\n",
        "\r\n",
        "train_regret_loser_13 = min_max_array(regret_loser_13)\r\n",
        "train_regret_winner_13 = min_max_array(regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13 = min(train_regret_loser_13)\r\n",
        "min_train_regret_winner_13 = min(train_regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13, min_train_regret_winner_13"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-8.708311010744215, -8.708311010744215)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uZlLJ1quwh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b36c84-7a38-4117-a9a6-352e52b92033"
      },
      "source": [
        "### Training regret minimization: run number = 14\r\n",
        "\r\n",
        "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_14 = np.log(y_global_orig - loser_output_14)\r\n",
        "regret_winner_14 = np.log(y_global_orig - winner_output_14)\r\n",
        "\r\n",
        "train_regret_loser_14 = min_max_array(regret_loser_14)\r\n",
        "train_regret_winner_14 = min_max_array(regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14 = min(train_regret_loser_14)\r\n",
        "min_train_regret_winner_14 = min(train_regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14, min_train_regret_winner_14"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.310681494569531, -6.166376924272546)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVBcHRiMuwjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "808e7644-5e86-4e38-e05f-7873abf219c2"
      },
      "source": [
        "### Training regret minimization: run number = 15\r\n",
        "\r\n",
        "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_15 = np.log(y_global_orig - loser_output_15)\r\n",
        "regret_winner_15 = np.log(y_global_orig - winner_output_15)\r\n",
        "\r\n",
        "train_regret_loser_15 = min_max_array(regret_loser_15)\r\n",
        "train_regret_winner_15 = min_max_array(regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15 = min(train_regret_loser_15)\r\n",
        "min_train_regret_winner_15 = min(train_regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15, min_train_regret_winner_15"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.622138313873521, -5.626134575567164)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8nZ_DrKuwnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb64544-8c71-47f3-bec3-45d6348d7b6a"
      },
      "source": [
        "### Training regret minimization: run number = 16\r\n",
        "\r\n",
        "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_16 = np.log(y_global_orig - loser_output_16)\r\n",
        "regret_winner_16 = np.log(y_global_orig - winner_output_16)\r\n",
        "\r\n",
        "train_regret_loser_16 = min_max_array(regret_loser_16)\r\n",
        "train_regret_winner_16 = min_max_array(regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16 = min(train_regret_loser_16)\r\n",
        "min_train_regret_winner_16 = min(train_regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16, min_train_regret_winner_16"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-6.1095947512514375, -6.1095947512514375)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg6qzzQFuwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f7b1a58-c0c7-494e-f54b-c7a28f3fba88"
      },
      "source": [
        "### Training regret minimization: run number = 17\r\n",
        "\r\n",
        "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_17 = np.log(y_global_orig - loser_output_17)\r\n",
        "regret_winner_17 = np.log(y_global_orig - winner_output_17)\r\n",
        "\r\n",
        "train_regret_loser_17 = min_max_array(regret_loser_17)\r\n",
        "train_regret_winner_17 = min_max_array(regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17 = min(train_regret_loser_17)\r\n",
        "min_train_regret_winner_17 = min(train_regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17, min_train_regret_winner_17"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.893161531911135, -5.893161531911135)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrRtDLMFuwsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53728171-acfd-48e5-bd62-4d819485adb9"
      },
      "source": [
        "### Training regret minimization: run number = 18\r\n",
        "\r\n",
        "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_18 = np.log(y_global_orig - loser_output_18)\r\n",
        "regret_winner_18 = np.log(y_global_orig - winner_output_18)\r\n",
        "\r\n",
        "train_regret_loser_18 = min_max_array(regret_loser_18)\r\n",
        "train_regret_winner_18 = min_max_array(regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18 = min(train_regret_loser_18)\r\n",
        "min_train_regret_winner_18 = min(train_regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18, min_train_regret_winner_18"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.0344672575848755, -5.0344672575848755)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkmSu4CUuwuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c694b78e-31f5-4418-b892-60ff0470c558"
      },
      "source": [
        "### Training regret minimization: run number = 19\r\n",
        "\r\n",
        "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_19 = np.log(y_global_orig - loser_output_19)\r\n",
        "regret_winner_19 = np.log(y_global_orig - winner_output_19)\r\n",
        "\r\n",
        "train_regret_loser_19 = min_max_array(regret_loser_19)\r\n",
        "train_regret_winner_19 = min_max_array(regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19 = min(train_regret_loser_19)\r\n",
        "min_train_regret_winner_19 = min(train_regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19, min_train_regret_winner_19"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-7.204853323630892, -7.204853323630892)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eymecjwkuwxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6ddd54-c16c-424f-bb43-e4663a041e5e"
      },
      "source": [
        "### Training regret minimization: run number = 20\r\n",
        "\r\n",
        "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_20 = np.log(y_global_orig - loser_output_20)\r\n",
        "regret_winner_20 = np.log(y_global_orig - winner_output_20)\r\n",
        "\r\n",
        "train_regret_loser_20 = min_max_array(regret_loser_20)\r\n",
        "train_regret_winner_20 = min_max_array(regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20 = min(train_regret_loser_20)\r\n",
        "min_train_regret_winner_20 = min(train_regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20, min_train_regret_winner_20"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-6.378142012561249, -6.378142012561249)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5P6kq2Kuw0d"
      },
      "source": [
        "# Iteration1 :\r\n",
        "\r\n",
        "slice1 = 0\r\n",
        "\r\n",
        "loser1 = [train_regret_loser_1[slice1],\r\n",
        "       train_regret_loser_2[slice1],\r\n",
        "       train_regret_loser_3[slice1],\r\n",
        "       train_regret_loser_4[slice1],\r\n",
        "       train_regret_loser_5[slice1],\r\n",
        "       train_regret_loser_6[slice1],\r\n",
        "       train_regret_loser_7[slice1],\r\n",
        "       train_regret_loser_8[slice1],\r\n",
        "       train_regret_loser_9[slice1],\r\n",
        "       train_regret_loser_10[slice1],\r\n",
        "       train_regret_loser_11[slice1],\r\n",
        "       train_regret_loser_12[slice1],\r\n",
        "       train_regret_loser_13[slice1],\r\n",
        "       train_regret_loser_14[slice1],\r\n",
        "       train_regret_loser_15[slice1],\r\n",
        "       train_regret_loser_16[slice1],\r\n",
        "       train_regret_loser_17[slice1],\r\n",
        "       train_regret_loser_18[slice1],\r\n",
        "       train_regret_loser_19[slice1],\r\n",
        "       train_regret_loser_20[slice1]]\r\n",
        "\r\n",
        "winner1 = [train_regret_winner_1[slice1],\r\n",
        "       train_regret_winner_2[slice1],\r\n",
        "       train_regret_winner_3[slice1],\r\n",
        "       train_regret_winner_4[slice1],\r\n",
        "       train_regret_winner_5[slice1],\r\n",
        "       train_regret_winner_6[slice1],\r\n",
        "       train_regret_winner_7[slice1],\r\n",
        "       train_regret_winner_8[slice1],\r\n",
        "       train_regret_winner_9[slice1],\r\n",
        "       train_regret_winner_10[slice1],\r\n",
        "       train_regret_winner_11[slice1],\r\n",
        "       train_regret_winner_12[slice1],\r\n",
        "       train_regret_winner_13[slice1],\r\n",
        "       train_regret_winner_14[slice1],\r\n",
        "       train_regret_winner_15[slice1],\r\n",
        "       train_regret_winner_16[slice1],\r\n",
        "       train_regret_winner_17[slice1],\r\n",
        "       train_regret_winner_18[slice1],\r\n",
        "       train_regret_winner_19[slice1],\r\n",
        "       train_regret_winner_20[slice1]]\r\n",
        "\r\n",
        "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\r\n",
        "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\r\n",
        "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\r\n",
        "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\r\n",
        "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\r\n",
        "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXcnj2pNuw3X"
      },
      "source": [
        "# Iteration11 :\r\n",
        "\r\n",
        "slice11 = 10\r\n",
        "\r\n",
        "loser11 = [train_regret_loser_1[slice11],\r\n",
        "       train_regret_loser_2[slice11],\r\n",
        "       train_regret_loser_3[slice11],\r\n",
        "       train_regret_loser_4[slice11],\r\n",
        "       train_regret_loser_5[slice11],\r\n",
        "       train_regret_loser_6[slice11],\r\n",
        "       train_regret_loser_7[slice11],\r\n",
        "       train_regret_loser_8[slice11],\r\n",
        "       train_regret_loser_9[slice11],\r\n",
        "       train_regret_loser_10[slice11],\r\n",
        "       train_regret_loser_11[slice11],\r\n",
        "       train_regret_loser_12[slice11],\r\n",
        "       train_regret_loser_13[slice11],\r\n",
        "       train_regret_loser_14[slice11],\r\n",
        "       train_regret_loser_15[slice11],\r\n",
        "       train_regret_loser_16[slice11],\r\n",
        "       train_regret_loser_17[slice11],\r\n",
        "       train_regret_loser_18[slice11],\r\n",
        "       train_regret_loser_19[slice11],\r\n",
        "       train_regret_loser_20[slice11]]\r\n",
        "\r\n",
        "winner11 = [train_regret_winner_1[slice11],\r\n",
        "       train_regret_winner_2[slice11],\r\n",
        "       train_regret_winner_3[slice11],\r\n",
        "       train_regret_winner_4[slice11],\r\n",
        "       train_regret_winner_5[slice11],\r\n",
        "       train_regret_winner_6[slice11],\r\n",
        "       train_regret_winner_7[slice11],\r\n",
        "       train_regret_winner_8[slice11],\r\n",
        "       train_regret_winner_9[slice11],\r\n",
        "       train_regret_winner_10[slice11],\r\n",
        "       train_regret_winner_11[slice11],\r\n",
        "       train_regret_winner_12[slice11],\r\n",
        "       train_regret_winner_13[slice11],\r\n",
        "       train_regret_winner_14[slice11],\r\n",
        "       train_regret_winner_15[slice11],\r\n",
        "       train_regret_winner_16[slice11],\r\n",
        "       train_regret_winner_17[slice11],\r\n",
        "       train_regret_winner_18[slice11],\r\n",
        "       train_regret_winner_19[slice11],\r\n",
        "       train_regret_winner_20[slice11]]\r\n",
        "\r\n",
        "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\r\n",
        "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\r\n",
        "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\r\n",
        "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\r\n",
        "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\r\n",
        "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxQa6BAVuw6L"
      },
      "source": [
        "# Iteration21 :\r\n",
        "\r\n",
        "slice21 = 20\r\n",
        "\r\n",
        "loser21 = [train_regret_loser_1[slice21],\r\n",
        "       train_regret_loser_2[slice21],\r\n",
        "       train_regret_loser_3[slice21],\r\n",
        "       train_regret_loser_4[slice21],\r\n",
        "       train_regret_loser_5[slice21],\r\n",
        "       train_regret_loser_6[slice21],\r\n",
        "       train_regret_loser_7[slice21],\r\n",
        "       train_regret_loser_8[slice21],\r\n",
        "       train_regret_loser_9[slice21],\r\n",
        "       train_regret_loser_10[slice21],\r\n",
        "       train_regret_loser_11[slice21],\r\n",
        "       train_regret_loser_12[slice21],\r\n",
        "       train_regret_loser_13[slice21],\r\n",
        "       train_regret_loser_14[slice21],\r\n",
        "       train_regret_loser_15[slice21],\r\n",
        "       train_regret_loser_16[slice21],\r\n",
        "       train_regret_loser_17[slice21],\r\n",
        "       train_regret_loser_18[slice21],\r\n",
        "       train_regret_loser_19[slice21],\r\n",
        "       train_regret_loser_20[slice21]]\r\n",
        "\r\n",
        "winner21 = [train_regret_winner_1[slice21],\r\n",
        "       train_regret_winner_2[slice21],\r\n",
        "       train_regret_winner_3[slice21],\r\n",
        "       train_regret_winner_4[slice21],\r\n",
        "       train_regret_winner_5[slice21],\r\n",
        "       train_regret_winner_6[slice21],\r\n",
        "       train_regret_winner_7[slice21],\r\n",
        "       train_regret_winner_8[slice21],\r\n",
        "       train_regret_winner_9[slice21],\r\n",
        "       train_regret_winner_10[slice21],\r\n",
        "       train_regret_winner_11[slice21],\r\n",
        "       train_regret_winner_12[slice21],\r\n",
        "       train_regret_winner_13[slice21],\r\n",
        "       train_regret_winner_14[slice21],\r\n",
        "       train_regret_winner_15[slice21],\r\n",
        "       train_regret_winner_16[slice21],\r\n",
        "       train_regret_winner_17[slice21],\r\n",
        "       train_regret_winner_18[slice21],\r\n",
        "       train_regret_winner_19[slice21],\r\n",
        "       train_regret_winner_20[slice21]]\r\n",
        "\r\n",
        "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\r\n",
        "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\r\n",
        "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\r\n",
        "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\r\n",
        "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\r\n",
        "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wdJdIsWuw8r"
      },
      "source": [
        "# Iteration31 :\r\n",
        "\r\n",
        "slice31 = 30\r\n",
        "\r\n",
        "loser31 = [train_regret_loser_1[slice31],\r\n",
        "       train_regret_loser_2[slice31],\r\n",
        "       train_regret_loser_3[slice31],\r\n",
        "       train_regret_loser_4[slice31],\r\n",
        "       train_regret_loser_5[slice31],\r\n",
        "       train_regret_loser_6[slice31],\r\n",
        "       train_regret_loser_7[slice31],\r\n",
        "       train_regret_loser_8[slice31],\r\n",
        "       train_regret_loser_9[slice31],\r\n",
        "       train_regret_loser_10[slice31],\r\n",
        "       train_regret_loser_11[slice31],\r\n",
        "       train_regret_loser_12[slice31],\r\n",
        "       train_regret_loser_13[slice31],\r\n",
        "       train_regret_loser_14[slice31],\r\n",
        "       train_regret_loser_15[slice31],\r\n",
        "       train_regret_loser_16[slice31],\r\n",
        "       train_regret_loser_17[slice31],\r\n",
        "       train_regret_loser_18[slice31],\r\n",
        "       train_regret_loser_19[slice31],\r\n",
        "       train_regret_loser_20[slice31]]\r\n",
        "\r\n",
        "winner31 = [train_regret_winner_1[slice31],\r\n",
        "       train_regret_winner_2[slice31],\r\n",
        "       train_regret_winner_3[slice31],\r\n",
        "       train_regret_winner_4[slice31],\r\n",
        "       train_regret_winner_5[slice31],\r\n",
        "       train_regret_winner_6[slice31],\r\n",
        "       train_regret_winner_7[slice31],\r\n",
        "       train_regret_winner_8[slice31],\r\n",
        "       train_regret_winner_9[slice31],\r\n",
        "       train_regret_winner_10[slice31],\r\n",
        "       train_regret_winner_11[slice31],\r\n",
        "       train_regret_winner_12[slice31],\r\n",
        "       train_regret_winner_13[slice31],\r\n",
        "       train_regret_winner_14[slice31],\r\n",
        "       train_regret_winner_15[slice31],\r\n",
        "       train_regret_winner_16[slice31],\r\n",
        "       train_regret_winner_17[slice31],\r\n",
        "       train_regret_winner_18[slice31],\r\n",
        "       train_regret_winner_19[slice31],\r\n",
        "       train_regret_winner_20[slice31]]\r\n",
        "\r\n",
        "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\r\n",
        "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\r\n",
        "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\r\n",
        "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\r\n",
        "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\r\n",
        "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWZBDufuw_Z"
      },
      "source": [
        "# Iteration41 :\r\n",
        "\r\n",
        "slice41 = 40\r\n",
        "\r\n",
        "loser41 = [train_regret_loser_1[slice41],\r\n",
        "       train_regret_loser_2[slice41],\r\n",
        "       train_regret_loser_3[slice41],\r\n",
        "       train_regret_loser_4[slice41],\r\n",
        "       train_regret_loser_5[slice41],\r\n",
        "       train_regret_loser_6[slice41],\r\n",
        "       train_regret_loser_7[slice41],\r\n",
        "       train_regret_loser_8[slice41],\r\n",
        "       train_regret_loser_9[slice41],\r\n",
        "       train_regret_loser_10[slice41],\r\n",
        "       train_regret_loser_11[slice41],\r\n",
        "       train_regret_loser_12[slice41],\r\n",
        "       train_regret_loser_13[slice41],\r\n",
        "       train_regret_loser_14[slice41],\r\n",
        "       train_regret_loser_15[slice41],\r\n",
        "       train_regret_loser_16[slice41],\r\n",
        "       train_regret_loser_17[slice41],\r\n",
        "       train_regret_loser_18[slice41],\r\n",
        "       train_regret_loser_19[slice41],\r\n",
        "       train_regret_loser_20[slice41]]\r\n",
        "\r\n",
        "winner41 = [train_regret_winner_1[slice41],\r\n",
        "       train_regret_winner_2[slice41],\r\n",
        "       train_regret_winner_3[slice41],\r\n",
        "       train_regret_winner_4[slice41],\r\n",
        "       train_regret_winner_5[slice41],\r\n",
        "       train_regret_winner_6[slice41],\r\n",
        "       train_regret_winner_7[slice41],\r\n",
        "       train_regret_winner_8[slice41],\r\n",
        "       train_regret_winner_9[slice41],\r\n",
        "       train_regret_winner_10[slice41],\r\n",
        "       train_regret_winner_11[slice41],\r\n",
        "       train_regret_winner_12[slice41],\r\n",
        "       train_regret_winner_13[slice41],\r\n",
        "       train_regret_winner_14[slice41],\r\n",
        "       train_regret_winner_15[slice41],\r\n",
        "       train_regret_winner_16[slice41],\r\n",
        "       train_regret_winner_17[slice41],\r\n",
        "       train_regret_winner_18[slice41],\r\n",
        "       train_regret_winner_19[slice41],\r\n",
        "       train_regret_winner_20[slice41]]\r\n",
        "\r\n",
        "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\r\n",
        "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\r\n",
        "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\r\n",
        "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\r\n",
        "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\r\n",
        "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwi_GzjiuxB6"
      },
      "source": [
        "# Iteration51 :\r\n",
        "\r\n",
        "slice51 = 50\r\n",
        "\r\n",
        "loser51 = [train_regret_loser_1[slice51],\r\n",
        "       train_regret_loser_2[slice51],\r\n",
        "       train_regret_loser_3[slice51],\r\n",
        "       train_regret_loser_4[slice51],\r\n",
        "       train_regret_loser_5[slice51],\r\n",
        "       train_regret_loser_6[slice51],\r\n",
        "       train_regret_loser_7[slice51],\r\n",
        "       train_regret_loser_8[slice51],\r\n",
        "       train_regret_loser_9[slice51],\r\n",
        "       train_regret_loser_10[slice51],\r\n",
        "       train_regret_loser_11[slice51],\r\n",
        "       train_regret_loser_12[slice51],\r\n",
        "       train_regret_loser_13[slice51],\r\n",
        "       train_regret_loser_14[slice51],\r\n",
        "       train_regret_loser_15[slice51],\r\n",
        "       train_regret_loser_16[slice51],\r\n",
        "       train_regret_loser_17[slice51],\r\n",
        "       train_regret_loser_18[slice51],\r\n",
        "       train_regret_loser_19[slice51],\r\n",
        "       train_regret_loser_20[slice51]]\r\n",
        "\r\n",
        "winner51 = [train_regret_winner_1[slice51],\r\n",
        "       train_regret_winner_2[slice51],\r\n",
        "       train_regret_winner_3[slice51],\r\n",
        "       train_regret_winner_4[slice51],\r\n",
        "       train_regret_winner_5[slice51],\r\n",
        "       train_regret_winner_6[slice51],\r\n",
        "       train_regret_winner_7[slice51],\r\n",
        "       train_regret_winner_8[slice51],\r\n",
        "       train_regret_winner_9[slice51],\r\n",
        "       train_regret_winner_10[slice51],\r\n",
        "       train_regret_winner_11[slice51],\r\n",
        "       train_regret_winner_12[slice51],\r\n",
        "       train_regret_winner_13[slice51],\r\n",
        "       train_regret_winner_14[slice51],\r\n",
        "       train_regret_winner_15[slice51],\r\n",
        "       train_regret_winner_16[slice51],\r\n",
        "       train_regret_winner_17[slice51],\r\n",
        "       train_regret_winner_18[slice51],\r\n",
        "       train_regret_winner_19[slice51],\r\n",
        "       train_regret_winner_20[slice51]]\r\n",
        "\r\n",
        "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\r\n",
        "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\r\n",
        "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\r\n",
        "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\r\n",
        "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\r\n",
        "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk9nlS6puxFZ"
      },
      "source": [
        "# Iteration61 :\r\n",
        "\r\n",
        "slice61 = 60\r\n",
        "\r\n",
        "loser61 = [train_regret_loser_1[slice61],\r\n",
        "       train_regret_loser_2[slice61],\r\n",
        "       train_regret_loser_3[slice61],\r\n",
        "       train_regret_loser_4[slice61],\r\n",
        "       train_regret_loser_5[slice61],\r\n",
        "       train_regret_loser_6[slice61],\r\n",
        "       train_regret_loser_7[slice61],\r\n",
        "       train_regret_loser_8[slice61],\r\n",
        "       train_regret_loser_9[slice61],\r\n",
        "       train_regret_loser_10[slice61],\r\n",
        "       train_regret_loser_11[slice61],\r\n",
        "       train_regret_loser_12[slice61],\r\n",
        "       train_regret_loser_13[slice61],\r\n",
        "       train_regret_loser_14[slice61],\r\n",
        "       train_regret_loser_15[slice61],\r\n",
        "       train_regret_loser_16[slice61],\r\n",
        "       train_regret_loser_17[slice61],\r\n",
        "       train_regret_loser_18[slice61],\r\n",
        "       train_regret_loser_19[slice61],\r\n",
        "       train_regret_loser_20[slice61]]\r\n",
        "\r\n",
        "winner61 = [train_regret_winner_1[slice61],\r\n",
        "       train_regret_winner_2[slice61],\r\n",
        "       train_regret_winner_3[slice61],\r\n",
        "       train_regret_winner_4[slice61],\r\n",
        "       train_regret_winner_5[slice61],\r\n",
        "       train_regret_winner_6[slice61],\r\n",
        "       train_regret_winner_7[slice61],\r\n",
        "       train_regret_winner_8[slice61],\r\n",
        "       train_regret_winner_9[slice61],\r\n",
        "       train_regret_winner_10[slice61],\r\n",
        "       train_regret_winner_11[slice61],\r\n",
        "       train_regret_winner_12[slice61],\r\n",
        "       train_regret_winner_13[slice61],\r\n",
        "       train_regret_winner_14[slice61],\r\n",
        "       train_regret_winner_15[slice61],\r\n",
        "       train_regret_winner_16[slice61],\r\n",
        "       train_regret_winner_17[slice61],\r\n",
        "       train_regret_winner_18[slice61],\r\n",
        "       train_regret_winner_19[slice61],\r\n",
        "       train_regret_winner_20[slice61]]\r\n",
        "\r\n",
        "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\r\n",
        "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\r\n",
        "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\r\n",
        "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\r\n",
        "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\r\n",
        "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMus9vDpuxHN"
      },
      "source": [
        "# Iteration71 :\r\n",
        "\r\n",
        "slice71 = 70\r\n",
        "\r\n",
        "loser71 = [train_regret_loser_1[slice71],\r\n",
        "       train_regret_loser_2[slice71],\r\n",
        "       train_regret_loser_3[slice71],\r\n",
        "       train_regret_loser_4[slice71],\r\n",
        "       train_regret_loser_5[slice71],\r\n",
        "       train_regret_loser_6[slice71],\r\n",
        "       train_regret_loser_7[slice71],\r\n",
        "       train_regret_loser_8[slice71],\r\n",
        "       train_regret_loser_9[slice71],\r\n",
        "       train_regret_loser_10[slice71],\r\n",
        "       train_regret_loser_11[slice71],\r\n",
        "       train_regret_loser_12[slice71],\r\n",
        "       train_regret_loser_13[slice71],\r\n",
        "       train_regret_loser_14[slice71],\r\n",
        "       train_regret_loser_15[slice71],\r\n",
        "       train_regret_loser_16[slice71],\r\n",
        "       train_regret_loser_17[slice71],\r\n",
        "       train_regret_loser_18[slice71],\r\n",
        "       train_regret_loser_19[slice71],\r\n",
        "       train_regret_loser_20[slice71]]\r\n",
        "\r\n",
        "winner71 = [train_regret_winner_1[slice71],\r\n",
        "       train_regret_winner_2[slice71],\r\n",
        "       train_regret_winner_3[slice71],\r\n",
        "       train_regret_winner_4[slice71],\r\n",
        "       train_regret_winner_5[slice71],\r\n",
        "       train_regret_winner_6[slice71],\r\n",
        "       train_regret_winner_7[slice71],\r\n",
        "       train_regret_winner_8[slice71],\r\n",
        "       train_regret_winner_9[slice71],\r\n",
        "       train_regret_winner_10[slice71],\r\n",
        "       train_regret_winner_11[slice71],\r\n",
        "       train_regret_winner_12[slice71],\r\n",
        "       train_regret_winner_13[slice71],\r\n",
        "       train_regret_winner_14[slice71],\r\n",
        "       train_regret_winner_15[slice71],\r\n",
        "       train_regret_winner_16[slice71],\r\n",
        "       train_regret_winner_17[slice71],\r\n",
        "       train_regret_winner_18[slice71],\r\n",
        "       train_regret_winner_19[slice71],\r\n",
        "       train_regret_winner_20[slice71]]\r\n",
        "\r\n",
        "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\r\n",
        "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\r\n",
        "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\r\n",
        "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\r\n",
        "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\r\n",
        "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Mg0x9vuxL4"
      },
      "source": [
        "# Iteration81 :\r\n",
        "\r\n",
        "slice81 = 80\r\n",
        "\r\n",
        "loser81 = [train_regret_loser_1[slice81],\r\n",
        "       train_regret_loser_2[slice81],\r\n",
        "       train_regret_loser_3[slice81],\r\n",
        "       train_regret_loser_4[slice81],\r\n",
        "       train_regret_loser_5[slice81],\r\n",
        "       train_regret_loser_6[slice81],\r\n",
        "       train_regret_loser_7[slice81],\r\n",
        "       train_regret_loser_8[slice81],\r\n",
        "       train_regret_loser_9[slice81],\r\n",
        "       train_regret_loser_10[slice81],\r\n",
        "       train_regret_loser_11[slice81],\r\n",
        "       train_regret_loser_12[slice81],\r\n",
        "       train_regret_loser_13[slice81],\r\n",
        "       train_regret_loser_14[slice81],\r\n",
        "       train_regret_loser_15[slice81],\r\n",
        "       train_regret_loser_16[slice81],\r\n",
        "       train_regret_loser_17[slice81],\r\n",
        "       train_regret_loser_18[slice81],\r\n",
        "       train_regret_loser_19[slice81],\r\n",
        "       train_regret_loser_20[slice81]]\r\n",
        "\r\n",
        "winner81 = [train_regret_winner_1[slice81],\r\n",
        "       train_regret_winner_2[slice81],\r\n",
        "       train_regret_winner_3[slice81],\r\n",
        "       train_regret_winner_4[slice81],\r\n",
        "       train_regret_winner_5[slice81],\r\n",
        "       train_regret_winner_6[slice81],\r\n",
        "       train_regret_winner_7[slice81],\r\n",
        "       train_regret_winner_8[slice81],\r\n",
        "       train_regret_winner_9[slice81],\r\n",
        "       train_regret_winner_10[slice81],\r\n",
        "       train_regret_winner_11[slice81],\r\n",
        "       train_regret_winner_12[slice81],\r\n",
        "       train_regret_winner_13[slice81],\r\n",
        "       train_regret_winner_14[slice81],\r\n",
        "       train_regret_winner_15[slice81],\r\n",
        "       train_regret_winner_16[slice81],\r\n",
        "       train_regret_winner_17[slice81],\r\n",
        "       train_regret_winner_18[slice81],\r\n",
        "       train_regret_winner_19[slice81],\r\n",
        "       train_regret_winner_20[slice81]]\r\n",
        "\r\n",
        "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\r\n",
        "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\r\n",
        "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\r\n",
        "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\r\n",
        "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\r\n",
        "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gu9K8XlteV4"
      },
      "source": [
        "# Iteration91 :\r\n",
        "\r\n",
        "slice91 = 90\r\n",
        "\r\n",
        "loser91 = [train_regret_loser_1[slice91],\r\n",
        "       train_regret_loser_2[slice91],\r\n",
        "       train_regret_loser_3[slice91],\r\n",
        "       train_regret_loser_4[slice91],\r\n",
        "       train_regret_loser_5[slice91],\r\n",
        "       train_regret_loser_6[slice91],\r\n",
        "       train_regret_loser_7[slice91],\r\n",
        "       train_regret_loser_8[slice91],\r\n",
        "       train_regret_loser_9[slice91],\r\n",
        "       train_regret_loser_10[slice91],\r\n",
        "       train_regret_loser_11[slice91],\r\n",
        "       train_regret_loser_12[slice91],\r\n",
        "       train_regret_loser_13[slice91],\r\n",
        "       train_regret_loser_14[slice91],\r\n",
        "       train_regret_loser_15[slice91],\r\n",
        "       train_regret_loser_16[slice91],\r\n",
        "       train_regret_loser_17[slice91],\r\n",
        "       train_regret_loser_18[slice91],\r\n",
        "       train_regret_loser_19[slice91],\r\n",
        "       train_regret_loser_20[slice91]]\r\n",
        "\r\n",
        "winner91 = [train_regret_winner_1[slice91],\r\n",
        "       train_regret_winner_2[slice91],\r\n",
        "       train_regret_winner_3[slice91],\r\n",
        "       train_regret_winner_4[slice91],\r\n",
        "       train_regret_winner_5[slice91],\r\n",
        "       train_regret_winner_6[slice91],\r\n",
        "       train_regret_winner_7[slice91],\r\n",
        "       train_regret_winner_8[slice91],\r\n",
        "       train_regret_winner_9[slice91],\r\n",
        "       train_regret_winner_10[slice91],\r\n",
        "       train_regret_winner_11[slice91],\r\n",
        "       train_regret_winner_12[slice91],\r\n",
        "       train_regret_winner_13[slice91],\r\n",
        "       train_regret_winner_14[slice91],\r\n",
        "       train_regret_winner_15[slice91],\r\n",
        "       train_regret_winner_16[slice91],\r\n",
        "       train_regret_winner_17[slice91],\r\n",
        "       train_regret_winner_18[slice91],\r\n",
        "       train_regret_winner_19[slice91],\r\n",
        "       train_regret_winner_20[slice91]]\r\n",
        "\r\n",
        "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\r\n",
        "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\r\n",
        "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\r\n",
        "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\r\n",
        "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\r\n",
        "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVVeft6steXv"
      },
      "source": [
        "# Iteration101 :\r\n",
        "\r\n",
        "slice101 = 100\r\n",
        "\r\n",
        "loser101 = [train_regret_loser_1[slice101],\r\n",
        "       train_regret_loser_2[slice101],\r\n",
        "       train_regret_loser_3[slice101],\r\n",
        "       train_regret_loser_4[slice101],\r\n",
        "       train_regret_loser_5[slice101],\r\n",
        "       train_regret_loser_6[slice101],\r\n",
        "       train_regret_loser_7[slice101],\r\n",
        "       train_regret_loser_8[slice101],\r\n",
        "       train_regret_loser_9[slice101],\r\n",
        "       train_regret_loser_10[slice101],\r\n",
        "       train_regret_loser_11[slice101],\r\n",
        "       train_regret_loser_12[slice101],\r\n",
        "       train_regret_loser_13[slice101],\r\n",
        "       train_regret_loser_14[slice101],\r\n",
        "       train_regret_loser_15[slice101],\r\n",
        "       train_regret_loser_16[slice101],\r\n",
        "       train_regret_loser_17[slice101],\r\n",
        "       train_regret_loser_18[slice101],\r\n",
        "       train_regret_loser_19[slice101],\r\n",
        "       train_regret_loser_20[slice101]]\r\n",
        "\r\n",
        "winner101 = [train_regret_winner_1[slice101],\r\n",
        "       train_regret_winner_2[slice101],\r\n",
        "       train_regret_winner_3[slice101],\r\n",
        "       train_regret_winner_4[slice101],\r\n",
        "       train_regret_winner_5[slice101],\r\n",
        "       train_regret_winner_6[slice101],\r\n",
        "       train_regret_winner_7[slice101],\r\n",
        "       train_regret_winner_8[slice101],\r\n",
        "       train_regret_winner_9[slice101],\r\n",
        "       train_regret_winner_10[slice101],\r\n",
        "       train_regret_winner_11[slice101],\r\n",
        "       train_regret_winner_12[slice101],\r\n",
        "       train_regret_winner_13[slice101],\r\n",
        "       train_regret_winner_14[slice101],\r\n",
        "       train_regret_winner_15[slice101],\r\n",
        "       train_regret_winner_16[slice101],\r\n",
        "       train_regret_winner_17[slice101],\r\n",
        "       train_regret_winner_18[slice101],\r\n",
        "       train_regret_winner_19[slice101],\r\n",
        "       train_regret_winner_20[slice101]]\r\n",
        "\r\n",
        "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\r\n",
        "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\r\n",
        "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\r\n",
        "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\r\n",
        "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\r\n",
        "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maqbtTXXtebT"
      },
      "source": [
        "# Iteration2 :\r\n",
        "\r\n",
        "slice2 = 1\r\n",
        "\r\n",
        "loser2 = [train_regret_loser_1[slice2],\r\n",
        "       train_regret_loser_2[slice2],\r\n",
        "       train_regret_loser_3[slice2],\r\n",
        "       train_regret_loser_4[slice2],\r\n",
        "       train_regret_loser_5[slice2],\r\n",
        "       train_regret_loser_6[slice2],\r\n",
        "       train_regret_loser_7[slice2],\r\n",
        "       train_regret_loser_8[slice2],\r\n",
        "       train_regret_loser_9[slice2],\r\n",
        "       train_regret_loser_10[slice2],\r\n",
        "       train_regret_loser_11[slice2],\r\n",
        "       train_regret_loser_12[slice2],\r\n",
        "       train_regret_loser_13[slice2],\r\n",
        "       train_regret_loser_14[slice2],\r\n",
        "       train_regret_loser_15[slice2],\r\n",
        "       train_regret_loser_16[slice2],\r\n",
        "       train_regret_loser_17[slice2],\r\n",
        "       train_regret_loser_18[slice2],\r\n",
        "       train_regret_loser_19[slice2],\r\n",
        "       train_regret_loser_20[slice2]]\r\n",
        "\r\n",
        "winner2 = [train_regret_winner_1[slice2],\r\n",
        "       train_regret_winner_2[slice2],\r\n",
        "       train_regret_winner_3[slice2],\r\n",
        "       train_regret_winner_4[slice2],\r\n",
        "       train_regret_winner_5[slice2],\r\n",
        "       train_regret_winner_6[slice2],\r\n",
        "       train_regret_winner_7[slice2],\r\n",
        "       train_regret_winner_8[slice2],\r\n",
        "       train_regret_winner_9[slice2],\r\n",
        "       train_regret_winner_10[slice2],\r\n",
        "       train_regret_winner_11[slice2],\r\n",
        "       train_regret_winner_12[slice2],\r\n",
        "       train_regret_winner_13[slice2],\r\n",
        "       train_regret_winner_14[slice2],\r\n",
        "       train_regret_winner_15[slice2],\r\n",
        "       train_regret_winner_16[slice2],\r\n",
        "       train_regret_winner_17[slice2],\r\n",
        "       train_regret_winner_18[slice2],\r\n",
        "       train_regret_winner_19[slice2],\r\n",
        "       train_regret_winner_20[slice2]]\r\n",
        "\r\n",
        "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\r\n",
        "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\r\n",
        "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\r\n",
        "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\r\n",
        "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\r\n",
        "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIBh0dQw-ZM"
      },
      "source": [
        "# Iteration12 :\r\n",
        "\r\n",
        "slice12 = 11\r\n",
        "\r\n",
        "loser12 = [train_regret_loser_1[slice12],\r\n",
        "       train_regret_loser_2[slice12],\r\n",
        "       train_regret_loser_3[slice12],\r\n",
        "       train_regret_loser_4[slice12],\r\n",
        "       train_regret_loser_5[slice12],\r\n",
        "       train_regret_loser_6[slice12],\r\n",
        "       train_regret_loser_7[slice12],\r\n",
        "       train_regret_loser_8[slice12],\r\n",
        "       train_regret_loser_9[slice12],\r\n",
        "       train_regret_loser_10[slice12],\r\n",
        "       train_regret_loser_11[slice12],\r\n",
        "       train_regret_loser_12[slice12],\r\n",
        "       train_regret_loser_13[slice12],\r\n",
        "       train_regret_loser_14[slice12],\r\n",
        "       train_regret_loser_15[slice12],\r\n",
        "       train_regret_loser_16[slice12],\r\n",
        "       train_regret_loser_17[slice12],\r\n",
        "       train_regret_loser_18[slice12],\r\n",
        "       train_regret_loser_19[slice12],\r\n",
        "       train_regret_loser_20[slice12]]\r\n",
        "\r\n",
        "winner12 = [train_regret_winner_1[slice12],\r\n",
        "       train_regret_winner_2[slice12],\r\n",
        "       train_regret_winner_3[slice12],\r\n",
        "       train_regret_winner_4[slice12],\r\n",
        "       train_regret_winner_5[slice12],\r\n",
        "       train_regret_winner_6[slice12],\r\n",
        "       train_regret_winner_7[slice12],\r\n",
        "       train_regret_winner_8[slice12],\r\n",
        "       train_regret_winner_9[slice12],\r\n",
        "       train_regret_winner_10[slice12],\r\n",
        "       train_regret_winner_11[slice12],\r\n",
        "       train_regret_winner_12[slice12],\r\n",
        "       train_regret_winner_13[slice12],\r\n",
        "       train_regret_winner_14[slice12],\r\n",
        "       train_regret_winner_15[slice12],\r\n",
        "       train_regret_winner_16[slice12],\r\n",
        "       train_regret_winner_17[slice12],\r\n",
        "       train_regret_winner_18[slice12],\r\n",
        "       train_regret_winner_19[slice12],\r\n",
        "       train_regret_winner_20[slice12]]\r\n",
        "\r\n",
        "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\r\n",
        "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\r\n",
        "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\r\n",
        "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\r\n",
        "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjIhsei3w-cV"
      },
      "source": [
        "# Iteration22 :\r\n",
        "\r\n",
        "slice22 = 21\r\n",
        "\r\n",
        "loser22 = [train_regret_loser_1[slice22],\r\n",
        "       train_regret_loser_2[slice22],\r\n",
        "       train_regret_loser_3[slice22],\r\n",
        "       train_regret_loser_4[slice22],\r\n",
        "       train_regret_loser_5[slice22],\r\n",
        "       train_regret_loser_6[slice22],\r\n",
        "       train_regret_loser_7[slice22],\r\n",
        "       train_regret_loser_8[slice22],\r\n",
        "       train_regret_loser_9[slice22],\r\n",
        "       train_regret_loser_10[slice22],\r\n",
        "       train_regret_loser_11[slice22],\r\n",
        "       train_regret_loser_12[slice22],\r\n",
        "       train_regret_loser_13[slice22],\r\n",
        "       train_regret_loser_14[slice22],\r\n",
        "       train_regret_loser_15[slice22],\r\n",
        "       train_regret_loser_16[slice22],\r\n",
        "       train_regret_loser_17[slice22],\r\n",
        "       train_regret_loser_18[slice22],\r\n",
        "       train_regret_loser_19[slice22],\r\n",
        "       train_regret_loser_20[slice22]]\r\n",
        "\r\n",
        "winner22 = [train_regret_winner_1[slice22],\r\n",
        "       train_regret_winner_2[slice22],\r\n",
        "       train_regret_winner_3[slice22],\r\n",
        "       train_regret_winner_4[slice22],\r\n",
        "       train_regret_winner_5[slice22],\r\n",
        "       train_regret_winner_6[slice22],\r\n",
        "       train_regret_winner_7[slice22],\r\n",
        "       train_regret_winner_8[slice22],\r\n",
        "       train_regret_winner_9[slice22],\r\n",
        "       train_regret_winner_10[slice22],\r\n",
        "       train_regret_winner_11[slice22],\r\n",
        "       train_regret_winner_12[slice22],\r\n",
        "       train_regret_winner_13[slice22],\r\n",
        "       train_regret_winner_14[slice22],\r\n",
        "       train_regret_winner_15[slice22],\r\n",
        "       train_regret_winner_16[slice22],\r\n",
        "       train_regret_winner_17[slice22],\r\n",
        "       train_regret_winner_18[slice22],\r\n",
        "       train_regret_winner_19[slice22],\r\n",
        "       train_regret_winner_20[slice22]]\r\n",
        "\r\n",
        "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\r\n",
        "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\r\n",
        "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\r\n",
        "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\r\n",
        "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\r\n",
        "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur9p9GkHw-fq"
      },
      "source": [
        "# Iteration32 :\r\n",
        "\r\n",
        "slice32 = 31\r\n",
        "\r\n",
        "loser32 = [train_regret_loser_1[slice32],\r\n",
        "       train_regret_loser_2[slice32],\r\n",
        "       train_regret_loser_3[slice32],\r\n",
        "       train_regret_loser_4[slice32],\r\n",
        "       train_regret_loser_5[slice32],\r\n",
        "       train_regret_loser_6[slice32],\r\n",
        "       train_regret_loser_7[slice32],\r\n",
        "       train_regret_loser_8[slice32],\r\n",
        "       train_regret_loser_9[slice32],\r\n",
        "       train_regret_loser_10[slice32],\r\n",
        "       train_regret_loser_11[slice32],\r\n",
        "       train_regret_loser_12[slice32],\r\n",
        "       train_regret_loser_13[slice32],\r\n",
        "       train_regret_loser_14[slice32],\r\n",
        "       train_regret_loser_15[slice32],\r\n",
        "       train_regret_loser_16[slice32],\r\n",
        "       train_regret_loser_17[slice32],\r\n",
        "       train_regret_loser_18[slice32],\r\n",
        "       train_regret_loser_19[slice32],\r\n",
        "       train_regret_loser_20[slice32]]\r\n",
        "\r\n",
        "winner32 = [train_regret_winner_1[slice32],\r\n",
        "       train_regret_winner_2[slice32],\r\n",
        "       train_regret_winner_3[slice32],\r\n",
        "       train_regret_winner_4[slice32],\r\n",
        "       train_regret_winner_5[slice32],\r\n",
        "       train_regret_winner_6[slice32],\r\n",
        "       train_regret_winner_7[slice32],\r\n",
        "       train_regret_winner_8[slice32],\r\n",
        "       train_regret_winner_9[slice32],\r\n",
        "       train_regret_winner_10[slice32],\r\n",
        "       train_regret_winner_11[slice32],\r\n",
        "       train_regret_winner_12[slice32],\r\n",
        "       train_regret_winner_13[slice32],\r\n",
        "       train_regret_winner_14[slice32],\r\n",
        "       train_regret_winner_15[slice32],\r\n",
        "       train_regret_winner_16[slice32],\r\n",
        "       train_regret_winner_17[slice32],\r\n",
        "       train_regret_winner_18[slice32],\r\n",
        "       train_regret_winner_19[slice32],\r\n",
        "       train_regret_winner_20[slice32]]\r\n",
        "\r\n",
        "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\r\n",
        "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\r\n",
        "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\r\n",
        "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\r\n",
        "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\r\n",
        "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_rzHB8w-hy"
      },
      "source": [
        "# Iteration42 :\r\n",
        "\r\n",
        "slice42 = 41\r\n",
        "\r\n",
        "loser42 = [train_regret_loser_1[slice42],\r\n",
        "       train_regret_loser_2[slice42],\r\n",
        "       train_regret_loser_3[slice42],\r\n",
        "       train_regret_loser_4[slice42],\r\n",
        "       train_regret_loser_5[slice42],\r\n",
        "       train_regret_loser_6[slice42],\r\n",
        "       train_regret_loser_7[slice42],\r\n",
        "       train_regret_loser_8[slice42],\r\n",
        "       train_regret_loser_9[slice42],\r\n",
        "       train_regret_loser_10[slice42],\r\n",
        "       train_regret_loser_11[slice42],\r\n",
        "       train_regret_loser_12[slice42],\r\n",
        "       train_regret_loser_13[slice42],\r\n",
        "       train_regret_loser_14[slice42],\r\n",
        "       train_regret_loser_15[slice42],\r\n",
        "       train_regret_loser_16[slice42],\r\n",
        "       train_regret_loser_17[slice42],\r\n",
        "       train_regret_loser_18[slice42],\r\n",
        "       train_regret_loser_19[slice42],\r\n",
        "       train_regret_loser_20[slice42]]\r\n",
        "\r\n",
        "winner42 = [train_regret_winner_1[slice42],\r\n",
        "       train_regret_winner_2[slice42],\r\n",
        "       train_regret_winner_3[slice42],\r\n",
        "       train_regret_winner_4[slice42],\r\n",
        "       train_regret_winner_5[slice42],\r\n",
        "       train_regret_winner_6[slice42],\r\n",
        "       train_regret_winner_7[slice42],\r\n",
        "       train_regret_winner_8[slice42],\r\n",
        "       train_regret_winner_9[slice42],\r\n",
        "       train_regret_winner_10[slice42],\r\n",
        "       train_regret_winner_11[slice42],\r\n",
        "       train_regret_winner_12[slice42],\r\n",
        "       train_regret_winner_13[slice42],\r\n",
        "       train_regret_winner_14[slice42],\r\n",
        "       train_regret_winner_15[slice42],\r\n",
        "       train_regret_winner_16[slice42],\r\n",
        "       train_regret_winner_17[slice42],\r\n",
        "       train_regret_winner_18[slice42],\r\n",
        "       train_regret_winner_19[slice42],\r\n",
        "       train_regret_winner_20[slice42]]\r\n",
        "\r\n",
        "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\r\n",
        "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\r\n",
        "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\r\n",
        "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\r\n",
        "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\r\n",
        "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKUUG6vAw-kz"
      },
      "source": [
        "# Iteration52 :\r\n",
        "\r\n",
        "slice52 = 51\r\n",
        "\r\n",
        "loser52 = [train_regret_loser_1[slice52],\r\n",
        "       train_regret_loser_2[slice52],\r\n",
        "       train_regret_loser_3[slice52],\r\n",
        "       train_regret_loser_4[slice52],\r\n",
        "       train_regret_loser_5[slice52],\r\n",
        "       train_regret_loser_6[slice52],\r\n",
        "       train_regret_loser_7[slice52],\r\n",
        "       train_regret_loser_8[slice52],\r\n",
        "       train_regret_loser_9[slice52],\r\n",
        "       train_regret_loser_10[slice52],\r\n",
        "       train_regret_loser_11[slice52],\r\n",
        "       train_regret_loser_12[slice52],\r\n",
        "       train_regret_loser_13[slice52],\r\n",
        "       train_regret_loser_14[slice52],\r\n",
        "       train_regret_loser_15[slice52],\r\n",
        "       train_regret_loser_16[slice52],\r\n",
        "       train_regret_loser_17[slice52],\r\n",
        "       train_regret_loser_18[slice52],\r\n",
        "       train_regret_loser_19[slice52],\r\n",
        "       train_regret_loser_20[slice52]]\r\n",
        "\r\n",
        "winner52 = [train_regret_winner_1[slice52],\r\n",
        "       train_regret_winner_2[slice52],\r\n",
        "       train_regret_winner_3[slice52],\r\n",
        "       train_regret_winner_4[slice52],\r\n",
        "       train_regret_winner_5[slice52],\r\n",
        "       train_regret_winner_6[slice52],\r\n",
        "       train_regret_winner_7[slice52],\r\n",
        "       train_regret_winner_8[slice52],\r\n",
        "       train_regret_winner_9[slice52],\r\n",
        "       train_regret_winner_10[slice52],\r\n",
        "       train_regret_winner_11[slice52],\r\n",
        "       train_regret_winner_12[slice52],\r\n",
        "       train_regret_winner_13[slice52],\r\n",
        "       train_regret_winner_14[slice52],\r\n",
        "       train_regret_winner_15[slice52],\r\n",
        "       train_regret_winner_16[slice52],\r\n",
        "       train_regret_winner_17[slice52],\r\n",
        "       train_regret_winner_18[slice52],\r\n",
        "       train_regret_winner_19[slice52],\r\n",
        "       train_regret_winner_20[slice52]]\r\n",
        "\r\n",
        "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\r\n",
        "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\r\n",
        "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\r\n",
        "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\r\n",
        "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\r\n",
        "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9cJNkew-nY"
      },
      "source": [
        "# Iteration62 :\r\n",
        "\r\n",
        "slice62 = 61\r\n",
        "\r\n",
        "loser62 = [train_regret_loser_1[slice62],\r\n",
        "       train_regret_loser_2[slice62],\r\n",
        "       train_regret_loser_3[slice62],\r\n",
        "       train_regret_loser_4[slice62],\r\n",
        "       train_regret_loser_5[slice62],\r\n",
        "       train_regret_loser_6[slice62],\r\n",
        "       train_regret_loser_7[slice62],\r\n",
        "       train_regret_loser_8[slice62],\r\n",
        "       train_regret_loser_9[slice62],\r\n",
        "       train_regret_loser_10[slice62],\r\n",
        "       train_regret_loser_11[slice62],\r\n",
        "       train_regret_loser_12[slice62],\r\n",
        "       train_regret_loser_13[slice62],\r\n",
        "       train_regret_loser_14[slice62],\r\n",
        "       train_regret_loser_15[slice62],\r\n",
        "       train_regret_loser_16[slice62],\r\n",
        "       train_regret_loser_17[slice62],\r\n",
        "       train_regret_loser_18[slice62],\r\n",
        "       train_regret_loser_19[slice62],\r\n",
        "       train_regret_loser_20[slice62]]\r\n",
        "\r\n",
        "winner62 = [train_regret_winner_1[slice62],\r\n",
        "       train_regret_winner_2[slice62],\r\n",
        "       train_regret_winner_3[slice62],\r\n",
        "       train_regret_winner_4[slice62],\r\n",
        "       train_regret_winner_5[slice62],\r\n",
        "       train_regret_winner_6[slice62],\r\n",
        "       train_regret_winner_7[slice62],\r\n",
        "       train_regret_winner_8[slice62],\r\n",
        "       train_regret_winner_9[slice62],\r\n",
        "       train_regret_winner_10[slice62],\r\n",
        "       train_regret_winner_11[slice62],\r\n",
        "       train_regret_winner_12[slice62],\r\n",
        "       train_regret_winner_13[slice62],\r\n",
        "       train_regret_winner_14[slice62],\r\n",
        "       train_regret_winner_15[slice62],\r\n",
        "       train_regret_winner_16[slice62],\r\n",
        "       train_regret_winner_17[slice62],\r\n",
        "       train_regret_winner_18[slice62],\r\n",
        "       train_regret_winner_19[slice62],\r\n",
        "       train_regret_winner_20[slice62]]\r\n",
        "\r\n",
        "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\r\n",
        "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\r\n",
        "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\r\n",
        "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\r\n",
        "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\r\n",
        "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DZ8kHHmw-qK"
      },
      "source": [
        "# Iteration72 :\r\n",
        "\r\n",
        "slice72 = 71\r\n",
        "\r\n",
        "loser72 = [train_regret_loser_1[slice72],\r\n",
        "       train_regret_loser_2[slice72],\r\n",
        "       train_regret_loser_3[slice72],\r\n",
        "       train_regret_loser_4[slice72],\r\n",
        "       train_regret_loser_5[slice72],\r\n",
        "       train_regret_loser_6[slice72],\r\n",
        "       train_regret_loser_7[slice72],\r\n",
        "       train_regret_loser_8[slice72],\r\n",
        "       train_regret_loser_9[slice72],\r\n",
        "       train_regret_loser_10[slice72],\r\n",
        "       train_regret_loser_11[slice72],\r\n",
        "       train_regret_loser_12[slice72],\r\n",
        "       train_regret_loser_13[slice72],\r\n",
        "       train_regret_loser_14[slice72],\r\n",
        "       train_regret_loser_15[slice72],\r\n",
        "       train_regret_loser_16[slice72],\r\n",
        "       train_regret_loser_17[slice72],\r\n",
        "       train_regret_loser_18[slice72],\r\n",
        "       train_regret_loser_19[slice72],\r\n",
        "       train_regret_loser_20[slice72]]\r\n",
        "\r\n",
        "winner72 = [train_regret_winner_1[slice72],\r\n",
        "       train_regret_winner_2[slice72],\r\n",
        "       train_regret_winner_3[slice72],\r\n",
        "       train_regret_winner_4[slice72],\r\n",
        "       train_regret_winner_5[slice72],\r\n",
        "       train_regret_winner_6[slice72],\r\n",
        "       train_regret_winner_7[slice72],\r\n",
        "       train_regret_winner_8[slice72],\r\n",
        "       train_regret_winner_9[slice72],\r\n",
        "       train_regret_winner_10[slice72],\r\n",
        "       train_regret_winner_11[slice72],\r\n",
        "       train_regret_winner_12[slice72],\r\n",
        "       train_regret_winner_13[slice72],\r\n",
        "       train_regret_winner_14[slice72],\r\n",
        "       train_regret_winner_15[slice72],\r\n",
        "       train_regret_winner_16[slice72],\r\n",
        "       train_regret_winner_17[slice72],\r\n",
        "       train_regret_winner_18[slice72],\r\n",
        "       train_regret_winner_19[slice72],\r\n",
        "       train_regret_winner_20[slice72]]\r\n",
        "\r\n",
        "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\r\n",
        "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\r\n",
        "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\r\n",
        "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\r\n",
        "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\r\n",
        "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBzBvnVyw-sz"
      },
      "source": [
        "# Iteration82 :\r\n",
        "\r\n",
        "slice82 = 81\r\n",
        "\r\n",
        "loser82 = [train_regret_loser_1[slice82],\r\n",
        "       train_regret_loser_2[slice82],\r\n",
        "       train_regret_loser_3[slice82],\r\n",
        "       train_regret_loser_4[slice82],\r\n",
        "       train_regret_loser_5[slice82],\r\n",
        "       train_regret_loser_6[slice82],\r\n",
        "       train_regret_loser_7[slice82],\r\n",
        "       train_regret_loser_8[slice82],\r\n",
        "       train_regret_loser_9[slice82],\r\n",
        "       train_regret_loser_10[slice82],\r\n",
        "       train_regret_loser_11[slice82],\r\n",
        "       train_regret_loser_12[slice82],\r\n",
        "       train_regret_loser_13[slice82],\r\n",
        "       train_regret_loser_14[slice82],\r\n",
        "       train_regret_loser_15[slice82],\r\n",
        "       train_regret_loser_16[slice82],\r\n",
        "       train_regret_loser_17[slice82],\r\n",
        "       train_regret_loser_18[slice82],\r\n",
        "       train_regret_loser_19[slice82],\r\n",
        "       train_regret_loser_20[slice82]]\r\n",
        "\r\n",
        "winner82 = [train_regret_winner_1[slice82],\r\n",
        "       train_regret_winner_2[slice82],\r\n",
        "       train_regret_winner_3[slice82],\r\n",
        "       train_regret_winner_4[slice82],\r\n",
        "       train_regret_winner_5[slice82],\r\n",
        "       train_regret_winner_6[slice82],\r\n",
        "       train_regret_winner_7[slice82],\r\n",
        "       train_regret_winner_8[slice82],\r\n",
        "       train_regret_winner_9[slice82],\r\n",
        "       train_regret_winner_10[slice82],\r\n",
        "       train_regret_winner_11[slice82],\r\n",
        "       train_regret_winner_12[slice82],\r\n",
        "       train_regret_winner_13[slice82],\r\n",
        "       train_regret_winner_14[slice82],\r\n",
        "       train_regret_winner_15[slice82],\r\n",
        "       train_regret_winner_16[slice82],\r\n",
        "       train_regret_winner_17[slice82],\r\n",
        "       train_regret_winner_18[slice82],\r\n",
        "       train_regret_winner_19[slice82],\r\n",
        "       train_regret_winner_20[slice82]]\r\n",
        "\r\n",
        "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\r\n",
        "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\r\n",
        "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\r\n",
        "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\r\n",
        "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\r\n",
        "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qj--n6Bw-y2"
      },
      "source": [
        "# Iteration92 :\r\n",
        "\r\n",
        "slice92 = 91\r\n",
        "\r\n",
        "loser92 = [train_regret_loser_1[slice92],\r\n",
        "       train_regret_loser_2[slice92],\r\n",
        "       train_regret_loser_3[slice92],\r\n",
        "       train_regret_loser_4[slice92],\r\n",
        "       train_regret_loser_5[slice92],\r\n",
        "       train_regret_loser_6[slice92],\r\n",
        "       train_regret_loser_7[slice92],\r\n",
        "       train_regret_loser_8[slice92],\r\n",
        "       train_regret_loser_9[slice92],\r\n",
        "       train_regret_loser_10[slice92],\r\n",
        "       train_regret_loser_11[slice92],\r\n",
        "       train_regret_loser_12[slice92],\r\n",
        "       train_regret_loser_13[slice92],\r\n",
        "       train_regret_loser_14[slice92],\r\n",
        "       train_regret_loser_15[slice92],\r\n",
        "       train_regret_loser_16[slice92],\r\n",
        "       train_regret_loser_17[slice92],\r\n",
        "       train_regret_loser_18[slice92],\r\n",
        "       train_regret_loser_19[slice92],\r\n",
        "       train_regret_loser_20[slice92]]\r\n",
        "\r\n",
        "winner92 = [train_regret_winner_1[slice92],\r\n",
        "       train_regret_winner_2[slice92],\r\n",
        "       train_regret_winner_3[slice92],\r\n",
        "       train_regret_winner_4[slice92],\r\n",
        "       train_regret_winner_5[slice92],\r\n",
        "       train_regret_winner_6[slice92],\r\n",
        "       train_regret_winner_7[slice92],\r\n",
        "       train_regret_winner_8[slice92],\r\n",
        "       train_regret_winner_9[slice92],\r\n",
        "       train_regret_winner_10[slice92],\r\n",
        "       train_regret_winner_11[slice92],\r\n",
        "       train_regret_winner_12[slice92],\r\n",
        "       train_regret_winner_13[slice92],\r\n",
        "       train_regret_winner_14[slice92],\r\n",
        "       train_regret_winner_15[slice92],\r\n",
        "       train_regret_winner_16[slice92],\r\n",
        "       train_regret_winner_17[slice92],\r\n",
        "       train_regret_winner_18[slice92],\r\n",
        "       train_regret_winner_19[slice92],\r\n",
        "       train_regret_winner_20[slice92]]\r\n",
        "\r\n",
        "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\r\n",
        "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\r\n",
        "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\r\n",
        "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\r\n",
        "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\r\n",
        "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJ5KqLdw-0L"
      },
      "source": [
        "# Iteration3 :\r\n",
        "\r\n",
        "slice3 = 2\r\n",
        "\r\n",
        "loser3 = [train_regret_loser_1[slice3],\r\n",
        "       train_regret_loser_2[slice3],\r\n",
        "       train_regret_loser_3[slice3],\r\n",
        "       train_regret_loser_4[slice3],\r\n",
        "       train_regret_loser_5[slice3],\r\n",
        "       train_regret_loser_6[slice3],\r\n",
        "       train_regret_loser_7[slice3],\r\n",
        "       train_regret_loser_8[slice3],\r\n",
        "       train_regret_loser_9[slice3],\r\n",
        "       train_regret_loser_10[slice3],\r\n",
        "       train_regret_loser_11[slice3],\r\n",
        "       train_regret_loser_12[slice3],\r\n",
        "       train_regret_loser_13[slice3],\r\n",
        "       train_regret_loser_14[slice3],\r\n",
        "       train_regret_loser_15[slice3],\r\n",
        "       train_regret_loser_16[slice3],\r\n",
        "       train_regret_loser_17[slice3],\r\n",
        "       train_regret_loser_18[slice3],\r\n",
        "       train_regret_loser_19[slice3],\r\n",
        "       train_regret_loser_20[slice3]]\r\n",
        "\r\n",
        "winner3 = [train_regret_winner_1[slice3],\r\n",
        "       train_regret_winner_2[slice3],\r\n",
        "       train_regret_winner_3[slice3],\r\n",
        "       train_regret_winner_4[slice3],\r\n",
        "       train_regret_winner_5[slice3],\r\n",
        "       train_regret_winner_6[slice3],\r\n",
        "       train_regret_winner_7[slice3],\r\n",
        "       train_regret_winner_8[slice3],\r\n",
        "       train_regret_winner_9[slice3],\r\n",
        "       train_regret_winner_10[slice3],\r\n",
        "       train_regret_winner_11[slice3],\r\n",
        "       train_regret_winner_12[slice3],\r\n",
        "       train_regret_winner_13[slice3],\r\n",
        "       train_regret_winner_14[slice3],\r\n",
        "       train_regret_winner_15[slice3],\r\n",
        "       train_regret_winner_16[slice3],\r\n",
        "       train_regret_winner_17[slice3],\r\n",
        "       train_regret_winner_18[slice3],\r\n",
        "       train_regret_winner_19[slice3],\r\n",
        "       train_regret_winner_20[slice3]]\r\n",
        "\r\n",
        "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\r\n",
        "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\r\n",
        "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\r\n",
        "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\r\n",
        "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\r\n",
        "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkaoT1dyw-16"
      },
      "source": [
        "# Iteration13 :\r\n",
        "\r\n",
        "slice13 = 12\r\n",
        "\r\n",
        "loser13 = [train_regret_loser_1[slice13],\r\n",
        "       train_regret_loser_2[slice13],\r\n",
        "       train_regret_loser_3[slice13],\r\n",
        "       train_regret_loser_4[slice13],\r\n",
        "       train_regret_loser_5[slice13],\r\n",
        "       train_regret_loser_6[slice13],\r\n",
        "       train_regret_loser_7[slice13],\r\n",
        "       train_regret_loser_8[slice13],\r\n",
        "       train_regret_loser_9[slice13],\r\n",
        "       train_regret_loser_10[slice13],\r\n",
        "       train_regret_loser_11[slice13],\r\n",
        "       train_regret_loser_12[slice13],\r\n",
        "       train_regret_loser_13[slice13],\r\n",
        "       train_regret_loser_14[slice13],\r\n",
        "       train_regret_loser_15[slice13],\r\n",
        "       train_regret_loser_16[slice13],\r\n",
        "       train_regret_loser_17[slice13],\r\n",
        "       train_regret_loser_18[slice13],\r\n",
        "       train_regret_loser_19[slice13],\r\n",
        "       train_regret_loser_20[slice13]]\r\n",
        "\r\n",
        "winner13 = [train_regret_winner_1[slice13],\r\n",
        "       train_regret_winner_2[slice13],\r\n",
        "       train_regret_winner_3[slice13],\r\n",
        "       train_regret_winner_4[slice13],\r\n",
        "       train_regret_winner_5[slice13],\r\n",
        "       train_regret_winner_6[slice13],\r\n",
        "       train_regret_winner_7[slice13],\r\n",
        "       train_regret_winner_8[slice13],\r\n",
        "       train_regret_winner_9[slice13],\r\n",
        "       train_regret_winner_10[slice13],\r\n",
        "       train_regret_winner_11[slice13],\r\n",
        "       train_regret_winner_12[slice13],\r\n",
        "       train_regret_winner_13[slice13],\r\n",
        "       train_regret_winner_14[slice13],\r\n",
        "       train_regret_winner_15[slice13],\r\n",
        "       train_regret_winner_16[slice13],\r\n",
        "       train_regret_winner_17[slice13],\r\n",
        "       train_regret_winner_18[slice13],\r\n",
        "       train_regret_winner_19[slice13],\r\n",
        "       train_regret_winner_20[slice13]]\r\n",
        "\r\n",
        "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\r\n",
        "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\r\n",
        "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\r\n",
        "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\r\n",
        "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZbVMrBaw-4B"
      },
      "source": [
        "# Iteration23 :\r\n",
        "\r\n",
        "slice23 = 22\r\n",
        "\r\n",
        "loser23 = [train_regret_loser_1[slice23],\r\n",
        "       train_regret_loser_2[slice23],\r\n",
        "       train_regret_loser_3[slice23],\r\n",
        "       train_regret_loser_4[slice23],\r\n",
        "       train_regret_loser_5[slice23],\r\n",
        "       train_regret_loser_6[slice23],\r\n",
        "       train_regret_loser_7[slice23],\r\n",
        "       train_regret_loser_8[slice23],\r\n",
        "       train_regret_loser_9[slice23],\r\n",
        "       train_regret_loser_10[slice23],\r\n",
        "       train_regret_loser_11[slice23],\r\n",
        "       train_regret_loser_12[slice23],\r\n",
        "       train_regret_loser_13[slice23],\r\n",
        "       train_regret_loser_14[slice23],\r\n",
        "       train_regret_loser_15[slice23],\r\n",
        "       train_regret_loser_16[slice23],\r\n",
        "       train_regret_loser_17[slice23],\r\n",
        "       train_regret_loser_18[slice23],\r\n",
        "       train_regret_loser_19[slice23],\r\n",
        "       train_regret_loser_20[slice23]]\r\n",
        "\r\n",
        "winner23 = [train_regret_winner_1[slice23],\r\n",
        "       train_regret_winner_2[slice23],\r\n",
        "       train_regret_winner_3[slice23],\r\n",
        "       train_regret_winner_4[slice23],\r\n",
        "       train_regret_winner_5[slice23],\r\n",
        "       train_regret_winner_6[slice23],\r\n",
        "       train_regret_winner_7[slice23],\r\n",
        "       train_regret_winner_8[slice23],\r\n",
        "       train_regret_winner_9[slice23],\r\n",
        "       train_regret_winner_10[slice23],\r\n",
        "       train_regret_winner_11[slice23],\r\n",
        "       train_regret_winner_12[slice23],\r\n",
        "       train_regret_winner_13[slice23],\r\n",
        "       train_regret_winner_14[slice23],\r\n",
        "       train_regret_winner_15[slice23],\r\n",
        "       train_regret_winner_16[slice23],\r\n",
        "       train_regret_winner_17[slice23],\r\n",
        "       train_regret_winner_18[slice23],\r\n",
        "       train_regret_winner_19[slice23],\r\n",
        "       train_regret_winner_20[slice23]]\r\n",
        "\r\n",
        "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\r\n",
        "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\r\n",
        "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\r\n",
        "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\r\n",
        "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\r\n",
        "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUa6HRCUw-7G"
      },
      "source": [
        "# Iteration33 :\r\n",
        "\r\n",
        "slice33 = 32\r\n",
        "\r\n",
        "loser33 = [train_regret_loser_1[slice33],\r\n",
        "       train_regret_loser_2[slice33],\r\n",
        "       train_regret_loser_3[slice33],\r\n",
        "       train_regret_loser_4[slice33],\r\n",
        "       train_regret_loser_5[slice33],\r\n",
        "       train_regret_loser_6[slice33],\r\n",
        "       train_regret_loser_7[slice33],\r\n",
        "       train_regret_loser_8[slice33],\r\n",
        "       train_regret_loser_9[slice33],\r\n",
        "       train_regret_loser_10[slice33],\r\n",
        "       train_regret_loser_11[slice33],\r\n",
        "       train_regret_loser_12[slice33],\r\n",
        "       train_regret_loser_13[slice33],\r\n",
        "       train_regret_loser_14[slice33],\r\n",
        "       train_regret_loser_15[slice33],\r\n",
        "       train_regret_loser_16[slice33],\r\n",
        "       train_regret_loser_17[slice33],\r\n",
        "       train_regret_loser_18[slice33],\r\n",
        "       train_regret_loser_19[slice33],\r\n",
        "       train_regret_loser_20[slice33]]\r\n",
        "\r\n",
        "winner33 = [train_regret_winner_1[slice33],\r\n",
        "       train_regret_winner_2[slice33],\r\n",
        "       train_regret_winner_3[slice33],\r\n",
        "       train_regret_winner_4[slice33],\r\n",
        "       train_regret_winner_5[slice33],\r\n",
        "       train_regret_winner_6[slice33],\r\n",
        "       train_regret_winner_7[slice33],\r\n",
        "       train_regret_winner_8[slice33],\r\n",
        "       train_regret_winner_9[slice33],\r\n",
        "       train_regret_winner_10[slice33],\r\n",
        "       train_regret_winner_11[slice33],\r\n",
        "       train_regret_winner_12[slice33],\r\n",
        "       train_regret_winner_13[slice33],\r\n",
        "       train_regret_winner_14[slice33],\r\n",
        "       train_regret_winner_15[slice33],\r\n",
        "       train_regret_winner_16[slice33],\r\n",
        "       train_regret_winner_17[slice33],\r\n",
        "       train_regret_winner_18[slice33],\r\n",
        "       train_regret_winner_19[slice33],\r\n",
        "       train_regret_winner_20[slice33]]\r\n",
        "\r\n",
        "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\r\n",
        "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\r\n",
        "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\r\n",
        "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\r\n",
        "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\r\n",
        "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EVxQwsxw-9n"
      },
      "source": [
        "# Iteration43 :\r\n",
        "\r\n",
        "slice43 = 42\r\n",
        "\r\n",
        "loser43 = [train_regret_loser_1[slice43],\r\n",
        "       train_regret_loser_2[slice43],\r\n",
        "       train_regret_loser_3[slice43],\r\n",
        "       train_regret_loser_4[slice43],\r\n",
        "       train_regret_loser_5[slice43],\r\n",
        "       train_regret_loser_6[slice43],\r\n",
        "       train_regret_loser_7[slice43],\r\n",
        "       train_regret_loser_8[slice43],\r\n",
        "       train_regret_loser_9[slice43],\r\n",
        "       train_regret_loser_10[slice43],\r\n",
        "       train_regret_loser_11[slice43],\r\n",
        "       train_regret_loser_12[slice43],\r\n",
        "       train_regret_loser_13[slice43],\r\n",
        "       train_regret_loser_14[slice43],\r\n",
        "       train_regret_loser_15[slice43],\r\n",
        "       train_regret_loser_16[slice43],\r\n",
        "       train_regret_loser_17[slice43],\r\n",
        "       train_regret_loser_18[slice43],\r\n",
        "       train_regret_loser_19[slice43],\r\n",
        "       train_regret_loser_20[slice43]]\r\n",
        "\r\n",
        "winner43 = [train_regret_winner_1[slice43],\r\n",
        "       train_regret_winner_2[slice43],\r\n",
        "       train_regret_winner_3[slice43],\r\n",
        "       train_regret_winner_4[slice43],\r\n",
        "       train_regret_winner_5[slice43],\r\n",
        "       train_regret_winner_6[slice43],\r\n",
        "       train_regret_winner_7[slice43],\r\n",
        "       train_regret_winner_8[slice43],\r\n",
        "       train_regret_winner_9[slice43],\r\n",
        "       train_regret_winner_10[slice43],\r\n",
        "       train_regret_winner_11[slice43],\r\n",
        "       train_regret_winner_12[slice43],\r\n",
        "       train_regret_winner_13[slice43],\r\n",
        "       train_regret_winner_14[slice43],\r\n",
        "       train_regret_winner_15[slice43],\r\n",
        "       train_regret_winner_16[slice43],\r\n",
        "       train_regret_winner_17[slice43],\r\n",
        "       train_regret_winner_18[slice43],\r\n",
        "       train_regret_winner_19[slice43],\r\n",
        "       train_regret_winner_20[slice43]]\r\n",
        "\r\n",
        "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\r\n",
        "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\r\n",
        "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\r\n",
        "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\r\n",
        "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\r\n",
        "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQtRyMv-w_AZ"
      },
      "source": [
        "# Iteration53 :\r\n",
        "\r\n",
        "slice53 = 52\r\n",
        "\r\n",
        "loser53 = [train_regret_loser_1[slice53],\r\n",
        "       train_regret_loser_2[slice53],\r\n",
        "       train_regret_loser_3[slice53],\r\n",
        "       train_regret_loser_4[slice53],\r\n",
        "       train_regret_loser_5[slice53],\r\n",
        "       train_regret_loser_6[slice53],\r\n",
        "       train_regret_loser_7[slice53],\r\n",
        "       train_regret_loser_8[slice53],\r\n",
        "       train_regret_loser_9[slice53],\r\n",
        "       train_regret_loser_10[slice53],\r\n",
        "       train_regret_loser_11[slice53],\r\n",
        "       train_regret_loser_12[slice53],\r\n",
        "       train_regret_loser_13[slice53],\r\n",
        "       train_regret_loser_14[slice53],\r\n",
        "       train_regret_loser_15[slice53],\r\n",
        "       train_regret_loser_16[slice53],\r\n",
        "       train_regret_loser_17[slice53],\r\n",
        "       train_regret_loser_18[slice53],\r\n",
        "       train_regret_loser_19[slice53],\r\n",
        "       train_regret_loser_20[slice53]]\r\n",
        "\r\n",
        "winner53 = [train_regret_winner_1[slice53],\r\n",
        "       train_regret_winner_2[slice53],\r\n",
        "       train_regret_winner_3[slice53],\r\n",
        "       train_regret_winner_4[slice53],\r\n",
        "       train_regret_winner_5[slice53],\r\n",
        "       train_regret_winner_6[slice53],\r\n",
        "       train_regret_winner_7[slice53],\r\n",
        "       train_regret_winner_8[slice53],\r\n",
        "       train_regret_winner_9[slice53],\r\n",
        "       train_regret_winner_10[slice53],\r\n",
        "       train_regret_winner_11[slice53],\r\n",
        "       train_regret_winner_12[slice53],\r\n",
        "       train_regret_winner_13[slice53],\r\n",
        "       train_regret_winner_14[slice53],\r\n",
        "       train_regret_winner_15[slice53],\r\n",
        "       train_regret_winner_16[slice53],\r\n",
        "       train_regret_winner_17[slice53],\r\n",
        "       train_regret_winner_18[slice53],\r\n",
        "       train_regret_winner_19[slice53],\r\n",
        "       train_regret_winner_20[slice53]]\r\n",
        "\r\n",
        "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\r\n",
        "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\r\n",
        "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\r\n",
        "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\r\n",
        "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\r\n",
        "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CIMEXXYw_DR"
      },
      "source": [
        "# Iteration63 :\r\n",
        "\r\n",
        "slice63 = 62\r\n",
        "\r\n",
        "loser63 = [train_regret_loser_1[slice63],\r\n",
        "       train_regret_loser_2[slice63],\r\n",
        "       train_regret_loser_3[slice63],\r\n",
        "       train_regret_loser_4[slice63],\r\n",
        "       train_regret_loser_5[slice63],\r\n",
        "       train_regret_loser_6[slice63],\r\n",
        "       train_regret_loser_7[slice63],\r\n",
        "       train_regret_loser_8[slice63],\r\n",
        "       train_regret_loser_9[slice63],\r\n",
        "       train_regret_loser_10[slice63],\r\n",
        "       train_regret_loser_11[slice63],\r\n",
        "       train_regret_loser_12[slice63],\r\n",
        "       train_regret_loser_13[slice63],\r\n",
        "       train_regret_loser_14[slice63],\r\n",
        "       train_regret_loser_15[slice63],\r\n",
        "       train_regret_loser_16[slice63],\r\n",
        "       train_regret_loser_17[slice63],\r\n",
        "       train_regret_loser_18[slice63],\r\n",
        "       train_regret_loser_19[slice63],\r\n",
        "       train_regret_loser_20[slice63]]\r\n",
        "\r\n",
        "winner63 = [train_regret_winner_1[slice63],\r\n",
        "       train_regret_winner_2[slice63],\r\n",
        "       train_regret_winner_3[slice63],\r\n",
        "       train_regret_winner_4[slice63],\r\n",
        "       train_regret_winner_5[slice63],\r\n",
        "       train_regret_winner_6[slice63],\r\n",
        "       train_regret_winner_7[slice63],\r\n",
        "       train_regret_winner_8[slice63],\r\n",
        "       train_regret_winner_9[slice63],\r\n",
        "       train_regret_winner_10[slice63],\r\n",
        "       train_regret_winner_11[slice63],\r\n",
        "       train_regret_winner_12[slice63],\r\n",
        "       train_regret_winner_13[slice63],\r\n",
        "       train_regret_winner_14[slice63],\r\n",
        "       train_regret_winner_15[slice63],\r\n",
        "       train_regret_winner_16[slice63],\r\n",
        "       train_regret_winner_17[slice63],\r\n",
        "       train_regret_winner_18[slice63],\r\n",
        "       train_regret_winner_19[slice63],\r\n",
        "       train_regret_winner_20[slice63]]\r\n",
        "\r\n",
        "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\r\n",
        "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\r\n",
        "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\r\n",
        "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\r\n",
        "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\r\n",
        "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg4GfwU_w_GF"
      },
      "source": [
        "# Iteration73 :\r\n",
        "\r\n",
        "slice73 = 72\r\n",
        "\r\n",
        "loser73 = [train_regret_loser_1[slice73],\r\n",
        "       train_regret_loser_2[slice73],\r\n",
        "       train_regret_loser_3[slice73],\r\n",
        "       train_regret_loser_4[slice73],\r\n",
        "       train_regret_loser_5[slice73],\r\n",
        "       train_regret_loser_6[slice73],\r\n",
        "       train_regret_loser_7[slice73],\r\n",
        "       train_regret_loser_8[slice73],\r\n",
        "       train_regret_loser_9[slice73],\r\n",
        "       train_regret_loser_10[slice73],\r\n",
        "       train_regret_loser_11[slice73],\r\n",
        "       train_regret_loser_12[slice73],\r\n",
        "       train_regret_loser_13[slice73],\r\n",
        "       train_regret_loser_14[slice73],\r\n",
        "       train_regret_loser_15[slice73],\r\n",
        "       train_regret_loser_16[slice73],\r\n",
        "       train_regret_loser_17[slice73],\r\n",
        "       train_regret_loser_18[slice73],\r\n",
        "       train_regret_loser_19[slice73],\r\n",
        "       train_regret_loser_20[slice73]]\r\n",
        "\r\n",
        "winner73 = [train_regret_winner_1[slice73],\r\n",
        "       train_regret_winner_2[slice73],\r\n",
        "       train_regret_winner_3[slice73],\r\n",
        "       train_regret_winner_4[slice73],\r\n",
        "       train_regret_winner_5[slice73],\r\n",
        "       train_regret_winner_6[slice73],\r\n",
        "       train_regret_winner_7[slice73],\r\n",
        "       train_regret_winner_8[slice73],\r\n",
        "       train_regret_winner_9[slice73],\r\n",
        "       train_regret_winner_10[slice73],\r\n",
        "       train_regret_winner_11[slice73],\r\n",
        "       train_regret_winner_12[slice73],\r\n",
        "       train_regret_winner_13[slice73],\r\n",
        "       train_regret_winner_14[slice73],\r\n",
        "       train_regret_winner_15[slice73],\r\n",
        "       train_regret_winner_16[slice73],\r\n",
        "       train_regret_winner_17[slice73],\r\n",
        "       train_regret_winner_18[slice73],\r\n",
        "       train_regret_winner_19[slice73],\r\n",
        "       train_regret_winner_20[slice73]]\r\n",
        "\r\n",
        "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\r\n",
        "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\r\n",
        "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\r\n",
        "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\r\n",
        "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\r\n",
        "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WmpcEV4w_I9"
      },
      "source": [
        "# Iteration83 :\r\n",
        "\r\n",
        "slice83 = 82\r\n",
        "\r\n",
        "loser83 = [train_regret_loser_1[slice83],\r\n",
        "       train_regret_loser_2[slice83],\r\n",
        "       train_regret_loser_3[slice83],\r\n",
        "       train_regret_loser_4[slice83],\r\n",
        "       train_regret_loser_5[slice83],\r\n",
        "       train_regret_loser_6[slice83],\r\n",
        "       train_regret_loser_7[slice83],\r\n",
        "       train_regret_loser_8[slice83],\r\n",
        "       train_regret_loser_9[slice83],\r\n",
        "       train_regret_loser_10[slice83],\r\n",
        "       train_regret_loser_11[slice83],\r\n",
        "       train_regret_loser_12[slice83],\r\n",
        "       train_regret_loser_13[slice83],\r\n",
        "       train_regret_loser_14[slice83],\r\n",
        "       train_regret_loser_15[slice83],\r\n",
        "       train_regret_loser_16[slice83],\r\n",
        "       train_regret_loser_17[slice83],\r\n",
        "       train_regret_loser_18[slice83],\r\n",
        "       train_regret_loser_19[slice83],\r\n",
        "       train_regret_loser_20[slice83]]\r\n",
        "\r\n",
        "winner83 = [train_regret_winner_1[slice83],\r\n",
        "       train_regret_winner_2[slice83],\r\n",
        "       train_regret_winner_3[slice83],\r\n",
        "       train_regret_winner_4[slice83],\r\n",
        "       train_regret_winner_5[slice83],\r\n",
        "       train_regret_winner_6[slice83],\r\n",
        "       train_regret_winner_7[slice83],\r\n",
        "       train_regret_winner_8[slice83],\r\n",
        "       train_regret_winner_9[slice83],\r\n",
        "       train_regret_winner_10[slice83],\r\n",
        "       train_regret_winner_11[slice83],\r\n",
        "       train_regret_winner_12[slice83],\r\n",
        "       train_regret_winner_13[slice83],\r\n",
        "       train_regret_winner_14[slice83],\r\n",
        "       train_regret_winner_15[slice83],\r\n",
        "       train_regret_winner_16[slice83],\r\n",
        "       train_regret_winner_17[slice83],\r\n",
        "       train_regret_winner_18[slice83],\r\n",
        "       train_regret_winner_19[slice83],\r\n",
        "       train_regret_winner_20[slice83]]\r\n",
        "\r\n",
        "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\r\n",
        "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\r\n",
        "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\r\n",
        "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\r\n",
        "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\r\n",
        "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGPwoBZyw_MF"
      },
      "source": [
        "# Iteration93 :\r\n",
        "\r\n",
        "slice93 = 92\r\n",
        "\r\n",
        "loser93 = [train_regret_loser_1[slice93],\r\n",
        "       train_regret_loser_2[slice93],\r\n",
        "       train_regret_loser_3[slice93],\r\n",
        "       train_regret_loser_4[slice93],\r\n",
        "       train_regret_loser_5[slice93],\r\n",
        "       train_regret_loser_6[slice93],\r\n",
        "       train_regret_loser_7[slice93],\r\n",
        "       train_regret_loser_8[slice93],\r\n",
        "       train_regret_loser_9[slice93],\r\n",
        "       train_regret_loser_10[slice93],\r\n",
        "       train_regret_loser_11[slice93],\r\n",
        "       train_regret_loser_12[slice93],\r\n",
        "       train_regret_loser_13[slice93],\r\n",
        "       train_regret_loser_14[slice93],\r\n",
        "       train_regret_loser_15[slice93],\r\n",
        "       train_regret_loser_16[slice93],\r\n",
        "       train_regret_loser_17[slice93],\r\n",
        "       train_regret_loser_18[slice93],\r\n",
        "       train_regret_loser_19[slice93],\r\n",
        "       train_regret_loser_20[slice93]]\r\n",
        "\r\n",
        "winner93 = [train_regret_winner_1[slice93],\r\n",
        "       train_regret_winner_2[slice93],\r\n",
        "       train_regret_winner_3[slice93],\r\n",
        "       train_regret_winner_4[slice93],\r\n",
        "       train_regret_winner_5[slice93],\r\n",
        "       train_regret_winner_6[slice93],\r\n",
        "       train_regret_winner_7[slice93],\r\n",
        "       train_regret_winner_8[slice93],\r\n",
        "       train_regret_winner_9[slice93],\r\n",
        "       train_regret_winner_10[slice93],\r\n",
        "       train_regret_winner_11[slice93],\r\n",
        "       train_regret_winner_12[slice93],\r\n",
        "       train_regret_winner_13[slice93],\r\n",
        "       train_regret_winner_14[slice93],\r\n",
        "       train_regret_winner_15[slice93],\r\n",
        "       train_regret_winner_16[slice93],\r\n",
        "       train_regret_winner_17[slice93],\r\n",
        "       train_regret_winner_18[slice93],\r\n",
        "       train_regret_winner_19[slice93],\r\n",
        "       train_regret_winner_20[slice93]]\r\n",
        "\r\n",
        "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\r\n",
        "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\r\n",
        "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\r\n",
        "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\r\n",
        "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\r\n",
        "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ghdGzPw_Oq"
      },
      "source": [
        "# Iteration4 :\r\n",
        "\r\n",
        "slice4 = 3\r\n",
        "\r\n",
        "loser4 = [train_regret_loser_1[slice4],\r\n",
        "       train_regret_loser_2[slice4],\r\n",
        "       train_regret_loser_3[slice4],\r\n",
        "       train_regret_loser_4[slice4],\r\n",
        "       train_regret_loser_5[slice4],\r\n",
        "       train_regret_loser_6[slice4],\r\n",
        "       train_regret_loser_7[slice4],\r\n",
        "       train_regret_loser_8[slice4],\r\n",
        "       train_regret_loser_9[slice4],\r\n",
        "       train_regret_loser_10[slice4],\r\n",
        "       train_regret_loser_11[slice4],\r\n",
        "       train_regret_loser_12[slice4],\r\n",
        "       train_regret_loser_13[slice4],\r\n",
        "       train_regret_loser_14[slice4],\r\n",
        "       train_regret_loser_15[slice4],\r\n",
        "       train_regret_loser_16[slice4],\r\n",
        "       train_regret_loser_17[slice4],\r\n",
        "       train_regret_loser_18[slice4],\r\n",
        "       train_regret_loser_19[slice4],\r\n",
        "       train_regret_loser_20[slice4]]\r\n",
        "\r\n",
        "winner4 = [train_regret_winner_1[slice4],\r\n",
        "       train_regret_winner_2[slice4],\r\n",
        "       train_regret_winner_3[slice4],\r\n",
        "       train_regret_winner_4[slice4],\r\n",
        "       train_regret_winner_5[slice4],\r\n",
        "       train_regret_winner_6[slice4],\r\n",
        "       train_regret_winner_7[slice4],\r\n",
        "       train_regret_winner_8[slice4],\r\n",
        "       train_regret_winner_9[slice4],\r\n",
        "       train_regret_winner_10[slice4],\r\n",
        "       train_regret_winner_11[slice4],\r\n",
        "       train_regret_winner_12[slice4],\r\n",
        "       train_regret_winner_13[slice4],\r\n",
        "       train_regret_winner_14[slice4],\r\n",
        "       train_regret_winner_15[slice4],\r\n",
        "       train_regret_winner_16[slice4],\r\n",
        "       train_regret_winner_17[slice4],\r\n",
        "       train_regret_winner_18[slice4],\r\n",
        "       train_regret_winner_19[slice4],\r\n",
        "       train_regret_winner_20[slice4]]\r\n",
        "\r\n",
        "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\r\n",
        "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\r\n",
        "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\r\n",
        "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\r\n",
        "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\r\n",
        "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNvDt4QMw_Rz"
      },
      "source": [
        "# Iteration14 :\r\n",
        "\r\n",
        "slice14 = 13\r\n",
        "\r\n",
        "loser14 = [train_regret_loser_1[slice14],\r\n",
        "       train_regret_loser_2[slice14],\r\n",
        "       train_regret_loser_3[slice14],\r\n",
        "       train_regret_loser_4[slice14],\r\n",
        "       train_regret_loser_5[slice14],\r\n",
        "       train_regret_loser_6[slice14],\r\n",
        "       train_regret_loser_7[slice14],\r\n",
        "       train_regret_loser_8[slice14],\r\n",
        "       train_regret_loser_9[slice14],\r\n",
        "       train_regret_loser_10[slice14],\r\n",
        "       train_regret_loser_11[slice14],\r\n",
        "       train_regret_loser_12[slice14],\r\n",
        "       train_regret_loser_13[slice14],\r\n",
        "       train_regret_loser_14[slice14],\r\n",
        "       train_regret_loser_15[slice14],\r\n",
        "       train_regret_loser_16[slice14],\r\n",
        "       train_regret_loser_17[slice14],\r\n",
        "       train_regret_loser_18[slice14],\r\n",
        "       train_regret_loser_19[slice14],\r\n",
        "       train_regret_loser_20[slice14]]\r\n",
        "\r\n",
        "winner14 = [train_regret_winner_1[slice14],\r\n",
        "       train_regret_winner_2[slice14],\r\n",
        "       train_regret_winner_3[slice14],\r\n",
        "       train_regret_winner_4[slice14],\r\n",
        "       train_regret_winner_5[slice14],\r\n",
        "       train_regret_winner_6[slice14],\r\n",
        "       train_regret_winner_7[slice14],\r\n",
        "       train_regret_winner_8[slice14],\r\n",
        "       train_regret_winner_9[slice14],\r\n",
        "       train_regret_winner_10[slice14],\r\n",
        "       train_regret_winner_11[slice14],\r\n",
        "       train_regret_winner_12[slice14],\r\n",
        "       train_regret_winner_13[slice14],\r\n",
        "       train_regret_winner_14[slice14],\r\n",
        "       train_regret_winner_15[slice14],\r\n",
        "       train_regret_winner_16[slice14],\r\n",
        "       train_regret_winner_17[slice14],\r\n",
        "       train_regret_winner_18[slice14],\r\n",
        "       train_regret_winner_19[slice14],\r\n",
        "       train_regret_winner_20[slice14]]\r\n",
        "\r\n",
        "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\r\n",
        "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\r\n",
        "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\r\n",
        "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\r\n",
        "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\r\n",
        "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3kMQV0w_Ua"
      },
      "source": [
        "# Iteration24 :\r\n",
        "\r\n",
        "slice24 = 23\r\n",
        "\r\n",
        "loser24 = [train_regret_loser_1[slice24],\r\n",
        "       train_regret_loser_2[slice24],\r\n",
        "       train_regret_loser_3[slice24],\r\n",
        "       train_regret_loser_4[slice24],\r\n",
        "       train_regret_loser_5[slice24],\r\n",
        "       train_regret_loser_6[slice24],\r\n",
        "       train_regret_loser_7[slice24],\r\n",
        "       train_regret_loser_8[slice24],\r\n",
        "       train_regret_loser_9[slice24],\r\n",
        "       train_regret_loser_10[slice24],\r\n",
        "       train_regret_loser_11[slice24],\r\n",
        "       train_regret_loser_12[slice24],\r\n",
        "       train_regret_loser_13[slice24],\r\n",
        "       train_regret_loser_14[slice24],\r\n",
        "       train_regret_loser_15[slice24],\r\n",
        "       train_regret_loser_16[slice24],\r\n",
        "       train_regret_loser_17[slice24],\r\n",
        "       train_regret_loser_18[slice24],\r\n",
        "       train_regret_loser_19[slice24],\r\n",
        "       train_regret_loser_20[slice24]]\r\n",
        "\r\n",
        "winner24 = [train_regret_winner_1[slice24],\r\n",
        "       train_regret_winner_2[slice24],\r\n",
        "       train_regret_winner_3[slice24],\r\n",
        "       train_regret_winner_4[slice24],\r\n",
        "       train_regret_winner_5[slice24],\r\n",
        "       train_regret_winner_6[slice24],\r\n",
        "       train_regret_winner_7[slice24],\r\n",
        "       train_regret_winner_8[slice24],\r\n",
        "       train_regret_winner_9[slice24],\r\n",
        "       train_regret_winner_10[slice24],\r\n",
        "       train_regret_winner_11[slice24],\r\n",
        "       train_regret_winner_12[slice24],\r\n",
        "       train_regret_winner_13[slice24],\r\n",
        "       train_regret_winner_14[slice24],\r\n",
        "       train_regret_winner_15[slice24],\r\n",
        "       train_regret_winner_16[slice24],\r\n",
        "       train_regret_winner_17[slice24],\r\n",
        "       train_regret_winner_18[slice24],\r\n",
        "       train_regret_winner_19[slice24],\r\n",
        "       train_regret_winner_20[slice24]]\r\n",
        "\r\n",
        "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\r\n",
        "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\r\n",
        "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\r\n",
        "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\r\n",
        "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\r\n",
        "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ljkyMfw_Xj"
      },
      "source": [
        "# Iteration34 :\r\n",
        "\r\n",
        "slice34 = 33\r\n",
        "\r\n",
        "loser34 = [train_regret_loser_1[slice34],\r\n",
        "       train_regret_loser_2[slice34],\r\n",
        "       train_regret_loser_3[slice34],\r\n",
        "       train_regret_loser_4[slice34],\r\n",
        "       train_regret_loser_5[slice34],\r\n",
        "       train_regret_loser_6[slice34],\r\n",
        "       train_regret_loser_7[slice34],\r\n",
        "       train_regret_loser_8[slice34],\r\n",
        "       train_regret_loser_9[slice34],\r\n",
        "       train_regret_loser_10[slice34],\r\n",
        "       train_regret_loser_11[slice34],\r\n",
        "       train_regret_loser_12[slice34],\r\n",
        "       train_regret_loser_13[slice34],\r\n",
        "       train_regret_loser_14[slice34],\r\n",
        "       train_regret_loser_15[slice34],\r\n",
        "       train_regret_loser_16[slice34],\r\n",
        "       train_regret_loser_17[slice34],\r\n",
        "       train_regret_loser_18[slice34],\r\n",
        "       train_regret_loser_19[slice34],\r\n",
        "       train_regret_loser_20[slice34]]\r\n",
        "\r\n",
        "winner34 = [train_regret_winner_1[slice34],\r\n",
        "       train_regret_winner_2[slice34],\r\n",
        "       train_regret_winner_3[slice34],\r\n",
        "       train_regret_winner_4[slice34],\r\n",
        "       train_regret_winner_5[slice34],\r\n",
        "       train_regret_winner_6[slice34],\r\n",
        "       train_regret_winner_7[slice34],\r\n",
        "       train_regret_winner_8[slice34],\r\n",
        "       train_regret_winner_9[slice34],\r\n",
        "       train_regret_winner_10[slice34],\r\n",
        "       train_regret_winner_11[slice34],\r\n",
        "       train_regret_winner_12[slice34],\r\n",
        "       train_regret_winner_13[slice34],\r\n",
        "       train_regret_winner_14[slice34],\r\n",
        "       train_regret_winner_15[slice34],\r\n",
        "       train_regret_winner_16[slice34],\r\n",
        "       train_regret_winner_17[slice34],\r\n",
        "       train_regret_winner_18[slice34],\r\n",
        "       train_regret_winner_19[slice34],\r\n",
        "       train_regret_winner_20[slice34]]\r\n",
        "\r\n",
        "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\r\n",
        "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\r\n",
        "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\r\n",
        "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\r\n",
        "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\r\n",
        "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGKj-NR8w_ae"
      },
      "source": [
        "# Iteration44 :\r\n",
        "\r\n",
        "slice44 = 43\r\n",
        "\r\n",
        "loser44 = [train_regret_loser_1[slice44],\r\n",
        "       train_regret_loser_2[slice44],\r\n",
        "       train_regret_loser_3[slice44],\r\n",
        "       train_regret_loser_4[slice44],\r\n",
        "       train_regret_loser_5[slice44],\r\n",
        "       train_regret_loser_6[slice44],\r\n",
        "       train_regret_loser_7[slice44],\r\n",
        "       train_regret_loser_8[slice44],\r\n",
        "       train_regret_loser_9[slice44],\r\n",
        "       train_regret_loser_10[slice44],\r\n",
        "       train_regret_loser_11[slice44],\r\n",
        "       train_regret_loser_12[slice44],\r\n",
        "       train_regret_loser_13[slice44],\r\n",
        "       train_regret_loser_14[slice44],\r\n",
        "       train_regret_loser_15[slice44],\r\n",
        "       train_regret_loser_16[slice44],\r\n",
        "       train_regret_loser_17[slice44],\r\n",
        "       train_regret_loser_18[slice44],\r\n",
        "       train_regret_loser_19[slice44],\r\n",
        "       train_regret_loser_20[slice44]]\r\n",
        "\r\n",
        "winner44 = [train_regret_winner_1[slice44],\r\n",
        "       train_regret_winner_2[slice44],\r\n",
        "       train_regret_winner_3[slice44],\r\n",
        "       train_regret_winner_4[slice44],\r\n",
        "       train_regret_winner_5[slice44],\r\n",
        "       train_regret_winner_6[slice44],\r\n",
        "       train_regret_winner_7[slice44],\r\n",
        "       train_regret_winner_8[slice44],\r\n",
        "       train_regret_winner_9[slice44],\r\n",
        "       train_regret_winner_10[slice44],\r\n",
        "       train_regret_winner_11[slice44],\r\n",
        "       train_regret_winner_12[slice44],\r\n",
        "       train_regret_winner_13[slice44],\r\n",
        "       train_regret_winner_14[slice44],\r\n",
        "       train_regret_winner_15[slice44],\r\n",
        "       train_regret_winner_16[slice44],\r\n",
        "       train_regret_winner_17[slice44],\r\n",
        "       train_regret_winner_18[slice44],\r\n",
        "       train_regret_winner_19[slice44],\r\n",
        "       train_regret_winner_20[slice44]]\r\n",
        "\r\n",
        "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\r\n",
        "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\r\n",
        "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\r\n",
        "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\r\n",
        "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\r\n",
        "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJzx094Fw_c_"
      },
      "source": [
        "# Iteration54 :\r\n",
        "\r\n",
        "slice54 = 53\r\n",
        "\r\n",
        "loser54 = [train_regret_loser_1[slice54],\r\n",
        "       train_regret_loser_2[slice54],\r\n",
        "       train_regret_loser_3[slice54],\r\n",
        "       train_regret_loser_4[slice54],\r\n",
        "       train_regret_loser_5[slice54],\r\n",
        "       train_regret_loser_6[slice54],\r\n",
        "       train_regret_loser_7[slice54],\r\n",
        "       train_regret_loser_8[slice54],\r\n",
        "       train_regret_loser_9[slice54],\r\n",
        "       train_regret_loser_10[slice54],\r\n",
        "       train_regret_loser_11[slice54],\r\n",
        "       train_regret_loser_12[slice54],\r\n",
        "       train_regret_loser_13[slice54],\r\n",
        "       train_regret_loser_14[slice54],\r\n",
        "       train_regret_loser_15[slice54],\r\n",
        "       train_regret_loser_16[slice54],\r\n",
        "       train_regret_loser_17[slice54],\r\n",
        "       train_regret_loser_18[slice54],\r\n",
        "       train_regret_loser_19[slice54],\r\n",
        "       train_regret_loser_20[slice54]]\r\n",
        "\r\n",
        "winner54 = [train_regret_winner_1[slice54],\r\n",
        "       train_regret_winner_2[slice54],\r\n",
        "       train_regret_winner_3[slice54],\r\n",
        "       train_regret_winner_4[slice54],\r\n",
        "       train_regret_winner_5[slice54],\r\n",
        "       train_regret_winner_6[slice54],\r\n",
        "       train_regret_winner_7[slice54],\r\n",
        "       train_regret_winner_8[slice54],\r\n",
        "       train_regret_winner_9[slice54],\r\n",
        "       train_regret_winner_10[slice54],\r\n",
        "       train_regret_winner_11[slice54],\r\n",
        "       train_regret_winner_12[slice54],\r\n",
        "       train_regret_winner_13[slice54],\r\n",
        "       train_regret_winner_14[slice54],\r\n",
        "       train_regret_winner_15[slice54],\r\n",
        "       train_regret_winner_16[slice54],\r\n",
        "       train_regret_winner_17[slice54],\r\n",
        "       train_regret_winner_18[slice54],\r\n",
        "       train_regret_winner_19[slice54],\r\n",
        "       train_regret_winner_20[slice54]]\r\n",
        "\r\n",
        "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\r\n",
        "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\r\n",
        "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\r\n",
        "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\r\n",
        "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\r\n",
        "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEVYMqj_w_fr"
      },
      "source": [
        "# Iteration64 :\r\n",
        "\r\n",
        "slice64 = 63\r\n",
        "\r\n",
        "loser64 = [train_regret_loser_1[slice64],\r\n",
        "       train_regret_loser_2[slice64],\r\n",
        "       train_regret_loser_3[slice64],\r\n",
        "       train_regret_loser_4[slice64],\r\n",
        "       train_regret_loser_5[slice64],\r\n",
        "       train_regret_loser_6[slice64],\r\n",
        "       train_regret_loser_7[slice64],\r\n",
        "       train_regret_loser_8[slice64],\r\n",
        "       train_regret_loser_9[slice64],\r\n",
        "       train_regret_loser_10[slice64],\r\n",
        "       train_regret_loser_11[slice64],\r\n",
        "       train_regret_loser_12[slice64],\r\n",
        "       train_regret_loser_13[slice64],\r\n",
        "       train_regret_loser_14[slice64],\r\n",
        "       train_regret_loser_15[slice64],\r\n",
        "       train_regret_loser_16[slice64],\r\n",
        "       train_regret_loser_17[slice64],\r\n",
        "       train_regret_loser_18[slice64],\r\n",
        "       train_regret_loser_19[slice64],\r\n",
        "       train_regret_loser_20[slice64]]\r\n",
        "\r\n",
        "winner64 = [train_regret_winner_1[slice64],\r\n",
        "       train_regret_winner_2[slice64],\r\n",
        "       train_regret_winner_3[slice64],\r\n",
        "       train_regret_winner_4[slice64],\r\n",
        "       train_regret_winner_5[slice64],\r\n",
        "       train_regret_winner_6[slice64],\r\n",
        "       train_regret_winner_7[slice64],\r\n",
        "       train_regret_winner_8[slice64],\r\n",
        "       train_regret_winner_9[slice64],\r\n",
        "       train_regret_winner_10[slice64],\r\n",
        "       train_regret_winner_11[slice64],\r\n",
        "       train_regret_winner_12[slice64],\r\n",
        "       train_regret_winner_13[slice64],\r\n",
        "       train_regret_winner_14[slice64],\r\n",
        "       train_regret_winner_15[slice64],\r\n",
        "       train_regret_winner_16[slice64],\r\n",
        "       train_regret_winner_17[slice64],\r\n",
        "       train_regret_winner_18[slice64],\r\n",
        "       train_regret_winner_19[slice64],\r\n",
        "       train_regret_winner_20[slice64]]\r\n",
        "\r\n",
        "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\r\n",
        "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\r\n",
        "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\r\n",
        "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\r\n",
        "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\r\n",
        "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvr3S_p0w_ii"
      },
      "source": [
        "# Iteration74 :\r\n",
        "\r\n",
        "slice74 = 73\r\n",
        "\r\n",
        "loser74 = [train_regret_loser_1[slice74],\r\n",
        "       train_regret_loser_2[slice74],\r\n",
        "       train_regret_loser_3[slice74],\r\n",
        "       train_regret_loser_4[slice74],\r\n",
        "       train_regret_loser_5[slice74],\r\n",
        "       train_regret_loser_6[slice74],\r\n",
        "       train_regret_loser_7[slice74],\r\n",
        "       train_regret_loser_8[slice74],\r\n",
        "       train_regret_loser_9[slice74],\r\n",
        "       train_regret_loser_10[slice74],\r\n",
        "       train_regret_loser_11[slice74],\r\n",
        "       train_regret_loser_12[slice74],\r\n",
        "       train_regret_loser_13[slice74],\r\n",
        "       train_regret_loser_14[slice74],\r\n",
        "       train_regret_loser_15[slice74],\r\n",
        "       train_regret_loser_16[slice74],\r\n",
        "       train_regret_loser_17[slice74],\r\n",
        "       train_regret_loser_18[slice74],\r\n",
        "       train_regret_loser_19[slice74],\r\n",
        "       train_regret_loser_20[slice74]]\r\n",
        "\r\n",
        "winner74 = [train_regret_winner_1[slice74],\r\n",
        "       train_regret_winner_2[slice74],\r\n",
        "       train_regret_winner_3[slice74],\r\n",
        "       train_regret_winner_4[slice74],\r\n",
        "       train_regret_winner_5[slice74],\r\n",
        "       train_regret_winner_6[slice74],\r\n",
        "       train_regret_winner_7[slice74],\r\n",
        "       train_regret_winner_8[slice74],\r\n",
        "       train_regret_winner_9[slice74],\r\n",
        "       train_regret_winner_10[slice74],\r\n",
        "       train_regret_winner_11[slice74],\r\n",
        "       train_regret_winner_12[slice74],\r\n",
        "       train_regret_winner_13[slice74],\r\n",
        "       train_regret_winner_14[slice74],\r\n",
        "       train_regret_winner_15[slice74],\r\n",
        "       train_regret_winner_16[slice74],\r\n",
        "       train_regret_winner_17[slice74],\r\n",
        "       train_regret_winner_18[slice74],\r\n",
        "       train_regret_winner_19[slice74],\r\n",
        "       train_regret_winner_20[slice74]]\r\n",
        "\r\n",
        "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\r\n",
        "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\r\n",
        "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\r\n",
        "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\r\n",
        "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\r\n",
        "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEAvUVK7w_lO"
      },
      "source": [
        "# Iteration84 :\r\n",
        "\r\n",
        "slice84 = 83\r\n",
        "\r\n",
        "loser84 = [train_regret_loser_1[slice84],\r\n",
        "       train_regret_loser_2[slice84],\r\n",
        "       train_regret_loser_3[slice84],\r\n",
        "       train_regret_loser_4[slice84],\r\n",
        "       train_regret_loser_5[slice84],\r\n",
        "       train_regret_loser_6[slice84],\r\n",
        "       train_regret_loser_7[slice84],\r\n",
        "       train_regret_loser_8[slice84],\r\n",
        "       train_regret_loser_9[slice84],\r\n",
        "       train_regret_loser_10[slice84],\r\n",
        "       train_regret_loser_11[slice84],\r\n",
        "       train_regret_loser_12[slice84],\r\n",
        "       train_regret_loser_13[slice84],\r\n",
        "       train_regret_loser_14[slice84],\r\n",
        "       train_regret_loser_15[slice84],\r\n",
        "       train_regret_loser_16[slice84],\r\n",
        "       train_regret_loser_17[slice84],\r\n",
        "       train_regret_loser_18[slice84],\r\n",
        "       train_regret_loser_19[slice84],\r\n",
        "       train_regret_loser_20[slice84]]\r\n",
        "\r\n",
        "winner84 = [train_regret_winner_1[slice84],\r\n",
        "       train_regret_winner_2[slice84],\r\n",
        "       train_regret_winner_3[slice84],\r\n",
        "       train_regret_winner_4[slice84],\r\n",
        "       train_regret_winner_5[slice84],\r\n",
        "       train_regret_winner_6[slice84],\r\n",
        "       train_regret_winner_7[slice84],\r\n",
        "       train_regret_winner_8[slice84],\r\n",
        "       train_regret_winner_9[slice84],\r\n",
        "       train_regret_winner_10[slice84],\r\n",
        "       train_regret_winner_11[slice84],\r\n",
        "       train_regret_winner_12[slice84],\r\n",
        "       train_regret_winner_13[slice84],\r\n",
        "       train_regret_winner_14[slice84],\r\n",
        "       train_regret_winner_15[slice84],\r\n",
        "       train_regret_winner_16[slice84],\r\n",
        "       train_regret_winner_17[slice84],\r\n",
        "       train_regret_winner_18[slice84],\r\n",
        "       train_regret_winner_19[slice84],\r\n",
        "       train_regret_winner_20[slice84]]\r\n",
        "\r\n",
        "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\r\n",
        "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\r\n",
        "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\r\n",
        "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\r\n",
        "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\r\n",
        "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMZzhdGw_oC"
      },
      "source": [
        "# Iteration94 :\r\n",
        "\r\n",
        "slice94 = 93\r\n",
        "\r\n",
        "loser94 = [train_regret_loser_1[slice94],\r\n",
        "       train_regret_loser_2[slice94],\r\n",
        "       train_regret_loser_3[slice94],\r\n",
        "       train_regret_loser_4[slice94],\r\n",
        "       train_regret_loser_5[slice94],\r\n",
        "       train_regret_loser_6[slice94],\r\n",
        "       train_regret_loser_7[slice94],\r\n",
        "       train_regret_loser_8[slice94],\r\n",
        "       train_regret_loser_9[slice94],\r\n",
        "       train_regret_loser_10[slice94],\r\n",
        "       train_regret_loser_11[slice94],\r\n",
        "       train_regret_loser_12[slice94],\r\n",
        "       train_regret_loser_13[slice94],\r\n",
        "       train_regret_loser_14[slice94],\r\n",
        "       train_regret_loser_15[slice94],\r\n",
        "       train_regret_loser_16[slice94],\r\n",
        "       train_regret_loser_17[slice94],\r\n",
        "       train_regret_loser_18[slice94],\r\n",
        "       train_regret_loser_19[slice94],\r\n",
        "       train_regret_loser_20[slice94]]\r\n",
        "\r\n",
        "winner94 = [train_regret_winner_1[slice94],\r\n",
        "       train_regret_winner_2[slice94],\r\n",
        "       train_regret_winner_3[slice94],\r\n",
        "       train_regret_winner_4[slice94],\r\n",
        "       train_regret_winner_5[slice94],\r\n",
        "       train_regret_winner_6[slice94],\r\n",
        "       train_regret_winner_7[slice94],\r\n",
        "       train_regret_winner_8[slice94],\r\n",
        "       train_regret_winner_9[slice94],\r\n",
        "       train_regret_winner_10[slice94],\r\n",
        "       train_regret_winner_11[slice94],\r\n",
        "       train_regret_winner_12[slice94],\r\n",
        "       train_regret_winner_13[slice94],\r\n",
        "       train_regret_winner_14[slice94],\r\n",
        "       train_regret_winner_15[slice94],\r\n",
        "       train_regret_winner_16[slice94],\r\n",
        "       train_regret_winner_17[slice94],\r\n",
        "       train_regret_winner_18[slice94],\r\n",
        "       train_regret_winner_19[slice94],\r\n",
        "       train_regret_winner_20[slice94]]\r\n",
        "\r\n",
        "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\r\n",
        "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\r\n",
        "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\r\n",
        "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\r\n",
        "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\r\n",
        "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMxHQOgaw_qu"
      },
      "source": [
        "# Iteration5 :\r\n",
        "\r\n",
        "slice5 = 4\r\n",
        "\r\n",
        "loser5 = [train_regret_loser_1[slice5],\r\n",
        "       train_regret_loser_2[slice5],\r\n",
        "       train_regret_loser_3[slice5],\r\n",
        "       train_regret_loser_4[slice5],\r\n",
        "       train_regret_loser_5[slice5],\r\n",
        "       train_regret_loser_6[slice5],\r\n",
        "       train_regret_loser_7[slice5],\r\n",
        "       train_regret_loser_8[slice5],\r\n",
        "       train_regret_loser_9[slice5],\r\n",
        "       train_regret_loser_10[slice5],\r\n",
        "       train_regret_loser_11[slice5],\r\n",
        "       train_regret_loser_12[slice5],\r\n",
        "       train_regret_loser_13[slice5],\r\n",
        "       train_regret_loser_14[slice5],\r\n",
        "       train_regret_loser_15[slice5],\r\n",
        "       train_regret_loser_16[slice5],\r\n",
        "       train_regret_loser_17[slice5],\r\n",
        "       train_regret_loser_18[slice5],\r\n",
        "       train_regret_loser_19[slice5],\r\n",
        "       train_regret_loser_20[slice5]]\r\n",
        "\r\n",
        "winner5 = [train_regret_winner_1[slice5],\r\n",
        "       train_regret_winner_2[slice5],\r\n",
        "       train_regret_winner_3[slice5],\r\n",
        "       train_regret_winner_4[slice5],\r\n",
        "       train_regret_winner_5[slice5],\r\n",
        "       train_regret_winner_6[slice5],\r\n",
        "       train_regret_winner_7[slice5],\r\n",
        "       train_regret_winner_8[slice5],\r\n",
        "       train_regret_winner_9[slice5],\r\n",
        "       train_regret_winner_10[slice5],\r\n",
        "       train_regret_winner_11[slice5],\r\n",
        "       train_regret_winner_12[slice5],\r\n",
        "       train_regret_winner_13[slice5],\r\n",
        "       train_regret_winner_14[slice5],\r\n",
        "       train_regret_winner_15[slice5],\r\n",
        "       train_regret_winner_16[slice5],\r\n",
        "       train_regret_winner_17[slice5],\r\n",
        "       train_regret_winner_18[slice5],\r\n",
        "       train_regret_winner_19[slice5],\r\n",
        "       train_regret_winner_20[slice5]]\r\n",
        "\r\n",
        "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\r\n",
        "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\r\n",
        "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\r\n",
        "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\r\n",
        "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\r\n",
        "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPVFQTfNw_wF"
      },
      "source": [
        "# Iteration15 :\r\n",
        "\r\n",
        "slice15 = 14\r\n",
        "\r\n",
        "loser15 = [train_regret_loser_1[slice15],\r\n",
        "       train_regret_loser_2[slice15],\r\n",
        "       train_regret_loser_3[slice15],\r\n",
        "       train_regret_loser_4[slice15],\r\n",
        "       train_regret_loser_5[slice15],\r\n",
        "       train_regret_loser_6[slice15],\r\n",
        "       train_regret_loser_7[slice15],\r\n",
        "       train_regret_loser_8[slice15],\r\n",
        "       train_regret_loser_9[slice15],\r\n",
        "       train_regret_loser_10[slice15],\r\n",
        "       train_regret_loser_11[slice15],\r\n",
        "       train_regret_loser_12[slice15],\r\n",
        "       train_regret_loser_13[slice15],\r\n",
        "       train_regret_loser_14[slice15],\r\n",
        "       train_regret_loser_15[slice15],\r\n",
        "       train_regret_loser_16[slice15],\r\n",
        "       train_regret_loser_17[slice15],\r\n",
        "       train_regret_loser_18[slice15],\r\n",
        "       train_regret_loser_19[slice15],\r\n",
        "       train_regret_loser_20[slice15]]\r\n",
        "\r\n",
        "winner15 = [train_regret_winner_1[slice15],\r\n",
        "       train_regret_winner_2[slice15],\r\n",
        "       train_regret_winner_3[slice15],\r\n",
        "       train_regret_winner_4[slice15],\r\n",
        "       train_regret_winner_5[slice15],\r\n",
        "       train_regret_winner_6[slice15],\r\n",
        "       train_regret_winner_7[slice15],\r\n",
        "       train_regret_winner_8[slice15],\r\n",
        "       train_regret_winner_9[slice15],\r\n",
        "       train_regret_winner_10[slice15],\r\n",
        "       train_regret_winner_11[slice15],\r\n",
        "       train_regret_winner_12[slice15],\r\n",
        "       train_regret_winner_13[slice15],\r\n",
        "       train_regret_winner_14[slice15],\r\n",
        "       train_regret_winner_15[slice15],\r\n",
        "       train_regret_winner_16[slice15],\r\n",
        "       train_regret_winner_17[slice15],\r\n",
        "       train_regret_winner_18[slice15],\r\n",
        "       train_regret_winner_19[slice15],\r\n",
        "       train_regret_winner_20[slice15]]\r\n",
        "\r\n",
        "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\r\n",
        "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\r\n",
        "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\r\n",
        "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\r\n",
        "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\r\n",
        "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADBmBA3ew_zK"
      },
      "source": [
        "# Iteration25 :\r\n",
        "\r\n",
        "slice25 = 24\r\n",
        "\r\n",
        "loser25 = [train_regret_loser_1[slice25],\r\n",
        "       train_regret_loser_2[slice25],\r\n",
        "       train_regret_loser_3[slice25],\r\n",
        "       train_regret_loser_4[slice25],\r\n",
        "       train_regret_loser_5[slice25],\r\n",
        "       train_regret_loser_6[slice25],\r\n",
        "       train_regret_loser_7[slice25],\r\n",
        "       train_regret_loser_8[slice25],\r\n",
        "       train_regret_loser_9[slice25],\r\n",
        "       train_regret_loser_10[slice25],\r\n",
        "       train_regret_loser_11[slice25],\r\n",
        "       train_regret_loser_12[slice25],\r\n",
        "       train_regret_loser_13[slice25],\r\n",
        "       train_regret_loser_14[slice25],\r\n",
        "       train_regret_loser_15[slice25],\r\n",
        "       train_regret_loser_16[slice25],\r\n",
        "       train_regret_loser_17[slice25],\r\n",
        "       train_regret_loser_18[slice25],\r\n",
        "       train_regret_loser_19[slice25],\r\n",
        "       train_regret_loser_20[slice25]]\r\n",
        "\r\n",
        "winner25 = [train_regret_winner_1[slice25],\r\n",
        "       train_regret_winner_2[slice25],\r\n",
        "       train_regret_winner_3[slice25],\r\n",
        "       train_regret_winner_4[slice25],\r\n",
        "       train_regret_winner_5[slice25],\r\n",
        "       train_regret_winner_6[slice25],\r\n",
        "       train_regret_winner_7[slice25],\r\n",
        "       train_regret_winner_8[slice25],\r\n",
        "       train_regret_winner_9[slice25],\r\n",
        "       train_regret_winner_10[slice25],\r\n",
        "       train_regret_winner_11[slice25],\r\n",
        "       train_regret_winner_12[slice25],\r\n",
        "       train_regret_winner_13[slice25],\r\n",
        "       train_regret_winner_14[slice25],\r\n",
        "       train_regret_winner_15[slice25],\r\n",
        "       train_regret_winner_16[slice25],\r\n",
        "       train_regret_winner_17[slice25],\r\n",
        "       train_regret_winner_18[slice25],\r\n",
        "       train_regret_winner_19[slice25],\r\n",
        "       train_regret_winner_20[slice25]]\r\n",
        "\r\n",
        "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\r\n",
        "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\r\n",
        "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\r\n",
        "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\r\n",
        "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\r\n",
        "upper_winner25 = np.asarray(winner25_results[14:15][0])[0]"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUk9UtSTw_2N"
      },
      "source": [
        "# Iteration35 :\r\n",
        "\r\n",
        "slice35 = 34\r\n",
        "\r\n",
        "loser35 = [train_regret_loser_1[slice35],\r\n",
        "       train_regret_loser_2[slice35],\r\n",
        "       train_regret_loser_3[slice35],\r\n",
        "       train_regret_loser_4[slice35],\r\n",
        "       train_regret_loser_5[slice35],\r\n",
        "       train_regret_loser_6[slice35],\r\n",
        "       train_regret_loser_7[slice35],\r\n",
        "       train_regret_loser_8[slice35],\r\n",
        "       train_regret_loser_9[slice35],\r\n",
        "       train_regret_loser_10[slice35],\r\n",
        "       train_regret_loser_11[slice35],\r\n",
        "       train_regret_loser_12[slice35],\r\n",
        "       train_regret_loser_13[slice35],\r\n",
        "       train_regret_loser_14[slice35],\r\n",
        "       train_regret_loser_15[slice35],\r\n",
        "       train_regret_loser_16[slice35],\r\n",
        "       train_regret_loser_17[slice35],\r\n",
        "       train_regret_loser_18[slice35],\r\n",
        "       train_regret_loser_19[slice35],\r\n",
        "       train_regret_loser_20[slice35]]\r\n",
        "\r\n",
        "winner35 = [train_regret_winner_1[slice35],\r\n",
        "       train_regret_winner_2[slice35],\r\n",
        "       train_regret_winner_3[slice35],\r\n",
        "       train_regret_winner_4[slice35],\r\n",
        "       train_regret_winner_5[slice35],\r\n",
        "       train_regret_winner_6[slice35],\r\n",
        "       train_regret_winner_7[slice35],\r\n",
        "       train_regret_winner_8[slice35],\r\n",
        "       train_regret_winner_9[slice35],\r\n",
        "       train_regret_winner_10[slice35],\r\n",
        "       train_regret_winner_11[slice35],\r\n",
        "       train_regret_winner_12[slice35],\r\n",
        "       train_regret_winner_13[slice35],\r\n",
        "       train_regret_winner_14[slice35],\r\n",
        "       train_regret_winner_15[slice35],\r\n",
        "       train_regret_winner_16[slice35],\r\n",
        "       train_regret_winner_17[slice35],\r\n",
        "       train_regret_winner_18[slice35],\r\n",
        "       train_regret_winner_19[slice35],\r\n",
        "       train_regret_winner_20[slice35]]\r\n",
        "\r\n",
        "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\r\n",
        "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\r\n",
        "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\r\n",
        "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\r\n",
        "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\r\n",
        "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX3VgqD_w_4z"
      },
      "source": [
        "# Iteration45 :\r\n",
        "\r\n",
        "slice45 = 44\r\n",
        "\r\n",
        "loser45 = [train_regret_loser_1[slice45],\r\n",
        "       train_regret_loser_2[slice45],\r\n",
        "       train_regret_loser_3[slice45],\r\n",
        "       train_regret_loser_4[slice45],\r\n",
        "       train_regret_loser_5[slice45],\r\n",
        "       train_regret_loser_6[slice45],\r\n",
        "       train_regret_loser_7[slice45],\r\n",
        "       train_regret_loser_8[slice45],\r\n",
        "       train_regret_loser_9[slice45],\r\n",
        "       train_regret_loser_10[slice45],\r\n",
        "       train_regret_loser_11[slice45],\r\n",
        "       train_regret_loser_12[slice45],\r\n",
        "       train_regret_loser_13[slice45],\r\n",
        "       train_regret_loser_14[slice45],\r\n",
        "       train_regret_loser_15[slice45],\r\n",
        "       train_regret_loser_16[slice45],\r\n",
        "       train_regret_loser_17[slice45],\r\n",
        "       train_regret_loser_18[slice45],\r\n",
        "       train_regret_loser_19[slice45],\r\n",
        "       train_regret_loser_20[slice45]]\r\n",
        "\r\n",
        "winner45 = [train_regret_winner_1[slice45],\r\n",
        "       train_regret_winner_2[slice45],\r\n",
        "       train_regret_winner_3[slice45],\r\n",
        "       train_regret_winner_4[slice45],\r\n",
        "       train_regret_winner_5[slice45],\r\n",
        "       train_regret_winner_6[slice45],\r\n",
        "       train_regret_winner_7[slice45],\r\n",
        "       train_regret_winner_8[slice45],\r\n",
        "       train_regret_winner_9[slice45],\r\n",
        "       train_regret_winner_10[slice45],\r\n",
        "       train_regret_winner_11[slice45],\r\n",
        "       train_regret_winner_12[slice45],\r\n",
        "       train_regret_winner_13[slice45],\r\n",
        "       train_regret_winner_14[slice45],\r\n",
        "       train_regret_winner_15[slice45],\r\n",
        "       train_regret_winner_16[slice45],\r\n",
        "       train_regret_winner_17[slice45],\r\n",
        "       train_regret_winner_18[slice45],\r\n",
        "       train_regret_winner_19[slice45],\r\n",
        "       train_regret_winner_20[slice45]]\r\n",
        "\r\n",
        "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\r\n",
        "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\r\n",
        "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\r\n",
        "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\r\n",
        "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\r\n",
        "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o35W-AOzw_8F"
      },
      "source": [
        "# Iteration55 :\r\n",
        "\r\n",
        "slice55 = 54\r\n",
        "\r\n",
        "loser55 = [train_regret_loser_1[slice55],\r\n",
        "       train_regret_loser_2[slice55],\r\n",
        "       train_regret_loser_3[slice55],\r\n",
        "       train_regret_loser_4[slice55],\r\n",
        "       train_regret_loser_5[slice55],\r\n",
        "       train_regret_loser_6[slice55],\r\n",
        "       train_regret_loser_7[slice55],\r\n",
        "       train_regret_loser_8[slice55],\r\n",
        "       train_regret_loser_9[slice55],\r\n",
        "       train_regret_loser_10[slice55],\r\n",
        "       train_regret_loser_11[slice55],\r\n",
        "       train_regret_loser_12[slice55],\r\n",
        "       train_regret_loser_13[slice55],\r\n",
        "       train_regret_loser_14[slice55],\r\n",
        "       train_regret_loser_15[slice55],\r\n",
        "       train_regret_loser_16[slice55],\r\n",
        "       train_regret_loser_17[slice55],\r\n",
        "       train_regret_loser_18[slice55],\r\n",
        "       train_regret_loser_19[slice55],\r\n",
        "       train_regret_loser_20[slice55]]\r\n",
        "\r\n",
        "winner55 = [train_regret_winner_1[slice55],\r\n",
        "       train_regret_winner_2[slice55],\r\n",
        "       train_regret_winner_3[slice55],\r\n",
        "       train_regret_winner_4[slice55],\r\n",
        "       train_regret_winner_5[slice55],\r\n",
        "       train_regret_winner_6[slice55],\r\n",
        "       train_regret_winner_7[slice55],\r\n",
        "       train_regret_winner_8[slice55],\r\n",
        "       train_regret_winner_9[slice55],\r\n",
        "       train_regret_winner_10[slice55],\r\n",
        "       train_regret_winner_11[slice55],\r\n",
        "       train_regret_winner_12[slice55],\r\n",
        "       train_regret_winner_13[slice55],\r\n",
        "       train_regret_winner_14[slice55],\r\n",
        "       train_regret_winner_15[slice55],\r\n",
        "       train_regret_winner_16[slice55],\r\n",
        "       train_regret_winner_17[slice55],\r\n",
        "       train_regret_winner_18[slice55],\r\n",
        "       train_regret_winner_19[slice55],\r\n",
        "       train_regret_winner_20[slice55]]\r\n",
        "\r\n",
        "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\r\n",
        "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\r\n",
        "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\r\n",
        "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\r\n",
        "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\r\n",
        "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUAjKXWdw__J"
      },
      "source": [
        "# Iteration65 :\r\n",
        "\r\n",
        "slice65 = 64\r\n",
        "\r\n",
        "loser65 = [train_regret_loser_1[slice65],\r\n",
        "       train_regret_loser_2[slice65],\r\n",
        "       train_regret_loser_3[slice65],\r\n",
        "       train_regret_loser_4[slice65],\r\n",
        "       train_regret_loser_5[slice65],\r\n",
        "       train_regret_loser_6[slice65],\r\n",
        "       train_regret_loser_7[slice65],\r\n",
        "       train_regret_loser_8[slice65],\r\n",
        "       train_regret_loser_9[slice65],\r\n",
        "       train_regret_loser_10[slice65],\r\n",
        "       train_regret_loser_11[slice65],\r\n",
        "       train_regret_loser_12[slice65],\r\n",
        "       train_regret_loser_13[slice65],\r\n",
        "       train_regret_loser_14[slice65],\r\n",
        "       train_regret_loser_15[slice65],\r\n",
        "       train_regret_loser_16[slice65],\r\n",
        "       train_regret_loser_17[slice65],\r\n",
        "       train_regret_loser_18[slice65],\r\n",
        "       train_regret_loser_19[slice65],\r\n",
        "       train_regret_loser_20[slice65]]\r\n",
        "\r\n",
        "winner65 = [train_regret_winner_1[slice65],\r\n",
        "       train_regret_winner_2[slice65],\r\n",
        "       train_regret_winner_3[slice65],\r\n",
        "       train_regret_winner_4[slice65],\r\n",
        "       train_regret_winner_5[slice65],\r\n",
        "       train_regret_winner_6[slice65],\r\n",
        "       train_regret_winner_7[slice65],\r\n",
        "       train_regret_winner_8[slice65],\r\n",
        "       train_regret_winner_9[slice65],\r\n",
        "       train_regret_winner_10[slice65],\r\n",
        "       train_regret_winner_11[slice65],\r\n",
        "       train_regret_winner_12[slice65],\r\n",
        "       train_regret_winner_13[slice65],\r\n",
        "       train_regret_winner_14[slice65],\r\n",
        "       train_regret_winner_15[slice65],\r\n",
        "       train_regret_winner_16[slice65],\r\n",
        "       train_regret_winner_17[slice65],\r\n",
        "       train_regret_winner_18[slice65],\r\n",
        "       train_regret_winner_19[slice65],\r\n",
        "       train_regret_winner_20[slice65]]\r\n",
        "\r\n",
        "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\r\n",
        "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\r\n",
        "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\r\n",
        "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\r\n",
        "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\r\n",
        "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D00FcmZwxAB9"
      },
      "source": [
        "# Iteration75 :\r\n",
        "\r\n",
        "slice75 = 74\r\n",
        "\r\n",
        "loser75 = [train_regret_loser_1[slice75],\r\n",
        "       train_regret_loser_2[slice75],\r\n",
        "       train_regret_loser_3[slice75],\r\n",
        "       train_regret_loser_4[slice75],\r\n",
        "       train_regret_loser_5[slice75],\r\n",
        "       train_regret_loser_6[slice75],\r\n",
        "       train_regret_loser_7[slice75],\r\n",
        "       train_regret_loser_8[slice75],\r\n",
        "       train_regret_loser_9[slice75],\r\n",
        "       train_regret_loser_10[slice75],\r\n",
        "       train_regret_loser_11[slice75],\r\n",
        "       train_regret_loser_12[slice75],\r\n",
        "       train_regret_loser_13[slice75],\r\n",
        "       train_regret_loser_14[slice75],\r\n",
        "       train_regret_loser_15[slice75],\r\n",
        "       train_regret_loser_16[slice75],\r\n",
        "       train_regret_loser_17[slice75],\r\n",
        "       train_regret_loser_18[slice75],\r\n",
        "       train_regret_loser_19[slice75],\r\n",
        "       train_regret_loser_20[slice75]]\r\n",
        "\r\n",
        "winner75 = [train_regret_winner_1[slice75],\r\n",
        "       train_regret_winner_2[slice75],\r\n",
        "       train_regret_winner_3[slice75],\r\n",
        "       train_regret_winner_4[slice75],\r\n",
        "       train_regret_winner_5[slice75],\r\n",
        "       train_regret_winner_6[slice75],\r\n",
        "       train_regret_winner_7[slice75],\r\n",
        "       train_regret_winner_8[slice75],\r\n",
        "       train_regret_winner_9[slice75],\r\n",
        "       train_regret_winner_10[slice75],\r\n",
        "       train_regret_winner_11[slice75],\r\n",
        "       train_regret_winner_12[slice75],\r\n",
        "       train_regret_winner_13[slice75],\r\n",
        "       train_regret_winner_14[slice75],\r\n",
        "       train_regret_winner_15[slice75],\r\n",
        "       train_regret_winner_16[slice75],\r\n",
        "       train_regret_winner_17[slice75],\r\n",
        "       train_regret_winner_18[slice75],\r\n",
        "       train_regret_winner_19[slice75],\r\n",
        "       train_regret_winner_20[slice75]]\r\n",
        "\r\n",
        "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\r\n",
        "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\r\n",
        "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\r\n",
        "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\r\n",
        "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\r\n",
        "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnzmprxxAFB"
      },
      "source": [
        "# Iteration85 :\r\n",
        "\r\n",
        "slice85 = 84\r\n",
        "\r\n",
        "loser85 = [train_regret_loser_1[slice85],\r\n",
        "       train_regret_loser_2[slice85],\r\n",
        "       train_regret_loser_3[slice85],\r\n",
        "       train_regret_loser_4[slice85],\r\n",
        "       train_regret_loser_5[slice85],\r\n",
        "       train_regret_loser_6[slice85],\r\n",
        "       train_regret_loser_7[slice85],\r\n",
        "       train_regret_loser_8[slice85],\r\n",
        "       train_regret_loser_9[slice85],\r\n",
        "       train_regret_loser_10[slice85],\r\n",
        "       train_regret_loser_11[slice85],\r\n",
        "       train_regret_loser_12[slice85],\r\n",
        "       train_regret_loser_13[slice85],\r\n",
        "       train_regret_loser_14[slice85],\r\n",
        "       train_regret_loser_15[slice85],\r\n",
        "       train_regret_loser_16[slice85],\r\n",
        "       train_regret_loser_17[slice85],\r\n",
        "       train_regret_loser_18[slice85],\r\n",
        "       train_regret_loser_19[slice85],\r\n",
        "       train_regret_loser_20[slice85]]\r\n",
        "\r\n",
        "winner85 = [train_regret_winner_1[slice85],\r\n",
        "       train_regret_winner_2[slice85],\r\n",
        "       train_regret_winner_3[slice85],\r\n",
        "       train_regret_winner_4[slice85],\r\n",
        "       train_regret_winner_5[slice85],\r\n",
        "       train_regret_winner_6[slice85],\r\n",
        "       train_regret_winner_7[slice85],\r\n",
        "       train_regret_winner_8[slice85],\r\n",
        "       train_regret_winner_9[slice85],\r\n",
        "       train_regret_winner_10[slice85],\r\n",
        "       train_regret_winner_11[slice85],\r\n",
        "       train_regret_winner_12[slice85],\r\n",
        "       train_regret_winner_13[slice85],\r\n",
        "       train_regret_winner_14[slice85],\r\n",
        "       train_regret_winner_15[slice85],\r\n",
        "       train_regret_winner_16[slice85],\r\n",
        "       train_regret_winner_17[slice85],\r\n",
        "       train_regret_winner_18[slice85],\r\n",
        "       train_regret_winner_19[slice85],\r\n",
        "       train_regret_winner_20[slice85]]\r\n",
        "\r\n",
        "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\r\n",
        "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\r\n",
        "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\r\n",
        "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\r\n",
        "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\r\n",
        "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztvJhrN6xAH1"
      },
      "source": [
        "# Iteration95 :\r\n",
        "\r\n",
        "slice95 = 94\r\n",
        "\r\n",
        "loser95 = [train_regret_loser_1[slice95],\r\n",
        "       train_regret_loser_2[slice95],\r\n",
        "       train_regret_loser_3[slice95],\r\n",
        "       train_regret_loser_4[slice95],\r\n",
        "       train_regret_loser_5[slice95],\r\n",
        "       train_regret_loser_6[slice95],\r\n",
        "       train_regret_loser_7[slice95],\r\n",
        "       train_regret_loser_8[slice95],\r\n",
        "       train_regret_loser_9[slice95],\r\n",
        "       train_regret_loser_10[slice95],\r\n",
        "       train_regret_loser_11[slice95],\r\n",
        "       train_regret_loser_12[slice95],\r\n",
        "       train_regret_loser_13[slice95],\r\n",
        "       train_regret_loser_14[slice95],\r\n",
        "       train_regret_loser_15[slice95],\r\n",
        "       train_regret_loser_16[slice95],\r\n",
        "       train_regret_loser_17[slice95],\r\n",
        "       train_regret_loser_18[slice95],\r\n",
        "       train_regret_loser_19[slice95],\r\n",
        "       train_regret_loser_20[slice95]]\r\n",
        "\r\n",
        "winner95 = [train_regret_winner_1[slice95],\r\n",
        "       train_regret_winner_2[slice95],\r\n",
        "       train_regret_winner_3[slice95],\r\n",
        "       train_regret_winner_4[slice95],\r\n",
        "       train_regret_winner_5[slice95],\r\n",
        "       train_regret_winner_6[slice95],\r\n",
        "       train_regret_winner_7[slice95],\r\n",
        "       train_regret_winner_8[slice95],\r\n",
        "       train_regret_winner_9[slice95],\r\n",
        "       train_regret_winner_10[slice95],\r\n",
        "       train_regret_winner_11[slice95],\r\n",
        "       train_regret_winner_12[slice95],\r\n",
        "       train_regret_winner_13[slice95],\r\n",
        "       train_regret_winner_14[slice95],\r\n",
        "       train_regret_winner_15[slice95],\r\n",
        "       train_regret_winner_16[slice95],\r\n",
        "       train_regret_winner_17[slice95],\r\n",
        "       train_regret_winner_18[slice95],\r\n",
        "       train_regret_winner_19[slice95],\r\n",
        "       train_regret_winner_20[slice95]]\r\n",
        "\r\n",
        "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\r\n",
        "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\r\n",
        "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\r\n",
        "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\r\n",
        "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\r\n",
        "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK-4nr-OxALA"
      },
      "source": [
        "# Iteration6 :\r\n",
        "\r\n",
        "slice6 = 5\r\n",
        "\r\n",
        "loser6 = [train_regret_loser_1[slice6],\r\n",
        "       train_regret_loser_2[slice6],\r\n",
        "       train_regret_loser_3[slice6],\r\n",
        "       train_regret_loser_4[slice6],\r\n",
        "       train_regret_loser_5[slice6],\r\n",
        "       train_regret_loser_6[slice6],\r\n",
        "       train_regret_loser_7[slice6],\r\n",
        "       train_regret_loser_8[slice6],\r\n",
        "       train_regret_loser_9[slice6],\r\n",
        "       train_regret_loser_10[slice6],\r\n",
        "       train_regret_loser_11[slice6],\r\n",
        "       train_regret_loser_12[slice6],\r\n",
        "       train_regret_loser_13[slice6],\r\n",
        "       train_regret_loser_14[slice6],\r\n",
        "       train_regret_loser_15[slice6],\r\n",
        "       train_regret_loser_16[slice6],\r\n",
        "       train_regret_loser_17[slice6],\r\n",
        "       train_regret_loser_18[slice6],\r\n",
        "       train_regret_loser_19[slice6],\r\n",
        "       train_regret_loser_20[slice6]]\r\n",
        "\r\n",
        "winner6 = [train_regret_winner_1[slice6],\r\n",
        "       train_regret_winner_2[slice6],\r\n",
        "       train_regret_winner_3[slice6],\r\n",
        "       train_regret_winner_4[slice6],\r\n",
        "       train_regret_winner_5[slice6],\r\n",
        "       train_regret_winner_6[slice6],\r\n",
        "       train_regret_winner_7[slice6],\r\n",
        "       train_regret_winner_8[slice6],\r\n",
        "       train_regret_winner_9[slice6],\r\n",
        "       train_regret_winner_10[slice6],\r\n",
        "       train_regret_winner_11[slice6],\r\n",
        "       train_regret_winner_12[slice6],\r\n",
        "       train_regret_winner_13[slice6],\r\n",
        "       train_regret_winner_14[slice6],\r\n",
        "       train_regret_winner_15[slice6],\r\n",
        "       train_regret_winner_16[slice6],\r\n",
        "       train_regret_winner_17[slice6],\r\n",
        "       train_regret_winner_18[slice6],\r\n",
        "       train_regret_winner_19[slice6],\r\n",
        "       train_regret_winner_20[slice6]]\r\n",
        "\r\n",
        "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\r\n",
        "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\r\n",
        "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\r\n",
        "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\r\n",
        "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\r\n",
        "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSwfNX5ExANt"
      },
      "source": [
        "# Iteration16 :\r\n",
        "\r\n",
        "slice16 = 15\r\n",
        "\r\n",
        "loser16 = [train_regret_loser_1[slice16],\r\n",
        "       train_regret_loser_2[slice16],\r\n",
        "       train_regret_loser_3[slice16],\r\n",
        "       train_regret_loser_4[slice16],\r\n",
        "       train_regret_loser_5[slice16],\r\n",
        "       train_regret_loser_6[slice16],\r\n",
        "       train_regret_loser_7[slice16],\r\n",
        "       train_regret_loser_8[slice16],\r\n",
        "       train_regret_loser_9[slice16],\r\n",
        "       train_regret_loser_10[slice16],\r\n",
        "       train_regret_loser_11[slice16],\r\n",
        "       train_regret_loser_12[slice16],\r\n",
        "       train_regret_loser_13[slice16],\r\n",
        "       train_regret_loser_14[slice16],\r\n",
        "       train_regret_loser_15[slice16],\r\n",
        "       train_regret_loser_16[slice16],\r\n",
        "       train_regret_loser_17[slice16],\r\n",
        "       train_regret_loser_18[slice16],\r\n",
        "       train_regret_loser_19[slice16],\r\n",
        "       train_regret_loser_20[slice16]]\r\n",
        "\r\n",
        "winner16 = [train_regret_winner_1[slice16],\r\n",
        "       train_regret_winner_2[slice16],\r\n",
        "       train_regret_winner_3[slice16],\r\n",
        "       train_regret_winner_4[slice16],\r\n",
        "       train_regret_winner_5[slice16],\r\n",
        "       train_regret_winner_6[slice16],\r\n",
        "       train_regret_winner_7[slice16],\r\n",
        "       train_regret_winner_8[slice16],\r\n",
        "       train_regret_winner_9[slice16],\r\n",
        "       train_regret_winner_10[slice16],\r\n",
        "       train_regret_winner_11[slice16],\r\n",
        "       train_regret_winner_12[slice16],\r\n",
        "       train_regret_winner_13[slice16],\r\n",
        "       train_regret_winner_14[slice16],\r\n",
        "       train_regret_winner_15[slice16],\r\n",
        "       train_regret_winner_16[slice16],\r\n",
        "       train_regret_winner_17[slice16],\r\n",
        "       train_regret_winner_18[slice16],\r\n",
        "       train_regret_winner_19[slice16],\r\n",
        "       train_regret_winner_20[slice16]]\r\n",
        "\r\n",
        "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\r\n",
        "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\r\n",
        "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\r\n",
        "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\r\n",
        "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\r\n",
        "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOB3hOLIxAQy"
      },
      "source": [
        "# Iteration26 :\r\n",
        "\r\n",
        "slice26 = 25\r\n",
        "\r\n",
        "loser26 = [train_regret_loser_1[slice26],\r\n",
        "       train_regret_loser_2[slice26],\r\n",
        "       train_regret_loser_3[slice26],\r\n",
        "       train_regret_loser_4[slice26],\r\n",
        "       train_regret_loser_5[slice26],\r\n",
        "       train_regret_loser_6[slice26],\r\n",
        "       train_regret_loser_7[slice26],\r\n",
        "       train_regret_loser_8[slice26],\r\n",
        "       train_regret_loser_9[slice26],\r\n",
        "       train_regret_loser_10[slice26],\r\n",
        "       train_regret_loser_11[slice26],\r\n",
        "       train_regret_loser_12[slice26],\r\n",
        "       train_regret_loser_13[slice26],\r\n",
        "       train_regret_loser_14[slice26],\r\n",
        "       train_regret_loser_15[slice26],\r\n",
        "       train_regret_loser_16[slice26],\r\n",
        "       train_regret_loser_17[slice26],\r\n",
        "       train_regret_loser_18[slice26],\r\n",
        "       train_regret_loser_19[slice26],\r\n",
        "       train_regret_loser_20[slice26]]\r\n",
        "\r\n",
        "winner26 = [train_regret_winner_1[slice26],\r\n",
        "       train_regret_winner_2[slice26],\r\n",
        "       train_regret_winner_3[slice26],\r\n",
        "       train_regret_winner_4[slice26],\r\n",
        "       train_regret_winner_5[slice26],\r\n",
        "       train_regret_winner_6[slice26],\r\n",
        "       train_regret_winner_7[slice26],\r\n",
        "       train_regret_winner_8[slice26],\r\n",
        "       train_regret_winner_9[slice26],\r\n",
        "       train_regret_winner_10[slice26],\r\n",
        "       train_regret_winner_11[slice26],\r\n",
        "       train_regret_winner_12[slice26],\r\n",
        "       train_regret_winner_13[slice26],\r\n",
        "       train_regret_winner_14[slice26],\r\n",
        "       train_regret_winner_15[slice26],\r\n",
        "       train_regret_winner_16[slice26],\r\n",
        "       train_regret_winner_17[slice26],\r\n",
        "       train_regret_winner_18[slice26],\r\n",
        "       train_regret_winner_19[slice26],\r\n",
        "       train_regret_winner_20[slice26]]\r\n",
        "\r\n",
        "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\r\n",
        "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\r\n",
        "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\r\n",
        "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\r\n",
        "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\r\n",
        "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-0hQMtOxATh"
      },
      "source": [
        "# Iteration36 :\r\n",
        "\r\n",
        "slice36 = 35\r\n",
        "\r\n",
        "loser36 = [train_regret_loser_1[slice36],\r\n",
        "       train_regret_loser_2[slice36],\r\n",
        "       train_regret_loser_3[slice36],\r\n",
        "       train_regret_loser_4[slice36],\r\n",
        "       train_regret_loser_5[slice36],\r\n",
        "       train_regret_loser_6[slice36],\r\n",
        "       train_regret_loser_7[slice36],\r\n",
        "       train_regret_loser_8[slice36],\r\n",
        "       train_regret_loser_9[slice36],\r\n",
        "       train_regret_loser_10[slice36],\r\n",
        "       train_regret_loser_11[slice36],\r\n",
        "       train_regret_loser_12[slice36],\r\n",
        "       train_regret_loser_13[slice36],\r\n",
        "       train_regret_loser_14[slice36],\r\n",
        "       train_regret_loser_15[slice36],\r\n",
        "       train_regret_loser_16[slice36],\r\n",
        "       train_regret_loser_17[slice36],\r\n",
        "       train_regret_loser_18[slice36],\r\n",
        "       train_regret_loser_19[slice36],\r\n",
        "       train_regret_loser_20[slice36]]\r\n",
        "\r\n",
        "winner36 = [train_regret_winner_1[slice36],\r\n",
        "       train_regret_winner_2[slice36],\r\n",
        "       train_regret_winner_3[slice36],\r\n",
        "       train_regret_winner_4[slice36],\r\n",
        "       train_regret_winner_5[slice36],\r\n",
        "       train_regret_winner_6[slice36],\r\n",
        "       train_regret_winner_7[slice36],\r\n",
        "       train_regret_winner_8[slice36],\r\n",
        "       train_regret_winner_9[slice36],\r\n",
        "       train_regret_winner_10[slice36],\r\n",
        "       train_regret_winner_11[slice36],\r\n",
        "       train_regret_winner_12[slice36],\r\n",
        "       train_regret_winner_13[slice36],\r\n",
        "       train_regret_winner_14[slice36],\r\n",
        "       train_regret_winner_15[slice36],\r\n",
        "       train_regret_winner_16[slice36],\r\n",
        "       train_regret_winner_17[slice36],\r\n",
        "       train_regret_winner_18[slice36],\r\n",
        "       train_regret_winner_19[slice36],\r\n",
        "       train_regret_winner_20[slice36]]\r\n",
        "\r\n",
        "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\r\n",
        "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\r\n",
        "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\r\n",
        "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\r\n",
        "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\r\n",
        "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSqBLqTgxAWs"
      },
      "source": [
        "# Iteration46 :\r\n",
        "\r\n",
        "slice46 = 45\r\n",
        "\r\n",
        "loser46 = [train_regret_loser_1[slice46],\r\n",
        "       train_regret_loser_2[slice46],\r\n",
        "       train_regret_loser_3[slice46],\r\n",
        "       train_regret_loser_4[slice46],\r\n",
        "       train_regret_loser_5[slice46],\r\n",
        "       train_regret_loser_6[slice46],\r\n",
        "       train_regret_loser_7[slice46],\r\n",
        "       train_regret_loser_8[slice46],\r\n",
        "       train_regret_loser_9[slice46],\r\n",
        "       train_regret_loser_10[slice46],\r\n",
        "       train_regret_loser_11[slice46],\r\n",
        "       train_regret_loser_12[slice46],\r\n",
        "       train_regret_loser_13[slice46],\r\n",
        "       train_regret_loser_14[slice46],\r\n",
        "       train_regret_loser_15[slice46],\r\n",
        "       train_regret_loser_16[slice46],\r\n",
        "       train_regret_loser_17[slice46],\r\n",
        "       train_regret_loser_18[slice46],\r\n",
        "       train_regret_loser_19[slice46],\r\n",
        "       train_regret_loser_20[slice46]]\r\n",
        "\r\n",
        "winner46 = [train_regret_winner_1[slice46],\r\n",
        "       train_regret_winner_2[slice46],\r\n",
        "       train_regret_winner_3[slice46],\r\n",
        "       train_regret_winner_4[slice46],\r\n",
        "       train_regret_winner_5[slice46],\r\n",
        "       train_regret_winner_6[slice46],\r\n",
        "       train_regret_winner_7[slice46],\r\n",
        "       train_regret_winner_8[slice46],\r\n",
        "       train_regret_winner_9[slice46],\r\n",
        "       train_regret_winner_10[slice46],\r\n",
        "       train_regret_winner_11[slice46],\r\n",
        "       train_regret_winner_12[slice46],\r\n",
        "       train_regret_winner_13[slice46],\r\n",
        "       train_regret_winner_14[slice46],\r\n",
        "       train_regret_winner_15[slice46],\r\n",
        "       train_regret_winner_16[slice46],\r\n",
        "       train_regret_winner_17[slice46],\r\n",
        "       train_regret_winner_18[slice46],\r\n",
        "       train_regret_winner_19[slice46],\r\n",
        "       train_regret_winner_20[slice46]]\r\n",
        "\r\n",
        "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\r\n",
        "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\r\n",
        "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\r\n",
        "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\r\n",
        "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\r\n",
        "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFwO80w6xAZZ"
      },
      "source": [
        "# Iteration56 :\r\n",
        "\r\n",
        "slice56 = 55\r\n",
        "\r\n",
        "loser56 = [train_regret_loser_1[slice56],\r\n",
        "       train_regret_loser_2[slice56],\r\n",
        "       train_regret_loser_3[slice56],\r\n",
        "       train_regret_loser_4[slice56],\r\n",
        "       train_regret_loser_5[slice56],\r\n",
        "       train_regret_loser_6[slice56],\r\n",
        "       train_regret_loser_7[slice56],\r\n",
        "       train_regret_loser_8[slice56],\r\n",
        "       train_regret_loser_9[slice56],\r\n",
        "       train_regret_loser_10[slice56],\r\n",
        "       train_regret_loser_11[slice56],\r\n",
        "       train_regret_loser_12[slice56],\r\n",
        "       train_regret_loser_13[slice56],\r\n",
        "       train_regret_loser_14[slice56],\r\n",
        "       train_regret_loser_15[slice56],\r\n",
        "       train_regret_loser_16[slice56],\r\n",
        "       train_regret_loser_17[slice56],\r\n",
        "       train_regret_loser_18[slice56],\r\n",
        "       train_regret_loser_19[slice56],\r\n",
        "       train_regret_loser_20[slice56]]\r\n",
        "\r\n",
        "winner56 = [train_regret_winner_1[slice56],\r\n",
        "       train_regret_winner_2[slice56],\r\n",
        "       train_regret_winner_3[slice56],\r\n",
        "       train_regret_winner_4[slice56],\r\n",
        "       train_regret_winner_5[slice56],\r\n",
        "       train_regret_winner_6[slice56],\r\n",
        "       train_regret_winner_7[slice56],\r\n",
        "       train_regret_winner_8[slice56],\r\n",
        "       train_regret_winner_9[slice56],\r\n",
        "       train_regret_winner_10[slice56],\r\n",
        "       train_regret_winner_11[slice56],\r\n",
        "       train_regret_winner_12[slice56],\r\n",
        "       train_regret_winner_13[slice56],\r\n",
        "       train_regret_winner_14[slice56],\r\n",
        "       train_regret_winner_15[slice56],\r\n",
        "       train_regret_winner_16[slice56],\r\n",
        "       train_regret_winner_17[slice56],\r\n",
        "       train_regret_winner_18[slice56],\r\n",
        "       train_regret_winner_19[slice56],\r\n",
        "       train_regret_winner_20[slice56]]\r\n",
        "\r\n",
        "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\r\n",
        "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\r\n",
        "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\r\n",
        "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\r\n",
        "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\r\n",
        "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfVZBDP8xAcC"
      },
      "source": [
        "# Iteration66 :\r\n",
        "\r\n",
        "slice66 = 65\r\n",
        "\r\n",
        "loser66 = [train_regret_loser_1[slice66],\r\n",
        "       train_regret_loser_2[slice66],\r\n",
        "       train_regret_loser_3[slice66],\r\n",
        "       train_regret_loser_4[slice66],\r\n",
        "       train_regret_loser_5[slice66],\r\n",
        "       train_regret_loser_6[slice66],\r\n",
        "       train_regret_loser_7[slice66],\r\n",
        "       train_regret_loser_8[slice66],\r\n",
        "       train_regret_loser_9[slice66],\r\n",
        "       train_regret_loser_10[slice66],\r\n",
        "       train_regret_loser_11[slice66],\r\n",
        "       train_regret_loser_12[slice66],\r\n",
        "       train_regret_loser_13[slice66],\r\n",
        "       train_regret_loser_14[slice66],\r\n",
        "       train_regret_loser_15[slice66],\r\n",
        "       train_regret_loser_16[slice66],\r\n",
        "       train_regret_loser_17[slice66],\r\n",
        "       train_regret_loser_18[slice66],\r\n",
        "       train_regret_loser_19[slice66],\r\n",
        "       train_regret_loser_20[slice66]]\r\n",
        "\r\n",
        "winner66 = [train_regret_winner_1[slice66],\r\n",
        "       train_regret_winner_2[slice66],\r\n",
        "       train_regret_winner_3[slice66],\r\n",
        "       train_regret_winner_4[slice66],\r\n",
        "       train_regret_winner_5[slice66],\r\n",
        "       train_regret_winner_6[slice66],\r\n",
        "       train_regret_winner_7[slice66],\r\n",
        "       train_regret_winner_8[slice66],\r\n",
        "       train_regret_winner_9[slice66],\r\n",
        "       train_regret_winner_10[slice66],\r\n",
        "       train_regret_winner_11[slice66],\r\n",
        "       train_regret_winner_12[slice66],\r\n",
        "       train_regret_winner_13[slice66],\r\n",
        "       train_regret_winner_14[slice66],\r\n",
        "       train_regret_winner_15[slice66],\r\n",
        "       train_regret_winner_16[slice66],\r\n",
        "       train_regret_winner_17[slice66],\r\n",
        "       train_regret_winner_18[slice66],\r\n",
        "       train_regret_winner_19[slice66],\r\n",
        "       train_regret_winner_20[slice66]]\r\n",
        "\r\n",
        "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\r\n",
        "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\r\n",
        "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\r\n",
        "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\r\n",
        "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\r\n",
        "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnSYoG7bxAfL"
      },
      "source": [
        "# Iteration76 :\r\n",
        "\r\n",
        "slice76 = 75\r\n",
        "\r\n",
        "loser76 = [train_regret_loser_1[slice76],\r\n",
        "       train_regret_loser_2[slice76],\r\n",
        "       train_regret_loser_3[slice76],\r\n",
        "       train_regret_loser_4[slice76],\r\n",
        "       train_regret_loser_5[slice76],\r\n",
        "       train_regret_loser_6[slice76],\r\n",
        "       train_regret_loser_7[slice76],\r\n",
        "       train_regret_loser_8[slice76],\r\n",
        "       train_regret_loser_9[slice76],\r\n",
        "       train_regret_loser_10[slice76],\r\n",
        "       train_regret_loser_11[slice76],\r\n",
        "       train_regret_loser_12[slice76],\r\n",
        "       train_regret_loser_13[slice76],\r\n",
        "       train_regret_loser_14[slice76],\r\n",
        "       train_regret_loser_15[slice76],\r\n",
        "       train_regret_loser_16[slice76],\r\n",
        "       train_regret_loser_17[slice76],\r\n",
        "       train_regret_loser_18[slice76],\r\n",
        "       train_regret_loser_19[slice76],\r\n",
        "       train_regret_loser_20[slice76]]\r\n",
        "\r\n",
        "winner76 = [train_regret_winner_1[slice76],\r\n",
        "       train_regret_winner_2[slice76],\r\n",
        "       train_regret_winner_3[slice76],\r\n",
        "       train_regret_winner_4[slice76],\r\n",
        "       train_regret_winner_5[slice76],\r\n",
        "       train_regret_winner_6[slice76],\r\n",
        "       train_regret_winner_7[slice76],\r\n",
        "       train_regret_winner_8[slice76],\r\n",
        "       train_regret_winner_9[slice76],\r\n",
        "       train_regret_winner_10[slice76],\r\n",
        "       train_regret_winner_11[slice76],\r\n",
        "       train_regret_winner_12[slice76],\r\n",
        "       train_regret_winner_13[slice76],\r\n",
        "       train_regret_winner_14[slice76],\r\n",
        "       train_regret_winner_15[slice76],\r\n",
        "       train_regret_winner_16[slice76],\r\n",
        "       train_regret_winner_17[slice76],\r\n",
        "       train_regret_winner_18[slice76],\r\n",
        "       train_regret_winner_19[slice76],\r\n",
        "       train_regret_winner_20[slice76]]\r\n",
        "\r\n",
        "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\r\n",
        "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\r\n",
        "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\r\n",
        "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\r\n",
        "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\r\n",
        "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ze_Oy9mxAiO"
      },
      "source": [
        "# Iteration86 :\r\n",
        "\r\n",
        "slice86 = 85\r\n",
        "\r\n",
        "loser86 = [train_regret_loser_1[slice86],\r\n",
        "       train_regret_loser_2[slice86],\r\n",
        "       train_regret_loser_3[slice86],\r\n",
        "       train_regret_loser_4[slice86],\r\n",
        "       train_regret_loser_5[slice86],\r\n",
        "       train_regret_loser_6[slice86],\r\n",
        "       train_regret_loser_7[slice86],\r\n",
        "       train_regret_loser_8[slice86],\r\n",
        "       train_regret_loser_9[slice86],\r\n",
        "       train_regret_loser_10[slice86],\r\n",
        "       train_regret_loser_11[slice86],\r\n",
        "       train_regret_loser_12[slice86],\r\n",
        "       train_regret_loser_13[slice86],\r\n",
        "       train_regret_loser_14[slice86],\r\n",
        "       train_regret_loser_15[slice86],\r\n",
        "       train_regret_loser_16[slice86],\r\n",
        "       train_regret_loser_17[slice86],\r\n",
        "       train_regret_loser_18[slice86],\r\n",
        "       train_regret_loser_19[slice86],\r\n",
        "       train_regret_loser_20[slice86]]\r\n",
        "\r\n",
        "winner86 = [train_regret_winner_1[slice86],\r\n",
        "       train_regret_winner_2[slice86],\r\n",
        "       train_regret_winner_3[slice86],\r\n",
        "       train_regret_winner_4[slice86],\r\n",
        "       train_regret_winner_5[slice86],\r\n",
        "       train_regret_winner_6[slice86],\r\n",
        "       train_regret_winner_7[slice86],\r\n",
        "       train_regret_winner_8[slice86],\r\n",
        "       train_regret_winner_9[slice86],\r\n",
        "       train_regret_winner_10[slice86],\r\n",
        "       train_regret_winner_11[slice86],\r\n",
        "       train_regret_winner_12[slice86],\r\n",
        "       train_regret_winner_13[slice86],\r\n",
        "       train_regret_winner_14[slice86],\r\n",
        "       train_regret_winner_15[slice86],\r\n",
        "       train_regret_winner_16[slice86],\r\n",
        "       train_regret_winner_17[slice86],\r\n",
        "       train_regret_winner_18[slice86],\r\n",
        "       train_regret_winner_19[slice86],\r\n",
        "       train_regret_winner_20[slice86]]\r\n",
        "\r\n",
        "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\r\n",
        "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\r\n",
        "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\r\n",
        "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\r\n",
        "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\r\n",
        "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glsE9_RPxAlS"
      },
      "source": [
        "# Iteration96 :\r\n",
        "\r\n",
        "slice96 = 95\r\n",
        "\r\n",
        "loser96 = [train_regret_loser_1[slice96],\r\n",
        "       train_regret_loser_2[slice96],\r\n",
        "       train_regret_loser_3[slice96],\r\n",
        "       train_regret_loser_4[slice96],\r\n",
        "       train_regret_loser_5[slice96],\r\n",
        "       train_regret_loser_6[slice96],\r\n",
        "       train_regret_loser_7[slice96],\r\n",
        "       train_regret_loser_8[slice96],\r\n",
        "       train_regret_loser_9[slice96],\r\n",
        "       train_regret_loser_10[slice96],\r\n",
        "       train_regret_loser_11[slice96],\r\n",
        "       train_regret_loser_12[slice96],\r\n",
        "       train_regret_loser_13[slice96],\r\n",
        "       train_regret_loser_14[slice96],\r\n",
        "       train_regret_loser_15[slice96],\r\n",
        "       train_regret_loser_16[slice96],\r\n",
        "       train_regret_loser_17[slice96],\r\n",
        "       train_regret_loser_18[slice96],\r\n",
        "       train_regret_loser_19[slice96],\r\n",
        "       train_regret_loser_20[slice96]]\r\n",
        "\r\n",
        "winner96 = [train_regret_winner_1[slice96],\r\n",
        "       train_regret_winner_2[slice96],\r\n",
        "       train_regret_winner_3[slice96],\r\n",
        "       train_regret_winner_4[slice96],\r\n",
        "       train_regret_winner_5[slice96],\r\n",
        "       train_regret_winner_6[slice96],\r\n",
        "       train_regret_winner_7[slice96],\r\n",
        "       train_regret_winner_8[slice96],\r\n",
        "       train_regret_winner_9[slice96],\r\n",
        "       train_regret_winner_10[slice96],\r\n",
        "       train_regret_winner_11[slice96],\r\n",
        "       train_regret_winner_12[slice96],\r\n",
        "       train_regret_winner_13[slice96],\r\n",
        "       train_regret_winner_14[slice96],\r\n",
        "       train_regret_winner_15[slice96],\r\n",
        "       train_regret_winner_16[slice96],\r\n",
        "       train_regret_winner_17[slice96],\r\n",
        "       train_regret_winner_18[slice96],\r\n",
        "       train_regret_winner_19[slice96],\r\n",
        "       train_regret_winner_20[slice96]]\r\n",
        "\r\n",
        "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\r\n",
        "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\r\n",
        "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\r\n",
        "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\r\n",
        "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\r\n",
        "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhsEfl1RxAoA"
      },
      "source": [
        "# Iteration7 :\r\n",
        "\r\n",
        "slice7 = 6\r\n",
        "\r\n",
        "loser7 = [train_regret_loser_1[slice7],\r\n",
        "       train_regret_loser_2[slice7],\r\n",
        "       train_regret_loser_3[slice7],\r\n",
        "       train_regret_loser_4[slice7],\r\n",
        "       train_regret_loser_5[slice7],\r\n",
        "       train_regret_loser_6[slice7],\r\n",
        "       train_regret_loser_7[slice7],\r\n",
        "       train_regret_loser_8[slice7],\r\n",
        "       train_regret_loser_9[slice7],\r\n",
        "       train_regret_loser_10[slice7],\r\n",
        "       train_regret_loser_11[slice7],\r\n",
        "       train_regret_loser_12[slice7],\r\n",
        "       train_regret_loser_13[slice7],\r\n",
        "       train_regret_loser_14[slice7],\r\n",
        "       train_regret_loser_15[slice7],\r\n",
        "       train_regret_loser_16[slice7],\r\n",
        "       train_regret_loser_17[slice7],\r\n",
        "       train_regret_loser_18[slice7],\r\n",
        "       train_regret_loser_19[slice7],\r\n",
        "       train_regret_loser_20[slice7]]\r\n",
        "\r\n",
        "winner7 = [train_regret_winner_1[slice7],\r\n",
        "       train_regret_winner_2[slice7],\r\n",
        "       train_regret_winner_3[slice7],\r\n",
        "       train_regret_winner_4[slice7],\r\n",
        "       train_regret_winner_5[slice7],\r\n",
        "       train_regret_winner_6[slice7],\r\n",
        "       train_regret_winner_7[slice7],\r\n",
        "       train_regret_winner_8[slice7],\r\n",
        "       train_regret_winner_9[slice7],\r\n",
        "       train_regret_winner_10[slice7],\r\n",
        "       train_regret_winner_11[slice7],\r\n",
        "       train_regret_winner_12[slice7],\r\n",
        "       train_regret_winner_13[slice7],\r\n",
        "       train_regret_winner_14[slice7],\r\n",
        "       train_regret_winner_15[slice7],\r\n",
        "       train_regret_winner_16[slice7],\r\n",
        "       train_regret_winner_17[slice7],\r\n",
        "       train_regret_winner_18[slice7],\r\n",
        "       train_regret_winner_19[slice7],\r\n",
        "       train_regret_winner_20[slice7]]\r\n",
        "\r\n",
        "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\r\n",
        "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\r\n",
        "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\r\n",
        "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\r\n",
        "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\r\n",
        "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUCMheLKxArL"
      },
      "source": [
        "# Iteration17 :\r\n",
        "\r\n",
        "slice17 = 16\r\n",
        "\r\n",
        "loser17 = [train_regret_loser_1[slice17],\r\n",
        "       train_regret_loser_2[slice17],\r\n",
        "       train_regret_loser_3[slice17],\r\n",
        "       train_regret_loser_4[slice17],\r\n",
        "       train_regret_loser_5[slice17],\r\n",
        "       train_regret_loser_6[slice17],\r\n",
        "       train_regret_loser_7[slice17],\r\n",
        "       train_regret_loser_8[slice17],\r\n",
        "       train_regret_loser_9[slice17],\r\n",
        "       train_regret_loser_10[slice17],\r\n",
        "       train_regret_loser_11[slice17],\r\n",
        "       train_regret_loser_12[slice17],\r\n",
        "       train_regret_loser_13[slice17],\r\n",
        "       train_regret_loser_14[slice17],\r\n",
        "       train_regret_loser_15[slice17],\r\n",
        "       train_regret_loser_16[slice17],\r\n",
        "       train_regret_loser_17[slice17],\r\n",
        "       train_regret_loser_18[slice17],\r\n",
        "       train_regret_loser_19[slice17],\r\n",
        "       train_regret_loser_20[slice17]]\r\n",
        "\r\n",
        "winner17 = [train_regret_winner_1[slice17],\r\n",
        "       train_regret_winner_2[slice17],\r\n",
        "       train_regret_winner_3[slice17],\r\n",
        "       train_regret_winner_4[slice17],\r\n",
        "       train_regret_winner_5[slice17],\r\n",
        "       train_regret_winner_6[slice17],\r\n",
        "       train_regret_winner_7[slice17],\r\n",
        "       train_regret_winner_8[slice17],\r\n",
        "       train_regret_winner_9[slice17],\r\n",
        "       train_regret_winner_10[slice17],\r\n",
        "       train_regret_winner_11[slice17],\r\n",
        "       train_regret_winner_12[slice17],\r\n",
        "       train_regret_winner_13[slice17],\r\n",
        "       train_regret_winner_14[slice17],\r\n",
        "       train_regret_winner_15[slice17],\r\n",
        "       train_regret_winner_16[slice17],\r\n",
        "       train_regret_winner_17[slice17],\r\n",
        "       train_regret_winner_18[slice17],\r\n",
        "       train_regret_winner_19[slice17],\r\n",
        "       train_regret_winner_20[slice17]]\r\n",
        "\r\n",
        "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\r\n",
        "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\r\n",
        "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\r\n",
        "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\r\n",
        "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\r\n",
        "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYnBmq7_xAuB"
      },
      "source": [
        "# Iteration27 :\r\n",
        "\r\n",
        "slice27 = 26\r\n",
        "\r\n",
        "loser27 = [train_regret_loser_1[slice27],\r\n",
        "       train_regret_loser_2[slice27],\r\n",
        "       train_regret_loser_3[slice27],\r\n",
        "       train_regret_loser_4[slice27],\r\n",
        "       train_regret_loser_5[slice27],\r\n",
        "       train_regret_loser_6[slice27],\r\n",
        "       train_regret_loser_7[slice27],\r\n",
        "       train_regret_loser_8[slice27],\r\n",
        "       train_regret_loser_9[slice27],\r\n",
        "       train_regret_loser_10[slice27],\r\n",
        "       train_regret_loser_11[slice27],\r\n",
        "       train_regret_loser_12[slice27],\r\n",
        "       train_regret_loser_13[slice27],\r\n",
        "       train_regret_loser_14[slice27],\r\n",
        "       train_regret_loser_15[slice27],\r\n",
        "       train_regret_loser_16[slice27],\r\n",
        "       train_regret_loser_17[slice27],\r\n",
        "       train_regret_loser_18[slice27],\r\n",
        "       train_regret_loser_19[slice27],\r\n",
        "       train_regret_loser_20[slice27]]\r\n",
        "\r\n",
        "winner27 = [train_regret_winner_1[slice27],\r\n",
        "       train_regret_winner_2[slice27],\r\n",
        "       train_regret_winner_3[slice27],\r\n",
        "       train_regret_winner_4[slice27],\r\n",
        "       train_regret_winner_5[slice27],\r\n",
        "       train_regret_winner_6[slice27],\r\n",
        "       train_regret_winner_7[slice27],\r\n",
        "       train_regret_winner_8[slice27],\r\n",
        "       train_regret_winner_9[slice27],\r\n",
        "       train_regret_winner_10[slice27],\r\n",
        "       train_regret_winner_11[slice27],\r\n",
        "       train_regret_winner_12[slice27],\r\n",
        "       train_regret_winner_13[slice27],\r\n",
        "       train_regret_winner_14[slice27],\r\n",
        "       train_regret_winner_15[slice27],\r\n",
        "       train_regret_winner_16[slice27],\r\n",
        "       train_regret_winner_17[slice27],\r\n",
        "       train_regret_winner_18[slice27],\r\n",
        "       train_regret_winner_19[slice27],\r\n",
        "       train_regret_winner_20[slice27]]\r\n",
        "\r\n",
        "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\r\n",
        "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\r\n",
        "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\r\n",
        "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\r\n",
        "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\r\n",
        "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ChA8KOUxAw6"
      },
      "source": [
        "# Iteration37 :\r\n",
        "\r\n",
        "slice37 = 36\r\n",
        "\r\n",
        "loser37 = [train_regret_loser_1[slice37],\r\n",
        "       train_regret_loser_2[slice37],\r\n",
        "       train_regret_loser_3[slice37],\r\n",
        "       train_regret_loser_4[slice37],\r\n",
        "       train_regret_loser_5[slice37],\r\n",
        "       train_regret_loser_6[slice37],\r\n",
        "       train_regret_loser_7[slice37],\r\n",
        "       train_regret_loser_8[slice37],\r\n",
        "       train_regret_loser_9[slice37],\r\n",
        "       train_regret_loser_10[slice37],\r\n",
        "       train_regret_loser_11[slice37],\r\n",
        "       train_regret_loser_12[slice37],\r\n",
        "       train_regret_loser_13[slice37],\r\n",
        "       train_regret_loser_14[slice37],\r\n",
        "       train_regret_loser_15[slice37],\r\n",
        "       train_regret_loser_16[slice37],\r\n",
        "       train_regret_loser_17[slice37],\r\n",
        "       train_regret_loser_18[slice37],\r\n",
        "       train_regret_loser_19[slice37],\r\n",
        "       train_regret_loser_20[slice37]]\r\n",
        "\r\n",
        "winner37 = [train_regret_winner_1[slice37],\r\n",
        "       train_regret_winner_2[slice37],\r\n",
        "       train_regret_winner_3[slice37],\r\n",
        "       train_regret_winner_4[slice37],\r\n",
        "       train_regret_winner_5[slice37],\r\n",
        "       train_regret_winner_6[slice37],\r\n",
        "       train_regret_winner_7[slice37],\r\n",
        "       train_regret_winner_8[slice37],\r\n",
        "       train_regret_winner_9[slice37],\r\n",
        "       train_regret_winner_10[slice37],\r\n",
        "       train_regret_winner_11[slice37],\r\n",
        "       train_regret_winner_12[slice37],\r\n",
        "       train_regret_winner_13[slice37],\r\n",
        "       train_regret_winner_14[slice37],\r\n",
        "       train_regret_winner_15[slice37],\r\n",
        "       train_regret_winner_16[slice37],\r\n",
        "       train_regret_winner_17[slice37],\r\n",
        "       train_regret_winner_18[slice37],\r\n",
        "       train_regret_winner_19[slice37],\r\n",
        "       train_regret_winner_20[slice37]]\r\n",
        "\r\n",
        "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\r\n",
        "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\r\n",
        "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\r\n",
        "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\r\n",
        "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\r\n",
        "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVsY1buDxAzx"
      },
      "source": [
        "# Iteration47 :\r\n",
        "\r\n",
        "slice47 = 46\r\n",
        "\r\n",
        "loser47 = [train_regret_loser_1[slice47],\r\n",
        "       train_regret_loser_2[slice47],\r\n",
        "       train_regret_loser_3[slice47],\r\n",
        "       train_regret_loser_4[slice47],\r\n",
        "       train_regret_loser_5[slice47],\r\n",
        "       train_regret_loser_6[slice47],\r\n",
        "       train_regret_loser_7[slice47],\r\n",
        "       train_regret_loser_8[slice47],\r\n",
        "       train_regret_loser_9[slice47],\r\n",
        "       train_regret_loser_10[slice47],\r\n",
        "       train_regret_loser_11[slice47],\r\n",
        "       train_regret_loser_12[slice47],\r\n",
        "       train_regret_loser_13[slice47],\r\n",
        "       train_regret_loser_14[slice47],\r\n",
        "       train_regret_loser_15[slice47],\r\n",
        "       train_regret_loser_16[slice47],\r\n",
        "       train_regret_loser_17[slice47],\r\n",
        "       train_regret_loser_18[slice47],\r\n",
        "       train_regret_loser_19[slice47],\r\n",
        "       train_regret_loser_20[slice47]]\r\n",
        "\r\n",
        "winner47 = [train_regret_winner_1[slice47],\r\n",
        "       train_regret_winner_2[slice47],\r\n",
        "       train_regret_winner_3[slice47],\r\n",
        "       train_regret_winner_4[slice47],\r\n",
        "       train_regret_winner_5[slice47],\r\n",
        "       train_regret_winner_6[slice47],\r\n",
        "       train_regret_winner_7[slice47],\r\n",
        "       train_regret_winner_8[slice47],\r\n",
        "       train_regret_winner_9[slice47],\r\n",
        "       train_regret_winner_10[slice47],\r\n",
        "       train_regret_winner_11[slice47],\r\n",
        "       train_regret_winner_12[slice47],\r\n",
        "       train_regret_winner_13[slice47],\r\n",
        "       train_regret_winner_14[slice47],\r\n",
        "       train_regret_winner_15[slice47],\r\n",
        "       train_regret_winner_16[slice47],\r\n",
        "       train_regret_winner_17[slice47],\r\n",
        "       train_regret_winner_18[slice47],\r\n",
        "       train_regret_winner_19[slice47],\r\n",
        "       train_regret_winner_20[slice47]]\r\n",
        "\r\n",
        "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\r\n",
        "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\r\n",
        "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\r\n",
        "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\r\n",
        "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\r\n",
        "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fIy_EyTxA20"
      },
      "source": [
        "# Iteration57 :\r\n",
        "\r\n",
        "slice57 = 56\r\n",
        "\r\n",
        "loser57 = [train_regret_loser_1[slice57],\r\n",
        "       train_regret_loser_2[slice57],\r\n",
        "       train_regret_loser_3[slice57],\r\n",
        "       train_regret_loser_4[slice57],\r\n",
        "       train_regret_loser_5[slice57],\r\n",
        "       train_regret_loser_6[slice57],\r\n",
        "       train_regret_loser_7[slice57],\r\n",
        "       train_regret_loser_8[slice57],\r\n",
        "       train_regret_loser_9[slice57],\r\n",
        "       train_regret_loser_10[slice57],\r\n",
        "       train_regret_loser_11[slice57],\r\n",
        "       train_regret_loser_12[slice57],\r\n",
        "       train_regret_loser_13[slice57],\r\n",
        "       train_regret_loser_14[slice57],\r\n",
        "       train_regret_loser_15[slice57],\r\n",
        "       train_regret_loser_16[slice57],\r\n",
        "       train_regret_loser_17[slice57],\r\n",
        "       train_regret_loser_18[slice57],\r\n",
        "       train_regret_loser_19[slice57],\r\n",
        "       train_regret_loser_20[slice57]]\r\n",
        "\r\n",
        "winner57 = [train_regret_winner_1[slice57],\r\n",
        "       train_regret_winner_2[slice57],\r\n",
        "       train_regret_winner_3[slice57],\r\n",
        "       train_regret_winner_4[slice57],\r\n",
        "       train_regret_winner_5[slice57],\r\n",
        "       train_regret_winner_6[slice57],\r\n",
        "       train_regret_winner_7[slice57],\r\n",
        "       train_regret_winner_8[slice57],\r\n",
        "       train_regret_winner_9[slice57],\r\n",
        "       train_regret_winner_10[slice57],\r\n",
        "       train_regret_winner_11[slice57],\r\n",
        "       train_regret_winner_12[slice57],\r\n",
        "       train_regret_winner_13[slice57],\r\n",
        "       train_regret_winner_14[slice57],\r\n",
        "       train_regret_winner_15[slice57],\r\n",
        "       train_regret_winner_16[slice57],\r\n",
        "       train_regret_winner_17[slice57],\r\n",
        "       train_regret_winner_18[slice57],\r\n",
        "       train_regret_winner_19[slice57],\r\n",
        "       train_regret_winner_20[slice57]]\r\n",
        "\r\n",
        "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\r\n",
        "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\r\n",
        "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\r\n",
        "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\r\n",
        "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\r\n",
        "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umwdBl8XxA5r"
      },
      "source": [
        "# Iteration67 :\r\n",
        "\r\n",
        "slice67 = 66\r\n",
        "\r\n",
        "loser67 = [train_regret_loser_1[slice67],\r\n",
        "       train_regret_loser_2[slice67],\r\n",
        "       train_regret_loser_3[slice67],\r\n",
        "       train_regret_loser_4[slice67],\r\n",
        "       train_regret_loser_5[slice67],\r\n",
        "       train_regret_loser_6[slice67],\r\n",
        "       train_regret_loser_7[slice67],\r\n",
        "       train_regret_loser_8[slice67],\r\n",
        "       train_regret_loser_9[slice67],\r\n",
        "       train_regret_loser_10[slice67],\r\n",
        "       train_regret_loser_11[slice67],\r\n",
        "       train_regret_loser_12[slice67],\r\n",
        "       train_regret_loser_13[slice67],\r\n",
        "       train_regret_loser_14[slice67],\r\n",
        "       train_regret_loser_15[slice67],\r\n",
        "       train_regret_loser_16[slice67],\r\n",
        "       train_regret_loser_17[slice67],\r\n",
        "       train_regret_loser_18[slice67],\r\n",
        "       train_regret_loser_19[slice67],\r\n",
        "       train_regret_loser_20[slice67]]\r\n",
        "\r\n",
        "winner67 = [train_regret_winner_1[slice67],\r\n",
        "       train_regret_winner_2[slice67],\r\n",
        "       train_regret_winner_3[slice67],\r\n",
        "       train_regret_winner_4[slice67],\r\n",
        "       train_regret_winner_5[slice67],\r\n",
        "       train_regret_winner_6[slice67],\r\n",
        "       train_regret_winner_7[slice67],\r\n",
        "       train_regret_winner_8[slice67],\r\n",
        "       train_regret_winner_9[slice67],\r\n",
        "       train_regret_winner_10[slice67],\r\n",
        "       train_regret_winner_11[slice67],\r\n",
        "       train_regret_winner_12[slice67],\r\n",
        "       train_regret_winner_13[slice67],\r\n",
        "       train_regret_winner_14[slice67],\r\n",
        "       train_regret_winner_15[slice67],\r\n",
        "       train_regret_winner_16[slice67],\r\n",
        "       train_regret_winner_17[slice67],\r\n",
        "       train_regret_winner_18[slice67],\r\n",
        "       train_regret_winner_19[slice67],\r\n",
        "       train_regret_winner_20[slice67]]\r\n",
        "\r\n",
        "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\r\n",
        "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\r\n",
        "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\r\n",
        "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\r\n",
        "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\r\n",
        "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF3KFX-ZxA8q"
      },
      "source": [
        "# Iteration77 :\r\n",
        "\r\n",
        "slice77 = 76\r\n",
        "\r\n",
        "loser77 = [train_regret_loser_1[slice77],\r\n",
        "       train_regret_loser_2[slice77],\r\n",
        "       train_regret_loser_3[slice77],\r\n",
        "       train_regret_loser_4[slice77],\r\n",
        "       train_regret_loser_5[slice77],\r\n",
        "       train_regret_loser_6[slice77],\r\n",
        "       train_regret_loser_7[slice77],\r\n",
        "       train_regret_loser_8[slice77],\r\n",
        "       train_regret_loser_9[slice77],\r\n",
        "       train_regret_loser_10[slice77],\r\n",
        "       train_regret_loser_11[slice77],\r\n",
        "       train_regret_loser_12[slice77],\r\n",
        "       train_regret_loser_13[slice77],\r\n",
        "       train_regret_loser_14[slice77],\r\n",
        "       train_regret_loser_15[slice77],\r\n",
        "       train_regret_loser_16[slice77],\r\n",
        "       train_regret_loser_17[slice77],\r\n",
        "       train_regret_loser_18[slice77],\r\n",
        "       train_regret_loser_19[slice77],\r\n",
        "       train_regret_loser_20[slice77]]\r\n",
        "\r\n",
        "winner77 = [train_regret_winner_1[slice77],\r\n",
        "       train_regret_winner_2[slice77],\r\n",
        "       train_regret_winner_3[slice77],\r\n",
        "       train_regret_winner_4[slice77],\r\n",
        "       train_regret_winner_5[slice77],\r\n",
        "       train_regret_winner_6[slice77],\r\n",
        "       train_regret_winner_7[slice77],\r\n",
        "       train_regret_winner_8[slice77],\r\n",
        "       train_regret_winner_9[slice77],\r\n",
        "       train_regret_winner_10[slice77],\r\n",
        "       train_regret_winner_11[slice77],\r\n",
        "       train_regret_winner_12[slice77],\r\n",
        "       train_regret_winner_13[slice77],\r\n",
        "       train_regret_winner_14[slice77],\r\n",
        "       train_regret_winner_15[slice77],\r\n",
        "       train_regret_winner_16[slice77],\r\n",
        "       train_regret_winner_17[slice77],\r\n",
        "       train_regret_winner_18[slice77],\r\n",
        "       train_regret_winner_19[slice77],\r\n",
        "       train_regret_winner_20[slice77]]\r\n",
        "\r\n",
        "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\r\n",
        "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\r\n",
        "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\r\n",
        "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\r\n",
        "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\r\n",
        "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVYft39xA_t"
      },
      "source": [
        "# Iteration87 :\r\n",
        "\r\n",
        "slice87 = 86\r\n",
        "\r\n",
        "loser87 = [train_regret_loser_1[slice87],\r\n",
        "       train_regret_loser_2[slice87],\r\n",
        "       train_regret_loser_3[slice87],\r\n",
        "       train_regret_loser_4[slice87],\r\n",
        "       train_regret_loser_5[slice87],\r\n",
        "       train_regret_loser_6[slice87],\r\n",
        "       train_regret_loser_7[slice87],\r\n",
        "       train_regret_loser_8[slice87],\r\n",
        "       train_regret_loser_9[slice87],\r\n",
        "       train_regret_loser_10[slice87],\r\n",
        "       train_regret_loser_11[slice87],\r\n",
        "       train_regret_loser_12[slice87],\r\n",
        "       train_regret_loser_13[slice87],\r\n",
        "       train_regret_loser_14[slice87],\r\n",
        "       train_regret_loser_15[slice87],\r\n",
        "       train_regret_loser_16[slice87],\r\n",
        "       train_regret_loser_17[slice87],\r\n",
        "       train_regret_loser_18[slice87],\r\n",
        "       train_regret_loser_19[slice87],\r\n",
        "       train_regret_loser_20[slice87]]\r\n",
        "\r\n",
        "winner87 = [train_regret_winner_1[slice87],\r\n",
        "       train_regret_winner_2[slice87],\r\n",
        "       train_regret_winner_3[slice87],\r\n",
        "       train_regret_winner_4[slice87],\r\n",
        "       train_regret_winner_5[slice87],\r\n",
        "       train_regret_winner_6[slice87],\r\n",
        "       train_regret_winner_7[slice87],\r\n",
        "       train_regret_winner_8[slice87],\r\n",
        "       train_regret_winner_9[slice87],\r\n",
        "       train_regret_winner_10[slice87],\r\n",
        "       train_regret_winner_11[slice87],\r\n",
        "       train_regret_winner_12[slice87],\r\n",
        "       train_regret_winner_13[slice87],\r\n",
        "       train_regret_winner_14[slice87],\r\n",
        "       train_regret_winner_15[slice87],\r\n",
        "       train_regret_winner_16[slice87],\r\n",
        "       train_regret_winner_17[slice87],\r\n",
        "       train_regret_winner_18[slice87],\r\n",
        "       train_regret_winner_19[slice87],\r\n",
        "       train_regret_winner_20[slice87]]\r\n",
        "\r\n",
        "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\r\n",
        "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\r\n",
        "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\r\n",
        "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\r\n",
        "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\r\n",
        "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gPkLC0yxBCs"
      },
      "source": [
        "# Iteration97 :\r\n",
        "\r\n",
        "slice97 = 96\r\n",
        "\r\n",
        "loser97 = [train_regret_loser_1[slice97],\r\n",
        "       train_regret_loser_2[slice97],\r\n",
        "       train_regret_loser_3[slice97],\r\n",
        "       train_regret_loser_4[slice97],\r\n",
        "       train_regret_loser_5[slice97],\r\n",
        "       train_regret_loser_6[slice97],\r\n",
        "       train_regret_loser_7[slice97],\r\n",
        "       train_regret_loser_8[slice97],\r\n",
        "       train_regret_loser_9[slice97],\r\n",
        "       train_regret_loser_10[slice97],\r\n",
        "       train_regret_loser_11[slice97],\r\n",
        "       train_regret_loser_12[slice97],\r\n",
        "       train_regret_loser_13[slice97],\r\n",
        "       train_regret_loser_14[slice97],\r\n",
        "       train_regret_loser_15[slice97],\r\n",
        "       train_regret_loser_16[slice97],\r\n",
        "       train_regret_loser_17[slice97],\r\n",
        "       train_regret_loser_18[slice97],\r\n",
        "       train_regret_loser_19[slice97],\r\n",
        "       train_regret_loser_20[slice97]]\r\n",
        "\r\n",
        "winner97 = [train_regret_winner_1[slice97],\r\n",
        "       train_regret_winner_2[slice97],\r\n",
        "       train_regret_winner_3[slice97],\r\n",
        "       train_regret_winner_4[slice97],\r\n",
        "       train_regret_winner_5[slice97],\r\n",
        "       train_regret_winner_6[slice97],\r\n",
        "       train_regret_winner_7[slice97],\r\n",
        "       train_regret_winner_8[slice97],\r\n",
        "       train_regret_winner_9[slice97],\r\n",
        "       train_regret_winner_10[slice97],\r\n",
        "       train_regret_winner_11[slice97],\r\n",
        "       train_regret_winner_12[slice97],\r\n",
        "       train_regret_winner_13[slice97],\r\n",
        "       train_regret_winner_14[slice97],\r\n",
        "       train_regret_winner_15[slice97],\r\n",
        "       train_regret_winner_16[slice97],\r\n",
        "       train_regret_winner_17[slice97],\r\n",
        "       train_regret_winner_18[slice97],\r\n",
        "       train_regret_winner_19[slice97],\r\n",
        "       train_regret_winner_20[slice97]]\r\n",
        "\r\n",
        "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\r\n",
        "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\r\n",
        "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\r\n",
        "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\r\n",
        "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\r\n",
        "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbgYEpYUxBFr"
      },
      "source": [
        "# Iteration8 :\r\n",
        "\r\n",
        "slice8 = 7\r\n",
        "\r\n",
        "loser8 = [train_regret_loser_1[slice8],\r\n",
        "       train_regret_loser_2[slice8],\r\n",
        "       train_regret_loser_3[slice8],\r\n",
        "       train_regret_loser_4[slice8],\r\n",
        "       train_regret_loser_5[slice8],\r\n",
        "       train_regret_loser_6[slice8],\r\n",
        "       train_regret_loser_7[slice8],\r\n",
        "       train_regret_loser_8[slice8],\r\n",
        "       train_regret_loser_9[slice8],\r\n",
        "       train_regret_loser_10[slice8],\r\n",
        "       train_regret_loser_11[slice8],\r\n",
        "       train_regret_loser_12[slice8],\r\n",
        "       train_regret_loser_13[slice8],\r\n",
        "       train_regret_loser_14[slice8],\r\n",
        "       train_regret_loser_15[slice8],\r\n",
        "       train_regret_loser_16[slice8],\r\n",
        "       train_regret_loser_17[slice8],\r\n",
        "       train_regret_loser_18[slice8],\r\n",
        "       train_regret_loser_19[slice8],\r\n",
        "       train_regret_loser_20[slice8]]\r\n",
        "\r\n",
        "winner8 = [train_regret_winner_1[slice8],\r\n",
        "       train_regret_winner_2[slice8],\r\n",
        "       train_regret_winner_3[slice8],\r\n",
        "       train_regret_winner_4[slice8],\r\n",
        "       train_regret_winner_5[slice8],\r\n",
        "       train_regret_winner_6[slice8],\r\n",
        "       train_regret_winner_7[slice8],\r\n",
        "       train_regret_winner_8[slice8],\r\n",
        "       train_regret_winner_9[slice8],\r\n",
        "       train_regret_winner_10[slice8],\r\n",
        "       train_regret_winner_11[slice8],\r\n",
        "       train_regret_winner_12[slice8],\r\n",
        "       train_regret_winner_13[slice8],\r\n",
        "       train_regret_winner_14[slice8],\r\n",
        "       train_regret_winner_15[slice8],\r\n",
        "       train_regret_winner_16[slice8],\r\n",
        "       train_regret_winner_17[slice8],\r\n",
        "       train_regret_winner_18[slice8],\r\n",
        "       train_regret_winner_19[slice8],\r\n",
        "       train_regret_winner_20[slice8]]\r\n",
        "\r\n",
        "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\r\n",
        "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\r\n",
        "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\r\n",
        "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\r\n",
        "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\r\n",
        "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im9dHUqvxBH9"
      },
      "source": [
        "# Iteration18 :\r\n",
        "\r\n",
        "slice18 = 17\r\n",
        "\r\n",
        "loser18 = [train_regret_loser_1[slice18],\r\n",
        "       train_regret_loser_2[slice18],\r\n",
        "       train_regret_loser_3[slice18],\r\n",
        "       train_regret_loser_4[slice18],\r\n",
        "       train_regret_loser_5[slice18],\r\n",
        "       train_regret_loser_6[slice18],\r\n",
        "       train_regret_loser_7[slice18],\r\n",
        "       train_regret_loser_8[slice18],\r\n",
        "       train_regret_loser_9[slice18],\r\n",
        "       train_regret_loser_10[slice18],\r\n",
        "       train_regret_loser_11[slice18],\r\n",
        "       train_regret_loser_12[slice18],\r\n",
        "       train_regret_loser_13[slice18],\r\n",
        "       train_regret_loser_14[slice18],\r\n",
        "       train_regret_loser_15[slice18],\r\n",
        "       train_regret_loser_16[slice18],\r\n",
        "       train_regret_loser_17[slice18],\r\n",
        "       train_regret_loser_18[slice18],\r\n",
        "       train_regret_loser_19[slice18],\r\n",
        "       train_regret_loser_20[slice18]]\r\n",
        "\r\n",
        "winner18 = [train_regret_winner_1[slice18],\r\n",
        "       train_regret_winner_2[slice18],\r\n",
        "       train_regret_winner_3[slice18],\r\n",
        "       train_regret_winner_4[slice18],\r\n",
        "       train_regret_winner_5[slice18],\r\n",
        "       train_regret_winner_6[slice18],\r\n",
        "       train_regret_winner_7[slice18],\r\n",
        "       train_regret_winner_8[slice18],\r\n",
        "       train_regret_winner_9[slice18],\r\n",
        "       train_regret_winner_10[slice18],\r\n",
        "       train_regret_winner_11[slice18],\r\n",
        "       train_regret_winner_12[slice18],\r\n",
        "       train_regret_winner_13[slice18],\r\n",
        "       train_regret_winner_14[slice18],\r\n",
        "       train_regret_winner_15[slice18],\r\n",
        "       train_regret_winner_16[slice18],\r\n",
        "       train_regret_winner_17[slice18],\r\n",
        "       train_regret_winner_18[slice18],\r\n",
        "       train_regret_winner_19[slice18],\r\n",
        "       train_regret_winner_20[slice18]]\r\n",
        "\r\n",
        "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\r\n",
        "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\r\n",
        "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\r\n",
        "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\r\n",
        "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\r\n",
        "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvqFjlBDxBK4"
      },
      "source": [
        "# Iteration28 :\r\n",
        "\r\n",
        "slice28 = 27\r\n",
        "\r\n",
        "loser28 = [train_regret_loser_1[slice28],\r\n",
        "       train_regret_loser_2[slice28],\r\n",
        "       train_regret_loser_3[slice28],\r\n",
        "       train_regret_loser_4[slice28],\r\n",
        "       train_regret_loser_5[slice28],\r\n",
        "       train_regret_loser_6[slice28],\r\n",
        "       train_regret_loser_7[slice28],\r\n",
        "       train_regret_loser_8[slice28],\r\n",
        "       train_regret_loser_9[slice28],\r\n",
        "       train_regret_loser_10[slice28],\r\n",
        "       train_regret_loser_11[slice28],\r\n",
        "       train_regret_loser_12[slice28],\r\n",
        "       train_regret_loser_13[slice28],\r\n",
        "       train_regret_loser_14[slice28],\r\n",
        "       train_regret_loser_15[slice28],\r\n",
        "       train_regret_loser_16[slice28],\r\n",
        "       train_regret_loser_17[slice28],\r\n",
        "       train_regret_loser_18[slice28],\r\n",
        "       train_regret_loser_19[slice28],\r\n",
        "       train_regret_loser_20[slice28]]\r\n",
        "\r\n",
        "winner28 = [train_regret_winner_1[slice28],\r\n",
        "       train_regret_winner_2[slice28],\r\n",
        "       train_regret_winner_3[slice28],\r\n",
        "       train_regret_winner_4[slice28],\r\n",
        "       train_regret_winner_5[slice28],\r\n",
        "       train_regret_winner_6[slice28],\r\n",
        "       train_regret_winner_7[slice28],\r\n",
        "       train_regret_winner_8[slice28],\r\n",
        "       train_regret_winner_9[slice28],\r\n",
        "       train_regret_winner_10[slice28],\r\n",
        "       train_regret_winner_11[slice28],\r\n",
        "       train_regret_winner_12[slice28],\r\n",
        "       train_regret_winner_13[slice28],\r\n",
        "       train_regret_winner_14[slice28],\r\n",
        "       train_regret_winner_15[slice28],\r\n",
        "       train_regret_winner_16[slice28],\r\n",
        "       train_regret_winner_17[slice28],\r\n",
        "       train_regret_winner_18[slice28],\r\n",
        "       train_regret_winner_19[slice28],\r\n",
        "       train_regret_winner_20[slice28]]\r\n",
        "\r\n",
        "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\r\n",
        "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\r\n",
        "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\r\n",
        "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\r\n",
        "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\r\n",
        "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It5daJCEted3"
      },
      "source": [
        "# Iteration38 :\r\n",
        "\r\n",
        "slice38 = 37\r\n",
        "\r\n",
        "loser38 = [train_regret_loser_1[slice38],\r\n",
        "       train_regret_loser_2[slice38],\r\n",
        "       train_regret_loser_3[slice38],\r\n",
        "       train_regret_loser_4[slice38],\r\n",
        "       train_regret_loser_5[slice38],\r\n",
        "       train_regret_loser_6[slice38],\r\n",
        "       train_regret_loser_7[slice38],\r\n",
        "       train_regret_loser_8[slice38],\r\n",
        "       train_regret_loser_9[slice38],\r\n",
        "       train_regret_loser_10[slice38],\r\n",
        "       train_regret_loser_11[slice38],\r\n",
        "       train_regret_loser_12[slice38],\r\n",
        "       train_regret_loser_13[slice38],\r\n",
        "       train_regret_loser_14[slice38],\r\n",
        "       train_regret_loser_15[slice38],\r\n",
        "       train_regret_loser_16[slice38],\r\n",
        "       train_regret_loser_17[slice38],\r\n",
        "       train_regret_loser_18[slice38],\r\n",
        "       train_regret_loser_19[slice38],\r\n",
        "       train_regret_loser_20[slice38]]\r\n",
        "\r\n",
        "winner38 = [train_regret_winner_1[slice38],\r\n",
        "       train_regret_winner_2[slice38],\r\n",
        "       train_regret_winner_3[slice38],\r\n",
        "       train_regret_winner_4[slice38],\r\n",
        "       train_regret_winner_5[slice38],\r\n",
        "       train_regret_winner_6[slice38],\r\n",
        "       train_regret_winner_7[slice38],\r\n",
        "       train_regret_winner_8[slice38],\r\n",
        "       train_regret_winner_9[slice38],\r\n",
        "       train_regret_winner_10[slice38],\r\n",
        "       train_regret_winner_11[slice38],\r\n",
        "       train_regret_winner_12[slice38],\r\n",
        "       train_regret_winner_13[slice38],\r\n",
        "       train_regret_winner_14[slice38],\r\n",
        "       train_regret_winner_15[slice38],\r\n",
        "       train_regret_winner_16[slice38],\r\n",
        "       train_regret_winner_17[slice38],\r\n",
        "       train_regret_winner_18[slice38],\r\n",
        "       train_regret_winner_19[slice38],\r\n",
        "       train_regret_winner_20[slice38]]\r\n",
        "\r\n",
        "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\r\n",
        "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\r\n",
        "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\r\n",
        "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\r\n",
        "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\r\n",
        "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GekXOf_gtegw"
      },
      "source": [
        "# Iteration48 :\r\n",
        "\r\n",
        "slice48 = 47\r\n",
        "\r\n",
        "loser48 = [train_regret_loser_1[slice48],\r\n",
        "       train_regret_loser_2[slice48],\r\n",
        "       train_regret_loser_3[slice48],\r\n",
        "       train_regret_loser_4[slice48],\r\n",
        "       train_regret_loser_5[slice48],\r\n",
        "       train_regret_loser_6[slice48],\r\n",
        "       train_regret_loser_7[slice48],\r\n",
        "       train_regret_loser_8[slice48],\r\n",
        "       train_regret_loser_9[slice48],\r\n",
        "       train_regret_loser_10[slice48],\r\n",
        "       train_regret_loser_11[slice48],\r\n",
        "       train_regret_loser_12[slice48],\r\n",
        "       train_regret_loser_13[slice48],\r\n",
        "       train_regret_loser_14[slice48],\r\n",
        "       train_regret_loser_15[slice48],\r\n",
        "       train_regret_loser_16[slice48],\r\n",
        "       train_regret_loser_17[slice48],\r\n",
        "       train_regret_loser_18[slice48],\r\n",
        "       train_regret_loser_19[slice48],\r\n",
        "       train_regret_loser_20[slice48]]\r\n",
        "\r\n",
        "winner48 = [train_regret_winner_1[slice48],\r\n",
        "       train_regret_winner_2[slice48],\r\n",
        "       train_regret_winner_3[slice48],\r\n",
        "       train_regret_winner_4[slice48],\r\n",
        "       train_regret_winner_5[slice48],\r\n",
        "       train_regret_winner_6[slice48],\r\n",
        "       train_regret_winner_7[slice48],\r\n",
        "       train_regret_winner_8[slice48],\r\n",
        "       train_regret_winner_9[slice48],\r\n",
        "       train_regret_winner_10[slice48],\r\n",
        "       train_regret_winner_11[slice48],\r\n",
        "       train_regret_winner_12[slice48],\r\n",
        "       train_regret_winner_13[slice48],\r\n",
        "       train_regret_winner_14[slice48],\r\n",
        "       train_regret_winner_15[slice48],\r\n",
        "       train_regret_winner_16[slice48],\r\n",
        "       train_regret_winner_17[slice48],\r\n",
        "       train_regret_winner_18[slice48],\r\n",
        "       train_regret_winner_19[slice48],\r\n",
        "       train_regret_winner_20[slice48]]\r\n",
        "\r\n",
        "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\r\n",
        "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\r\n",
        "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\r\n",
        "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\r\n",
        "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\r\n",
        "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNpV6knOtejv"
      },
      "source": [
        "# Iteration58 :\r\n",
        "\r\n",
        "slice58 = 57\r\n",
        "\r\n",
        "loser58 = [train_regret_loser_1[slice58],\r\n",
        "       train_regret_loser_2[slice58],\r\n",
        "       train_regret_loser_3[slice58],\r\n",
        "       train_regret_loser_4[slice58],\r\n",
        "       train_regret_loser_5[slice58],\r\n",
        "       train_regret_loser_6[slice58],\r\n",
        "       train_regret_loser_7[slice58],\r\n",
        "       train_regret_loser_8[slice58],\r\n",
        "       train_regret_loser_9[slice58],\r\n",
        "       train_regret_loser_10[slice58],\r\n",
        "       train_regret_loser_11[slice58],\r\n",
        "       train_regret_loser_12[slice58],\r\n",
        "       train_regret_loser_13[slice58],\r\n",
        "       train_regret_loser_14[slice58],\r\n",
        "       train_regret_loser_15[slice58],\r\n",
        "       train_regret_loser_16[slice58],\r\n",
        "       train_regret_loser_17[slice58],\r\n",
        "       train_regret_loser_18[slice58],\r\n",
        "       train_regret_loser_19[slice58],\r\n",
        "       train_regret_loser_20[slice58]]\r\n",
        "\r\n",
        "winner58 = [train_regret_winner_1[slice58],\r\n",
        "       train_regret_winner_2[slice58],\r\n",
        "       train_regret_winner_3[slice58],\r\n",
        "       train_regret_winner_4[slice58],\r\n",
        "       train_regret_winner_5[slice58],\r\n",
        "       train_regret_winner_6[slice58],\r\n",
        "       train_regret_winner_7[slice58],\r\n",
        "       train_regret_winner_8[slice58],\r\n",
        "       train_regret_winner_9[slice58],\r\n",
        "       train_regret_winner_10[slice58],\r\n",
        "       train_regret_winner_11[slice58],\r\n",
        "       train_regret_winner_12[slice58],\r\n",
        "       train_regret_winner_13[slice58],\r\n",
        "       train_regret_winner_14[slice58],\r\n",
        "       train_regret_winner_15[slice58],\r\n",
        "       train_regret_winner_16[slice58],\r\n",
        "       train_regret_winner_17[slice58],\r\n",
        "       train_regret_winner_18[slice58],\r\n",
        "       train_regret_winner_19[slice58],\r\n",
        "       train_regret_winner_20[slice58]]\r\n",
        "\r\n",
        "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\r\n",
        "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\r\n",
        "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\r\n",
        "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\r\n",
        "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\r\n",
        "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VFsqaBqtemn"
      },
      "source": [
        "# Iteration68 :\r\n",
        "\r\n",
        "slice68 = 67\r\n",
        "\r\n",
        "loser68 = [train_regret_loser_1[slice68],\r\n",
        "       train_regret_loser_2[slice68],\r\n",
        "       train_regret_loser_3[slice68],\r\n",
        "       train_regret_loser_4[slice68],\r\n",
        "       train_regret_loser_5[slice68],\r\n",
        "       train_regret_loser_6[slice68],\r\n",
        "       train_regret_loser_7[slice68],\r\n",
        "       train_regret_loser_8[slice68],\r\n",
        "       train_regret_loser_9[slice68],\r\n",
        "       train_regret_loser_10[slice68],\r\n",
        "       train_regret_loser_11[slice68],\r\n",
        "       train_regret_loser_12[slice68],\r\n",
        "       train_regret_loser_13[slice68],\r\n",
        "       train_regret_loser_14[slice68],\r\n",
        "       train_regret_loser_15[slice68],\r\n",
        "       train_regret_loser_16[slice68],\r\n",
        "       train_regret_loser_17[slice68],\r\n",
        "       train_regret_loser_18[slice68],\r\n",
        "       train_regret_loser_19[slice68],\r\n",
        "       train_regret_loser_20[slice68]]\r\n",
        "\r\n",
        "winner68 = [train_regret_winner_1[slice68],\r\n",
        "       train_regret_winner_2[slice68],\r\n",
        "       train_regret_winner_3[slice68],\r\n",
        "       train_regret_winner_4[slice68],\r\n",
        "       train_regret_winner_5[slice68],\r\n",
        "       train_regret_winner_6[slice68],\r\n",
        "       train_regret_winner_7[slice68],\r\n",
        "       train_regret_winner_8[slice68],\r\n",
        "       train_regret_winner_9[slice68],\r\n",
        "       train_regret_winner_10[slice68],\r\n",
        "       train_regret_winner_11[slice68],\r\n",
        "       train_regret_winner_12[slice68],\r\n",
        "       train_regret_winner_13[slice68],\r\n",
        "       train_regret_winner_14[slice68],\r\n",
        "       train_regret_winner_15[slice68],\r\n",
        "       train_regret_winner_16[slice68],\r\n",
        "       train_regret_winner_17[slice68],\r\n",
        "       train_regret_winner_18[slice68],\r\n",
        "       train_regret_winner_19[slice68],\r\n",
        "       train_regret_winner_20[slice68]]\r\n",
        "\r\n",
        "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\r\n",
        "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\r\n",
        "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\r\n",
        "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\r\n",
        "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\r\n",
        "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XlFH0Wxtepo"
      },
      "source": [
        "# Iteration78 :\r\n",
        "\r\n",
        "slice78 = 77\r\n",
        "\r\n",
        "loser78 = [train_regret_loser_1[slice78],\r\n",
        "       train_regret_loser_2[slice78],\r\n",
        "       train_regret_loser_3[slice78],\r\n",
        "       train_regret_loser_4[slice78],\r\n",
        "       train_regret_loser_5[slice78],\r\n",
        "       train_regret_loser_6[slice78],\r\n",
        "       train_regret_loser_7[slice78],\r\n",
        "       train_regret_loser_8[slice78],\r\n",
        "       train_regret_loser_9[slice78],\r\n",
        "       train_regret_loser_10[slice78],\r\n",
        "       train_regret_loser_11[slice78],\r\n",
        "       train_regret_loser_12[slice78],\r\n",
        "       train_regret_loser_13[slice78],\r\n",
        "       train_regret_loser_14[slice78],\r\n",
        "       train_regret_loser_15[slice78],\r\n",
        "       train_regret_loser_16[slice78],\r\n",
        "       train_regret_loser_17[slice78],\r\n",
        "       train_regret_loser_18[slice78],\r\n",
        "       train_regret_loser_19[slice78],\r\n",
        "       train_regret_loser_20[slice78]]\r\n",
        "\r\n",
        "winner78 = [train_regret_winner_1[slice78],\r\n",
        "       train_regret_winner_2[slice78],\r\n",
        "       train_regret_winner_3[slice78],\r\n",
        "       train_regret_winner_4[slice78],\r\n",
        "       train_regret_winner_5[slice78],\r\n",
        "       train_regret_winner_6[slice78],\r\n",
        "       train_regret_winner_7[slice78],\r\n",
        "       train_regret_winner_8[slice78],\r\n",
        "       train_regret_winner_9[slice78],\r\n",
        "       train_regret_winner_10[slice78],\r\n",
        "       train_regret_winner_11[slice78],\r\n",
        "       train_regret_winner_12[slice78],\r\n",
        "       train_regret_winner_13[slice78],\r\n",
        "       train_regret_winner_14[slice78],\r\n",
        "       train_regret_winner_15[slice78],\r\n",
        "       train_regret_winner_16[slice78],\r\n",
        "       train_regret_winner_17[slice78],\r\n",
        "       train_regret_winner_18[slice78],\r\n",
        "       train_regret_winner_19[slice78],\r\n",
        "       train_regret_winner_20[slice78]]\r\n",
        "\r\n",
        "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\r\n",
        "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\r\n",
        "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\r\n",
        "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\r\n",
        "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\r\n",
        "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsL0PJVetesC"
      },
      "source": [
        "# Iteration88 :\r\n",
        "\r\n",
        "slice88 = 87\r\n",
        "\r\n",
        "loser88 = [train_regret_loser_1[slice88],\r\n",
        "       train_regret_loser_2[slice88],\r\n",
        "       train_regret_loser_3[slice88],\r\n",
        "       train_regret_loser_4[slice88],\r\n",
        "       train_regret_loser_5[slice88],\r\n",
        "       train_regret_loser_6[slice88],\r\n",
        "       train_regret_loser_7[slice88],\r\n",
        "       train_regret_loser_8[slice88],\r\n",
        "       train_regret_loser_9[slice88],\r\n",
        "       train_regret_loser_10[slice88],\r\n",
        "       train_regret_loser_11[slice88],\r\n",
        "       train_regret_loser_12[slice88],\r\n",
        "       train_regret_loser_13[slice88],\r\n",
        "       train_regret_loser_14[slice88],\r\n",
        "       train_regret_loser_15[slice88],\r\n",
        "       train_regret_loser_16[slice88],\r\n",
        "       train_regret_loser_17[slice88],\r\n",
        "       train_regret_loser_18[slice88],\r\n",
        "       train_regret_loser_19[slice88],\r\n",
        "       train_regret_loser_20[slice88]]\r\n",
        "\r\n",
        "winner88 = [train_regret_winner_1[slice88],\r\n",
        "       train_regret_winner_2[slice88],\r\n",
        "       train_regret_winner_3[slice88],\r\n",
        "       train_regret_winner_4[slice88],\r\n",
        "       train_regret_winner_5[slice88],\r\n",
        "       train_regret_winner_6[slice88],\r\n",
        "       train_regret_winner_7[slice88],\r\n",
        "       train_regret_winner_8[slice88],\r\n",
        "       train_regret_winner_9[slice88],\r\n",
        "       train_regret_winner_10[slice88],\r\n",
        "       train_regret_winner_11[slice88],\r\n",
        "       train_regret_winner_12[slice88],\r\n",
        "       train_regret_winner_13[slice88],\r\n",
        "       train_regret_winner_14[slice88],\r\n",
        "       train_regret_winner_15[slice88],\r\n",
        "       train_regret_winner_16[slice88],\r\n",
        "       train_regret_winner_17[slice88],\r\n",
        "       train_regret_winner_18[slice88],\r\n",
        "       train_regret_winner_19[slice88],\r\n",
        "       train_regret_winner_20[slice88]]\r\n",
        "\r\n",
        "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\r\n",
        "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\r\n",
        "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\r\n",
        "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\r\n",
        "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\r\n",
        "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYWEWGku1o_9"
      },
      "source": [
        "# Iteration98 :\r\n",
        "\r\n",
        "slice98 = 97\r\n",
        "\r\n",
        "loser98 = [train_regret_loser_1[slice98],\r\n",
        "       train_regret_loser_2[slice98],\r\n",
        "       train_regret_loser_3[slice98],\r\n",
        "       train_regret_loser_4[slice98],\r\n",
        "       train_regret_loser_5[slice98],\r\n",
        "       train_regret_loser_6[slice98],\r\n",
        "       train_regret_loser_7[slice98],\r\n",
        "       train_regret_loser_8[slice98],\r\n",
        "       train_regret_loser_9[slice98],\r\n",
        "       train_regret_loser_10[slice98],\r\n",
        "       train_regret_loser_11[slice98],\r\n",
        "       train_regret_loser_12[slice98],\r\n",
        "       train_regret_loser_13[slice98],\r\n",
        "       train_regret_loser_14[slice98],\r\n",
        "       train_regret_loser_15[slice98],\r\n",
        "       train_regret_loser_16[slice98],\r\n",
        "       train_regret_loser_17[slice98],\r\n",
        "       train_regret_loser_18[slice98],\r\n",
        "       train_regret_loser_19[slice98],\r\n",
        "       train_regret_loser_20[slice98]]\r\n",
        "\r\n",
        "winner98 = [train_regret_winner_1[slice98],\r\n",
        "       train_regret_winner_2[slice98],\r\n",
        "       train_regret_winner_3[slice98],\r\n",
        "       train_regret_winner_4[slice98],\r\n",
        "       train_regret_winner_5[slice98],\r\n",
        "       train_regret_winner_6[slice98],\r\n",
        "       train_regret_winner_7[slice98],\r\n",
        "       train_regret_winner_8[slice98],\r\n",
        "       train_regret_winner_9[slice98],\r\n",
        "       train_regret_winner_10[slice98],\r\n",
        "       train_regret_winner_11[slice98],\r\n",
        "       train_regret_winner_12[slice98],\r\n",
        "       train_regret_winner_13[slice98],\r\n",
        "       train_regret_winner_14[slice98],\r\n",
        "       train_regret_winner_15[slice98],\r\n",
        "       train_regret_winner_16[slice98],\r\n",
        "       train_regret_winner_17[slice98],\r\n",
        "       train_regret_winner_18[slice98],\r\n",
        "       train_regret_winner_19[slice98],\r\n",
        "       train_regret_winner_20[slice98]]\r\n",
        "\r\n",
        "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\r\n",
        "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\r\n",
        "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\r\n",
        "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\r\n",
        "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\r\n",
        "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guLGgcX91pDK"
      },
      "source": [
        "# Iteration9 :\r\n",
        "\r\n",
        "slice9 = 8\r\n",
        "\r\n",
        "loser9 = [train_regret_loser_1[slice9],\r\n",
        "       train_regret_loser_2[slice9],\r\n",
        "       train_regret_loser_3[slice9],\r\n",
        "       train_regret_loser_4[slice9],\r\n",
        "       train_regret_loser_5[slice9],\r\n",
        "       train_regret_loser_6[slice9],\r\n",
        "       train_regret_loser_7[slice9],\r\n",
        "       train_regret_loser_8[slice9],\r\n",
        "       train_regret_loser_9[slice9],\r\n",
        "       train_regret_loser_10[slice9],\r\n",
        "       train_regret_loser_11[slice9],\r\n",
        "       train_regret_loser_12[slice9],\r\n",
        "       train_regret_loser_13[slice9],\r\n",
        "       train_regret_loser_14[slice9],\r\n",
        "       train_regret_loser_15[slice9],\r\n",
        "       train_regret_loser_16[slice9],\r\n",
        "       train_regret_loser_17[slice9],\r\n",
        "       train_regret_loser_18[slice9],\r\n",
        "       train_regret_loser_19[slice9],\r\n",
        "       train_regret_loser_20[slice9]]\r\n",
        "\r\n",
        "winner9 = [train_regret_winner_1[slice9],\r\n",
        "       train_regret_winner_2[slice9],\r\n",
        "       train_regret_winner_3[slice9],\r\n",
        "       train_regret_winner_4[slice9],\r\n",
        "       train_regret_winner_5[slice9],\r\n",
        "       train_regret_winner_6[slice9],\r\n",
        "       train_regret_winner_7[slice9],\r\n",
        "       train_regret_winner_8[slice9],\r\n",
        "       train_regret_winner_9[slice9],\r\n",
        "       train_regret_winner_10[slice9],\r\n",
        "       train_regret_winner_11[slice9],\r\n",
        "       train_regret_winner_12[slice9],\r\n",
        "       train_regret_winner_13[slice9],\r\n",
        "       train_regret_winner_14[slice9],\r\n",
        "       train_regret_winner_15[slice9],\r\n",
        "       train_regret_winner_16[slice9],\r\n",
        "       train_regret_winner_17[slice9],\r\n",
        "       train_regret_winner_18[slice9],\r\n",
        "       train_regret_winner_19[slice9],\r\n",
        "       train_regret_winner_20[slice9]]\r\n",
        "\r\n",
        "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\r\n",
        "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\r\n",
        "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\r\n",
        "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\r\n",
        "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\r\n",
        "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3_9-eoT1pGd"
      },
      "source": [
        "# Iteration19 :\r\n",
        "\r\n",
        "slice19 = 18\r\n",
        "\r\n",
        "loser19 = [train_regret_loser_1[slice19],\r\n",
        "       train_regret_loser_2[slice19],\r\n",
        "       train_regret_loser_3[slice19],\r\n",
        "       train_regret_loser_4[slice19],\r\n",
        "       train_regret_loser_5[slice19],\r\n",
        "       train_regret_loser_6[slice19],\r\n",
        "       train_regret_loser_7[slice19],\r\n",
        "       train_regret_loser_8[slice19],\r\n",
        "       train_regret_loser_9[slice19],\r\n",
        "       train_regret_loser_10[slice19],\r\n",
        "       train_regret_loser_11[slice19],\r\n",
        "       train_regret_loser_12[slice19],\r\n",
        "       train_regret_loser_13[slice19],\r\n",
        "       train_regret_loser_14[slice19],\r\n",
        "       train_regret_loser_15[slice19],\r\n",
        "       train_regret_loser_16[slice19],\r\n",
        "       train_regret_loser_17[slice19],\r\n",
        "       train_regret_loser_18[slice19],\r\n",
        "       train_regret_loser_19[slice19],\r\n",
        "       train_regret_loser_20[slice19]]\r\n",
        "\r\n",
        "winner19 = [train_regret_winner_1[slice19],\r\n",
        "       train_regret_winner_2[slice19],\r\n",
        "       train_regret_winner_3[slice19],\r\n",
        "       train_regret_winner_4[slice19],\r\n",
        "       train_regret_winner_5[slice19],\r\n",
        "       train_regret_winner_6[slice19],\r\n",
        "       train_regret_winner_7[slice19],\r\n",
        "       train_regret_winner_8[slice19],\r\n",
        "       train_regret_winner_9[slice19],\r\n",
        "       train_regret_winner_10[slice19],\r\n",
        "       train_regret_winner_11[slice19],\r\n",
        "       train_regret_winner_12[slice19],\r\n",
        "       train_regret_winner_13[slice19],\r\n",
        "       train_regret_winner_14[slice19],\r\n",
        "       train_regret_winner_15[slice19],\r\n",
        "       train_regret_winner_16[slice19],\r\n",
        "       train_regret_winner_17[slice19],\r\n",
        "       train_regret_winner_18[slice19],\r\n",
        "       train_regret_winner_19[slice19],\r\n",
        "       train_regret_winner_20[slice19]]\r\n",
        "\r\n",
        "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\r\n",
        "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\r\n",
        "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\r\n",
        "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\r\n",
        "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\r\n",
        "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ae5x13M1pRp"
      },
      "source": [
        "# Iteration29 :\r\n",
        "\r\n",
        "slice29 = 28\r\n",
        "\r\n",
        "loser29 = [train_regret_loser_1[slice29],\r\n",
        "       train_regret_loser_2[slice29],\r\n",
        "       train_regret_loser_3[slice29],\r\n",
        "       train_regret_loser_4[slice29],\r\n",
        "       train_regret_loser_5[slice29],\r\n",
        "       train_regret_loser_6[slice29],\r\n",
        "       train_regret_loser_7[slice29],\r\n",
        "       train_regret_loser_8[slice29],\r\n",
        "       train_regret_loser_9[slice29],\r\n",
        "       train_regret_loser_10[slice29],\r\n",
        "       train_regret_loser_11[slice29],\r\n",
        "       train_regret_loser_12[slice29],\r\n",
        "       train_regret_loser_13[slice29],\r\n",
        "       train_regret_loser_14[slice29],\r\n",
        "       train_regret_loser_15[slice29],\r\n",
        "       train_regret_loser_16[slice29],\r\n",
        "       train_regret_loser_17[slice29],\r\n",
        "       train_regret_loser_18[slice29],\r\n",
        "       train_regret_loser_19[slice29],\r\n",
        "       train_regret_loser_20[slice29]]\r\n",
        "\r\n",
        "winner29 = [train_regret_winner_1[slice29],\r\n",
        "       train_regret_winner_2[slice29],\r\n",
        "       train_regret_winner_3[slice29],\r\n",
        "       train_regret_winner_4[slice29],\r\n",
        "       train_regret_winner_5[slice29],\r\n",
        "       train_regret_winner_6[slice29],\r\n",
        "       train_regret_winner_7[slice29],\r\n",
        "       train_regret_winner_8[slice29],\r\n",
        "       train_regret_winner_9[slice29],\r\n",
        "       train_regret_winner_10[slice29],\r\n",
        "       train_regret_winner_11[slice29],\r\n",
        "       train_regret_winner_12[slice29],\r\n",
        "       train_regret_winner_13[slice29],\r\n",
        "       train_regret_winner_14[slice29],\r\n",
        "       train_regret_winner_15[slice29],\r\n",
        "       train_regret_winner_16[slice29],\r\n",
        "       train_regret_winner_17[slice29],\r\n",
        "       train_regret_winner_18[slice29],\r\n",
        "       train_regret_winner_19[slice29],\r\n",
        "       train_regret_winner_20[slice29]]\r\n",
        "\r\n",
        "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\r\n",
        "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\r\n",
        "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\r\n",
        "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\r\n",
        "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\r\n",
        "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR9aPvBy1pUc"
      },
      "source": [
        "# Iteration39 :\r\n",
        "\r\n",
        "slice39 = 38\r\n",
        "\r\n",
        "loser39 = [train_regret_loser_1[slice39],\r\n",
        "       train_regret_loser_2[slice39],\r\n",
        "       train_regret_loser_3[slice39],\r\n",
        "       train_regret_loser_4[slice39],\r\n",
        "       train_regret_loser_5[slice39],\r\n",
        "       train_regret_loser_6[slice39],\r\n",
        "       train_regret_loser_7[slice39],\r\n",
        "       train_regret_loser_8[slice39],\r\n",
        "       train_regret_loser_9[slice39],\r\n",
        "       train_regret_loser_10[slice39],\r\n",
        "       train_regret_loser_11[slice39],\r\n",
        "       train_regret_loser_12[slice39],\r\n",
        "       train_regret_loser_13[slice39],\r\n",
        "       train_regret_loser_14[slice39],\r\n",
        "       train_regret_loser_15[slice39],\r\n",
        "       train_regret_loser_16[slice39],\r\n",
        "       train_regret_loser_17[slice39],\r\n",
        "       train_regret_loser_18[slice39],\r\n",
        "       train_regret_loser_19[slice39],\r\n",
        "       train_regret_loser_20[slice39]]\r\n",
        "\r\n",
        "winner39 = [train_regret_winner_1[slice39],\r\n",
        "       train_regret_winner_2[slice39],\r\n",
        "       train_regret_winner_3[slice39],\r\n",
        "       train_regret_winner_4[slice39],\r\n",
        "       train_regret_winner_5[slice39],\r\n",
        "       train_regret_winner_6[slice39],\r\n",
        "       train_regret_winner_7[slice39],\r\n",
        "       train_regret_winner_8[slice39],\r\n",
        "       train_regret_winner_9[slice39],\r\n",
        "       train_regret_winner_10[slice39],\r\n",
        "       train_regret_winner_11[slice39],\r\n",
        "       train_regret_winner_12[slice39],\r\n",
        "       train_regret_winner_13[slice39],\r\n",
        "       train_regret_winner_14[slice39],\r\n",
        "       train_regret_winner_15[slice39],\r\n",
        "       train_regret_winner_16[slice39],\r\n",
        "       train_regret_winner_17[slice39],\r\n",
        "       train_regret_winner_18[slice39],\r\n",
        "       train_regret_winner_19[slice39],\r\n",
        "       train_regret_winner_20[slice39]]\r\n",
        "\r\n",
        "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\r\n",
        "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\r\n",
        "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\r\n",
        "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\r\n",
        "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\r\n",
        "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vb7vh2K1pXp"
      },
      "source": [
        "# Iteration49 :\r\n",
        "\r\n",
        "slice49 = 48\r\n",
        "\r\n",
        "loser49 = [train_regret_loser_1[slice49],\r\n",
        "       train_regret_loser_2[slice49],\r\n",
        "       train_regret_loser_3[slice49],\r\n",
        "       train_regret_loser_4[slice49],\r\n",
        "       train_regret_loser_5[slice49],\r\n",
        "       train_regret_loser_6[slice49],\r\n",
        "       train_regret_loser_7[slice49],\r\n",
        "       train_regret_loser_8[slice49],\r\n",
        "       train_regret_loser_9[slice49],\r\n",
        "       train_regret_loser_10[slice49],\r\n",
        "       train_regret_loser_11[slice49],\r\n",
        "       train_regret_loser_12[slice49],\r\n",
        "       train_regret_loser_13[slice49],\r\n",
        "       train_regret_loser_14[slice49],\r\n",
        "       train_regret_loser_15[slice49],\r\n",
        "       train_regret_loser_16[slice49],\r\n",
        "       train_regret_loser_17[slice49],\r\n",
        "       train_regret_loser_18[slice49],\r\n",
        "       train_regret_loser_19[slice49],\r\n",
        "       train_regret_loser_20[slice49]]\r\n",
        "\r\n",
        "winner49 = [train_regret_winner_1[slice49],\r\n",
        "       train_regret_winner_2[slice49],\r\n",
        "       train_regret_winner_3[slice49],\r\n",
        "       train_regret_winner_4[slice49],\r\n",
        "       train_regret_winner_5[slice49],\r\n",
        "       train_regret_winner_6[slice49],\r\n",
        "       train_regret_winner_7[slice49],\r\n",
        "       train_regret_winner_8[slice49],\r\n",
        "       train_regret_winner_9[slice49],\r\n",
        "       train_regret_winner_10[slice49],\r\n",
        "       train_regret_winner_11[slice49],\r\n",
        "       train_regret_winner_12[slice49],\r\n",
        "       train_regret_winner_13[slice49],\r\n",
        "       train_regret_winner_14[slice49],\r\n",
        "       train_regret_winner_15[slice49],\r\n",
        "       train_regret_winner_16[slice49],\r\n",
        "       train_regret_winner_17[slice49],\r\n",
        "       train_regret_winner_18[slice49],\r\n",
        "       train_regret_winner_19[slice49],\r\n",
        "       train_regret_winner_20[slice49]]\r\n",
        "\r\n",
        "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\r\n",
        "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\r\n",
        "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\r\n",
        "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\r\n",
        "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\r\n",
        "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wN7DWwt1paM"
      },
      "source": [
        "# Iteration59 :\r\n",
        "\r\n",
        "slice59 = 58\r\n",
        "\r\n",
        "loser59 = [train_regret_loser_1[slice59],\r\n",
        "       train_regret_loser_2[slice59],\r\n",
        "       train_regret_loser_3[slice59],\r\n",
        "       train_regret_loser_4[slice59],\r\n",
        "       train_regret_loser_5[slice59],\r\n",
        "       train_regret_loser_6[slice59],\r\n",
        "       train_regret_loser_7[slice59],\r\n",
        "       train_regret_loser_8[slice59],\r\n",
        "       train_regret_loser_9[slice59],\r\n",
        "       train_regret_loser_10[slice59],\r\n",
        "       train_regret_loser_11[slice59],\r\n",
        "       train_regret_loser_12[slice59],\r\n",
        "       train_regret_loser_13[slice59],\r\n",
        "       train_regret_loser_14[slice59],\r\n",
        "       train_regret_loser_15[slice59],\r\n",
        "       train_regret_loser_16[slice59],\r\n",
        "       train_regret_loser_17[slice59],\r\n",
        "       train_regret_loser_18[slice59],\r\n",
        "       train_regret_loser_19[slice59],\r\n",
        "       train_regret_loser_20[slice59]]\r\n",
        "\r\n",
        "winner59 = [train_regret_winner_1[slice59],\r\n",
        "       train_regret_winner_2[slice59],\r\n",
        "       train_regret_winner_3[slice59],\r\n",
        "       train_regret_winner_4[slice59],\r\n",
        "       train_regret_winner_5[slice59],\r\n",
        "       train_regret_winner_6[slice59],\r\n",
        "       train_regret_winner_7[slice59],\r\n",
        "       train_regret_winner_8[slice59],\r\n",
        "       train_regret_winner_9[slice59],\r\n",
        "       train_regret_winner_10[slice59],\r\n",
        "       train_regret_winner_11[slice59],\r\n",
        "       train_regret_winner_12[slice59],\r\n",
        "       train_regret_winner_13[slice59],\r\n",
        "       train_regret_winner_14[slice59],\r\n",
        "       train_regret_winner_15[slice59],\r\n",
        "       train_regret_winner_16[slice59],\r\n",
        "       train_regret_winner_17[slice59],\r\n",
        "       train_regret_winner_18[slice59],\r\n",
        "       train_regret_winner_19[slice59],\r\n",
        "       train_regret_winner_20[slice59]]\r\n",
        "\r\n",
        "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\r\n",
        "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\r\n",
        "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\r\n",
        "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\r\n",
        "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\r\n",
        "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqtgekml1pde"
      },
      "source": [
        "# Iteration69 :\r\n",
        "\r\n",
        "slice69 = 68\r\n",
        "\r\n",
        "loser69 = [train_regret_loser_1[slice69],\r\n",
        "       train_regret_loser_2[slice69],\r\n",
        "       train_regret_loser_3[slice69],\r\n",
        "       train_regret_loser_4[slice69],\r\n",
        "       train_regret_loser_5[slice69],\r\n",
        "       train_regret_loser_6[slice69],\r\n",
        "       train_regret_loser_7[slice69],\r\n",
        "       train_regret_loser_8[slice69],\r\n",
        "       train_regret_loser_9[slice69],\r\n",
        "       train_regret_loser_10[slice69],\r\n",
        "       train_regret_loser_11[slice69],\r\n",
        "       train_regret_loser_12[slice69],\r\n",
        "       train_regret_loser_13[slice69],\r\n",
        "       train_regret_loser_14[slice69],\r\n",
        "       train_regret_loser_15[slice69],\r\n",
        "       train_regret_loser_16[slice69],\r\n",
        "       train_regret_loser_17[slice69],\r\n",
        "       train_regret_loser_18[slice69],\r\n",
        "       train_regret_loser_19[slice69],\r\n",
        "       train_regret_loser_20[slice69]]\r\n",
        "\r\n",
        "winner69 = [train_regret_winner_1[slice69],\r\n",
        "       train_regret_winner_2[slice69],\r\n",
        "       train_regret_winner_3[slice69],\r\n",
        "       train_regret_winner_4[slice69],\r\n",
        "       train_regret_winner_5[slice69],\r\n",
        "       train_regret_winner_6[slice69],\r\n",
        "       train_regret_winner_7[slice69],\r\n",
        "       train_regret_winner_8[slice69],\r\n",
        "       train_regret_winner_9[slice69],\r\n",
        "       train_regret_winner_10[slice69],\r\n",
        "       train_regret_winner_11[slice69],\r\n",
        "       train_regret_winner_12[slice69],\r\n",
        "       train_regret_winner_13[slice69],\r\n",
        "       train_regret_winner_14[slice69],\r\n",
        "       train_regret_winner_15[slice69],\r\n",
        "       train_regret_winner_16[slice69],\r\n",
        "       train_regret_winner_17[slice69],\r\n",
        "       train_regret_winner_18[slice69],\r\n",
        "       train_regret_winner_19[slice69],\r\n",
        "       train_regret_winner_20[slice69]]\r\n",
        "\r\n",
        "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\r\n",
        "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\r\n",
        "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\r\n",
        "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\r\n",
        "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\r\n",
        "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPFTa1171pgF"
      },
      "source": [
        "# Iteration79 :\r\n",
        "\r\n",
        "slice79 = 78\r\n",
        "\r\n",
        "loser79 = [train_regret_loser_1[slice79],\r\n",
        "       train_regret_loser_2[slice79],\r\n",
        "       train_regret_loser_3[slice79],\r\n",
        "       train_regret_loser_4[slice79],\r\n",
        "       train_regret_loser_5[slice79],\r\n",
        "       train_regret_loser_6[slice79],\r\n",
        "       train_regret_loser_7[slice79],\r\n",
        "       train_regret_loser_8[slice79],\r\n",
        "       train_regret_loser_9[slice79],\r\n",
        "       train_regret_loser_10[slice79],\r\n",
        "       train_regret_loser_11[slice79],\r\n",
        "       train_regret_loser_12[slice79],\r\n",
        "       train_regret_loser_13[slice79],\r\n",
        "       train_regret_loser_14[slice79],\r\n",
        "       train_regret_loser_15[slice79],\r\n",
        "       train_regret_loser_16[slice79],\r\n",
        "       train_regret_loser_17[slice79],\r\n",
        "       train_regret_loser_18[slice79],\r\n",
        "       train_regret_loser_19[slice79],\r\n",
        "       train_regret_loser_20[slice79]]\r\n",
        "\r\n",
        "winner79 = [train_regret_winner_1[slice79],\r\n",
        "       train_regret_winner_2[slice79],\r\n",
        "       train_regret_winner_3[slice79],\r\n",
        "       train_regret_winner_4[slice79],\r\n",
        "       train_regret_winner_5[slice79],\r\n",
        "       train_regret_winner_6[slice79],\r\n",
        "       train_regret_winner_7[slice79],\r\n",
        "       train_regret_winner_8[slice79],\r\n",
        "       train_regret_winner_9[slice79],\r\n",
        "       train_regret_winner_10[slice79],\r\n",
        "       train_regret_winner_11[slice79],\r\n",
        "       train_regret_winner_12[slice79],\r\n",
        "       train_regret_winner_13[slice79],\r\n",
        "       train_regret_winner_14[slice79],\r\n",
        "       train_regret_winner_15[slice79],\r\n",
        "       train_regret_winner_16[slice79],\r\n",
        "       train_regret_winner_17[slice79],\r\n",
        "       train_regret_winner_18[slice79],\r\n",
        "       train_regret_winner_19[slice79],\r\n",
        "       train_regret_winner_20[slice79]]\r\n",
        "\r\n",
        "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\r\n",
        "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\r\n",
        "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\r\n",
        "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\r\n",
        "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\r\n",
        "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTnRF4lx1pio"
      },
      "source": [
        "# Iteration89 :\r\n",
        "\r\n",
        "slice89 = 88\r\n",
        "\r\n",
        "loser89 = [train_regret_loser_1[slice89],\r\n",
        "       train_regret_loser_2[slice89],\r\n",
        "       train_regret_loser_3[slice89],\r\n",
        "       train_regret_loser_4[slice89],\r\n",
        "       train_regret_loser_5[slice89],\r\n",
        "       train_regret_loser_6[slice89],\r\n",
        "       train_regret_loser_7[slice89],\r\n",
        "       train_regret_loser_8[slice89],\r\n",
        "       train_regret_loser_9[slice89],\r\n",
        "       train_regret_loser_10[slice89],\r\n",
        "       train_regret_loser_11[slice89],\r\n",
        "       train_regret_loser_12[slice89],\r\n",
        "       train_regret_loser_13[slice89],\r\n",
        "       train_regret_loser_14[slice89],\r\n",
        "       train_regret_loser_15[slice89],\r\n",
        "       train_regret_loser_16[slice89],\r\n",
        "       train_regret_loser_17[slice89],\r\n",
        "       train_regret_loser_18[slice89],\r\n",
        "       train_regret_loser_19[slice89],\r\n",
        "       train_regret_loser_20[slice89]]\r\n",
        "\r\n",
        "winner89 = [train_regret_winner_1[slice89],\r\n",
        "       train_regret_winner_2[slice89],\r\n",
        "       train_regret_winner_3[slice89],\r\n",
        "       train_regret_winner_4[slice89],\r\n",
        "       train_regret_winner_5[slice89],\r\n",
        "       train_regret_winner_6[slice89],\r\n",
        "       train_regret_winner_7[slice89],\r\n",
        "       train_regret_winner_8[slice89],\r\n",
        "       train_regret_winner_9[slice89],\r\n",
        "       train_regret_winner_10[slice89],\r\n",
        "       train_regret_winner_11[slice89],\r\n",
        "       train_regret_winner_12[slice89],\r\n",
        "       train_regret_winner_13[slice89],\r\n",
        "       train_regret_winner_14[slice89],\r\n",
        "       train_regret_winner_15[slice89],\r\n",
        "       train_regret_winner_16[slice89],\r\n",
        "       train_regret_winner_17[slice89],\r\n",
        "       train_regret_winner_18[slice89],\r\n",
        "       train_regret_winner_19[slice89],\r\n",
        "       train_regret_winner_20[slice89]]\r\n",
        "\r\n",
        "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\r\n",
        "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\r\n",
        "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\r\n",
        "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\r\n",
        "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\r\n",
        "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiLAF38i1plO"
      },
      "source": [
        "# Iteration99 :\r\n",
        "\r\n",
        "slice99 = 98\r\n",
        "\r\n",
        "loser99 = [train_regret_loser_1[slice99],\r\n",
        "       train_regret_loser_2[slice99],\r\n",
        "       train_regret_loser_3[slice99],\r\n",
        "       train_regret_loser_4[slice99],\r\n",
        "       train_regret_loser_5[slice99],\r\n",
        "       train_regret_loser_6[slice99],\r\n",
        "       train_regret_loser_7[slice99],\r\n",
        "       train_regret_loser_8[slice99],\r\n",
        "       train_regret_loser_9[slice99],\r\n",
        "       train_regret_loser_10[slice99],\r\n",
        "       train_regret_loser_11[slice99],\r\n",
        "       train_regret_loser_12[slice99],\r\n",
        "       train_regret_loser_13[slice99],\r\n",
        "       train_regret_loser_14[slice99],\r\n",
        "       train_regret_loser_15[slice99],\r\n",
        "       train_regret_loser_16[slice99],\r\n",
        "       train_regret_loser_17[slice99],\r\n",
        "       train_regret_loser_18[slice99],\r\n",
        "       train_regret_loser_19[slice99],\r\n",
        "       train_regret_loser_20[slice99]]\r\n",
        "\r\n",
        "winner99 = [train_regret_winner_1[slice99],\r\n",
        "       train_regret_winner_2[slice99],\r\n",
        "       train_regret_winner_3[slice99],\r\n",
        "       train_regret_winner_4[slice99],\r\n",
        "       train_regret_winner_5[slice99],\r\n",
        "       train_regret_winner_6[slice99],\r\n",
        "       train_regret_winner_7[slice99],\r\n",
        "       train_regret_winner_8[slice99],\r\n",
        "       train_regret_winner_9[slice99],\r\n",
        "       train_regret_winner_10[slice99],\r\n",
        "       train_regret_winner_11[slice99],\r\n",
        "       train_regret_winner_12[slice99],\r\n",
        "       train_regret_winner_13[slice99],\r\n",
        "       train_regret_winner_14[slice99],\r\n",
        "       train_regret_winner_15[slice99],\r\n",
        "       train_regret_winner_16[slice99],\r\n",
        "       train_regret_winner_17[slice99],\r\n",
        "       train_regret_winner_18[slice99],\r\n",
        "       train_regret_winner_19[slice99],\r\n",
        "       train_regret_winner_20[slice99]]\r\n",
        "\r\n",
        "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\r\n",
        "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\r\n",
        "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\r\n",
        "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\r\n",
        "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\r\n",
        "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBd_N0F21pn0"
      },
      "source": [
        "# Iteration10 :\r\n",
        "\r\n",
        "slice10 = 9\r\n",
        "\r\n",
        "loser10 = [train_regret_loser_1[slice10],\r\n",
        "       train_regret_loser_2[slice10],\r\n",
        "       train_regret_loser_3[slice10],\r\n",
        "       train_regret_loser_4[slice10],\r\n",
        "       train_regret_loser_5[slice10],\r\n",
        "       train_regret_loser_6[slice10],\r\n",
        "       train_regret_loser_7[slice10],\r\n",
        "       train_regret_loser_8[slice10],\r\n",
        "       train_regret_loser_9[slice10],\r\n",
        "       train_regret_loser_10[slice10],\r\n",
        "       train_regret_loser_11[slice10],\r\n",
        "       train_regret_loser_12[slice10],\r\n",
        "       train_regret_loser_13[slice10],\r\n",
        "       train_regret_loser_14[slice10],\r\n",
        "       train_regret_loser_15[slice10],\r\n",
        "       train_regret_loser_16[slice10],\r\n",
        "       train_regret_loser_17[slice10],\r\n",
        "       train_regret_loser_18[slice10],\r\n",
        "       train_regret_loser_19[slice10],\r\n",
        "       train_regret_loser_20[slice10]]\r\n",
        "\r\n",
        "winner10 = [train_regret_winner_1[slice10],\r\n",
        "       train_regret_winner_2[slice10],\r\n",
        "       train_regret_winner_3[slice10],\r\n",
        "       train_regret_winner_4[slice10],\r\n",
        "       train_regret_winner_5[slice10],\r\n",
        "       train_regret_winner_6[slice10],\r\n",
        "       train_regret_winner_7[slice10],\r\n",
        "       train_regret_winner_8[slice10],\r\n",
        "       train_regret_winner_9[slice10],\r\n",
        "       train_regret_winner_10[slice10],\r\n",
        "       train_regret_winner_11[slice10],\r\n",
        "       train_regret_winner_12[slice10],\r\n",
        "       train_regret_winner_13[slice10],\r\n",
        "       train_regret_winner_14[slice10],\r\n",
        "       train_regret_winner_15[slice10],\r\n",
        "       train_regret_winner_16[slice10],\r\n",
        "       train_regret_winner_17[slice10],\r\n",
        "       train_regret_winner_18[slice10],\r\n",
        "       train_regret_winner_19[slice10],\r\n",
        "       train_regret_winner_20[slice10]]\r\n",
        "\r\n",
        "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\r\n",
        "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\r\n",
        "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\r\n",
        "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\r\n",
        "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\r\n",
        "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXjdqc4s1pqb"
      },
      "source": [
        "# Iteration20 :\r\n",
        "\r\n",
        "slice20 = 19\r\n",
        "\r\n",
        "loser20 = [train_regret_loser_1[slice20],\r\n",
        "       train_regret_loser_2[slice20],\r\n",
        "       train_regret_loser_3[slice20],\r\n",
        "       train_regret_loser_4[slice20],\r\n",
        "       train_regret_loser_5[slice20],\r\n",
        "       train_regret_loser_6[slice20],\r\n",
        "       train_regret_loser_7[slice20],\r\n",
        "       train_regret_loser_8[slice20],\r\n",
        "       train_regret_loser_9[slice20],\r\n",
        "       train_regret_loser_10[slice20],\r\n",
        "       train_regret_loser_11[slice20],\r\n",
        "       train_regret_loser_12[slice20],\r\n",
        "       train_regret_loser_13[slice20],\r\n",
        "       train_regret_loser_14[slice20],\r\n",
        "       train_regret_loser_15[slice20],\r\n",
        "       train_regret_loser_16[slice20],\r\n",
        "       train_regret_loser_17[slice20],\r\n",
        "       train_regret_loser_18[slice20],\r\n",
        "       train_regret_loser_19[slice20],\r\n",
        "       train_regret_loser_20[slice20]]\r\n",
        "\r\n",
        "winner20 = [train_regret_winner_1[slice20],\r\n",
        "       train_regret_winner_2[slice20],\r\n",
        "       train_regret_winner_3[slice20],\r\n",
        "       train_regret_winner_4[slice20],\r\n",
        "       train_regret_winner_5[slice20],\r\n",
        "       train_regret_winner_6[slice20],\r\n",
        "       train_regret_winner_7[slice20],\r\n",
        "       train_regret_winner_8[slice20],\r\n",
        "       train_regret_winner_9[slice20],\r\n",
        "       train_regret_winner_10[slice20],\r\n",
        "       train_regret_winner_11[slice20],\r\n",
        "       train_regret_winner_12[slice20],\r\n",
        "       train_regret_winner_13[slice20],\r\n",
        "       train_regret_winner_14[slice20],\r\n",
        "       train_regret_winner_15[slice20],\r\n",
        "       train_regret_winner_16[slice20],\r\n",
        "       train_regret_winner_17[slice20],\r\n",
        "       train_regret_winner_18[slice20],\r\n",
        "       train_regret_winner_19[slice20],\r\n",
        "       train_regret_winner_20[slice20]]\r\n",
        "\r\n",
        "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\r\n",
        "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\r\n",
        "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\r\n",
        "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\r\n",
        "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\r\n",
        "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQBVS6dx1ptB"
      },
      "source": [
        "# Iteration30 :\r\n",
        "\r\n",
        "slice30 = 29\r\n",
        "\r\n",
        "loser30 = [train_regret_loser_1[slice30],\r\n",
        "       train_regret_loser_2[slice30],\r\n",
        "       train_regret_loser_3[slice30],\r\n",
        "       train_regret_loser_4[slice30],\r\n",
        "       train_regret_loser_5[slice30],\r\n",
        "       train_regret_loser_6[slice30],\r\n",
        "       train_regret_loser_7[slice30],\r\n",
        "       train_regret_loser_8[slice30],\r\n",
        "       train_regret_loser_9[slice30],\r\n",
        "       train_regret_loser_10[slice30],\r\n",
        "       train_regret_loser_11[slice30],\r\n",
        "       train_regret_loser_12[slice30],\r\n",
        "       train_regret_loser_13[slice30],\r\n",
        "       train_regret_loser_14[slice30],\r\n",
        "       train_regret_loser_15[slice30],\r\n",
        "       train_regret_loser_16[slice30],\r\n",
        "       train_regret_loser_17[slice30],\r\n",
        "       train_regret_loser_18[slice30],\r\n",
        "       train_regret_loser_19[slice30],\r\n",
        "       train_regret_loser_20[slice30]]\r\n",
        "\r\n",
        "winner30 = [train_regret_winner_1[slice30],\r\n",
        "       train_regret_winner_2[slice30],\r\n",
        "       train_regret_winner_3[slice30],\r\n",
        "       train_regret_winner_4[slice30],\r\n",
        "       train_regret_winner_5[slice30],\r\n",
        "       train_regret_winner_6[slice30],\r\n",
        "       train_regret_winner_7[slice30],\r\n",
        "       train_regret_winner_8[slice30],\r\n",
        "       train_regret_winner_9[slice30],\r\n",
        "       train_regret_winner_10[slice30],\r\n",
        "       train_regret_winner_11[slice30],\r\n",
        "       train_regret_winner_12[slice30],\r\n",
        "       train_regret_winner_13[slice30],\r\n",
        "       train_regret_winner_14[slice30],\r\n",
        "       train_regret_winner_15[slice30],\r\n",
        "       train_regret_winner_16[slice30],\r\n",
        "       train_regret_winner_17[slice30],\r\n",
        "       train_regret_winner_18[slice30],\r\n",
        "       train_regret_winner_19[slice30],\r\n",
        "       train_regret_winner_20[slice30]]\r\n",
        "\r\n",
        "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\r\n",
        "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\r\n",
        "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\r\n",
        "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\r\n",
        "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\r\n",
        "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-iWfLkj1pvn"
      },
      "source": [
        "# Iteration40 :\r\n",
        "\r\n",
        "slice40 = 39\r\n",
        "\r\n",
        "loser40 = [train_regret_loser_1[slice40],\r\n",
        "       train_regret_loser_2[slice40],\r\n",
        "       train_regret_loser_3[slice40],\r\n",
        "       train_regret_loser_4[slice40],\r\n",
        "       train_regret_loser_5[slice40],\r\n",
        "       train_regret_loser_6[slice40],\r\n",
        "       train_regret_loser_7[slice40],\r\n",
        "       train_regret_loser_8[slice40],\r\n",
        "       train_regret_loser_9[slice40],\r\n",
        "       train_regret_loser_10[slice40],\r\n",
        "       train_regret_loser_11[slice40],\r\n",
        "       train_regret_loser_12[slice40],\r\n",
        "       train_regret_loser_13[slice40],\r\n",
        "       train_regret_loser_14[slice40],\r\n",
        "       train_regret_loser_15[slice40],\r\n",
        "       train_regret_loser_16[slice40],\r\n",
        "       train_regret_loser_17[slice40],\r\n",
        "       train_regret_loser_18[slice40],\r\n",
        "       train_regret_loser_19[slice40],\r\n",
        "       train_regret_loser_20[slice40]]\r\n",
        "\r\n",
        "winner40 = [train_regret_winner_1[slice40],\r\n",
        "       train_regret_winner_2[slice40],\r\n",
        "       train_regret_winner_3[slice40],\r\n",
        "       train_regret_winner_4[slice40],\r\n",
        "       train_regret_winner_5[slice40],\r\n",
        "       train_regret_winner_6[slice40],\r\n",
        "       train_regret_winner_7[slice40],\r\n",
        "       train_regret_winner_8[slice40],\r\n",
        "       train_regret_winner_9[slice40],\r\n",
        "       train_regret_winner_10[slice40],\r\n",
        "       train_regret_winner_11[slice40],\r\n",
        "       train_regret_winner_12[slice40],\r\n",
        "       train_regret_winner_13[slice40],\r\n",
        "       train_regret_winner_14[slice40],\r\n",
        "       train_regret_winner_15[slice40],\r\n",
        "       train_regret_winner_16[slice40],\r\n",
        "       train_regret_winner_17[slice40],\r\n",
        "       train_regret_winner_18[slice40],\r\n",
        "       train_regret_winner_19[slice40],\r\n",
        "       train_regret_winner_20[slice40]]\r\n",
        "\r\n",
        "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\r\n",
        "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\r\n",
        "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\r\n",
        "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\r\n",
        "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\r\n",
        "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G5i-Cjw2U7K"
      },
      "source": [
        "# Iteration50 :\r\n",
        "\r\n",
        "slice50 = 49\r\n",
        "\r\n",
        "loser50 = [train_regret_loser_1[slice50],\r\n",
        "       train_regret_loser_2[slice50],\r\n",
        "       train_regret_loser_3[slice50],\r\n",
        "       train_regret_loser_4[slice50],\r\n",
        "       train_regret_loser_5[slice50],\r\n",
        "       train_regret_loser_6[slice50],\r\n",
        "       train_regret_loser_7[slice50],\r\n",
        "       train_regret_loser_8[slice50],\r\n",
        "       train_regret_loser_9[slice50],\r\n",
        "       train_regret_loser_10[slice50],\r\n",
        "       train_regret_loser_11[slice50],\r\n",
        "       train_regret_loser_12[slice50],\r\n",
        "       train_regret_loser_13[slice50],\r\n",
        "       train_regret_loser_14[slice50],\r\n",
        "       train_regret_loser_15[slice50],\r\n",
        "       train_regret_loser_16[slice50],\r\n",
        "       train_regret_loser_17[slice50],\r\n",
        "       train_regret_loser_18[slice50],\r\n",
        "       train_regret_loser_19[slice50],\r\n",
        "       train_regret_loser_20[slice50]]\r\n",
        "\r\n",
        "winner50 = [train_regret_winner_1[slice50],\r\n",
        "       train_regret_winner_2[slice50],\r\n",
        "       train_regret_winner_3[slice50],\r\n",
        "       train_regret_winner_4[slice50],\r\n",
        "       train_regret_winner_5[slice50],\r\n",
        "       train_regret_winner_6[slice50],\r\n",
        "       train_regret_winner_7[slice50],\r\n",
        "       train_regret_winner_8[slice50],\r\n",
        "       train_regret_winner_9[slice50],\r\n",
        "       train_regret_winner_10[slice50],\r\n",
        "       train_regret_winner_11[slice50],\r\n",
        "       train_regret_winner_12[slice50],\r\n",
        "       train_regret_winner_13[slice50],\r\n",
        "       train_regret_winner_14[slice50],\r\n",
        "       train_regret_winner_15[slice50],\r\n",
        "       train_regret_winner_16[slice50],\r\n",
        "       train_regret_winner_17[slice50],\r\n",
        "       train_regret_winner_18[slice50],\r\n",
        "       train_regret_winner_19[slice50],\r\n",
        "       train_regret_winner_20[slice50]]\r\n",
        "\r\n",
        "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\r\n",
        "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\r\n",
        "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\r\n",
        "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\r\n",
        "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\r\n",
        "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY6qxZQA2U-r"
      },
      "source": [
        "# Iteration60 :\r\n",
        "\r\n",
        "slice60 = 59\r\n",
        "\r\n",
        "loser60 = [train_regret_loser_1[slice60],\r\n",
        "       train_regret_loser_2[slice60],\r\n",
        "       train_regret_loser_3[slice60],\r\n",
        "       train_regret_loser_4[slice60],\r\n",
        "       train_regret_loser_5[slice60],\r\n",
        "       train_regret_loser_6[slice60],\r\n",
        "       train_regret_loser_7[slice60],\r\n",
        "       train_regret_loser_8[slice60],\r\n",
        "       train_regret_loser_9[slice60],\r\n",
        "       train_regret_loser_10[slice60],\r\n",
        "       train_regret_loser_11[slice60],\r\n",
        "       train_regret_loser_12[slice60],\r\n",
        "       train_regret_loser_13[slice60],\r\n",
        "       train_regret_loser_14[slice60],\r\n",
        "       train_regret_loser_15[slice60],\r\n",
        "       train_regret_loser_16[slice60],\r\n",
        "       train_regret_loser_17[slice60],\r\n",
        "       train_regret_loser_18[slice60],\r\n",
        "       train_regret_loser_19[slice60],\r\n",
        "       train_regret_loser_20[slice60]]\r\n",
        "\r\n",
        "winner60 = [train_regret_winner_1[slice60],\r\n",
        "       train_regret_winner_2[slice60],\r\n",
        "       train_regret_winner_3[slice60],\r\n",
        "       train_regret_winner_4[slice60],\r\n",
        "       train_regret_winner_5[slice60],\r\n",
        "       train_regret_winner_6[slice60],\r\n",
        "       train_regret_winner_7[slice60],\r\n",
        "       train_regret_winner_8[slice60],\r\n",
        "       train_regret_winner_9[slice60],\r\n",
        "       train_regret_winner_10[slice60],\r\n",
        "       train_regret_winner_11[slice60],\r\n",
        "       train_regret_winner_12[slice60],\r\n",
        "       train_regret_winner_13[slice60],\r\n",
        "       train_regret_winner_14[slice60],\r\n",
        "       train_regret_winner_15[slice60],\r\n",
        "       train_regret_winner_16[slice60],\r\n",
        "       train_regret_winner_17[slice60],\r\n",
        "       train_regret_winner_18[slice60],\r\n",
        "       train_regret_winner_19[slice60],\r\n",
        "       train_regret_winner_20[slice60]]\r\n",
        "\r\n",
        "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\r\n",
        "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\r\n",
        "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\r\n",
        "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\r\n",
        "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\r\n",
        "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqgVxVH2VCU"
      },
      "source": [
        "# Iteration70 :\r\n",
        "\r\n",
        "slice70 = 69\r\n",
        "\r\n",
        "loser70 = [train_regret_loser_1[slice70],\r\n",
        "       train_regret_loser_2[slice70],\r\n",
        "       train_regret_loser_3[slice70],\r\n",
        "       train_regret_loser_4[slice70],\r\n",
        "       train_regret_loser_5[slice70],\r\n",
        "       train_regret_loser_6[slice70],\r\n",
        "       train_regret_loser_7[slice70],\r\n",
        "       train_regret_loser_8[slice70],\r\n",
        "       train_regret_loser_9[slice70],\r\n",
        "       train_regret_loser_10[slice70],\r\n",
        "       train_regret_loser_11[slice70],\r\n",
        "       train_regret_loser_12[slice70],\r\n",
        "       train_regret_loser_13[slice70],\r\n",
        "       train_regret_loser_14[slice70],\r\n",
        "       train_regret_loser_15[slice70],\r\n",
        "       train_regret_loser_16[slice70],\r\n",
        "       train_regret_loser_17[slice70],\r\n",
        "       train_regret_loser_18[slice70],\r\n",
        "       train_regret_loser_19[slice70],\r\n",
        "       train_regret_loser_20[slice70]]\r\n",
        "\r\n",
        "winner70 = [train_regret_winner_1[slice70],\r\n",
        "       train_regret_winner_2[slice70],\r\n",
        "       train_regret_winner_3[slice70],\r\n",
        "       train_regret_winner_4[slice70],\r\n",
        "       train_regret_winner_5[slice70],\r\n",
        "       train_regret_winner_6[slice70],\r\n",
        "       train_regret_winner_7[slice70],\r\n",
        "       train_regret_winner_8[slice70],\r\n",
        "       train_regret_winner_9[slice70],\r\n",
        "       train_regret_winner_10[slice70],\r\n",
        "       train_regret_winner_11[slice70],\r\n",
        "       train_regret_winner_12[slice70],\r\n",
        "       train_regret_winner_13[slice70],\r\n",
        "       train_regret_winner_14[slice70],\r\n",
        "       train_regret_winner_15[slice70],\r\n",
        "       train_regret_winner_16[slice70],\r\n",
        "       train_regret_winner_17[slice70],\r\n",
        "       train_regret_winner_18[slice70],\r\n",
        "       train_regret_winner_19[slice70],\r\n",
        "       train_regret_winner_20[slice70]]\r\n",
        "\r\n",
        "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\r\n",
        "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\r\n",
        "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\r\n",
        "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\r\n",
        "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\r\n",
        "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF7mlDZL1pyN"
      },
      "source": [
        "# Iteration80 :\r\n",
        "\r\n",
        "slice80 = 79\r\n",
        "\r\n",
        "loser80 = [train_regret_loser_1[slice80],\r\n",
        "       train_regret_loser_2[slice80],\r\n",
        "       train_regret_loser_3[slice80],\r\n",
        "       train_regret_loser_4[slice80],\r\n",
        "       train_regret_loser_5[slice80],\r\n",
        "       train_regret_loser_6[slice80],\r\n",
        "       train_regret_loser_7[slice80],\r\n",
        "       train_regret_loser_8[slice80],\r\n",
        "       train_regret_loser_9[slice80],\r\n",
        "       train_regret_loser_10[slice80],\r\n",
        "       train_regret_loser_11[slice80],\r\n",
        "       train_regret_loser_12[slice80],\r\n",
        "       train_regret_loser_13[slice80],\r\n",
        "       train_regret_loser_14[slice80],\r\n",
        "       train_regret_loser_15[slice80],\r\n",
        "       train_regret_loser_16[slice80],\r\n",
        "       train_regret_loser_17[slice80],\r\n",
        "       train_regret_loser_18[slice80],\r\n",
        "       train_regret_loser_19[slice80],\r\n",
        "       train_regret_loser_20[slice80]]\r\n",
        "\r\n",
        "winner80 = [train_regret_winner_1[slice80],\r\n",
        "       train_regret_winner_2[slice80],\r\n",
        "       train_regret_winner_3[slice80],\r\n",
        "       train_regret_winner_4[slice80],\r\n",
        "       train_regret_winner_5[slice80],\r\n",
        "       train_regret_winner_6[slice80],\r\n",
        "       train_regret_winner_7[slice80],\r\n",
        "       train_regret_winner_8[slice80],\r\n",
        "       train_regret_winner_9[slice80],\r\n",
        "       train_regret_winner_10[slice80],\r\n",
        "       train_regret_winner_11[slice80],\r\n",
        "       train_regret_winner_12[slice80],\r\n",
        "       train_regret_winner_13[slice80],\r\n",
        "       train_regret_winner_14[slice80],\r\n",
        "       train_regret_winner_15[slice80],\r\n",
        "       train_regret_winner_16[slice80],\r\n",
        "       train_regret_winner_17[slice80],\r\n",
        "       train_regret_winner_18[slice80],\r\n",
        "       train_regret_winner_19[slice80],\r\n",
        "       train_regret_winner_20[slice80]]\r\n",
        "\r\n",
        "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\r\n",
        "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\r\n",
        "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\r\n",
        "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\r\n",
        "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\r\n",
        "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Rw8IkD1p0z"
      },
      "source": [
        "# Iteration90 :\r\n",
        "\r\n",
        "slice90 = 89\r\n",
        "\r\n",
        "loser90 = [train_regret_loser_1[slice90],\r\n",
        "       train_regret_loser_2[slice90],\r\n",
        "       train_regret_loser_3[slice90],\r\n",
        "       train_regret_loser_4[slice90],\r\n",
        "       train_regret_loser_5[slice90],\r\n",
        "       train_regret_loser_6[slice90],\r\n",
        "       train_regret_loser_7[slice90],\r\n",
        "       train_regret_loser_8[slice90],\r\n",
        "       train_regret_loser_9[slice90],\r\n",
        "       train_regret_loser_10[slice90],\r\n",
        "       train_regret_loser_11[slice90],\r\n",
        "       train_regret_loser_12[slice90],\r\n",
        "       train_regret_loser_13[slice90],\r\n",
        "       train_regret_loser_14[slice90],\r\n",
        "       train_regret_loser_15[slice90],\r\n",
        "       train_regret_loser_16[slice90],\r\n",
        "       train_regret_loser_17[slice90],\r\n",
        "       train_regret_loser_18[slice90],\r\n",
        "       train_regret_loser_19[slice90],\r\n",
        "       train_regret_loser_20[slice90]]\r\n",
        "\r\n",
        "winner90 = [train_regret_winner_1[slice90],\r\n",
        "       train_regret_winner_2[slice90],\r\n",
        "       train_regret_winner_3[slice90],\r\n",
        "       train_regret_winner_4[slice90],\r\n",
        "       train_regret_winner_5[slice90],\r\n",
        "       train_regret_winner_6[slice90],\r\n",
        "       train_regret_winner_7[slice90],\r\n",
        "       train_regret_winner_8[slice90],\r\n",
        "       train_regret_winner_9[slice90],\r\n",
        "       train_regret_winner_10[slice90],\r\n",
        "       train_regret_winner_11[slice90],\r\n",
        "       train_regret_winner_12[slice90],\r\n",
        "       train_regret_winner_13[slice90],\r\n",
        "       train_regret_winner_14[slice90],\r\n",
        "       train_regret_winner_15[slice90],\r\n",
        "       train_regret_winner_16[slice90],\r\n",
        "       train_regret_winner_17[slice90],\r\n",
        "       train_regret_winner_18[slice90],\r\n",
        "       train_regret_winner_19[slice90],\r\n",
        "       train_regret_winner_20[slice90]]\r\n",
        "\r\n",
        "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\r\n",
        "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\r\n",
        "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\r\n",
        "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\r\n",
        "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\r\n",
        "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiljCrq21p3a"
      },
      "source": [
        "# Iteration100 :\r\n",
        "\r\n",
        "slice100 = 99\r\n",
        "\r\n",
        "loser100 = [train_regret_loser_1[slice100],\r\n",
        "       train_regret_loser_2[slice100],\r\n",
        "       train_regret_loser_3[slice100],\r\n",
        "       train_regret_loser_4[slice100],\r\n",
        "       train_regret_loser_5[slice100],\r\n",
        "       train_regret_loser_6[slice100],\r\n",
        "       train_regret_loser_7[slice100],\r\n",
        "       train_regret_loser_8[slice100],\r\n",
        "       train_regret_loser_9[slice100],\r\n",
        "       train_regret_loser_10[slice100],\r\n",
        "       train_regret_loser_11[slice100],\r\n",
        "       train_regret_loser_12[slice100],\r\n",
        "       train_regret_loser_13[slice100],\r\n",
        "       train_regret_loser_14[slice100],\r\n",
        "       train_regret_loser_15[slice100],\r\n",
        "       train_regret_loser_16[slice100],\r\n",
        "       train_regret_loser_17[slice100],\r\n",
        "       train_regret_loser_18[slice100],\r\n",
        "       train_regret_loser_19[slice100],\r\n",
        "       train_regret_loser_20[slice100]]\r\n",
        "\r\n",
        "winner100 = [train_regret_winner_1[slice100],\r\n",
        "       train_regret_winner_2[slice100],\r\n",
        "       train_regret_winner_3[slice100],\r\n",
        "       train_regret_winner_4[slice100],\r\n",
        "       train_regret_winner_5[slice100],\r\n",
        "       train_regret_winner_6[slice100],\r\n",
        "       train_regret_winner_7[slice100],\r\n",
        "       train_regret_winner_8[slice100],\r\n",
        "       train_regret_winner_9[slice100],\r\n",
        "       train_regret_winner_10[slice100],\r\n",
        "       train_regret_winner_11[slice100],\r\n",
        "       train_regret_winner_12[slice100],\r\n",
        "       train_regret_winner_13[slice100],\r\n",
        "       train_regret_winner_14[slice100],\r\n",
        "       train_regret_winner_15[slice100],\r\n",
        "       train_regret_winner_16[slice100],\r\n",
        "       train_regret_winner_17[slice100],\r\n",
        "       train_regret_winner_18[slice100],\r\n",
        "       train_regret_winner_19[slice100],\r\n",
        "       train_regret_winner_20[slice100]]\r\n",
        "\r\n",
        "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\r\n",
        "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\r\n",
        "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\r\n",
        "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\r\n",
        "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\r\n",
        "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OItJgqUT2oNb"
      },
      "source": [
        "### Summarize arrays: 'Loser'\r\n",
        "\r\n",
        "lower_loser = [lower_loser1,\r\n",
        "            lower_loser2,\r\n",
        "            lower_loser3,\r\n",
        "            lower_loser4,\r\n",
        "            lower_loser5,\r\n",
        "            lower_loser6,\r\n",
        "            lower_loser7,\r\n",
        "            lower_loser8,\r\n",
        "            lower_loser9,\r\n",
        "            lower_loser10,\r\n",
        "            lower_loser11,\r\n",
        "            lower_loser12,\r\n",
        "            lower_loser13,\r\n",
        "            lower_loser14,\r\n",
        "            lower_loser15,\r\n",
        "            lower_loser16,\r\n",
        "            lower_loser17,\r\n",
        "            lower_loser18,\r\n",
        "            lower_loser19,\r\n",
        "            lower_loser20,\r\n",
        "            lower_loser21,\r\n",
        "            lower_loser22,\r\n",
        "            lower_loser23,\r\n",
        "            lower_loser24,\r\n",
        "            lower_loser25,\r\n",
        "            lower_loser26,\r\n",
        "            lower_loser27,\r\n",
        "            lower_loser28,\r\n",
        "            lower_loser29,\r\n",
        "            lower_loser30,\r\n",
        "            lower_loser31,\r\n",
        "            lower_loser32,\r\n",
        "            lower_loser33,\r\n",
        "            lower_loser34,\r\n",
        "            lower_loser35,\r\n",
        "            lower_loser36,\r\n",
        "            lower_loser37,\r\n",
        "            lower_loser38,\r\n",
        "            lower_loser39,\r\n",
        "            lower_loser40,\r\n",
        "            lower_loser41,\r\n",
        "            lower_loser42,\r\n",
        "            lower_loser43,\r\n",
        "            lower_loser44,\r\n",
        "            lower_loser45,\r\n",
        "            lower_loser46,\r\n",
        "            lower_loser47,\r\n",
        "            lower_loser48,\r\n",
        "            lower_loser49,\r\n",
        "            lower_loser50,\r\n",
        "            lower_loser51,\r\n",
        "            lower_loser52,\r\n",
        "            lower_loser53,\r\n",
        "            lower_loser54,\r\n",
        "            lower_loser55,\r\n",
        "            lower_loser56,\r\n",
        "            lower_loser57,\r\n",
        "            lower_loser58,\r\n",
        "            lower_loser59,\r\n",
        "            lower_loser60,\r\n",
        "            lower_loser61,\r\n",
        "            lower_loser62,\r\n",
        "            lower_loser63,\r\n",
        "            lower_loser64,\r\n",
        "            lower_loser65,\r\n",
        "            lower_loser66,\r\n",
        "            lower_loser67,\r\n",
        "            lower_loser68,\r\n",
        "            lower_loser69,\r\n",
        "            lower_loser70,\r\n",
        "            lower_loser71,\r\n",
        "            lower_loser72,\r\n",
        "            lower_loser73,\r\n",
        "            lower_loser74,\r\n",
        "            lower_loser75,\r\n",
        "            lower_loser76,\r\n",
        "            lower_loser77,\r\n",
        "            lower_loser78,\r\n",
        "            lower_loser79,\r\n",
        "            lower_loser80,\r\n",
        "            lower_loser81,\r\n",
        "            lower_loser82,\r\n",
        "            lower_loser83,\r\n",
        "            lower_loser84,\r\n",
        "            lower_loser85,\r\n",
        "            lower_loser86,\r\n",
        "            lower_loser87,\r\n",
        "            lower_loser88,\r\n",
        "            lower_loser89,\r\n",
        "            lower_loser90,\r\n",
        "            lower_loser91,\r\n",
        "            lower_loser92,\r\n",
        "            lower_loser93,\r\n",
        "            lower_loser94,\r\n",
        "            lower_loser95,\r\n",
        "            lower_loser96,\r\n",
        "            lower_loser97,\r\n",
        "            lower_loser98,\r\n",
        "            lower_loser99,\r\n",
        "            lower_loser100,\r\n",
        "            lower_loser101]\r\n",
        "\r\n",
        "median_loser = [median_loser1,\r\n",
        "            median_loser2,\r\n",
        "            median_loser3,\r\n",
        "            median_loser4,\r\n",
        "            median_loser5,\r\n",
        "            median_loser6,\r\n",
        "            median_loser7,\r\n",
        "            median_loser8,\r\n",
        "            median_loser9,\r\n",
        "            median_loser10,\r\n",
        "            median_loser11,\r\n",
        "            median_loser12,\r\n",
        "            median_loser13,\r\n",
        "            median_loser14,\r\n",
        "            median_loser15,\r\n",
        "            median_loser16,\r\n",
        "            median_loser17,\r\n",
        "            median_loser18,\r\n",
        "            median_loser19,\r\n",
        "            median_loser20,\r\n",
        "            median_loser21,\r\n",
        "            median_loser22,\r\n",
        "            median_loser23,\r\n",
        "            median_loser24,\r\n",
        "            median_loser25,\r\n",
        "            median_loser26,\r\n",
        "            median_loser27,\r\n",
        "            median_loser28,\r\n",
        "            median_loser29,\r\n",
        "            median_loser30,\r\n",
        "            median_loser31,\r\n",
        "            median_loser32,\r\n",
        "            median_loser33,\r\n",
        "            median_loser34,\r\n",
        "            median_loser35,\r\n",
        "            median_loser36,\r\n",
        "            median_loser37,\r\n",
        "            median_loser38,\r\n",
        "            median_loser39,\r\n",
        "            median_loser40,\r\n",
        "            median_loser41,\r\n",
        "            median_loser42,\r\n",
        "            median_loser43,\r\n",
        "            median_loser44,\r\n",
        "            median_loser45,\r\n",
        "            median_loser46,\r\n",
        "            median_loser47,\r\n",
        "            median_loser48,\r\n",
        "            median_loser49,\r\n",
        "            median_loser50,\r\n",
        "            median_loser51,\r\n",
        "            median_loser52,\r\n",
        "            median_loser53,\r\n",
        "            median_loser54,\r\n",
        "            median_loser55,\r\n",
        "            median_loser56,\r\n",
        "            median_loser57,\r\n",
        "            median_loser58,\r\n",
        "            median_loser59,\r\n",
        "            median_loser60,\r\n",
        "            median_loser61,\r\n",
        "            median_loser62,\r\n",
        "            median_loser63,\r\n",
        "            median_loser64,\r\n",
        "            median_loser65,\r\n",
        "            median_loser66,\r\n",
        "            median_loser67,\r\n",
        "            median_loser68,\r\n",
        "            median_loser69,\r\n",
        "            median_loser70,\r\n",
        "            median_loser71,\r\n",
        "            median_loser72,\r\n",
        "            median_loser73,\r\n",
        "            median_loser74,\r\n",
        "            median_loser75,\r\n",
        "            median_loser76,\r\n",
        "            median_loser77,\r\n",
        "            median_loser78,\r\n",
        "            median_loser79,\r\n",
        "            median_loser80,\r\n",
        "            median_loser81,\r\n",
        "            median_loser82,\r\n",
        "            median_loser83,\r\n",
        "            median_loser84,\r\n",
        "            median_loser85,\r\n",
        "            median_loser86,\r\n",
        "            median_loser87,\r\n",
        "            median_loser88,\r\n",
        "            median_loser89,\r\n",
        "            median_loser90,\r\n",
        "            median_loser91,\r\n",
        "            median_loser92,\r\n",
        "            median_loser93,\r\n",
        "            median_loser94,\r\n",
        "            median_loser95,\r\n",
        "            median_loser96,\r\n",
        "            median_loser97,\r\n",
        "            median_loser98,\r\n",
        "            median_loser99,\r\n",
        "            median_loser100,\r\n",
        "            median_loser101]\r\n",
        "\r\n",
        "upper_loser = [upper_loser1,\r\n",
        "            upper_loser2,\r\n",
        "            upper_loser3,\r\n",
        "            upper_loser4,\r\n",
        "            upper_loser5,\r\n",
        "            upper_loser6,\r\n",
        "            upper_loser7,\r\n",
        "            upper_loser8,\r\n",
        "            upper_loser9,\r\n",
        "            upper_loser10,\r\n",
        "            upper_loser11,\r\n",
        "            upper_loser12,\r\n",
        "            upper_loser13,\r\n",
        "            upper_loser14,\r\n",
        "            upper_loser15,\r\n",
        "            upper_loser16,\r\n",
        "            upper_loser17,\r\n",
        "            upper_loser18,\r\n",
        "            upper_loser19,\r\n",
        "            upper_loser20,\r\n",
        "            upper_loser21,\r\n",
        "            upper_loser22,\r\n",
        "            upper_loser23,\r\n",
        "            upper_loser24,\r\n",
        "            upper_loser25,\r\n",
        "            upper_loser26,\r\n",
        "            upper_loser27,\r\n",
        "            upper_loser28,\r\n",
        "            upper_loser29,\r\n",
        "            upper_loser30,\r\n",
        "            upper_loser31,\r\n",
        "            upper_loser32,\r\n",
        "            upper_loser33,\r\n",
        "            upper_loser34,\r\n",
        "            upper_loser35,\r\n",
        "            upper_loser36,\r\n",
        "            upper_loser37,\r\n",
        "            upper_loser38,\r\n",
        "            upper_loser39,\r\n",
        "            upper_loser40,\r\n",
        "            upper_loser41,\r\n",
        "            upper_loser42,\r\n",
        "            upper_loser43,\r\n",
        "            upper_loser44,\r\n",
        "            upper_loser45,\r\n",
        "            upper_loser46,\r\n",
        "            upper_loser47,\r\n",
        "            upper_loser48,\r\n",
        "            upper_loser49,\r\n",
        "            upper_loser50,\r\n",
        "            upper_loser51,\r\n",
        "            upper_loser52,\r\n",
        "            upper_loser53,\r\n",
        "            upper_loser54,\r\n",
        "            upper_loser55,\r\n",
        "            upper_loser56,\r\n",
        "            upper_loser57,\r\n",
        "            upper_loser58,\r\n",
        "            upper_loser59,\r\n",
        "            upper_loser60,\r\n",
        "            upper_loser61,\r\n",
        "            upper_loser62,\r\n",
        "            upper_loser63,\r\n",
        "            upper_loser64,\r\n",
        "            upper_loser65,\r\n",
        "            upper_loser66,\r\n",
        "            upper_loser67,\r\n",
        "            upper_loser68,\r\n",
        "            upper_loser69,\r\n",
        "            upper_loser70,\r\n",
        "            upper_loser71,\r\n",
        "            upper_loser72,\r\n",
        "            upper_loser73,\r\n",
        "            upper_loser74,\r\n",
        "            upper_loser75,\r\n",
        "            upper_loser76,\r\n",
        "            upper_loser77,\r\n",
        "            upper_loser78,\r\n",
        "            upper_loser79,\r\n",
        "            upper_loser80,\r\n",
        "            upper_loser81,\r\n",
        "            upper_loser82,\r\n",
        "            upper_loser83,\r\n",
        "            upper_loser84,\r\n",
        "            upper_loser85,\r\n",
        "            upper_loser86,\r\n",
        "            upper_loser87,\r\n",
        "            upper_loser88,\r\n",
        "            upper_loser89,\r\n",
        "            upper_loser90,\r\n",
        "            upper_loser91,\r\n",
        "            upper_loser92,\r\n",
        "            upper_loser93,\r\n",
        "            upper_loser94,\r\n",
        "            upper_loser95,\r\n",
        "            upper_loser96,\r\n",
        "            upper_loser97,\r\n",
        "            upper_loser98,\r\n",
        "            upper_loser99,\r\n",
        "            upper_loser100,\r\n",
        "            upper_loser101]"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u19FndLT2oU3"
      },
      "source": [
        "### Summarize arrays: 'Winner'\r\n",
        "\r\n",
        "lower_winner = [lower_winner1,\r\n",
        "            lower_winner2,\r\n",
        "            lower_winner3,\r\n",
        "            lower_winner4,\r\n",
        "            lower_winner5,\r\n",
        "            lower_winner6,\r\n",
        "            lower_winner7,\r\n",
        "            lower_winner8,\r\n",
        "            lower_winner9,\r\n",
        "            lower_winner10,\r\n",
        "            lower_winner11,\r\n",
        "            lower_winner12,\r\n",
        "            lower_winner13,\r\n",
        "            lower_winner14,\r\n",
        "            lower_winner15,\r\n",
        "            lower_winner16,\r\n",
        "            lower_winner17,\r\n",
        "            lower_winner18,\r\n",
        "            lower_winner19,\r\n",
        "            lower_winner20,\r\n",
        "            lower_winner21,\r\n",
        "            lower_winner22,\r\n",
        "            lower_winner23,\r\n",
        "            lower_winner24,\r\n",
        "            lower_winner25,\r\n",
        "            lower_winner26,\r\n",
        "            lower_winner27,\r\n",
        "            lower_winner28,\r\n",
        "            lower_winner29,\r\n",
        "            lower_winner30,\r\n",
        "            lower_winner31,\r\n",
        "            lower_winner32,\r\n",
        "            lower_winner33,\r\n",
        "            lower_winner34,\r\n",
        "            lower_winner35,\r\n",
        "            lower_winner36,\r\n",
        "            lower_winner37,\r\n",
        "            lower_winner38,\r\n",
        "            lower_winner39,\r\n",
        "            lower_winner40,\r\n",
        "            lower_winner41,\r\n",
        "            lower_winner42,\r\n",
        "            lower_winner43,\r\n",
        "            lower_winner44,\r\n",
        "            lower_winner45,\r\n",
        "            lower_winner46,\r\n",
        "            lower_winner47,\r\n",
        "            lower_winner48,\r\n",
        "            lower_winner49,\r\n",
        "            lower_winner50,\r\n",
        "            lower_winner51,\r\n",
        "            lower_winner52,\r\n",
        "            lower_winner53,\r\n",
        "            lower_winner54,\r\n",
        "            lower_winner55,\r\n",
        "            lower_winner56,\r\n",
        "            lower_winner57,\r\n",
        "            lower_winner58,\r\n",
        "            lower_winner59,\r\n",
        "            lower_winner60,\r\n",
        "            lower_winner61,\r\n",
        "            lower_winner62,\r\n",
        "            lower_winner63,\r\n",
        "            lower_winner64,\r\n",
        "            lower_winner65,\r\n",
        "            lower_winner66,\r\n",
        "            lower_winner67,\r\n",
        "            lower_winner68,\r\n",
        "            lower_winner69,\r\n",
        "            lower_winner70,\r\n",
        "            lower_winner71,\r\n",
        "            lower_winner72,\r\n",
        "            lower_winner73,\r\n",
        "            lower_winner74,\r\n",
        "            lower_winner75,\r\n",
        "            lower_winner76,\r\n",
        "            lower_winner77,\r\n",
        "            lower_winner78,\r\n",
        "            lower_winner79,\r\n",
        "            lower_winner80,\r\n",
        "            lower_winner81,\r\n",
        "            lower_winner82,\r\n",
        "            lower_winner83,\r\n",
        "            lower_winner84,\r\n",
        "            lower_winner85,\r\n",
        "            lower_winner86,\r\n",
        "            lower_winner87,\r\n",
        "            lower_winner88,\r\n",
        "            lower_winner89,\r\n",
        "            lower_winner90,\r\n",
        "            lower_winner91,\r\n",
        "            lower_winner92,\r\n",
        "            lower_winner93,\r\n",
        "            lower_winner94,\r\n",
        "            lower_winner95,\r\n",
        "            lower_winner96,\r\n",
        "            lower_winner97,\r\n",
        "            lower_winner98,\r\n",
        "            lower_winner99,\r\n",
        "            lower_winner100,\r\n",
        "            lower_winner101]\r\n",
        "\r\n",
        "median_winner = [median_winner1,\r\n",
        "            median_winner2,\r\n",
        "            median_winner3,\r\n",
        "            median_winner4,\r\n",
        "            median_winner5,\r\n",
        "            median_winner6,\r\n",
        "            median_winner7,\r\n",
        "            median_winner8,\r\n",
        "            median_winner9,\r\n",
        "            median_winner10,\r\n",
        "            median_winner11,\r\n",
        "            median_winner12,\r\n",
        "            median_winner13,\r\n",
        "            median_winner14,\r\n",
        "            median_winner15,\r\n",
        "            median_winner16,\r\n",
        "            median_winner17,\r\n",
        "            median_winner18,\r\n",
        "            median_winner19,\r\n",
        "            median_winner20,\r\n",
        "            median_winner21,\r\n",
        "            median_winner22,\r\n",
        "            median_winner23,\r\n",
        "            median_winner24,\r\n",
        "            median_winner25,\r\n",
        "            median_winner26,\r\n",
        "            median_winner27,\r\n",
        "            median_winner28,\r\n",
        "            median_winner29,\r\n",
        "            median_winner30,\r\n",
        "            median_winner31,\r\n",
        "            median_winner32,\r\n",
        "            median_winner33,\r\n",
        "            median_winner34,\r\n",
        "            median_winner35,\r\n",
        "            median_winner36,\r\n",
        "            median_winner37,\r\n",
        "            median_winner38,\r\n",
        "            median_winner39,\r\n",
        "            median_winner40,\r\n",
        "            median_winner41,\r\n",
        "            median_winner42,\r\n",
        "            median_winner43,\r\n",
        "            median_winner44,\r\n",
        "            median_winner45,\r\n",
        "            median_winner46,\r\n",
        "            median_winner47,\r\n",
        "            median_winner48,\r\n",
        "            median_winner49,\r\n",
        "            median_winner50,\r\n",
        "            median_winner51,\r\n",
        "            median_winner52,\r\n",
        "            median_winner53,\r\n",
        "            median_winner54,\r\n",
        "            median_winner55,\r\n",
        "            median_winner56,\r\n",
        "            median_winner57,\r\n",
        "            median_winner58,\r\n",
        "            median_winner59,\r\n",
        "            median_winner60,\r\n",
        "            median_winner61,\r\n",
        "            median_winner62,\r\n",
        "            median_winner63,\r\n",
        "            median_winner64,\r\n",
        "            median_winner65,\r\n",
        "            median_winner66,\r\n",
        "            median_winner67,\r\n",
        "            median_winner68,\r\n",
        "            median_winner69,\r\n",
        "            median_winner70,\r\n",
        "            median_winner71,\r\n",
        "            median_winner72,\r\n",
        "            median_winner73,\r\n",
        "            median_winner74,\r\n",
        "            median_winner75,\r\n",
        "            median_winner76,\r\n",
        "            median_winner77,\r\n",
        "            median_winner78,\r\n",
        "            median_winner79,\r\n",
        "            median_winner80,\r\n",
        "            median_winner81,\r\n",
        "            median_winner82,\r\n",
        "            median_winner83,\r\n",
        "            median_winner84,\r\n",
        "            median_winner85,\r\n",
        "            median_winner86,\r\n",
        "            median_winner87,\r\n",
        "            median_winner88,\r\n",
        "            median_winner89,\r\n",
        "            median_winner90,\r\n",
        "            median_winner91,\r\n",
        "            median_winner92,\r\n",
        "            median_winner93,\r\n",
        "            median_winner94,\r\n",
        "            median_winner95,\r\n",
        "            median_winner96,\r\n",
        "            median_winner97,\r\n",
        "            median_winner98,\r\n",
        "            median_winner99,\r\n",
        "            median_winner100,\r\n",
        "            median_winner101]\r\n",
        "\r\n",
        "upper_winner = [upper_winner1,\r\n",
        "            upper_winner2,\r\n",
        "            upper_winner3,\r\n",
        "            upper_winner4,\r\n",
        "            upper_winner5,\r\n",
        "            upper_winner6,\r\n",
        "            upper_winner7,\r\n",
        "            upper_winner8,\r\n",
        "            upper_winner9,\r\n",
        "            upper_winner10,\r\n",
        "            upper_winner11,\r\n",
        "            upper_winner12,\r\n",
        "            upper_winner13,\r\n",
        "            upper_winner14,\r\n",
        "            upper_winner15,\r\n",
        "            upper_winner16,\r\n",
        "            upper_winner17,\r\n",
        "            upper_winner18,\r\n",
        "            upper_winner19,\r\n",
        "            upper_winner20,\r\n",
        "            upper_winner21,\r\n",
        "            upper_winner22,\r\n",
        "            upper_winner23,\r\n",
        "            upper_winner24,\r\n",
        "            upper_winner25,\r\n",
        "            upper_winner26,\r\n",
        "            upper_winner27,\r\n",
        "            upper_winner28,\r\n",
        "            upper_winner29,\r\n",
        "            upper_winner30,\r\n",
        "            upper_winner31,\r\n",
        "            upper_winner32,\r\n",
        "            upper_winner33,\r\n",
        "            upper_winner34,\r\n",
        "            upper_winner35,\r\n",
        "            upper_winner36,\r\n",
        "            upper_winner37,\r\n",
        "            upper_winner38,\r\n",
        "            upper_winner39,\r\n",
        "            upper_winner40,\r\n",
        "            upper_winner41,\r\n",
        "            upper_winner42,\r\n",
        "            upper_winner43,\r\n",
        "            upper_winner44,\r\n",
        "            upper_winner45,\r\n",
        "            upper_winner46,\r\n",
        "            upper_winner47,\r\n",
        "            upper_winner48,\r\n",
        "            upper_winner49,\r\n",
        "            upper_winner50,\r\n",
        "            upper_winner51,\r\n",
        "            upper_winner52,\r\n",
        "            upper_winner53,\r\n",
        "            upper_winner54,\r\n",
        "            upper_winner55,\r\n",
        "            upper_winner56,\r\n",
        "            upper_winner57,\r\n",
        "            upper_winner58,\r\n",
        "            upper_winner59,\r\n",
        "            upper_winner60,\r\n",
        "            upper_winner61,\r\n",
        "            upper_winner62,\r\n",
        "            upper_winner63,\r\n",
        "            upper_winner64,\r\n",
        "            upper_winner65,\r\n",
        "            upper_winner66,\r\n",
        "            upper_winner67,\r\n",
        "            upper_winner68,\r\n",
        "            upper_winner69,\r\n",
        "            upper_winner70,\r\n",
        "            upper_winner71,\r\n",
        "            upper_winner72,\r\n",
        "            upper_winner73,\r\n",
        "            upper_winner74,\r\n",
        "            upper_winner75,\r\n",
        "            upper_winner76,\r\n",
        "            upper_winner77,\r\n",
        "            upper_winner78,\r\n",
        "            upper_winner79,\r\n",
        "            upper_winner80,\r\n",
        "            upper_winner81,\r\n",
        "            upper_winner82,\r\n",
        "            upper_winner83,\r\n",
        "            upper_winner84,\r\n",
        "            upper_winner85,\r\n",
        "            upper_winner86,\r\n",
        "            upper_winner87,\r\n",
        "            upper_winner88,\r\n",
        "            upper_winner89,\r\n",
        "            upper_winner90,\r\n",
        "            upper_winner91,\r\n",
        "            upper_winner92,\r\n",
        "            upper_winner93,\r\n",
        "            upper_winner94,\r\n",
        "            upper_winner95,\r\n",
        "            upper_winner96,\r\n",
        "            upper_winner97,\r\n",
        "            upper_winner98,\r\n",
        "            upper_winner99,\r\n",
        "            upper_winner100,\r\n",
        "            upper_winner101]"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84XIzmWD2oba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "98c05a2f-43fb-4037-9118-6cf8aa36660e"
      },
      "source": [
        "### Visualize!\r\n",
        "\r\n",
        "title = obj_func\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "\r\n",
        "plt.plot(median_loser, color = 'Green')\r\n",
        "plt.plot(median_winner, color = 'Yellow')\r\n",
        "\r\n",
        "xstar = np.arange(0, max_iter+1, step=1)\r\n",
        "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Green', alpha=0.4, label='GP EI Regret IQR: dEI')\r\n",
        "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Yellow', alpha=0.4, label='GP ERM Regret IQR: dERM')\r\n",
        "\r\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\r\n",
        "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') \r\n",
        "plt.ylabel('log(Regret)', weight = 'bold', family = 'Arial') \r\n",
        "plt.legend(loc=1) # add plot legend\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5dkR2Id2oiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e11d0f-214f-423c-cfcf-2685ed32e7c0"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3455.143899679184, 3516.657824754715)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNLBF57p2opi"
      },
      "source": [
        ""
      ],
      "execution_count": 183,
      "outputs": []
    }
  ]
}