{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 3a__Rosenbrock.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQo_XsusPbm"
      },
      "source": [
        "Rosenbrock synthetic function:\r\n",
        "\r\n",
        "GP EI: Newton-CG (exact GP EI gradients + exact GP EI Hessian) vs. L-BFGS-B (exact GP EI gradients, without Hessian)\r\n",
        "\r\n",
        "https://www.sfu.ca/~ssurjano/rosen.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhgucjJgsYfm",
        "outputId": "b9da524f-dea3-495a-c907-64332e58a570"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyGPGO\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/87/a113c91ba014708114f7635d5c0f6a5e5c773480c5f0a537b257a02d180d/pyGPGO-0.4.0.dev1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n",
            "Building wheels for collected packages: pyGPGO\n",
            "  Building wheel for pyGPGO (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyGPGO: filename=pyGPGO-0.4.0.dev1-cp37-none-any.whl size=19866 sha256=d1d2279570769e728b2790c61f18bc54689b30f2b2bd53f99a92af45e344c3db\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/27/04/c4fa3bfe194d36e3cd51542132f43415a6813114a5e8301acb\n",
            "Successfully built pyGPGO\n",
            "Installing collected packages: pyGPGO\n",
            "Successfully installed pyGPGO-0.4.0.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E99qMZverfrP"
      },
      "source": [
        "### Import modules:\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import scipy as sp\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import random\r\n",
        "import warnings\r\n",
        "import time\r\n",
        "\r\n",
        "from pyGPGO.logger import EventLogger\r\n",
        "from pyGPGO.GPGO import GPGO\r\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\r\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess, logpdf\r\n",
        "from pyGPGO.acquisition import Acquisition\r\n",
        "from pyGPGO.covfunc import squaredExponential\r\n",
        "\r\n",
        "from collections import OrderedDict\r\n",
        "from joblib import Parallel, delayed\r\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\r\n",
        "from scipy.optimize import minimize\r\n",
        "from scipy.spatial.distance import cdist\r\n",
        "from scipy.special import gamma\r\n",
        "from scipy.stats import norm, t\r\n",
        "from matplotlib.pyplot import rc\r\n",
        "\r\n",
        "rc('text', usetex=False)\r\n",
        "plt.rcParams['text.latex.preamble']=[r'\\usepackage{amsmath}']\r\n",
        "plt.rcParams['text.latex.preamble'] = [r'\\boldmath']\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\r\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\r\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzPUbCs3s9nu"
      },
      "source": [
        "### Inputs:\r\n",
        "\r\n",
        "n_start_AcqFunc = 100\r\n",
        "\r\n",
        "obj_func = 'Rosenbrock'\r\n",
        "n_test = n_test = n_start_AcqFunc # test points\r\n",
        "\r\n",
        "util_loser = 'dEI_GP'\r\n",
        "util_winner = 'dEI_GP'\r\n",
        "n_init = 5 # random initialisations"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AMqjmpbtAb1"
      },
      "source": [
        "### Objective function:\r\n",
        "\r\n",
        "if obj_func == 'Rosenbrock':\r\n",
        "            \r\n",
        "    # True y bounds:\r\n",
        "    y_lb = 0\r\n",
        "    operator = -1 # targets global minimum \r\n",
        "    y_global_orig = y_lb * operator # targets global minimum\r\n",
        "            \r\n",
        "# Constraints:\r\n",
        "    lb = -2.048 \r\n",
        "    ub = +2.048 \r\n",
        "    \r\n",
        "# Input array dimension(s):\r\n",
        "    dim = 2\r\n",
        "\r\n",
        "# 2-D inputs' parameter bounds:\r\n",
        "    param = {'x1_training': ('cont', [lb, ub]),\r\n",
        "             'x2_training': ('cont', [lb, ub])}\r\n",
        "    \r\n",
        "    max_iter = 100  # iterations of Bayesian optimisation\r\n",
        "    \r\n",
        "# Test data:\r\n",
        "    x1_test = np.linspace(lb, ub, n_test)\r\n",
        "    x2_test = np.linspace(lb, ub, n_test)\r\n",
        "    Xstar_d = np.column_stack((x1_test, x2_test))\r\n",
        "    \r\n",
        "    def f_syn_polarity(x1_training, x2_training):\r\n",
        "        return operator * (100 * (x2_training - x1_training ** 2) ** 2 + (x1_training - 1) ** 2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyQX-59DtC7-"
      },
      "source": [
        "### Cumulative Regret Calculator:\r\n",
        "\r\n",
        "def min_max_array(x):\r\n",
        "    new_list = []\r\n",
        "    for i, num in enumerate(x):\r\n",
        "            new_list.append(np.min(x[0:i+1]))\r\n",
        "    return new_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE_I_YSntHxl"
      },
      "source": [
        "### Set-seeds:\r\n",
        "\r\n",
        "run_num_1 = 111\r\n",
        "run_num_2 = 113\r\n",
        "run_num_3 = 3333\r\n",
        "run_num_4 = 444\r\n",
        "run_num_5 = 5555\r\n",
        "run_num_6 = 6\r\n",
        "run_num_7 = 777\r\n",
        "run_num_8 = 887\r\n",
        "run_num_9 = 99\r\n",
        "run_num_10 = 1000\r\n",
        "run_num_11 = 1113\r\n",
        "run_num_12 = 1234\r\n",
        "run_num_13 = 2345\r\n",
        "run_num_14 = 88\r\n",
        "run_num_15 = 1556\r\n",
        "run_num_16 = 1666\r\n",
        "run_num_17 = 717\r\n",
        "run_num_18 = 8\r\n",
        "run_num_19 = 1998\r\n",
        "run_num_20 = 2000"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe29v5N5tKjF"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\r\n",
        "\r\n",
        "def l2norm_(X, Xstar):\r\n",
        "    \r\n",
        "    return cdist(X, Xstar)\r\n",
        "\r\n",
        "def kronDelta(X, Xstar):\r\n",
        "\r\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\r\n",
        "\r\n",
        "class squaredExponentialDeriv(squaredExponential):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1\r\n",
        "    sigman = 1e-6\r\n",
        "\r\n",
        "    def K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\r\n",
        "        return K\r\n",
        "    \r\n",
        "    def dK(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\r\n",
        "        return dK\r\n",
        "    \r\n",
        "    def d2K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (r **2 - 1)\r\n",
        "        return d2K\r\n",
        "    \r\n",
        "cov_func = squaredExponentialDeriv()\r\n",
        "d_cov_func = squaredExponentialDeriv()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEBX5l7TtNSN"
      },
      "source": [
        "### Acquisition function derivatives:\r\n",
        "\r\n",
        "class Acquisition_new(Acquisition):    \r\n",
        "    def __init__(self, mode, eps=1e-08, **params):\r\n",
        "        \r\n",
        "        self.params = params\r\n",
        "        self.eps = eps\r\n",
        "\r\n",
        "        mode_dict = {\r\n",
        "            'dEI_GP': self.dEI_GP\r\n",
        "        }\r\n",
        "\r\n",
        "        self.f = mode_dict[mode]\r\n",
        "    \r\n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\r\n",
        "        \r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\r\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\r\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\r\n",
        "        \r\n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\r\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\r\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\r\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\r\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\r\n",
        "            \r\n",
        "        return f, df, d2f\r\n",
        "    \r\n",
        "    def _eval(self, tau, mean, std):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, **self.params)\r\n",
        "    \r\n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, **self.params)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SdY8rHwtRJ_"
      },
      "source": [
        "### Surrogate derivatives: \r\n",
        "\r\n",
        "from scipy.linalg import cholesky, solve\r\n",
        "\r\n",
        "class dGaussianProcess(GaussianProcess):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1\r\n",
        "    sigman = 1e-6\r\n",
        "\r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\r\n",
        "        L = cholesky(K).T\r\n",
        "        alpha = solve(L.T, solve(L, self.y))\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(self.L, Kstar.T)\r\n",
        "        dv = solve(self.L, dKstar.T)\r\n",
        "        d2v = solve(self.L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -2 * np.dot(dv.T, v)\r\n",
        "        dvdv = np.dot(dv.T, dv)\r\n",
        "        d2s = -2 * (dvdv + d2v)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha)\r\n",
        "        d2m = np.dot(d2Kstar, alpha)\r\n",
        "        return ds, dm, dvdv, d2v, d2m"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNv4Z0EqtUi2"
      },
      "source": [
        "class dGPGO(GPGO):  \r\n",
        "    n_start = n_start_AcqFunc\r\n",
        "    eps = 1e-08\r\n",
        "        \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expMSaSUtcB-"
      },
      "source": [
        "### d2GPGO - BayesOpt class: Exact Hessian\r\n",
        "\r\n",
        "class d2GPGO(GPGO):  \r\n",
        "    n_start = n_start_AcqFunc\r\n",
        "    p = np.full((n_start,1),1)*0 + 1\r\n",
        "    eps = 1e-08\r\n",
        "    \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "    \r\n",
        "    def hessp_nonzero(self, xnew, p):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        df2 = np.empty((self.n_start,))\r\n",
        "        df2 = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[2]\r\n",
        "        H2 = np.empty((self.n_start,))\r\n",
        "        df2 = np.asarray(df2)\r\n",
        "        p = np.asarray(p)\r\n",
        "        H2 = np.multiply(df2,p)\r\n",
        "        return H2\r\n",
        "\r\n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\r\n",
        "        \r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\r\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\r\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\r\n",
        "        \r\n",
        "        d2f = (z * norm.cdf(z) + norm.pdf(z)[0]) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\r\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\r\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx\r\n",
        "\r\n",
        "        return d2f\r\n",
        "\r\n",
        "    def hessp_nonzero1(self, xnew, p, *args):\r\n",
        "      new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "      new_std = np.sqrt(new_var + 1e-6)\r\n",
        "      ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "      df2 = np.empty((self.n_start,))\r\n",
        "      df2 = -self.dEI_GP(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0] * p\r\n",
        "      return df2\r\n",
        "\r\n",
        "    def d_optimizeAcq(self, method='Newton-CG', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,                  \r\n",
        "                                                                 hessp = self.hessp_nonzero1,                      \r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        \r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "\r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):    \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umKPgDuEtcxH",
        "outputId": "260a3c77-00c5-4a51-9b6e-ecbaf1cd7985"
      },
      "source": [
        "start_lose = time.time()\r\n",
        "start_lose"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1616061917.2202933"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfEuNIqvtdSv",
        "outputId": "2b24c0e5-a8af-44e0-9a39-043998e4a783"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_loser_1 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_1 = d2GPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.45944904 -1.35549029]. \t  -245.71064611316496 \t -108.5713485785257\n",
            "init   \t [-0.26190226  1.10289909]. \t  -108.5713485785257 \t -108.5713485785257\n",
            "init   \t [-0.83834755 -1.43702853]. \t  -461.2775269355244 \t -108.5713485785257\n",
            "init   \t [-1.95592878 -0.32676048]. \t  -1732.9949421003257 \t -108.5713485785257\n",
            "init   \t [-1.07035795 -0.66496024]. \t  -332.12317010404104 \t -108.5713485785257\n",
            "1      \t [-0.45795185 -0.30236194]. \t  \u001b[92m-28.3484050352548\u001b[0m \t -28.3484050352548\n",
            "2      \t [ 1.77122139 -1.9380348 ]. \t  -2576.421199974247 \t -28.3484050352548\n",
            "3      \t [-0.12226583 -0.75195384]. \t  -60.073467399432175 \t -28.3484050352548\n",
            "4      \t [1.88493603 1.44424789]. \t  -445.45983372037045 \t -28.3484050352548\n",
            "5      \t [-0.50551879  1.86210364]. \t  -260.36828725301234 \t -28.3484050352548\n",
            "6      \t [ 1.77169217 -0.26658693]. \t  -1160.3249553860608 \t -28.3484050352548\n",
            "7      \t [ 1.78803192 -0.60648493]. \t  -1447.3149830671293 \t -28.3484050352548\n",
            "8      \t [-0.54544849 -0.38504307]. \t  -48.97683286981722 \t -28.3484050352548\n",
            "9      \t [-0.08330601  0.31488921]. \t  \u001b[92m-10.656830259258513\u001b[0m \t -10.656830259258513\n",
            "10     \t [-1.46718824  0.91875406]. \t  -158.33479736047443 \t -10.656830259258513\n",
            "11     \t [-1.70623014  1.59082448]. \t  -181.66845577531566 \t -10.656830259258513\n",
            "12     \t [-0.76725301  0.86871125]. \t  -10.96509119669025 \t -10.656830259258513\n",
            "13     \t [-0.43734871  0.30238234]. \t  \u001b[92m-3.300480025442071\u001b[0m \t -3.300480025442071\n",
            "14     \t [0.9684561 1.7270646]. \t  -62.27793206271763 \t -3.300480025442071\n",
            "15     \t [-0.13798151  1.2120092 ]. \t  -143.61281532895606 \t -3.300480025442071\n",
            "16     \t [-0.43805569  0.3064678 ]. \t  -3.380747551456502 \t -3.300480025442071\n",
            "17     \t [1.0529725  2.03277781]. \t  -85.38534566433506 \t -3.300480025442071\n",
            "18     \t [ 0.74775298 -0.63960254]. \t  -143.7606812354837 \t -3.300480025442071\n",
            "19     \t [ 0.20328348 -0.21801166]. \t  -7.36026430048109 \t -3.300480025442071\n",
            "20     \t [-0.32144689  0.1849261 ]. \t  \u001b[92m-2.412045281903419\u001b[0m \t -2.412045281903419\n",
            "21     \t [4.45530668 5.07556937]. \t  -21839.602829798165 \t -2.412045281903419\n",
            "22     \t [-0.48004043 -1.57795417]. \t  -329.21903601179304 \t -2.412045281903419\n",
            "23     \t [1.06089113 1.82174536]. \t  -48.480860367576014 \t -2.412045281903419\n",
            "24     \t [-1.10776178  1.06507703]. \t  -7.068976453073132 \t -2.412045281903419\n",
            "25     \t [-0.37739478  0.12074502]. \t  \u001b[92m-1.944226425128506\u001b[0m \t -1.944226425128506\n",
            "26     \t [-0.08235995  0.39477545]. \t  -16.22530470798019 \t -1.944226425128506\n",
            "27     \t [ 1.160157   -0.46970718]. \t  -329.69192664755565 \t -1.944226425128506\n",
            "28     \t [0.51520454 0.44658014]. \t  -3.5163566238982207 \t -1.944226425128506\n",
            "29     \t [0.72271756 0.04804541]. \t  -22.570588291340652 \t -1.944226425128506\n",
            "30     \t [1.51037191 0.4337747 ]. \t  -341.56711141881 \t -1.944226425128506\n",
            "31     \t [0.94980596 0.89421683]. \t  \u001b[92m-0.008783433047088498\u001b[0m \t -0.008783433047088498\n",
            "32     \t [0.53386496 0.02594546]. \t  -6.928818537696993 \t -0.008783433047088498\n",
            "33     \t [-1.39253331 -0.59085225]. \t  -645.8148494698042 \t -0.008783433047088498\n",
            "34     \t [-0.85840066  1.22147162]. \t  -26.939300528904813 \t -0.008783433047088498\n",
            "35     \t [ 0.81411133 -0.95859836]. \t  -262.92044163522456 \t -0.008783433047088498\n",
            "36     \t [0.30181479 0.10000502]. \t  -0.49540648562156264 \t -0.008783433047088498\n",
            "37     \t [-1.22067096  1.44149904]. \t  -5.166978555590179 \t -0.008783433047088498\n",
            "38     \t [-0.26683513 -0.66227277]. \t  -55.40324609566223 \t -0.008783433047088498\n",
            "39     \t [-0.74217935  0.49954978]. \t  -3.2981568426071792 \t -0.008783433047088498\n",
            "40     \t [1.41436754 2.03815277]. \t  -0.3139593674149343 \t -0.008783433047088498\n",
            "41     \t [0.79303851 0.6411886 ]. \t  -0.05790927776543954 \t -0.008783433047088498\n",
            "42     \t [0.08183052 1.23046565]. \t  -150.60419279831945 \t -0.008783433047088498\n",
            "43     \t [-1.16223016 -1.42849377]. \t  -777.1109183233424 \t -0.008783433047088498\n",
            "44     \t [-1.04316404  1.15873988]. \t  -4.672230813198803 \t -0.008783433047088498\n",
            "45     \t [0.74327672 1.96714473]. \t  -200.19911334585981 \t -0.008783433047088498\n",
            "46     \t [-2.01047749 -0.74297611]. \t  -2298.681505002316 \t -0.008783433047088498\n",
            "47     \t [-1.21082569  1.68390588]. \t  -9.631740102570523 \t -0.008783433047088498\n",
            "48     \t [-1.33437787  0.68815167]. \t  -124.78585917988336 \t -0.008783433047088498\n",
            "49     \t [-0.23094754 -1.61276938]. \t  -279.10620252253784 \t -0.008783433047088498\n",
            "50     \t [0.66014216 1.64749383]. \t  -146.93868689891644 \t -0.008783433047088498\n",
            "51     \t [0.02199437 0.05615879]. \t  -1.2664659375657172 \t -0.008783433047088498\n",
            "52     \t [1.22158747 1.08415614]. \t  -16.70527922550893 \t -0.008783433047088498\n",
            "53     \t [ 0.16555488 -0.53612718]. \t  -32.4535352858786 \t -0.008783433047088498\n",
            "54     \t [ 0.42173736 -1.26611057]. \t  -208.84018173094518 \t -0.008783433047088498\n",
            "55     \t [-0.5189422  -1.68782909]. \t  -385.34301020541506 \t -0.008783433047088498\n",
            "56     \t [-0.95070309  1.48236624]. \t  -37.27492459488175 \t -0.008783433047088498\n",
            "57     \t [ 0.54384183 -0.89280069]. \t  -141.4766651669655 \t -0.008783433047088498\n",
            "58     \t [0.8051323  0.49947153]. \t  -2.2511201711580875 \t -0.008783433047088498\n",
            "59     \t [0.40937437 1.18707475]. \t  -104.28428965658428 \t -0.008783433047088498\n",
            "60     \t [1.1768558  1.41838114]. \t  -0.14277768349791176 \t -0.008783433047088498\n",
            "61     \t [-0.54492688 -0.24007982]. \t  -31.226397664342578 \t -0.008783433047088498\n",
            "62     \t [1.05179601 1.09619178]. \t  -0.012849666693023955 \t -0.008783433047088498\n",
            "63     \t [-1.12854391  1.29562523]. \t  -4.57916022324821 \t -0.008783433047088498\n",
            "64     \t [1.12735359 1.31255164]. \t  -0.18948740880205142 \t -0.008783433047088498\n",
            "65     \t [-0.21515472  1.99157176]. \t  -379.8881065908075 \t -0.008783433047088498\n",
            "66     \t [-0.05750687 -0.24363503]. \t  -7.216359370591485 \t -0.008783433047088498\n",
            "67     \t [ 1.46348283 -1.48988797]. \t  -1319.1174901038878 \t -0.008783433047088498\n",
            "68     \t [-1.95064235 -0.88388403]. \t  -2207.274882433743 \t -0.008783433047088498\n",
            "69     \t [0.16198391 0.10840563]. \t  -1.3774100518971923 \t -0.008783433047088498\n",
            "70     \t [-1.55333206 -1.38547729]. \t  -1449.24130480916 \t -0.008783433047088498\n",
            "71     \t [1.27468998 0.27710306]. \t  -181.71346766613016 \t -0.008783433047088498\n",
            "72     \t [-0.28009762  1.23351318]. \t  -135.0546635396757 \t -0.008783433047088498\n",
            "73     \t [0.48728597 1.70008284]. \t  -214.19305277710876 \t -0.008783433047088498\n",
            "74     \t [-0.27488168 -1.9321867 ]. \t  -404.7299793637241 \t -0.008783433047088498\n",
            "75     \t [-0.24921281  0.39865629]. \t  -12.887073322359871 \t -0.008783433047088498\n",
            "76     \t [-0.52180996 -0.0714736 ]. \t  -14.132946809213406 \t -0.008783433047088498\n",
            "77     \t [1.10267427 1.26710472]. \t  -0.27283114831775146 \t -0.008783433047088498\n",
            "78     \t [1.02999374 2.00653251]. \t  -89.42542473466041 \t -0.008783433047088498\n",
            "79     \t [-0.4332399 -0.1926707]. \t  -16.522120495331716 \t -0.008783433047088498\n",
            "80     \t [1.0573334  1.06879388]. \t  -0.2449581118713727 \t -0.008783433047088498\n",
            "81     \t [ 0.66521893 -0.23347934]. \t  -45.80907842624932 \t -0.008783433047088498\n",
            "82     \t [1.06649139 1.1029999 ]. \t  -0.12278460417748603 \t -0.008783433047088498\n",
            "83     \t [ 0.74413541 -1.9884645 ]. \t  -646.3445759354339 \t -0.008783433047088498\n",
            "84     \t [1.18993284 1.65277456]. \t  -5.6451275735506075 \t -0.008783433047088498\n",
            "85     \t [-1.82086888  0.66929264]. \t  -708.232246310807 \t -0.008783433047088498\n",
            "86     \t [ 0.74271154 -1.83825245]. \t  -571.2154314512803 \t -0.008783433047088498\n",
            "87     \t [-0.39927098  1.30131624]. \t  -132.3512741366938 \t -0.008783433047088498\n",
            "88     \t [0.54256713 0.31466783]. \t  -0.25040812350418595 \t -0.008783433047088498\n",
            "89     \t [0.88647147 0.72135748]. \t  -0.42858072349503895 \t -0.008783433047088498\n",
            "90     \t [ 1.21960029 -0.11987661]. \t  -258.3900302193461 \t -0.008783433047088498\n",
            "91     \t [0.26039215 0.32972883]. \t  -7.407477600251035 \t -0.008783433047088498\n",
            "92     \t [ 1.46806628 -0.6833924 ]. \t  -805.9903261107626 \t -0.008783433047088498\n",
            "93     \t [1.10093847 1.23767273]. \t  -0.07576147281820803 \t -0.008783433047088498\n",
            "94     \t [-1.17118342  0.76949043]. \t  -40.976133562082566 \t -0.008783433047088498\n",
            "95     \t [ 1.65361628 -1.69356049]. \t  -1961.1520765663745 \t -0.008783433047088498\n",
            "96     \t [-1.65372101 -1.45526157]. \t  -1762.698113551249 \t -0.008783433047088498\n",
            "97     \t [1.05498416 1.12829335]. \t  -0.026437674680498402 \t -0.008783433047088498\n",
            "98     \t [ 1.72114382 -0.85529876]. \t  -1457.9536047382583 \t -0.008783433047088498\n",
            "99     \t [0.34268958 0.38226986]. \t  -7.445746616971624 \t -0.008783433047088498\n",
            "100    \t [-1.0954881   0.33887054]. \t  -78.56168510444935 \t -0.008783433047088498\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j6UBhkdtdVe",
        "outputId": "bc492dd8-9a7a-4c95-f274-31c558fd0c37"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_loser_2 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_2 = d2GPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.44173258 -1.74529086]. \t  -1462.4037722675753 \t -1.3013277264983028\n",
            "init   \t [ 1.6176405  -0.26012243]. \t  -828.0271838688934 \t -1.3013277264983028\n",
            "init   \t [-1.525032    0.31071385]. \t  -412.40181251788414 \t -1.3013277264983028\n",
            "init   \t [ 1.39456889 -0.26574622]. \t  -488.8170397507174 \t -1.3013277264983028\n",
            "init   \t [0.80244966 0.75627765]. \t  -1.3013277264983028 \t -1.3013277264983028\n",
            "1      \t [ 0.43483271 -0.2655693 ]. \t  -20.989965664982826 \t -1.3013277264983028\n",
            "2      \t [0.70750854 0.05612926]. \t  -19.838160610929243 \t -1.3013277264983028\n",
            "3      \t [0.906422   1.79209788]. \t  -94.19520550936414 \t -1.3013277264983028\n",
            "4      \t [ 0.54724866 -0.48268329]. \t  -61.383096561513206 \t -1.3013277264983028\n",
            "5      \t [-0.36818969 -0.36297751]. \t  -26.726271491426207 \t -1.3013277264983028\n",
            "6      \t [0.85433046 1.0916157 ]. \t  -13.106452817727035 \t -1.3013277264983028\n",
            "7      \t [-0.04265851  1.11462236]. \t  -124.92010246749348 \t -1.3013277264983028\n",
            "8      \t [ 0.96743136 -0.15673713]. \t  -119.39177174495406 \t -1.3013277264983028\n",
            "9      \t [0.36145592 0.53912274]. \t  -17.092705269250054 \t -1.3013277264983028\n",
            "10     \t [-0.81274135 -1.46312101]. \t  -454.28325187718 \t -1.3013277264983028\n",
            "11     \t [-1.18252285  2.04422297]. \t  -46.47726813375165 \t -1.3013277264983028\n",
            "12     \t [-0.35622333  1.91499104]. \t  -321.5680662335156 \t -1.3013277264983028\n",
            "13     \t [-2.00865767  1.81043093]. \t  -503.79181494077164 \t -1.3013277264983028\n",
            "14     \t [1.54581982 1.98791258]. \t  -16.429895980058618 \t -1.3013277264983028\n",
            "15     \t [ 0.02516796 -0.25451335]. \t  -7.460285200291719 \t -1.3013277264983028\n",
            "16     \t [ 1.82364634 -1.95191717]. \t  -2785.987904868276 \t -1.3013277264983028\n",
            "17     \t [0.43308809 0.08979191]. \t  \u001b[92m-1.277352653642426\u001b[0m \t -1.277352653642426\n",
            "18     \t [0.59813564 0.696945  ]. \t  -11.665717482847484 \t -1.277352653642426\n",
            "19     \t [1.18740811 1.08980558]. \t  -10.283599433902245 \t -1.277352653642426\n",
            "20     \t [1.2851498  1.47373646]. \t  -3.2452103047506706 \t -1.277352653642426\n",
            "21     \t [1.0861605 1.2183568]. \t  \u001b[92m-0.15651363494701398\u001b[0m \t -0.15651363494701398\n",
            "22     \t [-1.8893095  -0.46960809]. \t  -1639.7797752822478 \t -0.15651363494701398\n",
            "23     \t [-0.83063858  0.84232153]. \t  -5.672627588205044 \t -0.15651363494701398\n",
            "24     \t [-0.80199266  0.57659333]. \t  -3.690718889652942 \t -0.15651363494701398\n",
            "25     \t [1.1025212  1.23366077]. \t  \u001b[92m-0.043299717652430215\u001b[0m \t -0.043299717652430215\n",
            "26     \t [0.57381717 0.36090918]. \t  -0.2817599820003396 \t -0.043299717652430215\n",
            "27     \t [0.46466716 1.54737497]. \t  -177.56499598933985 \t -0.043299717652430215\n",
            "28     \t [0.15102971 1.10510574]. \t  -117.8571634189896 \t -0.043299717652430215\n",
            "29     \t [2.03504526 0.92232824]. \t  -1037.3195379050223 \t -0.043299717652430215\n",
            "30     \t [0.92086323 0.91748086]. \t  -0.489173225997131 \t -0.043299717652430215\n",
            "31     \t [0.9960668  1.00835707]. \t  \u001b[92m-0.02628542354851871\u001b[0m \t -0.02628542354851871\n",
            "32     \t [1.04632362 1.10818383]. \t  \u001b[92m-0.020077011697160443\u001b[0m \t -0.020077011697160443\n",
            "33     \t [-0.24541195 -1.78732495]. \t  -342.8958826553927 \t -0.020077011697160443\n",
            "34     \t [0.99081535 1.04466817]. \t  -0.396393892714814 \t -0.020077011697160443\n",
            "35     \t [-0.68739673 -0.15970921]. \t  -42.81796004380665 \t -0.020077011697160443\n",
            "36     \t [ 1.74293882 -0.80536341]. \t  -1477.5699245440094 \t -0.020077011697160443\n",
            "37     \t [1.19934537 0.68577621]. \t  -56.6884084518539 \t -0.020077011697160443\n",
            "38     \t [0.98309222 0.91418125]. \t  -0.27370051266393847 \t -0.020077011697160443\n",
            "39     \t [0.84065146 0.69970027]. \t  -0.030284404489958974 \t -0.020077011697160443\n",
            "40     \t [-0.80967392 -0.64047878]. \t  -171.24964427357978 \t -0.020077011697160443\n",
            "41     \t [1.03165866 0.66417548]. \t  -16.01253240286429 \t -0.020077011697160443\n",
            "42     \t [-0.34286574  1.94133637]. \t  -334.4204397773866 \t -0.020077011697160443\n",
            "43     \t [-0.75738889  0.35083364]. \t  -8.05259127383539 \t -0.020077011697160443\n",
            "44     \t [-1.55338422  1.07956766]. \t  -184.32462558936453 \t -0.020077011697160443\n",
            "45     \t [ 1.99501367 -1.7679637 ]. \t  -3304.990185125095 \t -0.020077011697160443\n",
            "46     \t [-1.75491246  0.43709123]. \t  -705.9370292132911 \t -0.020077011697160443\n",
            "47     \t [-1.92446685  2.01946897]. \t  -292.1730256583349 \t -0.020077011697160443\n",
            "48     \t [-0.79094099 -1.53717212]. \t  -470.96045277147226 \t -0.020077011697160443\n",
            "49     \t [-1.10241157  1.24311903]. \t  -4.497461530222756 \t -0.020077011697160443\n",
            "50     \t [-1.00488951  1.05451804]. \t  -4.2195259766314015 \t -0.020077011697160443\n",
            "51     \t [1.05914411 1.12345495]. \t  \u001b[92m-0.0037764840180860245\u001b[0m \t -0.0037764840180860245\n",
            "52     \t [-1.51848669  1.61216493]. \t  -54.45599160830821 \t -0.0037764840180860245\n",
            "53     \t [-1.28014554  1.69347956]. \t  -5.498348840885934 \t -0.0037764840180860245\n",
            "54     \t [-0.57362638  0.37193407]. \t  -2.6602281661743064 \t -0.0037764840180860245\n",
            "55     \t [1.0984034  1.23754025]. \t  -0.106094846697602 \t -0.0037764840180860245\n",
            "56     \t [0.13073798 0.02316751]. \t  -0.7593071371995133 \t -0.0037764840180860245\n",
            "57     \t [1.96646943 0.26738163]. \t  -1296.6607706834602 \t -0.0037764840180860245\n",
            "58     \t [-1.93116689 -0.85024414]. \t  -2105.910875856383 \t -0.0037764840180860245\n",
            "59     \t [1.30393919 1.74517803]. \t  -0.29416518842338735 \t -0.0037764840180860245\n",
            "60     \t [0.28483277 1.61744199]. \t  -236.53700601606423 \t -0.0037764840180860245\n",
            "61     \t [1.57259458 0.00595459]. \t  -608.9856696190132 \t -0.0037764840180860245\n",
            "62     \t [1.85709005 1.58285552]. \t  -348.9033138942158 \t -0.0037764840180860245\n",
            "63     \t [-0.58054926 -0.49970011]. \t  -72.51110784024942 \t -0.0037764840180860245\n",
            "64     \t [1.11360539 1.2707486 ]. \t  -0.10673584069019053 \t -0.0037764840180860245\n",
            "65     \t [-0.11136909  1.32217248]. \t  -172.78473047588355 \t -0.0037764840180860245\n",
            "66     \t [ 0.08406173 -2.02016692]. \t  -411.80642514590784 \t -0.0037764840180860245\n",
            "67     \t [ 0.60840368 -1.00349936]. \t  -188.84598804941893 \t -0.0037764840180860245\n",
            "68     \t [1.37280024 1.86685824]. \t  -0.17038784874380064 \t -0.0037764840180860245\n",
            "69     \t [-1.14585149 -1.04130742]. \t  -558.8695561563712 \t -0.0037764840180860245\n",
            "70     \t [ 0.27362341 -0.21059029]. \t  -8.676367778288038 \t -0.0037764840180860245\n",
            "71     \t [0.74365306 0.54086756]. \t  -0.08048163691127136 \t -0.0037764840180860245\n",
            "72     \t [1.44667576 2.01200447]. \t  -0.8534549216445936 \t -0.0037764840180860245\n",
            "73     \t [0.95314975 0.73193607]. \t  -3.1194807181052044 \t -0.0037764840180860245\n",
            "74     \t [1.23918153 1.55888753]. \t  -0.1115744329865202 \t -0.0037764840180860245\n",
            "75     \t [-1.9517589  -0.69069591]. \t  -2033.76572776341 \t -0.0037764840180860245\n",
            "76     \t [1.76927873 1.12586904]. \t  -402.38507314741815 \t -0.0037764840180860245\n",
            "77     \t [-1.18605588  1.00865149]. \t  -20.62537567388703 \t -0.0037764840180860245\n",
            "78     \t [ 0.59733697 -0.66741213]. \t  -105.06553138860436 \t -0.0037764840180860245\n",
            "79     \t [-1.03351079  1.23317937]. \t  -6.858815070908965 \t -0.0037764840180860245\n",
            "80     \t [-1.03425542  0.62109077]. \t  -24.261808152218876 \t -0.0037764840180860245\n",
            "81     \t [0.54678719 1.70335149]. \t  -197.43238918791414 \t -0.0037764840180860245\n",
            "82     \t [1.44339084 2.0638615 ]. \t  -0.2346813328632704 \t -0.0037764840180860245\n",
            "83     \t [2.010771   1.52827972]. \t  -633.504066675221 \t -0.0037764840180860245\n",
            "84     \t [ 1.63095484 -0.77756562]. \t  -1182.0932557353174 \t -0.0037764840180860245\n",
            "85     \t [-0.08660451 -0.70069306]. \t  -51.33449912311258 \t -0.0037764840180860245\n",
            "86     \t [1.46932108 1.16443248]. \t  -99.11770622554333 \t -0.0037764840180860245\n",
            "87     \t [1.889115   0.21074836]. \t  -1128.411706159457 \t -0.0037764840180860245\n",
            "88     \t [-0.34497038 -1.43593559]. \t  -243.5928354559929 \t -0.0037764840180860245\n",
            "89     \t [ 1.42530268 -0.94849089]. \t  -888.2081439184545 \t -0.0037764840180860245\n",
            "90     \t [1.67435419 1.56277203]. \t  -154.38590161338132 \t -0.0037764840180860245\n",
            "91     \t [-0.98007518  1.8084631 ]. \t  -75.81680806136724 \t -0.0037764840180860245\n",
            "92     \t [-0.28495107 -1.45635923]. \t  -238.05904995480884 \t -0.0037764840180860245\n",
            "93     \t [-0.35843956  0.40250036]. \t  -9.35413291051952 \t -0.0037764840180860245\n",
            "94     \t [-1.60692968  1.75495421]. \t  -75.2334457526766 \t -0.0037764840180860245\n",
            "95     \t [ 1.29687332 -1.13775409]. \t  -795.1219996279622 \t -0.0037764840180860245\n",
            "96     \t [1.17085781 0.86407361]. \t  -25.717303713322334 \t -0.0037764840180860245\n",
            "97     \t [-0.54378274  1.59506367]. \t  -171.2179454991696 \t -0.0037764840180860245\n",
            "98     \t [-1.20891657  0.77681768]. \t  -51.75546165606691 \t -0.0037764840180860245\n",
            "99     \t [ 1.04183685 -0.83097723]. \t  -367.26112762398185 \t -0.0037764840180860245\n",
            "100    \t [-0.67481668  0.98829967]. \t  -31.20560964966402 \t -0.0037764840180860245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWnL1rwztdYG",
        "outputId": "4f3a6c5c-8ecb-419f-b162-d32dd908ae56"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_loser_3 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_3 = d2GPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.03045369 -1.60050678]. \t  -708.8071948417274 \t -1.118465165857483\n",
            "init   \t [-0.05684724 -0.0006915 ]. \t  -1.118465165857483 \t -1.118465165857483\n",
            "init   \t [ 0.64352256 -1.08181622]. \t  -223.90997740226942 \t -1.118465165857483\n",
            "init   \t [ 0.46200787 -1.55790376]. \t  -314.05929962166397 \t -1.118465165857483\n",
            "init   \t [ 0.86918061 -0.52199202]. \t  -163.20929387450548 \t -1.118465165857483\n",
            "1      \t [-0.4017771  -1.04590239]. \t  -147.728883612653 \t -1.118465165857483\n",
            "2      \t [-0.780274    1.80164718]. \t  -145.45124892612696 \t -1.118465165857483\n",
            "3      \t [ 0.19510124 -0.39571949]. \t  -19.464716969441064 \t -1.118465165857483\n",
            "4      \t [-0.29682683  0.42104675]. \t  -12.766702899455076 \t -1.118465165857483\n",
            "5      \t [1.68125163 2.00920736]. \t  -67.2783258604902 \t -1.118465165857483\n",
            "6      \t [0.15160251 0.01246474]. \t  \u001b[92m-0.7308423449833116\u001b[0m \t -0.7308423449833116\n",
            "7      \t [1.01799814 0.68687832]. \t  -12.21128808354645 \t -0.7308423449833116\n",
            "8      \t [1.1408016  1.07110639]. \t  -5.324642340417902 \t -0.7308423449833116\n",
            "9      \t [0.23738752 0.41555916]. \t  -13.484496564086381 \t -0.7308423449833116\n",
            "10     \t [1.95990213 1.25202671]. \t  -671.3117070662171 \t -0.7308423449833116\n",
            "11     \t [0.63924825 2.47205041]. \t  -425.89708599183257 \t -0.7308423449833116\n",
            "12     \t [-1.49843761  0.76257806]. \t  -226.09315394636212 \t -0.7308423449833116\n",
            "13     \t [0.85685074 0.86258795]. \t  -1.6690130039219107 \t -0.7308423449833116\n",
            "14     \t [1.10904017 1.42359805]. \t  -3.761067948901795 \t -0.7308423449833116\n",
            "15     \t [1.38246397 1.98811779]. \t  -0.7378111115235559 \t -0.7308423449833116\n",
            "16     \t [-1.75863345  1.99593531]. \t  -127.91943118917429 \t -0.7308423449833116\n",
            "17     \t [0.8575366  1.97026682]. \t  -152.51755456713286 \t -0.7308423449833116\n",
            "18     \t [0.13089464 0.13779516]. \t  -2.2112700556467138 \t -0.7308423449833116\n",
            "19     \t [1.01735736 1.1186248 ]. \t  \u001b[92m-0.6993443400299859\u001b[0m \t -0.6993443400299859\n",
            "20     \t [-0.75983516 -0.10071525]. \t  -49.074195396815135 \t -0.6993443400299859\n",
            "21     \t [1.36773606 1.84825038]. \t  \u001b[92m-0.1856370387517617\u001b[0m \t -0.1856370387517617\n",
            "22     \t [3.92693614 3.95197001]. \t  -13162.035997790297 \t -0.1856370387517617\n",
            "23     \t [-0.84638391  0.30055093]. \t  -20.699327559089415 \t -0.1856370387517617\n",
            "24     \t [-0.86774288  0.63332426]. \t  -4.920158121310172 \t -0.1856370387517617\n",
            "25     \t [-0.78239733  0.64759719]. \t  -3.302621860117641 \t -0.1856370387517617\n",
            "26     \t [ 0.86474998 -1.06134866]. \t  -327.31747625263705 \t -0.1856370387517617\n",
            "27     \t [0.89931942 0.75340109]. \t  -0.3167682437691932 \t -0.1856370387517617\n",
            "28     \t [0.92563577 0.87398109]. \t  \u001b[92m-0.035043593780174784\u001b[0m \t -0.035043593780174784\n",
            "29     \t [1.83979201 0.74160758]. \t  -699.3701751977119 \t -0.035043593780174784\n",
            "30     \t [-1.88548814  0.78108077]. \t  -777.8251909301468 \t -0.035043593780174784\n",
            "31     \t [-0.82056586  0.83637581]. \t  -5.97290838859537 \t -0.035043593780174784\n",
            "32     \t [-0.98261309  0.04318056]. \t  -89.00332324256519 \t -0.035043593780174784\n",
            "33     \t [-0.67499788  1.75886749]. \t  -172.6504632335947 \t -0.035043593780174784\n",
            "34     \t [-1.28509462  0.25780746]. \t  -199.45068044631728 \t -0.035043593780174784\n",
            "35     \t [-1.30914363  1.5639094 ]. \t  -7.580573497748772 \t -0.035043593780174784\n",
            "36     \t [-1.4435152   2.00925156]. \t  -6.525561451447095 \t -0.035043593780174784\n",
            "37     \t [0.99711775 0.96500943]. \t  -0.08547319757174346 \t -0.035043593780174784\n",
            "38     \t [-1.94580847 -0.71166838]. \t  -2031.7333252118815 \t -0.035043593780174784\n",
            "39     \t [-1.03861437 -1.56074171]. \t  -700.8316600995099 \t -0.035043593780174784\n",
            "40     \t [1.63843289 1.36021904]. \t  -175.76962581466148 \t -0.035043593780174784\n",
            "41     \t [ 0.78733669 -2.04448324]. \t  -709.9385288558885 \t -0.035043593780174784\n",
            "42     \t [0.49967729 1.38096823]. \t  -128.23221812997554 \t -0.035043593780174784\n",
            "43     \t [-0.3711829 -0.3645665]. \t  -27.115015768712222 \t -0.035043593780174784\n",
            "44     \t [0.8911853  0.76385037]. \t  -0.1040188783536322 \t -0.035043593780174784\n",
            "45     \t [1.06944825 1.07655838]. \t  -0.45588543891170014 \t -0.035043593780174784\n",
            "46     \t [ 0.34057347 -0.16994831]. \t  -8.610931318744916 \t -0.035043593780174784\n",
            "47     \t [0.7946948  0.64826948]. \t  -0.07013838891524227 \t -0.035043593780174784\n",
            "48     \t [1.62675981 0.68831337]. \t  -383.7825862242523 \t -0.035043593780174784\n",
            "49     \t [-1.92677455  0.16661211]. \t  -1265.869850209115 \t -0.035043593780174784\n",
            "50     \t [-1.43196859  1.68221133]. \t  -19.48063398134607 \t -0.035043593780174784\n",
            "51     \t [-1.54677484 -1.42960587]. \t  -1467.344875545267 \t -0.035043593780174784\n",
            "52     \t [ 0.85535277 -0.34463053]. \t  -115.85424275582528 \t -0.035043593780174784\n",
            "53     \t [1.13269135 1.26759145]. \t  -0.04131755720586214 \t -0.035043593780174784\n",
            "54     \t [1.38281945 0.84828167]. \t  -113.33656656331196 \t -0.035043593780174784\n",
            "55     \t [-1.58578633  0.99741321]. \t  -236.90775479420387 \t -0.035043593780174784\n",
            "56     \t [-0.93135105 -0.28698016]. \t  -136.99288509973206 \t -0.035043593780174784\n",
            "57     \t [-1.39071243  0.62388476]. \t  -177.3769387689481 \t -0.035043593780174784\n",
            "58     \t [0.87472748 1.96633851]. \t  -144.3015198463999 \t -0.035043593780174784\n",
            "59     \t [-1.1281658   0.19057678]. \t  -121.6407261095112 \t -0.035043593780174784\n",
            "60     \t [-0.94243415  0.15022679]. \t  -58.23085882279816 \t -0.035043593780174784\n",
            "61     \t [0.81456657 0.65549692]. \t  -0.04082045478636227 \t -0.035043593780174784\n",
            "62     \t [-0.40346979 -0.06644562]. \t  -7.22452655935794 \t -0.035043593780174784\n",
            "63     \t [-1.72920729  0.0166434 ]. \t  -891.627389019488 \t -0.035043593780174784\n",
            "64     \t [1.32731887 1.68482455]. \t  -0.6992806086671052 \t -0.035043593780174784\n",
            "65     \t [0.37365461 0.11444889]. \t  -0.45565581182211695 \t -0.035043593780174784\n",
            "66     \t [0.62882367 0.37553572]. \t  -0.17730714549863352 \t -0.035043593780174784\n",
            "67     \t [1.16085906 1.35068609]. \t  \u001b[92m-0.02683189292412615\u001b[0m \t -0.02683189292412615\n",
            "68     \t [0.60207405 0.38762294]. \t  -0.22149561162241863 \t -0.02683189292412615\n",
            "69     \t [0.97054361 0.76680745]. \t  -3.0685303410528384 \t -0.02683189292412615\n",
            "70     \t [1.76609451 0.29053984]. \t  -800.6563950055495 \t -0.02683189292412615\n",
            "71     \t [-0.33250964 -0.39777081]. \t  -27.615873630731052 \t -0.02683189292412615\n",
            "72     \t [ 1.73459313 -1.33251473]. \t  -1885.252561455859 \t -0.02683189292412615\n",
            "73     \t [ 1.07843284 -1.26873019]. \t  -591.3457791951057 \t -0.02683189292412615\n",
            "74     \t [ 0.05185248 -0.61927705]. \t  -39.58312041762906 \t -0.02683189292412615\n",
            "75     \t [1.07504385 1.15794208]. \t  \u001b[92m-0.006125661922954601\u001b[0m \t -0.006125661922954601\n",
            "76     \t [1.06176258 1.1392823 ]. \t  -0.018077009257239162 \t -0.006125661922954601\n",
            "77     \t [-1.13297187 -0.62810777]. \t  -370.021887695414 \t -0.006125661922954601\n",
            "78     \t [-0.16329517 -0.12542853]. \t  -3.6665093913833675 \t -0.006125661922954601\n",
            "79     \t [1.12645294 0.53893632]. \t  -53.30013520835561 \t -0.006125661922954601\n",
            "80     \t [ 1.59495137 -0.17072792]. \t  -737.2580887986887 \t -0.006125661922954601\n",
            "81     \t [-0.28444708 -0.11423451]. \t  -5.457947591478899 \t -0.006125661922954601\n",
            "82     \t [0.68399128 0.74996957]. \t  -8.059340982336861 \t -0.006125661922954601\n",
            "83     \t [-1.37415569  1.40566405]. \t  -28.9307325057905 \t -0.006125661922954601\n",
            "84     \t [-0.71861415  0.91425582]. \t  -18.78205873832965 \t -0.006125661922954601\n",
            "85     \t [0.78662684 0.64899484]. \t  -0.13681100995555326 \t -0.006125661922954601\n",
            "86     \t [0.37447845 0.96941186]. \t  -69.14485198697025 \t -0.006125661922954601\n",
            "87     \t [ 0.13406096 -0.30988621]. \t  -11.49897327350787 \t -0.006125661922954601\n",
            "88     \t [-1.97757266  0.61735958]. \t  -1093.5367051180658 \t -0.006125661922954601\n",
            "89     \t [-0.04372883 -0.81460824]. \t  -67.75993521587327 \t -0.006125661922954601\n",
            "90     \t [1.14099452 1.31584089]. \t  -0.039402237396146705 \t -0.006125661922954601\n",
            "91     \t [0.51069402 0.31521316]. \t  -0.5354083256716863 \t -0.006125661922954601\n",
            "92     \t [ 0.56971754 -0.26063979]. \t  -34.43313749931704 \t -0.006125661922954601\n",
            "93     \t [ 0.71186019 -0.97858099]. \t  -220.70233264226943 \t -0.006125661922954601\n",
            "94     \t [-0.68390632 -0.72930392]. \t  -146.12404581908285 \t -0.006125661922954601\n",
            "95     \t [0.64162173 0.4169937 ]. \t  -0.13126018234566295 \t -0.006125661922954601\n",
            "96     \t [ 1.7242307  -0.10450261]. \t  -947.6092010408778 \t -0.006125661922954601\n",
            "97     \t [1.40673123 1.95221858]. \t  -0.23658149334126774 \t -0.006125661922954601\n",
            "98     \t [-1.8188858  -2.01500327]. \t  -2841.750389796985 \t -0.006125661922954601\n",
            "99     \t [0.28598232 1.17276022]. \t  -119.53232007701105 \t -0.006125661922954601\n",
            "100    \t [ 1.94508247 -0.03896131]. \t  -1461.8963614633797 \t -0.006125661922954601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zciLjgNltda2",
        "outputId": "b73e2168-c0dc-4954-f269-bc00d8fef130"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_loser_4 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_4 = d2GPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.39002285 1.31602418]. \t  -38.11488737154776 \t -12.122423820878506\n",
            "init   \t [0.58248055 0.68494384]. \t  -12.122423820878506 \t -12.122423820878506\n",
            "init   \t [-1.88768295 -1.7790059 ]. \t  -2862.4120613667637 \t -12.122423820878506\n",
            "init   \t [-0.91545338 -0.75218738]. \t  -256.5560079635026 \t -12.122423820878506\n",
            "init   \t [-1.25504548 -1.24070756]. \t  -797.98450090518 \t -12.122423820878506\n",
            "1      \t [ 0.0784139  -1.99165818]. \t  -399.9725697606536 \t -12.122423820878506\n",
            "2      \t [0.36848897 1.24803051]. \t  -124.10800925982912 \t -12.122423820878506\n",
            "3      \t [1.30511175 0.39567731]. \t  -171.08516983793774 \t -12.122423820878506\n",
            "4      \t [2.04167073 2.00984239]. \t  -467.03053841253114 \t -12.122423820878506\n",
            "5      \t [0.91014057 1.11600244]. \t  \u001b[92m-8.282130516761175\u001b[0m \t -8.282130516761175\n",
            "6      \t [0.99877269 0.92140453]. \t  \u001b[92m-0.5797673235093339\u001b[0m \t -0.5797673235093339\n",
            "7      \t [ 1.73111495 -0.26146777]. \t  -1062.138680699351 \t -0.5797673235093339\n",
            "8      \t [1.03180018 1.96169696]. \t  -80.4772237978564 \t -0.5797673235093339\n",
            "9      \t [ 1.56120513 -1.00487715]. \t  -1185.2156248763604 \t -0.5797673235093339\n",
            "10     \t [0.507293   0.21831102]. \t  \u001b[92m-0.3951345907402283\u001b[0m \t -0.3951345907402283\n",
            "11     \t [-0.68497996  0.16259641]. \t  -12.239583504391522 \t -0.3951345907402283\n",
            "12     \t [-1.62980095  0.77295962]. \t  -361.59455113070555 \t -0.3951345907402283\n",
            "13     \t [-0.37533707  0.35972214]. \t  -6.680831570459256 \t -0.3951345907402283\n",
            "14     \t [-0.67002381 -1.75175791]. \t  -487.092544410677 \t -0.3951345907402283\n",
            "15     \t [-0.17325487  1.62640431]. \t  -256.22169244829337 \t -0.3951345907402283\n",
            "16     \t [0.49647802 0.40052582]. \t  -2.6262245943538023 \t -0.3951345907402283\n",
            "17     \t [-0.69170413  0.71530072]. \t  -8.471471087849487 \t -0.3951345907402283\n",
            "18     \t [-0.62811217  0.50849248]. \t  -3.949610268984114 \t -0.3951345907402283\n",
            "19     \t [-1.06584612  1.83667076]. \t  -53.35775496550255 \t -0.3951345907402283\n",
            "20     \t [-0.61916606 -0.43957734]. \t  -70.34537379874358 \t -0.3951345907402283\n",
            "21     \t [ 0.18298389 -0.0400949 ]. \t  -1.208887648019661 \t -0.3951345907402283\n",
            "22     \t [-0.4167332   0.14111697]. \t  -2.1130804890522543 \t -0.3951345907402283\n",
            "23     \t [0.62746085 0.4133797 ]. \t  \u001b[92m-0.17748647980056836\u001b[0m \t -0.17748647980056836\n",
            "24     \t [1.83420092 1.92805264]. \t  -206.97453394502486 \t -0.17748647980056836\n",
            "25     \t [1.32368235 1.82664946]. \t  -0.6600112264576352 \t -0.17748647980056836\n",
            "26     \t [1.49698166 2.01078795]. \t  -5.544636425029403 \t -0.17748647980056836\n",
            "27     \t [1.40046922 1.97908209]. \t  -0.19194594668516246 \t -0.17748647980056836\n",
            "28     \t [-0.55290055  1.30781461]. \t  -102.83506472840799 \t -0.17748647980056836\n",
            "29     \t [0.36909874 0.11682959]. \t  -0.4356890685645787 \t -0.17748647980056836\n",
            "30     \t [-1.92875429 -0.65237303]. \t  -1920.4236213733727 \t -0.17748647980056836\n",
            "31     \t [0.83406118 0.68926091]. \t  \u001b[92m-0.0316280354659526\u001b[0m \t -0.0316280354659526\n",
            "32     \t [-0.55885019 -1.36792651]. \t  -284.7506778298577 \t -0.0316280354659526\n",
            "33     \t [0.24363327 1.74337258]. \t  -284.16287928828496 \t -0.0316280354659526\n",
            "34     \t [-0.01587209 -0.58220972]. \t  -34.958152717907716 \t -0.0316280354659526\n",
            "35     \t [0.69657884 0.52144821]. \t  -0.22329761987255303 \t -0.0316280354659526\n",
            "36     \t [1.35609206 1.88597738]. \t  -0.3476236022260277 \t -0.0316280354659526\n",
            "37     \t [-1.59275152  0.6595924 ]. \t  -359.134746813209 \t -0.0316280354659526\n",
            "38     \t [0.83354712 0.71581111]. \t  -0.07184983233924042 \t -0.0316280354659526\n",
            "39     \t [-1.12292196  1.80136345]. \t  -33.71106413908456 \t -0.0316280354659526\n",
            "40     \t [-1.34069039  1.8482245 ]. \t  -5.7366291978190525 \t -0.0316280354659526\n",
            "41     \t [-0.25533525 -0.95263609]. \t  -105.17410174748937 \t -0.0316280354659526\n",
            "42     \t [0.84407654 0.68294179]. \t  -0.11147532535355575 \t -0.0316280354659526\n",
            "43     \t [0.75738609 0.54010884]. \t  -0.17125309205107903 \t -0.0316280354659526\n",
            "44     \t [-0.00051807 -0.49995919]. \t  -25.996982315917172 \t -0.0316280354659526\n",
            "45     \t [-1.37429839  1.73310574]. \t  -8.058127826750235 \t -0.0316280354659526\n",
            "46     \t [-0.23860944 -1.69672867]. \t  -309.06759363010303 \t -0.0316280354659526\n",
            "47     \t [0.1361602  0.91973458]. \t  -81.9614576900326 \t -0.0316280354659526\n",
            "48     \t [0.26502029 1.06660612]. \t  -99.81558628550876 \t -0.0316280354659526\n",
            "49     \t [-0.85724841 -1.36641014]. \t  -444.9892310789567 \t -0.0316280354659526\n",
            "50     \t [-1.91655642 -0.80527284]. \t  -2014.1679084374393 \t -0.0316280354659526\n",
            "51     \t [1.80553109 0.05877185]. \t  -1025.398245789344 \t -0.0316280354659526\n",
            "52     \t [ 0.00230208 -0.0283874 ]. \t  -1.0760156635369806 \t -0.0316280354659526\n",
            "53     \t [1.88164587 0.89465385]. \t  -700.875734542644 \t -0.0316280354659526\n",
            "54     \t [-1.56327956  2.03914531]. \t  -22.948422571998705 \t -0.0316280354659526\n",
            "55     \t [ 0.78927822 -1.02652579]. \t  -272.12477832565867 \t -0.0316280354659526\n",
            "56     \t [-1.09340419 -0.90661056]. \t  -446.28298085426337 \t -0.0316280354659526\n",
            "57     \t [0.0025451  1.09913131]. \t  -121.80245700293212 \t -0.0316280354659526\n",
            "58     \t [-1.13628881 -0.58089985]. \t  -355.02164384040896 \t -0.0316280354659526\n",
            "59     \t [-1.90671123  0.45571025]. \t  -1019.5855922763678 \t -0.0316280354659526\n",
            "60     \t [ 1.69350482 -0.70147587]. \t  -1274.5671653469997 \t -0.0316280354659526\n",
            "61     \t [1.41071839 1.93700481]. \t  -0.4508797500146591 \t -0.0316280354659526\n",
            "62     \t [-0.81246147  0.63327293]. \t  -3.356951636991144 \t -0.0316280354659526\n",
            "63     \t [-0.68900583 -0.93015724]. \t  -200.2232850113495 \t -0.0316280354659526\n",
            "64     \t [-1.9511115   1.59976625]. \t  -495.82478171662814 \t -0.0316280354659526\n",
            "65     \t [0.1433744  0.57671401]. \t  -31.66495620399632 \t -0.0316280354659526\n",
            "66     \t [1.83795572 3.98961035]. \t  -38.098957618978545 \t -0.0316280354659526\n",
            "67     \t [1.06085995 3.60827466]. \t  -616.4585280330157 \t -0.0316280354659526\n",
            "68     \t [0.1155818 0.9244811]. \t  -83.79651624949977 \t -0.0316280354659526\n",
            "69     \t [0.9302396  0.89922877]. \t  -0.11967264793471892 \t -0.0316280354659526\n",
            "70     \t [2.17705684 4.63256773]. \t  -2.53054968818815 \t -0.0316280354659526\n",
            "71     \t [-0.60496585  0.88407464]. \t  -29.41773966484951 \t -0.0316280354659526\n",
            "72     \t [-1.03864479  1.00783317]. \t  -4.659460335256755 \t -0.0316280354659526\n",
            "73     \t [0.81149774 0.67475577]. \t  -0.06186525973497762 \t -0.0316280354659526\n",
            "74     \t [0.82571521 0.72114958]. \t  -0.18517002918348852 \t -0.0316280354659526\n",
            "75     \t [ 1.60969256 -1.00479884]. \t  -1293.4278517665257 \t -0.0316280354659526\n",
            "76     \t [0.33941695 0.41635123]. \t  -9.50534310900239 \t -0.0316280354659526\n",
            "77     \t [1.43984719 1.69352848]. \t  -14.605468984402297 \t -0.0316280354659526\n",
            "78     \t [1.74876814 1.68137527]. \t  -190.12253463601016 \t -0.0316280354659526\n",
            "79     \t [0.59607989 0.36541864]. \t  -0.17336742519789472 \t -0.0316280354659526\n",
            "80     \t [0.68103099 0.44398436]. \t  -0.14101994206855661 \t -0.0316280354659526\n",
            "81     \t [0.77102632 0.77151141]. \t  -3.1863844894260773 \t -0.0316280354659526\n",
            "82     \t [0.53008699 0.22216843]. \t  -0.5668420168764012 \t -0.0316280354659526\n",
            "83     \t [ 0.15937131 -1.33503194]. \t  -185.78394798982714 \t -0.0316280354659526\n",
            "84     \t [1.01586111 1.01434984]. \t  \u001b[92m-0.03131192845511335\u001b[0m \t -0.03131192845511335\n",
            "85     \t [1.78889777 0.96399803]. \t  -500.6622668173832 \t -0.03131192845511335\n",
            "86     \t [ 0.62086675 -1.99945441]. \t  -568.9328188065471 \t -0.03131192845511335\n",
            "87     \t [-1.99508571  1.16711341]. \t  -800.410112024754 \t -0.03131192845511335\n",
            "88     \t [0.90655456 0.85412588]. \t  -0.11296224057320409 \t -0.03131192845511335\n",
            "89     \t [-0.75829718 -1.78029594]. \t  -557.8403897443384 \t -0.03131192845511335\n",
            "90     \t [ 1.26053213 -0.50401093]. \t  -438.1127623760935 \t -0.03131192845511335\n",
            "91     \t [2.04206484 1.39503674]. \t  -771.1439943589678 \t -0.03131192845511335\n",
            "92     \t [ 1.1015313  -0.52844185]. \t  -303.40158362456754 \t -0.03131192845511335\n",
            "93     \t [1.11703424 1.1815487 ]. \t  -0.4521633044595778 \t -0.03131192845511335\n",
            "94     \t [0.97817863 0.96346351]. \t  \u001b[92m-0.0048719716429606475\u001b[0m \t -0.0048719716429606475\n",
            "95     \t [-0.26471549 -0.51119417]. \t  -35.38680766315234 \t -0.0048719716429606475\n",
            "96     \t [ 1.35705638 -0.0890798 ]. \t  -372.880721250297 \t -0.0048719716429606475\n",
            "97     \t [0.79248782 0.00551502]. \t  -38.79641589678887 \t -0.0048719716429606475\n",
            "98     \t [-1.22520577  1.49539825]. \t  -4.954825042362605 \t -0.0048719716429606475\n",
            "99     \t [-0.26276442  1.13854958]. \t  -115.97854910800217 \t -0.0048719716429606475\n",
            "100    \t [ 1.28981407 -0.97803313]. \t  -697.9172874987738 \t -0.0048719716429606475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH76hlE2tddm",
        "outputId": "3423ba85-a409-4cc2-88f4-173a8ddbb4c6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_loser_5 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_5 = d2GPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.23491901 0.17105153]. \t  -1.9278091788796494 \t -1.9278091788796494\n",
            "init   \t [ 0.24749582 -0.48313101]. \t  -30.201785940713883 \t -1.9278091788796494\n",
            "init   \t [-1.56019024  1.41721238]. \t  -109.97965348801837 \t -1.9278091788796494\n",
            "init   \t [0.98198164 1.77093658]. \t  -65.06852657321838 \t -1.9278091788796494\n",
            "init   \t [1.47180752 0.85239655]. \t  -172.83511538815276 \t -1.9278091788796494\n",
            "1      \t [-0.26305134  0.52527889]. \t  -22.396458381172856 \t -1.9278091788796494\n",
            "2      \t [-2.0143131  -1.31251332]. \t  -2892.7444960501225 \t -1.9278091788796494\n",
            "3      \t [ 0.99390592 -1.25402384]. \t  -502.5994088777894 \t -1.9278091788796494\n",
            "4      \t [-0.07196585  0.07270944]. \t  \u001b[92m-1.6051456622453808\u001b[0m \t -1.6051456622453808\n",
            "5      \t [-0.53202063  1.9093367 ]. \t  -266.82924569916423 \t -1.6051456622453808\n",
            "6      \t [ 0.19547418 -0.20209885]. \t  -6.422103437492146 \t -1.6051456622453808\n",
            "7      \t [0.1821193  0.56549208]. \t  -29.005881434771503 \t -1.6051456622453808\n",
            "8      \t [1.90153703 1.90786623]. \t  -292.5312582941719 \t -1.6051456622453808\n",
            "9      \t [0.0734613  0.04404317]. \t  \u001b[92m-1.0078300177523136\u001b[0m \t -1.0078300177523136\n",
            "10     \t [0.6743064 0.3535177]. \t  -1.129642156271495 \t -1.0078300177523136\n",
            "11     \t [-0.34021369  0.21351898]. \t  -2.752140999584523 \t -1.0078300177523136\n",
            "12     \t [ 0.54762529 -0.19125539]. \t  -24.327362249039442 \t -1.0078300177523136\n",
            "13     \t [-1.07397932  2.01020652]. \t  -77.70771821226292 \t -1.0078300177523136\n",
            "14     \t [0.5134028  0.26269689]. \t  \u001b[92m-0.2368552504730209\u001b[0m \t -0.2368552504730209\n",
            "15     \t [-2.0041177   1.87863332]. \t  -466.06688216037895 \t -0.2368552504730209\n",
            "16     \t [-1.20400563  1.61743151]. \t  -7.673390676574341 \t -0.2368552504730209\n",
            "17     \t [-1.17769894  1.3811026 ]. \t  -4.745820924115678 \t -0.2368552504730209\n",
            "18     \t [-1.56321837  0.0803017 ]. \t  -565.1123929380714 \t -0.2368552504730209\n",
            "19     \t [-0.17517263  0.69141474]. \t  -45.03734983140083 \t -0.2368552504730209\n",
            "20     \t [0.60150756 0.31001185]. \t  -0.42711499888294424 \t -0.2368552504730209\n",
            "21     \t [ 0.43346255 -0.75667882]. \t  -89.54194982904822 \t -0.2368552504730209\n",
            "22     \t [ 0.91814057 -0.34450911]. \t  -141.02024019634268 \t -0.2368552504730209\n",
            "23     \t [0.59587717 0.26728552]. \t  -0.9339197665094424 \t -0.2368552504730209\n",
            "24     \t [ 0.81500145 -0.12707556]. \t  -62.650256265456946 \t -0.2368552504730209\n",
            "25     \t [-1.33177649 -1.0812734 ]. \t  -820.4837430337147 \t -0.2368552504730209\n",
            "26     \t [-0.43014507 -2.29003677]. \t  -614.6382847583409 \t -0.2368552504730209\n",
            "27     \t [-0.60415394 -0.74063953]. \t  -124.81762510119327 \t -0.2368552504730209\n",
            "28     \t [ 0.87329433 -0.30051235]. \t  -113.04598083597291 \t -0.2368552504730209\n",
            "29     \t [-0.64761208 -0.63941521]. \t  -114.82388800391946 \t -0.2368552504730209\n",
            "30     \t [-1.47691244 -1.57602495]. \t  -1417.861890443338 \t -0.2368552504730209\n",
            "31     \t [ 0.4327339  -1.35970691]. \t  -239.6320259130426 \t -0.2368552504730209\n",
            "32     \t [-1.18032486 -1.07964242]. \t  -616.2323504683599 \t -0.2368552504730209\n",
            "33     \t [0.83320423 0.78043435]. \t  -0.7709521183300825 \t -0.2368552504730209\n",
            "34     \t [1.09830144 1.56154684]. \t  -12.632106730370626 \t -0.2368552504730209\n",
            "35     \t [-1.67606772 -0.89015035]. \t  -1375.682863433497 \t -0.2368552504730209\n",
            "36     \t [1.01742684 1.1535825 ]. \t  -1.4027546256230963 \t -0.2368552504730209\n",
            "37     \t [-1.67958539  0.41857656]. \t  -584.3474178954291 \t -0.2368552504730209\n",
            "38     \t [-0.75365745 -1.90976445]. \t  -617.0067622668932 \t -0.2368552504730209\n",
            "39     \t [-0.82794695 -1.66106782]. \t  -553.9776385316743 \t -0.2368552504730209\n",
            "40     \t [0.59473711 0.31057476]. \t  -0.35032214577757537 \t -0.2368552504730209\n",
            "41     \t [-1.90053733 -1.91620189]. \t  -3064.5613349294117 \t -0.2368552504730209\n",
            "42     \t [-1.62986014  0.30449881]. \t  -560.0808238757978 \t -0.2368552504730209\n",
            "43     \t [-0.11340036  0.017187  ]. \t  -1.2415329644255486 \t -0.2368552504730209\n",
            "44     \t [-1.4610998   1.76453335]. \t  -19.767685852850942 \t -0.2368552504730209\n",
            "45     \t [-0.25640987  0.21172911]. \t  -3.7096718553017576 \t -0.2368552504730209\n",
            "46     \t [ 0.15915991 -0.78903105]. \t  -67.02570961865136 \t -0.2368552504730209\n",
            "47     \t [-1.5604199  -1.26951857]. \t  -1378.8350388257602 \t -0.2368552504730209\n",
            "48     \t [0.47897968 0.23845816]. \t  -0.27962824284233395 \t -0.2368552504730209\n",
            "49     \t [-0.12274723  1.13054738]. \t  -125.69023654816054 \t -0.2368552504730209\n",
            "50     \t [0.49470264 0.26641495]. \t  -0.3023461028942937 \t -0.2368552504730209\n",
            "51     \t [-0.15643341 -0.63785765]. \t  -45.20531701437525 \t -0.2368552504730209\n",
            "52     \t [-1.26879899 -0.26761964]. \t  -357.63700430421267 \t -0.2368552504730209\n",
            "53     \t [-0.5858811   0.26979212]. \t  -3.0547227988076906 \t -0.2368552504730209\n",
            "54     \t [0.49963123 0.23086254]. \t  -0.2855958144241081 \t -0.2368552504730209\n",
            "55     \t [ 0.82248618 -1.04625839]. \t  -296.8154796675672 \t -0.2368552504730209\n",
            "56     \t [-0.64689927  1.63209789]. \t  -149.9994387123553 \t -0.2368552504730209\n",
            "57     \t [0.49053467 0.21234673]. \t  -0.3395167970269144 \t -0.2368552504730209\n",
            "58     \t [-1.45316867 -0.38311089]. \t  -628.4257640568197 \t -0.2368552504730209\n",
            "59     \t [1.58170114 0.41906204]. \t  -434.1091590560216 \t -0.2368552504730209\n",
            "60     \t [1.03662994 0.97811476]. \t  -0.9323134917241241 \t -0.2368552504730209\n",
            "61     \t [-0.81535936  0.58043406]. \t  -4.007474413946899 \t -0.2368552504730209\n",
            "62     \t [-0.97376053 -1.4571699 ]. \t  -582.480767503922 \t -0.2368552504730209\n",
            "63     \t [-0.61116527 -1.07885359]. \t  -213.53562625344438 \t -0.2368552504730209\n",
            "64     \t [0.83871275 0.71732964]. \t  \u001b[92m-0.0453083360267387\u001b[0m \t -0.0453083360267387\n",
            "65     \t [0.77624217 0.60127275]. \t  -0.050231189568076255 \t -0.0453083360267387\n",
            "66     \t [0.99036749 0.33968709]. \t  -41.10622914377682 \t -0.0453083360267387\n",
            "67     \t [0.71829433 0.51397338]. \t  -0.07974750030707736 \t -0.0453083360267387\n",
            "68     \t [1.18565431 0.24203744]. \t  -135.46324582697912 \t -0.0453083360267387\n",
            "69     \t [1.10271346 1.1649134 ]. \t  -0.27129901041038773 \t -0.0453083360267387\n",
            "70     \t [-1.63445622 -1.84554485]. \t  -2047.2620122828723 \t -0.0453083360267387\n",
            "71     \t [ 1.47354817 -0.45096802]. \t  -687.8763878924361 \t -0.0453083360267387\n",
            "72     \t [ 0.51006207 -0.81553677]. \t  -115.9531049663765 \t -0.0453083360267387\n",
            "73     \t [-1.70649306  0.19121674]. \t  -747.6557790250947 \t -0.0453083360267387\n",
            "74     \t [1.13872968 1.32783194]. \t  -0.11613274787740212 \t -0.0453083360267387\n",
            "75     \t [-1.52326452 -0.02977024]. \t  -558.6662390049898 \t -0.0453083360267387\n",
            "76     \t [ 0.25685491 -1.73171831]. \t  -323.72218821205206 \t -0.0453083360267387\n",
            "77     \t [1.95511732 1.8879273 ]. \t  -375.16310905375974 \t -0.0453083360267387\n",
            "78     \t [1.49630918 1.91027544]. \t  -11.048438675077728 \t -0.0453083360267387\n",
            "79     \t [-1.16481664  1.87316636]. \t  -31.35007917044794 \t -0.0453083360267387\n",
            "80     \t [ 0.20382741 -0.87757499]. \t  -85.11215914353261 \t -0.0453083360267387\n",
            "81     \t [1.08945152 1.21870873]. \t  -0.10915175792669088 \t -0.0453083360267387\n",
            "82     \t [-1.48490288  1.04258879]. \t  -141.27997684480601 \t -0.0453083360267387\n",
            "83     \t [0.39968827 1.86533819]. \t  -291.263238083348 \t -0.0453083360267387\n",
            "84     \t [-0.14592709  1.77647586]. \t  -309.37923450719995 \t -0.0453083360267387\n",
            "85     \t [-0.93470523  0.42585779]. \t  -23.79700799426778 \t -0.0453083360267387\n",
            "86     \t [0.525904   0.88949121]. \t  -37.79139283682128 \t -0.0453083360267387\n",
            "87     \t [-1.57565981  0.39355628]. \t  -443.08777565153105 \t -0.0453083360267387\n",
            "88     \t [ 0.32296706 -1.50958795]. \t  -260.9242971549489 \t -0.0453083360267387\n",
            "89     \t [ 1.48541484 -0.86606207]. \t  -944.273132645884 \t -0.0453083360267387\n",
            "90     \t [1.09513682 1.16603463]. \t  -0.11987352626725699 \t -0.0453083360267387\n",
            "91     \t [ 1.87252561 -0.1611803 ]. \t  -1345.8407490325253 \t -0.0453083360267387\n",
            "92     \t [1.06391531 1.16313568]. \t  -0.10155338780814412 \t -0.0453083360267387\n",
            "93     \t [2.73195842 3.25200592]. \t  -1776.749446186191 \t -0.0453083360267387\n",
            "94     \t [0.92587285 0.83305454]. \t  -0.06399105243606627 \t -0.0453083360267387\n",
            "95     \t [ 1.7290815  -1.47455581]. \t  -1993.509957799454 \t -0.0453083360267387\n",
            "96     \t [1.62807101 0.40241991]. \t  -505.8326830375496 \t -0.0453083360267387\n",
            "97     \t [ 1.8800602  -0.87843687]. \t  -1948.2872100578213 \t -0.0453083360267387\n",
            "98     \t [-1.81468614  1.5420402 ]. \t  -314.5385193068393 \t -0.0453083360267387\n",
            "99     \t [-0.74573766  0.83727171]. \t  -10.951966768460675 \t -0.0453083360267387\n",
            "100    \t [-0.72720043 -1.80539374]. \t  -547.8388160624896 \t -0.0453083360267387\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh_Amb8TtdgO",
        "outputId": "679b6b15-dda8-462e-e7f5-5039238d9c14"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_loser_6 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_6 = d2GPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.60915518 -0.68821072]. \t  -1074.6314195925434 \t -3.0269049669752817\n",
            "init   \t [ 1.31575449 -1.87721062]. \t  -1302.169546932896 \t -3.0269049669752817\n",
            "init   \t [-1.60703824  0.38933325]. \t  -487.8262244570432 \t -3.0269049669752817\n",
            "init   \t [ 0.12213192 -0.33256477]. \t  -12.844955340992902 \t -3.0269049669752817\n",
            "init   \t [-0.67416945  0.50183959]. \t  -3.0269049669752817 \t -3.0269049669752817\n",
            "1      \t [-0.06756934  0.5044222 ]. \t  -26.125364719936726 \t -3.0269049669752817\n",
            "2      \t [-0.59581319 -0.51645495]. \t  -78.48883417844245 \t -3.0269049669752817\n",
            "3      \t [-0.21403066  0.06785647]. \t  \u001b[92m-1.522479008471028\u001b[0m \t -1.522479008471028\n",
            "4      \t [-0.59379378  1.76072913]. \t  -200.82546294985917 \t -1.522479008471028\n",
            "5      \t [-0.27273802  0.67394355]. \t  -37.56678420682764 \t -1.522479008471028\n",
            "6      \t [0.19834264 0.0774736 ]. \t  \u001b[92m-0.7880732058879137\u001b[0m \t -0.7880732058879137\n",
            "7      \t [1.55447224 1.39475231]. \t  -104.6805577806699 \t -0.7880732058879137\n",
            "8      \t [-0.69397195 -1.65661098]. \t  -460.06290670914524 \t -0.7880732058879137\n",
            "9      \t [0.60042579 0.50870782]. \t  -2.3558855274538892 \t -0.7880732058879137\n",
            "10     \t [ 1.62127626 -1.6671004 ]. \t  -1845.635789466035 \t -0.7880732058879137\n",
            "11     \t [0.76717462 0.31531899]. \t  -7.520103373107979 \t -0.7880732058879137\n",
            "12     \t [-0.62194866  0.32952288]. \t  -2.9590149731347335 \t -0.7880732058879137\n",
            "13     \t [0.5179572 0.3834948]. \t  -1.5598180090753493 \t -0.7880732058879137\n",
            "14     \t [-0.7829217  -0.95478987]. \t  -248.9647789528599 \t -0.7880732058879137\n",
            "15     \t [0.79208752 0.54493302]. \t  \u001b[92m-0.7233512959305186\u001b[0m \t -0.7233512959305186\n",
            "16     \t [1.17722931 0.04356452]. \t  -180.20950318768865 \t -0.7233512959305186\n",
            "17     \t [0.26282348 0.02154315]. \t  -0.7693680794170482 \t -0.7233512959305186\n",
            "18     \t [-0.22311957  0.67483924]. \t  -40.565634442430984 \t -0.7233512959305186\n",
            "19     \t [-0.23267667 -1.00058447]. \t  -112.76353295205944 \t -0.7233512959305186\n",
            "20     \t [0.34600884 1.45306496]. \t  -178.20801948167232 \t -0.7233512959305186\n",
            "21     \t [-0.411375    0.25813474]. \t  -2.782395382470321 \t -0.7233512959305186\n",
            "22     \t [-0.02091457 -0.09066323]. \t  -1.8721994257420942 \t -0.7233512959305186\n",
            "23     \t [0.01844167 0.44839901]. \t  -21.039136270192785 \t -0.7233512959305186\n",
            "24     \t [ 1.22131312 -1.63164916]. \t  -975.5210941657798 \t -0.7233512959305186\n",
            "25     \t [0.66937659 0.45736443]. \t  \u001b[92m-0.11795973730971027\u001b[0m \t -0.11795973730971027\n",
            "26     \t [0.65709915 0.42449169]. \t  -0.12289190937165485 \t -0.11795973730971027\n",
            "27     \t [ 0.34297426 -1.7788984 ]. \t  -360.1141872460066 \t -0.11795973730971027\n",
            "28     \t [0.10568514 0.11910562]. \t  -1.9648230673134135 \t -0.11795973730971027\n",
            "29     \t [-1.16042126 -0.48243   ]. \t  -339.1942622308875 \t -0.11795973730971027\n",
            "30     \t [ 0.70648111 -1.04731814]. \t  -239.23187114732363 \t -0.11795973730971027\n",
            "31     \t [ 0.48119433 -0.05490514]. \t  -8.47469889811514 \t -0.11795973730971027\n",
            "32     \t [-1.74169083  1.27475939]. \t  -316.8291374869721 \t -0.11795973730971027\n",
            "33     \t [1.7123502  0.67173204]. \t  -511.4533086722718 \t -0.11795973730971027\n",
            "34     \t [1.30986107 2.01562808]. \t  -9.089538358042942 \t -0.11795973730971027\n",
            "35     \t [1.02701608 1.37227186]. \t  -10.081979447543015 \t -0.11795973730971027\n",
            "36     \t [0.94326674 0.84868926]. \t  -0.17183476954902127 \t -0.11795973730971027\n",
            "37     \t [1.56497463 2.04310239]. \t  -16.806303238786754 \t -0.11795973730971027\n",
            "38     \t [0.71253773 0.50293746]. \t  \u001b[92m-0.08491228159486391\u001b[0m \t -0.08491228159486391\n",
            "39     \t [ 1.21764807 -1.67881643]. \t  -999.5450123676623 \t -0.08491228159486391\n",
            "40     \t [ 0.32029479 -1.97746247]. \t  -433.1233074488358 \t -0.08491228159486391\n",
            "41     \t [0.24322351 0.69817329]. \t  -41.40680608386912 \t -0.08491228159486391\n",
            "42     \t [2.01611511 0.40900302]. \t  -1337.4592511891349 \t -0.08491228159486391\n",
            "43     \t [-0.42277262  0.56208339]. \t  -16.719750907684208 \t -0.08491228159486391\n",
            "44     \t [ 1.58689748 -1.98885517]. \t  -2031.7383830266047 \t -0.08491228159486391\n",
            "45     \t [-0.06960103  0.39329284]. \t  -16.233272792355127 \t -0.08491228159486391\n",
            "46     \t [ 1.48293339 -1.80569008]. \t  -1604.0607230964827 \t -0.08491228159486391\n",
            "47     \t [1.1860111  0.69200666]. \t  -51.10215552529505 \t -0.08491228159486391\n",
            "48     \t [ 1.09019811 -0.85902057]. \t  -419.2552535068078 \t -0.08491228159486391\n",
            "49     \t [0.07221105 0.65215451]. \t  -42.71393786596149 \t -0.08491228159486391\n",
            "50     \t [-1.06352219 -0.92286626]. \t  -426.12742213244945 \t -0.08491228159486391\n",
            "51     \t [-0.82692897  0.95904257]. \t  -10.912882895944993 \t -0.08491228159486391\n",
            "52     \t [-0.93002234 -0.66266491]. \t  -237.0831376175313 \t -0.08491228159486391\n",
            "53     \t [-0.14395639  0.83083395]. \t  -66.93653951463152 \t -0.08491228159486391\n",
            "54     \t [ 1.64283333 -0.54736037]. \t  -1054.234750181094 \t -0.08491228159486391\n",
            "55     \t [ 1.26456752 -1.16271586]. \t  -762.8498168433456 \t -0.08491228159486391\n",
            "56     \t [-1.31227773  2.03619975]. \t  -15.214200315819033 \t -0.08491228159486391\n",
            "57     \t [-0.80713641 -0.74818458]. \t  -199.16880468917404 \t -0.08491228159486391\n",
            "58     \t [0.65318575 1.34301448]. \t  -84.09236747648043 \t -0.08491228159486391\n",
            "59     \t [ 1.18645898 -0.35980089]. \t  -312.43536793777577 \t -0.08491228159486391\n",
            "60     \t [-1.34258878 -1.30591657]. \t  -971.7408229441094 \t -0.08491228159486391\n",
            "61     \t [1.41971308 1.58634581]. \t  -18.60080807169307 \t -0.08491228159486391\n",
            "62     \t [-0.00653096  1.4474278 ]. \t  -210.50548033462462 \t -0.08491228159486391\n",
            "63     \t [1.28816371 1.64764937]. \t  -0.09676569284381095 \t -0.08491228159486391\n",
            "64     \t [-1.59878125  2.0132772 ]. \t  -36.219485364696766 \t -0.08491228159486391\n",
            "65     \t [-1.91902188 -1.03299139]. \t  -2232.243308095427 \t -0.08491228159486391\n",
            "66     \t [1.37437738 1.79664855]. \t  -0.9914345262741188 \t -0.08491228159486391\n",
            "67     \t [0.48556553 0.58101052]. \t  -12.183476168587521 \t -0.08491228159486391\n",
            "68     \t [-0.55155062  0.27051137]. \t  -2.520856175085368 \t -0.08491228159486391\n",
            "69     \t [-1.28137897 -1.19756858]. \t  -811.4810884152644 \t -0.08491228159486391\n",
            "70     \t [1.78871846 1.50721248]. \t  -287.01042744204386 \t -0.08491228159486391\n",
            "71     \t [-1.52580274 -1.09844367]. \t  -1180.4820266939364 \t -0.08491228159486391\n",
            "72     \t [1.74924623 1.41644118]. \t  -270.64469116755384 \t -0.08491228159486391\n",
            "73     \t [ 0.26996934 -0.00881265]. \t  -1.2003699574098554 \t -0.08491228159486391\n",
            "74     \t [ 0.39624454 -1.5080447 ]. \t  -277.60514967002473 \t -0.08491228159486391\n",
            "75     \t [-1.13532691  1.12907122]. \t  -7.11629317883051 \t -0.08491228159486391\n",
            "76     \t [-1.34554228  1.70428807]. \t  -6.629327038494695 \t -0.08491228159486391\n",
            "77     \t [1.31389352 1.69902415]. \t  -0.17301461096386167 \t -0.08491228159486391\n",
            "78     \t [ 0.45875963 -1.83421845]. \t  -418.36409948972675 \t -0.08491228159486391\n",
            "79     \t [-0.54559481 -2.02115196]. \t  -540.0841043685225 \t -0.08491228159486391\n",
            "80     \t [-0.14442587  1.30848992]. \t  -167.10909260384756 \t -0.08491228159486391\n",
            "81     \t [1.16813263 0.97525035]. \t  -15.182431900590537 \t -0.08491228159486391\n",
            "82     \t [-1.66291297  0.60205752]. \t  -475.0440655718367 \t -0.08491228159486391\n",
            "83     \t [0.66975161 0.45602927]. \t  -0.11463222060463235 \t -0.08491228159486391\n",
            "84     \t [1.19608754 1.36726084]. \t  -0.43995727016533925 \t -0.08491228159486391\n",
            "85     \t [-0.40389518 -1.25094207]. \t  -201.93127424599834 \t -0.08491228159486391\n",
            "86     \t [1.49181292 1.1552911 ]. \t  -114.77783114989622 \t -0.08491228159486391\n",
            "87     \t [1.43558455 2.02005754]. \t  -0.35656903643962984 \t -0.08491228159486391\n",
            "88     \t [-0.74225263  0.62892851]. \t  -3.643681056909558 \t -0.08491228159486391\n",
            "89     \t [0.62204972 0.36654171]. \t  -0.18447931589427974 \t -0.08491228159486391\n",
            "90     \t [-0.80526177 -0.21591912]. \t  -77.97176485445083 \t -0.08491228159486391\n",
            "91     \t [-1.68156932 -1.93210104]. \t  -2272.737961281629 \t -0.08491228159486391\n",
            "92     \t [ 1.55930984 -1.22208842]. \t  -1335.1450532415909 \t -0.08491228159486391\n",
            "93     \t [-0.73903477  0.13573331]. \t  -19.87026636959404 \t -0.08491228159486391\n",
            "94     \t [1.18306743 1.43030098]. \t  -0.12747094498765982 \t -0.08491228159486391\n",
            "95     \t [1.34555748 1.79746949]. \t  -0.13645445664675504 \t -0.08491228159486391\n",
            "96     \t [-1.80615082 -1.30792707]. \t  -2096.4630705644536 \t -0.08491228159486391\n",
            "97     \t [-1.51158698 -1.05923967]. \t  -1124.6318739826365 \t -0.08491228159486391\n",
            "98     \t [-1.21562782 -1.17840815]. \t  -710.4271513891267 \t -0.08491228159486391\n",
            "99     \t [-0.62834832  1.35196072]. \t  -94.26304497966618 \t -0.08491228159486391\n",
            "100    \t [1.16802336 1.32788128]. \t  -0.16070816763035162 \t -0.08491228159486391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNmoDTuUtdi3",
        "outputId": "f81f648b-42f3-4084-ee4e-8d633977a4dd"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_loser_7 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_7 = d2GPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.42268934 -0.80954733]. \t  -808.7939502616841 \t -2.0077595729598063\n",
            "init   \t [-1.79389885 -0.16441204]. \t  -1151.9264213711547 \t -2.0077595729598063\n",
            "init   \t [1.37319786 1.74897991]. \t  -2.0077595729598063 \t -2.0077595729598063\n",
            "init   \t [0.92974688 1.09976052]. \t  -5.543015908052858 \t -2.0077595729598063\n",
            "init   \t [-0.94533605  0.58994398]. \t  -13.008689168416776 \t -2.0077595729598063\n",
            "1      \t [-0.59759223  0.9308147 ]. \t  -35.46526561710063 \t -2.0077595729598063\n",
            "2      \t [-0.0058376  -0.35079779]. \t  -13.32000895935528 \t -2.0077595729598063\n",
            "3      \t [1.76201881 0.51049049]. \t  -673.5783036707471 \t -2.0077595729598063\n",
            "4      \t [-0.27048104  0.34323078]. \t  -8.907945256304291 \t -2.0077595729598063\n",
            "5      \t [1.06374379 1.49601535]. \t  -13.287499901069403 \t -2.0077595729598063\n",
            "6      \t [ 0.15581952 -1.8339356 ]. \t  -346.0090608303989 \t -2.0077595729598063\n",
            "7      \t [-1.24563186  1.17574742]. \t  -19.16928354355723 \t -2.0077595729598063\n",
            "8      \t [1.95674177 1.98388934]. \t  -341.2990453474088 \t -2.0077595729598063\n",
            "9      \t [0.51624661 0.0680271 ]. \t  -4.173585918485305 \t -2.0077595729598063\n",
            "10     \t [-0.8378277   0.68657241]. \t  -3.4012738508923444 \t -2.0077595729598063\n",
            "11     \t [0.33874999 0.54871364]. \t  -19.2695610751511 \t -2.0077595729598063\n",
            "12     \t [-1.16571307  2.02626115]. \t  -49.229144107198465 \t -2.0077595729598063\n",
            "13     \t [-0.47201161  0.0977218 ]. \t  -3.7311476132522454 \t -2.0077595729598063\n",
            "14     \t [0.02798137 0.09430709]. \t  \u001b[92m-1.8194965119198905\u001b[0m \t -1.8194965119198905\n",
            "15     \t [1.23803088 1.64253822]. \t  \u001b[92m-1.262652709411629\u001b[0m \t -1.262652709411629\n",
            "16     \t [1.14535763 2.01738997]. \t  -49.80062490669945 \t -1.262652709411629\n",
            "17     \t [ 0.24371301 -0.26863165]. \t  -11.332186000814197 \t -1.262652709411629\n",
            "18     \t [1.42863203 1.91312377]. \t  -1.8186894093825778 \t -1.262652709411629\n",
            "19     \t [-0.64859781  0.45281372]. \t  -2.8211380162944386 \t -1.262652709411629\n",
            "20     \t [ 0.02751354 -0.11254956]. \t  -2.2295673767103628 \t -1.262652709411629\n",
            "21     \t [1.29938909 1.67758776]. \t  \u001b[92m-0.10135025223940512\u001b[0m \t -0.10135025223940512\n",
            "22     \t [-0.1101198  -0.01429359]. \t  -1.3021673869637564 \t -0.10135025223940512\n",
            "23     \t [0.42247463 0.1024085 ]. \t  -0.9122961668477317 \t -0.10135025223940512\n",
            "24     \t [1.30589016 0.52928269]. \t  -138.40678783944495 \t -0.10135025223940512\n",
            "25     \t [0.9330717  0.56596293]. \t  -9.28624341657171 \t -0.10135025223940512\n",
            "26     \t [ 0.5899706  -1.56765579]. \t  -367.1668559668683 \t -0.10135025223940512\n",
            "27     \t [-1.51502555 -0.57082521]. \t  -827.7941113195253 \t -0.10135025223940512\n",
            "28     \t [-1.87469876 -0.93454369]. \t  -1987.6588142819153 \t -0.10135025223940512\n",
            "29     \t [-1.73811005  2.00823053]. \t  -110.07282670345559 \t -0.10135025223940512\n",
            "30     \t [-1.93949334  1.69699813]. \t  -434.9129180923196 \t -0.10135025223940512\n",
            "31     \t [-1.14666624  1.50980024]. \t  -8.4089901439262 \t -0.10135025223940512\n",
            "32     \t [ 1.54002649 -1.83787344]. \t  -1772.3269971951645 \t -0.10135025223940512\n",
            "33     \t [-1.8431759  -1.77951983]. \t  -2688.0272945478355 \t -0.10135025223940512\n",
            "34     \t [1.31530313 1.74904088]. \t  -0.13558661457542254 \t -0.10135025223940512\n",
            "35     \t [1.38158823 1.95906057]. \t  -0.39836238741930424 \t -0.10135025223940512\n",
            "36     \t [-0.63864392  1.52983507]. \t  -128.5665997916627 \t -0.10135025223940512\n",
            "37     \t [ 0.14065466 -0.02402529]. \t  -0.9303974829919991 \t -0.10135025223940512\n",
            "38     \t [-1.76261228  1.06688063]. \t  -423.75995997652166 \t -0.10135025223940512\n",
            "39     \t [-1.72517692 -0.55026939]. \t  -1251.0501918294053 \t -0.10135025223940512\n",
            "40     \t [0.05564954 0.60649055]. \t  -37.300190512549484 \t -0.10135025223940512\n",
            "41     \t [-1.82254402  1.35506658]. \t  -394.71835903777065 \t -0.10135025223940512\n",
            "42     \t [1.4493702  1.88796319]. \t  -4.72652160481193 \t -0.10135025223940512\n",
            "43     \t [0.8199031  1.16600161]. \t  -24.41237951303199 \t -0.10135025223940512\n",
            "44     \t [-0.86486021  0.97471617]. \t  -8.61848851357872 \t -0.10135025223940512\n",
            "45     \t [1.34803695 0.03901215]. \t  -316.3176151563217 \t -0.10135025223940512\n",
            "46     \t [1.27246343 1.60397822]. \t  \u001b[92m-0.09729457969826462\u001b[0m \t -0.09729457969826462\n",
            "47     \t [ 0.83852599 -0.34052251]. \t  -108.9462605973516 \t -0.09729457969826462\n",
            "48     \t [ 1.28403975 -1.80099872]. \t  -1190.162876097512 \t -0.09729457969826462\n",
            "49     \t [1.02569276 1.1038686 ]. \t  -0.2692219984662732 \t -0.09729457969826462\n",
            "50     \t [-0.34422207  1.24450668]. \t  -128.59855232461769 \t -0.09729457969826462\n",
            "51     \t [-1.01553322 -0.07176961]. \t  -125.74033204142796 \t -0.09729457969826462\n",
            "52     \t [-1.41605369  2.04337698]. \t  -5.983002071749153 \t -0.09729457969826462\n",
            "53     \t [1.27684505 0.33886227]. \t  -166.86638213510824 \t -0.09729457969826462\n",
            "54     \t [ 1.95242307 -1.48669605]. \t  -2808.478305233238 \t -0.09729457969826462\n",
            "55     \t [ 0.40544191 -1.72069599]. \t  -355.7058359961106 \t -0.09729457969826462\n",
            "56     \t [ 0.96780316 -0.78044245]. \t  -294.8392690826092 \t -0.09729457969826462\n",
            "57     \t [-0.46836524  0.05583377]. \t  -4.830375481641471 \t -0.09729457969826462\n",
            "58     \t [ 0.99548805 -1.06068493]. \t  -420.93966873039165 \t -0.09729457969826462\n",
            "59     \t [-1.23998148 -0.56124367]. \t  -445.5127150577988 \t -0.09729457969826462\n",
            "60     \t [-0.48624993  1.60285367]. \t  -188.91784469060016 \t -0.09729457969826462\n",
            "61     \t [ 1.7078212  -0.23497913]. \t  -993.779676556309 \t -0.09729457969826462\n",
            "62     \t [1.93673741 0.97984521]. \t  -768.7806515093562 \t -0.09729457969826462\n",
            "63     \t [ 1.97666469 -0.70501524]. \t  -2128.209855573222 \t -0.09729457969826462\n",
            "64     \t [ 0.76668352 -0.00948033]. \t  -35.7292483226988 \t -0.09729457969826462\n",
            "65     \t [0.97108911 0.96030652]. \t  \u001b[92m-0.030738761196914842\u001b[0m \t -0.030738761196914842\n",
            "66     \t [0.9893409 0.9655526]. \t  \u001b[92m-0.017650821030332026\u001b[0m \t -0.017650821030332026\n",
            "67     \t [ 1.29866214 -0.78551704]. \t  -611.1875774241657 \t -0.017650821030332026\n",
            "68     \t [1.05159626 1.0477697 ]. \t  -0.34004878839789376 \t -0.017650821030332026\n",
            "69     \t [1.37400725 1.91437731]. \t  -0.2100078781757343 \t -0.017650821030332026\n",
            "70     \t [-1.453335    0.70777001]. \t  -203.2563341481985 \t -0.017650821030332026\n",
            "71     \t [0.92628316 0.87905167]. \t  -0.0497493904416827 \t -0.017650821030332026\n",
            "72     \t [-0.12450811 -1.3949929 ]. \t  -200.21418176685694 \t -0.017650821030332026\n",
            "73     \t [-0.93108572  1.6070963 ]. \t  -58.51509767547627 \t -0.017650821030332026\n",
            "74     \t [1.32791237 1.75857392]. \t  -0.10980881641314008 \t -0.017650821030332026\n",
            "75     \t [-0.62409191 -0.74202643]. \t  -130.6707793379426 \t -0.017650821030332026\n",
            "76     \t [ 1.84142169 -0.68915833]. \t  -1665.3415997891302 \t -0.017650821030332026\n",
            "77     \t [-1.11398796 -0.24648974]. \t  -225.72234792939003 \t -0.017650821030332026\n",
            "78     \t [-0.09797941  0.996038  ]. \t  -98.51155747480169 \t -0.017650821030332026\n",
            "79     \t [1.1508533 0.387372 ]. \t  -87.83676818002708 \t -0.017650821030332026\n",
            "80     \t [0.70587209 0.46014561]. \t  -0.2317468799219714 \t -0.017650821030332026\n",
            "81     \t [0.20377698 0.76973986]. \t  -53.66365112401365 \t -0.017650821030332026\n",
            "82     \t [1.0043492  1.01149819]. \t  \u001b[92m-0.0007922403871347229\u001b[0m \t -0.0007922403871347229\n",
            "83     \t [1.84719157 0.07493504]. \t  -1114.3958721029478 \t -0.0007922403871347229\n",
            "84     \t [ 1.10464633 -1.45254347]. \t  -714.3899783273297 \t -0.0007922403871347229\n",
            "85     \t [1.12337412 1.26949402]. \t  -0.020883145044259768 \t -0.0007922403871347229\n",
            "86     \t [-1.15538548 -0.32404392]. \t  -279.8603611900302 \t -0.0007922403871347229\n",
            "87     \t [-0.43085748 -1.0387392 ]. \t  -151.95734865395278 \t -0.0007922403871347229\n",
            "88     \t [-0.33939423  0.24049692]. \t  -3.364198389531944 \t -0.0007922403871347229\n",
            "89     \t [1.02569239 1.07832956]. \t  -0.06974857757381787 \t -0.0007922403871347229\n",
            "90     \t [-1.9380262   0.13610953]. \t  -1318.9532876497697 \t -0.0007922403871347229\n",
            "91     \t [ 0.9721382  -0.41205174]. \t  -184.17401640929336 \t -0.0007922403871347229\n",
            "92     \t [ 0.14583905 -0.10642548]. \t  -2.3601797106240374 \t -0.0007922403871347229\n",
            "93     \t [0.01802314 2.0015417 ]. \t  -401.4511721215828 \t -0.0007922403871347229\n",
            "94     \t [ 0.15498551 -0.88449689]. \t  -83.25443505500101 \t -0.0007922403871347229\n",
            "95     \t [-0.52232349 -0.34714472]. \t  -40.7533218818055 \t -0.0007922403871347229\n",
            "96     \t [0.95429483 0.92879834]. \t  -0.03492137016452186 \t -0.0007922403871347229\n",
            "97     \t [1.36083287 1.87749507]. \t  -0.1958847502149283 \t -0.0007922403871347229\n",
            "98     \t [ 1.15095058 -1.48563159]. \t  -789.8119743499391 \t -0.0007922403871347229\n",
            "99     \t [1.36192892 1.85451107]. \t  -0.13100405754265665 \t -0.0007922403871347229\n",
            "100    \t [-0.54071186  1.67082572]. \t  -192.38799757088222 \t -0.0007922403871347229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_21yVprtdlu",
        "outputId": "1dd40845-e310-4882-ffad-51fa63999e5b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_loser_8 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_8 = d2GPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.59948851 -1.8750873 ]. \t  -1972.3059423071145 \t -7.238275380208551\n",
            "init   \t [ 0.40834083 -0.81972959]. \t  -97.66272647293047 \t -7.238275380208551\n",
            "init   \t [ 1.21492186 -0.54806067]. \t  -409.7425700197761 \t -7.238275380208551\n",
            "init   \t [ 1.55897178 -0.82804067]. \t  -1062.051446884513 \t -7.238275380208551\n",
            "init   \t [-0.69999303  0.28146449]. \t  -7.238275380208551 \t -7.238275380208551\n",
            "1      \t [0.61855617 0.33213955]. \t  \u001b[92m-0.4002435137165303\u001b[0m \t -0.4002435137165303\n",
            "2      \t [-1.3524013   0.89573053]. \t  -92.6309820204483 \t -0.4002435137165303\n",
            "3      \t [ 0.67131218 -0.12161085]. \t  -32.857433495954204 \t -0.4002435137165303\n",
            "4      \t [1.64636284 1.02033468]. \t  -286.0872466555437 \t -0.4002435137165303\n",
            "5      \t [0.09846217 1.18140845]. \t  -138.10405818058203 \t -0.4002435137165303\n",
            "6      \t [-0.95718583  0.49937523]. \t  -21.20525766744173 \t -0.4002435137165303\n",
            "7      \t [0.88703472 0.31092939]. \t  -22.660956151978393 \t -0.4002435137165303\n",
            "8      \t [-0.5966981   0.47264577]. \t  -3.9089342320248748 \t -0.4002435137165303\n",
            "9      \t [0.35192774 0.41213092]. \t  -8.730405921494043 \t -0.4002435137165303\n",
            "10     \t [1.23148034 1.44633301]. \t  -0.5465390943467198 \t -0.4002435137165303\n",
            "11     \t [0.95840511 1.32722821]. \t  -16.704305392218807 \t -0.4002435137165303\n",
            "12     \t [1.645183   1.85400241]. \t  -73.11315058379616 \t -0.4002435137165303\n",
            "13     \t [1.22365818 1.5305697 ]. \t  \u001b[92m-0.16044863786787977\u001b[0m \t -0.16044863786787977\n",
            "14     \t [-1.07680911 -0.66245332]. \t  -336.27103308075306 \t -0.16044863786787977\n",
            "15     \t [0.81481161 0.54401382]. \t  -1.471995011987517 \t -0.16044863786787977\n",
            "16     \t [0.72419757 0.60541254]. \t  -0.731363958795894 \t -0.16044863786787977\n",
            "17     \t [ 0.46042485 -1.16658716]. \t  -190.3389276320275 \t -0.16044863786787977\n",
            "18     \t [-1.54848356 -1.82686842]. \t  -1791.2782240225968 \t -0.16044863786787977\n",
            "19     \t [-0.3511832  -1.70540376]. \t  -336.25227998344593 \t -0.16044863786787977\n",
            "20     \t [1.22078653 1.526807  ]. \t  -0.1818786264357145 \t -0.16044863786787977\n",
            "21     \t [-0.09416479 -2.01791579]. \t  -411.98204758939727 \t -0.16044863786787977\n",
            "22     \t [-0.25896759  1.05540738]. \t  -99.26722111254946 \t -0.16044863786787977\n",
            "23     \t [-1.10123641  1.96747632]. \t  -61.38065911826277 \t -0.16044863786787977\n",
            "24     \t [-0.96524174 -1.36581502]. \t  -531.715849495629 \t -0.16044863786787977\n",
            "25     \t [0.47475314 1.75949646]. \t  -235.62398067075662 \t -0.16044863786787977\n",
            "26     \t [0.76448522 0.55931744]. \t  \u001b[92m-0.11856968169982886\u001b[0m \t -0.11856968169982886\n",
            "27     \t [-0.48949062  0.73595592]. \t  -26.85539619748126 \t -0.11856968169982886\n",
            "28     \t [-1.86854705  1.98860107]. \t  -234.08948491033163 \t -0.11856968169982886\n",
            "29     \t [-1.49964567  1.78875104]. \t  -27.425351780715324 \t -0.11856968169982886\n",
            "30     \t [1.99728149 1.79335453]. \t  -483.1390356175105 \t -0.11856968169982886\n",
            "31     \t [1.49971949 1.91974502]. \t  -11.101047318048673 \t -0.11856968169982886\n",
            "32     \t [-1.23562811  1.52096098]. \t  -5.0014154604999 \t -0.11856968169982886\n",
            "33     \t [-1.25330541  1.95501977]. \t  -19.841831903727883 \t -0.11856968169982886\n",
            "34     \t [1.15383887 1.31869236]. \t  \u001b[92m-0.03967313247682354\u001b[0m \t -0.03967313247682354\n",
            "35     \t [-1.65658863  0.95732344]. \t  -326.3809421815248 \t -0.03967313247682354\n",
            "36     \t [-0.95828785  1.12241918]. \t  -8.000718584478319 \t -0.03967313247682354\n",
            "37     \t [-2.04007732  1.73210541]. \t  -599.6397734779823 \t -0.03967313247682354\n",
            "38     \t [-0.69666159  2.01108644]. \t  -235.66968390762443 \t -0.03967313247682354\n",
            "39     \t [1.20837209 1.48361912]. \t  -0.0984374179090221 \t -0.03967313247682354\n",
            "40     \t [-0.23965993  0.0785304 ]. \t  -1.581250391022937 \t -0.03967313247682354\n",
            "41     \t [2.02060323 1.4606836 ]. \t  -688.6106859946037 \t -0.03967313247682354\n",
            "42     \t [-0.16098447 -0.93811381]. \t  -94.28323298300744 \t -0.03967313247682354\n",
            "43     \t [0.67703737 0.52165016]. \t  -0.5046212177954222 \t -0.03967313247682354\n",
            "44     \t [-0.66904423  1.19340861]. \t  -58.40574725562008 \t -0.03967313247682354\n",
            "45     \t [1.25805159 0.19123451]. \t  -193.68248995741166 \t -0.03967313247682354\n",
            "46     \t [-1.07582552  1.90269457]. \t  -59.855369374280805 \t -0.03967313247682354\n",
            "47     \t [-0.51424044 -0.06635342]. \t  -13.235566372013212 \t -0.03967313247682354\n",
            "48     \t [-1.11227828  0.93402843]. \t  -13.650774336140383 \t -0.03967313247682354\n",
            "49     \t [0.85892397 0.34161258]. \t  -15.7124183297375 \t -0.03967313247682354\n",
            "50     \t [1.14713505 1.38075619]. \t  -0.4420372642961823 \t -0.03967313247682354\n",
            "51     \t [1.03799728 1.22229839]. \t  -2.0998868809733406 \t -0.03967313247682354\n",
            "52     \t [-0.56482597 -1.12583464]. \t  -211.21159278875356 \t -0.03967313247682354\n",
            "53     \t [-1.92850496  0.52318824]. \t  -1029.9814077744916 \t -0.03967313247682354\n",
            "54     \t [-1.18082414  1.75523799]. \t  -17.78032166588062 \t -0.03967313247682354\n",
            "55     \t [0.65047701 0.4520622 ]. \t  -0.20592939490010814 \t -0.03967313247682354\n",
            "56     \t [-1.25725437 -0.32600639]. \t  -368.64375370499255 \t -0.03967313247682354\n",
            "57     \t [-1.48500591 -1.00094029]. \t  -1034.1360980208867 \t -0.03967313247682354\n",
            "58     \t [0.88169297 1.8141501 ]. \t  -107.50270556616614 \t -0.03967313247682354\n",
            "59     \t [0.89389391 0.81594664]. \t  -0.039820613055831275 \t -0.03967313247682354\n",
            "60     \t [-1.07729279  0.11680071]. \t  -113.25844116075896 \t -0.03967313247682354\n",
            "61     \t [1.22988235 1.51959968]. \t  -0.05773061435510794 \t -0.03967313247682354\n",
            "62     \t [-1.43699318  0.90086473]. \t  -141.448244037027 \t -0.03967313247682354\n",
            "63     \t [1.01776515 0.99545522]. \t  -0.16345631233532976 \t -0.03967313247682354\n",
            "64     \t [ 1.58178491 -1.94189443]. \t  -1975.1968996050994 \t -0.03967313247682354\n",
            "65     \t [1.85635975 1.58021445]. \t  -348.87561045171907 \t -0.03967313247682354\n",
            "66     \t [-2.02223264 -0.51849968]. \t  -2132.4307428489506 \t -0.03967313247682354\n",
            "67     \t [-1.37121146  0.53976074]. \t  -185.30597907635834 \t -0.03967313247682354\n",
            "68     \t [-1.10591708 -1.40193983]. \t  -693.4934082066238 \t -0.03967313247682354\n",
            "69     \t [0.88375916 0.93238652]. \t  -2.3043837843615584 \t -0.03967313247682354\n",
            "70     \t [ 0.20673218 -0.15698146]. \t  -4.618067879582819 \t -0.03967313247682354\n",
            "71     \t [0.54063257 0.2417269 ]. \t  -0.4666162759416262 \t -0.03967313247682354\n",
            "72     \t [1.30857924 1.70700531]. \t  -0.09810947870956521 \t -0.03967313247682354\n",
            "73     \t [1.03712128 1.0729572 ]. \t  \u001b[92m-0.002087330507328505\u001b[0m \t -0.002087330507328505\n",
            "74     \t [1.19884127 1.43670789]. \t  -0.039564116197895835 \t -0.002087330507328505\n",
            "75     \t [-0.83864115 -0.48317136]. \t  -144.15653311972133 \t -0.002087330507328505\n",
            "76     \t [ 1.81540232 -0.88185794]. \t  -1745.8518633501558 \t -0.002087330507328505\n",
            "77     \t [-1.21131546e+00 -4.31045776e-04]. \t  -220.30899682730103 \t -0.002087330507328505\n",
            "78     \t [ 2.02210299 -1.73191346]. \t  -3389.2322236048617 \t -0.002087330507328505\n",
            "79     \t [-0.12581057 -1.97756492]. \t  -398.6291010451451 \t -0.002087330507328505\n",
            "80     \t [1.4028137  2.04755223]. \t  -0.796925196137526 \t -0.002087330507328505\n",
            "81     \t [1.31792467 1.73789654]. \t  -0.10117040049309 \t -0.002087330507328505\n",
            "82     \t [1.25964485 1.57139516]. \t  -0.09085505731981444 \t -0.002087330507328505\n",
            "83     \t [1.16164647 1.33929551]. \t  -0.03638518460486336 \t -0.002087330507328505\n",
            "84     \t [-1.10985455  1.86904196]. \t  -45.0621325530082 \t -0.002087330507328505\n",
            "85     \t [-0.31437303 -1.03790435]. \t  -130.94416630874125 \t -0.002087330507328505\n",
            "86     \t [ 0.190941   -0.04878897]. \t  -1.3812889957609409 \t -0.002087330507328505\n",
            "87     \t [0.26139067 0.5017304 ]. \t  -19.329560711259465 \t -0.002087330507328505\n",
            "88     \t [-0.93060338  0.17375074]. \t  -51.651269276420294 \t -0.002087330507328505\n",
            "89     \t [1.27691555 1.57321124]. \t  -0.4050352272256026 \t -0.002087330507328505\n",
            "90     \t [ 0.13619448 -0.65511577]. \t  -46.12857340817518 \t -0.002087330507328505\n",
            "91     \t [-1.33951156  1.72886835]. \t  -5.901329482203336 \t -0.002087330507328505\n",
            "92     \t [-0.08108414  1.84567641]. \t  -339.39827472078963 \t -0.002087330507328505\n",
            "93     \t [ 1.55037833 -1.34548242]. \t  -1405.9195318452576 \t -0.002087330507328505\n",
            "94     \t [0.65952851 1.43503437]. \t  -100.12722412902917 \t -0.002087330507328505\n",
            "95     \t [-0.8577131   1.03479741]. \t  -12.398713195129323 \t -0.002087330507328505\n",
            "96     \t [-2.01567355 -0.66988734]. \t  -2249.0596095799283 \t -0.002087330507328505\n",
            "97     \t [ 2.02019527 -1.57878686]. \t  -3204.5733881370375 \t -0.002087330507328505\n",
            "98     \t [1.32691398 1.67984406]. \t  -0.7606523802234818 \t -0.002087330507328505\n",
            "99     \t [-0.05600739 -1.1276132 ]. \t  -128.97471474266496 \t -0.002087330507328505\n",
            "100    \t [-2.04035406 -0.23039551]. \t  -1939.4754235779617 \t -0.002087330507328505\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkq32q1utdoO",
        "outputId": "dc531c87-0f82-467c-b743-a12c3bae7a98"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_loser_9 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_9 = d2GPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.70565298 -0.04883088]. \t  -29.9831488845538 \t -29.9831488845538\n",
            "init   \t [ 1.33322823 -1.9191956 ]. \t  -1366.6650412963722 \t -29.9831488845538\n",
            "init   \t [1.26177265 0.26876895]. \t  -175.18114988457984 \t -29.9831488845538\n",
            "init   \t [-0.82893825 -1.85673433]. \t  -650.4739702903224 \t -29.9831488845538\n",
            "init   \t [ 2.00960983 -2.0200418 ]. \t  -3671.650548034952 \t -29.9831488845538\n",
            "1      \t [ 0.44784845 -1.80353056]. \t  -401.94607065972144 \t -29.9831488845538\n",
            "2      \t [-0.58928516  0.82048528]. \t  \u001b[92m-24.92032815745389\u001b[0m \t -24.92032815745389\n",
            "3      \t [-3.02552544 -0.6151535 ]. \t  -9559.458315845724 \t -24.92032815745389\n",
            "4      \t [-0.28788858  1.19428114]. \t  -125.17994398064035 \t -24.92032815745389\n",
            "5      \t [0.25347767 0.21240218]. \t  \u001b[92m-2.752174917878664\u001b[0m \t -2.752174917878664\n",
            "6      \t [-0.07778183  0.42296554]. \t  -18.543469322798174 \t -2.752174917878664\n",
            "7      \t [-1.08381231  1.62898573]. \t  -24.984448064080173 \t -2.752174917878664\n",
            "8      \t [-0.15828987  1.24297839]. \t  -149.67520799598637 \t -2.752174917878664\n",
            "9      \t [1.07230747 2.01506979]. \t  -74.86691334483989 \t -2.752174917878664\n",
            "10     \t [-2.01346326 -1.00148458]. \t  -2564.9080532207845 \t -2.752174917878664\n",
            "11     \t [-0.8277207   0.00382517]. \t  -49.757039216590954 \t -2.752174917878664\n",
            "12     \t [2.02242738 2.02849163]. \t  -426.11464967883586 \t -2.752174917878664\n",
            "13     \t [0.37035625 0.28543105]. \t  \u001b[92m-2.5947703949075933\u001b[0m \t -2.5947703949075933\n",
            "14     \t [0.43730765 0.13402628]. \t  \u001b[92m-0.6439405193178424\u001b[0m \t -0.6439405193178424\n",
            "15     \t [ 0.3754054  -1.02590489]. \t  -136.54030096647267 \t -0.6439405193178424\n",
            "16     \t [-1.10321213 -1.28709092]. \t  -631.5092029173569 \t -0.6439405193178424\n",
            "17     \t [-0.76999926  0.22556009]. \t  -16.62667402711779 \t -0.6439405193178424\n",
            "18     \t [0.90689455 1.0899462 ]. \t  -7.163676581600024 \t -0.6439405193178424\n",
            "19     \t [ 0.5452945  -0.50417316]. \t  -64.45006829625832 \t -0.6439405193178424\n",
            "20     \t [-0.41516783  0.11758421]. \t  -2.302786170838536 \t -0.6439405193178424\n",
            "21     \t [1.01653111 1.33700388]. \t  -9.221722310844507 \t -0.6439405193178424\n",
            "22     \t [1.24071924 1.92822824]. \t  -15.177911978567126 \t -0.6439405193178424\n",
            "23     \t [-1.08559383  1.34192711]. \t  -7.02008770599282 \t -0.6439405193178424\n",
            "24     \t [-1.68924522  1.89809867]. \t  -98.52065567975696 \t -0.6439405193178424\n",
            "25     \t [0.39623998 0.18979761]. \t  \u001b[92m-0.4720542992219571\u001b[0m \t -0.4720542992219571\n",
            "26     \t [-1.88587156  0.91344747]. \t  -706.9070146986345 \t -0.4720542992219571\n",
            "27     \t [0.68341223 0.49129365]. \t  \u001b[92m-0.15899224711341994\u001b[0m \t -0.15899224711341994\n",
            "28     \t [-0.0795336  -1.23139253]. \t  -154.36000755826518 \t -0.15899224711341994\n",
            "29     \t [-0.11052903 -0.50338427]. \t  -27.817707774326482 \t -0.15899224711341994\n",
            "30     \t [0.56014003 0.34063091]. \t  -0.26569828191779954 \t -0.15899224711341994\n",
            "31     \t [0.58181308 1.3615467 ]. \t  -104.83601437806144 \t -0.15899224711341994\n",
            "32     \t [1.24554173 1.59958294]. \t  -0.29269901063787895 \t -0.15899224711341994\n",
            "33     \t [-1.20712966  1.52928728]. \t  -5.391626693766622 \t -0.15899224711341994\n",
            "34     \t [ 0.16959348 -0.00591545]. \t  -0.8098271575674274 \t -0.15899224711341994\n",
            "35     \t [-0.37583209 -1.61138224]. \t  -309.064807839783 \t -0.15899224711341994\n",
            "36     \t [ 1.63076731 -1.45423047]. \t  -1692.5951079213537 \t -0.15899224711341994\n",
            "37     \t [-0.33797863  0.41658592]. \t  -10.932124152684748 \t -0.15899224711341994\n",
            "38     \t [-0.36329081  1.32777155]. \t  -144.85025260403364 \t -0.15899224711341994\n",
            "39     \t [0.61655079 0.40186858]. \t  -0.1942686856145894 \t -0.15899224711341994\n",
            "40     \t [-1.56920135 -0.12707265]. \t  -677.1339649108769 \t -0.15899224711341994\n",
            "41     \t [-0.20199301 -0.00120491]. \t  -1.6212383059338433 \t -0.15899224711341994\n",
            "42     \t [-0.15119024 -1.71306133]. \t  -302.6670012948018 \t -0.15899224711341994\n",
            "43     \t [0.52071767 0.2959925 ]. \t  -0.2914419882737949 \t -0.15899224711341994\n",
            "44     \t [0.99127909 1.00634457]. \t  \u001b[92m-0.05629401970937036\u001b[0m \t -0.05629401970937036\n",
            "45     \t [1.22345803 1.40894298]. \t  -0.8226899480791133 \t -0.05629401970937036\n",
            "46     \t [-1.87945054 -0.53187245]. \t  -1660.0689019354684 \t -0.05629401970937036\n",
            "47     \t [-0.94126862 -1.11145889]. \t  -402.7473815286371 \t -0.05629401970937036\n",
            "48     \t [0.64058855 1.94804941]. \t  -236.57998974621958 \t -0.05629401970937036\n",
            "49     \t [-1.72312315 -0.3832916 ]. \t  -1131.3041455848916 \t -0.05629401970937036\n",
            "50     \t [ 1.98436426 -0.06430682]. \t  -1602.5760280513484 \t -0.05629401970937036\n",
            "51     \t [0.95369749 0.9549156 ]. \t  -0.2080483927142317 \t -0.05629401970937036\n",
            "52     \t [0.95245211 0.8600227 ]. \t  -0.22450061156619044 \t -0.05629401970937036\n",
            "53     \t [-1.54536301  0.97857225]. \t  -205.16892502915746 \t -0.05629401970937036\n",
            "54     \t [-1.03742352  0.77025954]. \t  -13.513961478754117 \t -0.05629401970937036\n",
            "55     \t [0.85546452 0.75828284]. \t  -0.09092106429121846 \t -0.05629401970937036\n",
            "56     \t [ 1.63554429 -0.09981155]. \t  -770.3646643128745 \t -0.05629401970937036\n",
            "57     \t [-0.54231303  0.00288144]. \t  -10.859753918600669 \t -0.05629401970937036\n",
            "58     \t [-1.49595357 -0.91197093]. \t  -998.3840314171645 \t -0.05629401970937036\n",
            "59     \t [0.65487724 0.95429416]. \t  -27.726773753029832 \t -0.05629401970937036\n",
            "60     \t [0.81810211 0.68371592]. \t  \u001b[92m-0.05389450541083844\u001b[0m \t -0.05389450541083844\n",
            "61     \t [ 1.846403  -1.6901432]. \t  -2601.050623124933 \t -0.05389450541083844\n",
            "62     \t [0.93230244 0.87977952]. \t  \u001b[92m-0.01580131273905228\u001b[0m \t -0.01580131273905228\n",
            "63     \t [0.46873837 1.96743036]. \t  -305.7329054062828 \t -0.01580131273905228\n",
            "64     \t [ 0.27674062 -1.32846936]. \t  -197.9409839920002 \t -0.01580131273905228\n",
            "65     \t [0.72963728 0.53897645]. \t  -0.07745978597048989 \t -0.01580131273905228\n",
            "66     \t [0.00140331 1.36362061]. \t  -186.94277558703737 \t -0.01580131273905228\n",
            "67     \t [0.47213099 1.05764644]. \t  -69.95752629871546 \t -0.01580131273905228\n",
            "68     \t [-1.79204133 -1.16368841]. \t  -1921.9459548040177 \t -0.01580131273905228\n",
            "69     \t [-0.55761127 -1.33865854]. \t  -274.54049687577555 \t -0.01580131273905228\n",
            "70     \t [ 1.2980682  -1.25427281]. \t  -864.0101684671865 \t -0.01580131273905228\n",
            "71     \t [-1.84667548 -0.8660456 ]. \t  -1836.7400488056433 \t -0.01580131273905228\n",
            "72     \t [0.35377083 0.55626988]. \t  -19.00371957437456 \t -0.01580131273905228\n",
            "73     \t [ 0.23754772 -0.61027856]. \t  -45.03121945417927 \t -0.01580131273905228\n",
            "74     \t [-1.66305092 -0.65620777]. \t  -1178.0633699721684 \t -0.01580131273905228\n",
            "75     \t [ 0.13847559 -0.18332592]. \t  -4.842906597165351 \t -0.01580131273905228\n",
            "76     \t [-0.88307779  0.82963019]. \t  -3.7940238790094662 \t -0.01580131273905228\n",
            "77     \t [0.81461897 0.64916221]. \t  -0.05522282730712497 \t -0.01580131273905228\n",
            "78     \t [-0.69654673 -1.22160209]. \t  -294.18787606883797 \t -0.01580131273905228\n",
            "79     \t [-0.46708971 -1.31363253]. \t  -236.7951087892674 \t -0.01580131273905228\n",
            "80     \t [ 0.05482189 -1.28791705]. \t  -167.54144919181485 \t -0.01580131273905228\n",
            "81     \t [0.79924822 0.21251998]. \t  -18.211571521285908 \t -0.01580131273905228\n",
            "82     \t [-1.85316016  0.25770396]. \t  -1017.154878475344 \t -0.01580131273905228\n",
            "83     \t [ 1.69336897 -0.61239054]. \t  -1211.4435039774137 \t -0.01580131273905228\n",
            "84     \t [-0.65216879 -0.88035975]. \t  -173.21070243163263 \t -0.01580131273905228\n",
            "85     \t [-0.5255028   0.31883678]. \t  -2.509347690056986 \t -0.01580131273905228\n",
            "86     \t [1.78829037 0.77377538]. \t  -588.2993861148012 \t -0.01580131273905228\n",
            "87     \t [-0.2330249  -0.80892747]. \t  -76.03662216236455 \t -0.01580131273905228\n",
            "88     \t [-1.43296824  0.75859945]. \t  -173.569656950802 \t -0.01580131273905228\n",
            "89     \t [-0.93161299 -0.72401088]. \t  -257.1500312220242 \t -0.01580131273905228\n",
            "90     \t [1.14179758 1.48818296]. \t  -3.423439666435232 \t -0.01580131273905228\n",
            "91     \t [-1.10498356 -0.99634577]. \t  -496.0881529291035 \t -0.01580131273905228\n",
            "92     \t [-1.09491229 -1.51316321]. \t  -739.8809613632862 \t -0.01580131273905228\n",
            "93     \t [ 0.88961009 -0.45274637]. \t  -154.8037243800122 \t -0.01580131273905228\n",
            "94     \t [-1.25086189 -0.9100251 ]. \t  -617.4707731834394 \t -0.01580131273905228\n",
            "95     \t [-1.19440196 -1.04886507]. \t  -617.6061689884486 \t -0.01580131273905228\n",
            "96     \t [ 1.98571538 -1.49840793]. \t  -2961.9350309063184 \t -0.01580131273905228\n",
            "97     \t [0.94988028 0.92605334]. \t  -0.05906458166288756 \t -0.01580131273905228\n",
            "98     \t [-0.65541128  0.80886722]. \t  -17.12748433920213 \t -0.01580131273905228\n",
            "99     \t [-0.45261262  0.90300121]. \t  -50.85045173697143 \t -0.01580131273905228\n",
            "100    \t [ 1.19856704 -0.13210183]. \t  -246.11034989199715 \t -0.01580131273905228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Aur-aRtdq-",
        "outputId": "e000f08e-2393-4463-a9f0-f8104b6f11cc"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_loser_10 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_10 = d2GPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.62910294 -1.57693156]. \t  -389.29291138113445 \t -8.580376531587937\n",
            "init   \t [ 1.84435861 -0.07294402]. \t  -1207.999341247548 \t -8.580376531587937\n",
            "init   \t [ 1.5256557  -1.17828534]. \t  -1229.4172568797705 \t -8.580376531587937\n",
            "init   \t [-1.88125338 -0.42109149]. \t  -1576.6245828972787 \t -8.580376531587937\n",
            "init   \t [-1.09309052  1.39977001]. \t  -8.580376531587937 \t -8.580376531587937\n",
            "1      \t [-0.86019812  2.03750805]. \t  -171.82841453589126 \t -8.580376531587937\n",
            "2      \t [-0.5337917   1.15217037]. \t  -77.5624817749036 \t -8.580376531587937\n",
            "3      \t [-1.92496425  1.90332315]. \t  -333.33499540157186 \t -8.580376531587937\n",
            "4      \t [-0.38210857 -2.34134613]. \t  -620.6027604998372 \t -8.580376531587937\n",
            "5      \t [-0.45198894  1.79927878]. \t  -256.50591736736254 \t -8.580376531587937\n",
            "6      \t [-0.89443395  1.32897856]. \t  -31.569432174542456 \t -8.580376531587937\n",
            "7      \t [0.76191163 0.20393692]. \t  -14.237364089085258 \t -8.580376531587937\n",
            "8      \t [0.10527521 0.29373689]. \t  -8.789862043700118 \t -8.580376531587937\n",
            "9      \t [0.65210088 0.83593868]. \t  -16.988738873674894 \t -8.580376531587937\n",
            "10     \t [0.46361979 0.4205371 ]. \t  \u001b[92m-4.514584417459624\u001b[0m \t -4.514584417459624\n",
            "11     \t [1.1895611 1.767425 ]. \t  -12.452351781803818 \t -4.514584417459624\n",
            "12     \t [ 0.47499881 -0.05707135]. \t  -8.267285334496398 \t -4.514584417459624\n",
            "13     \t [1.17171262 1.0997644 ]. \t  -7.490362089228181 \t -4.514584417459624\n",
            "14     \t [-1.14626072  1.11079591]. \t  -8.732116019164996 \t -4.514584417459624\n",
            "15     \t [0.83008009 0.63274649]. \t  \u001b[92m-0.3456893396336701\u001b[0m \t -0.3456893396336701\n",
            "16     \t [0.98226372 1.2697215 ]. \t  -9.295464012565606 \t -0.3456893396336701\n",
            "17     \t [2.16403031 2.36782007]. \t  -537.3733706598347 \t -0.3456893396336701\n",
            "18     \t [-0.70446954 -0.55312607]. \t  -113.0299672388526 \t -0.3456893396336701\n",
            "19     \t [-0.19482126 -0.08611306]. \t  -2.9668942995261545 \t -0.3456893396336701\n",
            "20     \t [-0.3804647   0.45357847]. \t  -11.442976118299542 \t -0.3456893396336701\n",
            "21     \t [ 0.01392263 -0.46017202]. \t  -22.166020770957235 \t -0.3456893396336701\n",
            "22     \t [-1.16847338  1.34990476]. \t  -4.726070717599731 \t -0.3456893396336701\n",
            "23     \t [0.41370699 0.06430788]. \t  -1.485337650326521 \t -0.3456893396336701\n",
            "24     \t [-0.08262942  1.92456551]. \t  -368.94394876270246 \t -0.3456893396336701\n",
            "25     \t [-1.42911955 -1.23177359]. \t  -1077.9105586898488 \t -0.3456893396336701\n",
            "26     \t [1.45733524 0.6290517 ]. \t  -223.64417807412963 \t -0.3456893396336701\n",
            "27     \t [0.45360384 0.17719118]. \t  -0.38014620926712106 \t -0.3456893396336701\n",
            "28     \t [0.5718052 0.3732082]. \t  -0.3972293811355653 \t -0.3456893396336701\n",
            "29     \t [0.77032883 0.59514591]. \t  \u001b[92m-0.05305140144310027\u001b[0m \t -0.05305140144310027\n",
            "30     \t [0.99194882 0.10029902]. \t  -78.08617211182937 \t -0.05305140144310027\n",
            "31     \t [0.97112853 0.90237227]. \t  -0.16663192538170346 \t -0.05305140144310027\n",
            "32     \t [-0.31793576  0.82467337]. \t  -54.09523653176086 \t -0.05305140144310027\n",
            "33     \t [-0.39543354  0.18365716]. \t  -2.0217063104276574 \t -0.05305140144310027\n",
            "34     \t [-0.18459878 -0.99157824]. \t  -106.60008293430585 \t -0.05305140144310027\n",
            "35     \t [ 2.04545002 -1.72379955]. \t  -3491.1439590465025 \t -0.05305140144310027\n",
            "36     \t [-1.08562463 -1.84139658]. \t  -916.3761885599844 \t -0.05305140144310027\n",
            "37     \t [-0.49143264  0.98704416]. \t  -57.80707919856127 \t -0.05305140144310027\n",
            "38     \t [-1.9731415   1.30257183]. \t  -680.0202743820246 \t -0.05305140144310027\n",
            "39     \t [ 0.55732215 -1.45954397]. \t  -313.5397560590302 \t -0.05305140144310027\n",
            "40     \t [-0.71931561  1.74910406]. \t  -154.66185238171147 \t -0.05305140144310027\n",
            "41     \t [-0.84225432  0.84199187]. \t  -5.152164407456888 \t -0.05305140144310027\n",
            "42     \t [-1.17017422 -0.53387238]. \t  -366.9190964188499 \t -0.05305140144310027\n",
            "43     \t [ 1.09423273 -1.03254202]. \t  -497.2486072284271 \t -0.05305140144310027\n",
            "44     \t [-1.87395402  0.49278705]. \t  -919.6453670223667 \t -0.05305140144310027\n",
            "45     \t [1.82708159 0.67678285]. \t  -709.0126391146559 \t -0.05305140144310027\n",
            "46     \t [-0.74185732  0.48856017]. \t  -3.4158934199801827 \t -0.05305140144310027\n",
            "47     \t [0.10495403 0.01032357]. \t  -0.8011551527155836 \t -0.05305140144310027\n",
            "48     \t [0.34835018 1.21836145]. \t  -120.76853083741302 \t -0.05305140144310027\n",
            "49     \t [0.99621325 0.9759489 ]. \t  \u001b[92m-0.027212720838110355\u001b[0m \t -0.027212720838110355\n",
            "50     \t [1.1227541  0.79666545]. \t  -21.536438712554748 \t -0.027212720838110355\n",
            "51     \t [ 0.75641574 -1.40332656]. \t  -390.315934888768 \t -0.027212720838110355\n",
            "52     \t [-0.84502623 -1.83974622]. \t  -655.6015088704874 \t -0.027212720838110355\n",
            "53     \t [0.84464057 1.90600766]. \t  -142.25122111186738 \t -0.027212720838110355\n",
            "54     \t [1.90861306 1.11266196]. \t  -640.9873500056517 \t -0.027212720838110355\n",
            "55     \t [-1.49630139  1.46255007]. \t  -66.50621539864875 \t -0.027212720838110355\n",
            "56     \t [-0.77159888  0.08083053]. \t  -29.613117616473627 \t -0.027212720838110355\n",
            "57     \t [-1.33059952  1.88454276]. \t  -6.732381099281367 \t -0.027212720838110355\n",
            "58     \t [1.18092966 1.41135911]. \t  -0.060839586759303836 \t -0.027212720838110355\n",
            "59     \t [1.31417954 1.98169538]. \t  -6.582225985731442 \t -0.027212720838110355\n",
            "60     \t [ 1.95664623 -1.40562122]. \t  -2740.4804751994247 \t -0.027212720838110355\n",
            "61     \t [-1.66270093 -1.55400635]. \t  -1872.1039474825202 \t -0.027212720838110355\n",
            "62     \t [ 1.8351678  -0.07827489]. \t  -1188.26887670644 \t -0.027212720838110355\n",
            "63     \t [-1.81823939  1.2990678 ]. \t  -410.71794193666307 \t -0.027212720838110355\n",
            "64     \t [0.47241056 1.13269949]. \t  -83.00242487243595 \t -0.027212720838110355\n",
            "65     \t [ 1.2739886  -1.98423576]. \t  -1301.3239220270361 \t -0.027212720838110355\n",
            "66     \t [-1.4064444  -0.22099504]. \t  -489.3866516394009 \t -0.027212720838110355\n",
            "67     \t [1.08865535 1.22581037]. \t  -0.1730199260313738 \t -0.027212720838110355\n",
            "68     \t [-0.34965343 -1.95956872]. \t  -435.221610964645 \t -0.027212720838110355\n",
            "69     \t [ 1.51152057 -1.89587699]. \t  -1747.9794032094064 \t -0.027212720838110355\n",
            "70     \t [-1.40558332 -1.91251359]. \t  -1517.579699438475 \t -0.027212720838110355\n",
            "71     \t [ 1.40517301 -0.85657528]. \t  -801.669225988052 \t -0.027212720838110355\n",
            "72     \t [ 0.30886344 -0.26784294]. \t  -13.67196772240941 \t -0.027212720838110355\n",
            "73     \t [ 1.63415265 -1.63483115]. \t  -1853.9509454567055 \t -0.027212720838110355\n",
            "74     \t [2.03618554 0.66993058]. \t  -1209.415371467293 \t -0.027212720838110355\n",
            "75     \t [-1.73400181  1.72905703]. \t  -170.72783899706164 \t -0.027212720838110355\n",
            "76     \t [ 0.66620021 -1.06959045]. \t  -229.15336766252585 \t -0.027212720838110355\n",
            "77     \t [0.62080722 0.40307486]. \t  -0.17502154320294153 \t -0.027212720838110355\n",
            "78     \t [ 0.29383177 -1.78799774]. \t  -351.8117852221128 \t -0.027212720838110355\n",
            "79     \t [0.35188336 2.00297952]. \t  -353.54339364049775 \t -0.027212720838110355\n",
            "80     \t [-1.91804923  0.41488613]. \t  -1073.902053974256 \t -0.027212720838110355\n",
            "81     \t [ 2.02131551 -1.66183259]. \t  -3304.4750197844164 \t -0.027212720838110355\n",
            "82     \t [1.39795819 1.13389889]. \t  -67.46205269700383 \t -0.027212720838110355\n",
            "83     \t [-1.16956642  1.40716474]. \t  -4.861303510122351 \t -0.027212720838110355\n",
            "84     \t [0.93293501 0.90391066]. \t  -0.11701051986194459 \t -0.027212720838110355\n",
            "85     \t [ 1.66561392 -1.2937774 ]. \t  -1655.3437923316073 \t -0.027212720838110355\n",
            "86     \t [ 0.22527611 -1.7089182 ]. \t  -310.24317612133376 \t -0.027212720838110355\n",
            "87     \t [0.94173384 0.90664859]. \t  -0.04254340689019411 \t -0.027212720838110355\n",
            "88     \t [ 1.45792617 -0.2476256 ]. \t  -563.4053281955346 \t -0.027212720838110355\n",
            "89     \t [-1.67514889 -0.19090665]. \t  -905.3755728724991 \t -0.027212720838110355\n",
            "90     \t [1.01082264 1.0467674 ]. \t  -0.06264205804972182 \t -0.027212720838110355\n",
            "91     \t [ 0.46183315 -0.73357907]. \t  -89.94570007824322 \t -0.027212720838110355\n",
            "92     \t [0.78705217 0.06078357]. \t  -31.25628888559174 \t -0.027212720838110355\n",
            "93     \t [1.19064195 1.63726879]. \t  -4.860540991330615 \t -0.027212720838110355\n",
            "94     \t [ 0.45046954 -1.93188098]. \t  -456.04070283784597 \t -0.027212720838110355\n",
            "95     \t [-0.29073196 -0.0322581 ]. \t  -3.0298199791996803 \t -0.027212720838110355\n",
            "96     \t [0.70079929 0.47927986]. \t  -0.10353912784283456 \t -0.027212720838110355\n",
            "97     \t [-0.79374399  1.80956559]. \t  -142.348050764057 \t -0.027212720838110355\n",
            "98     \t [-0.65314311 -0.35198627]. \t  -63.351906424072624 \t -0.027212720838110355\n",
            "99     \t [ 1.97285981 -1.2448696 ]. \t  -2639.870017628281 \t -0.027212720838110355\n",
            "100    \t [2.01579377 0.71686644]. \t  -1120.9769394817645 \t -0.027212720838110355\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMu7U9NNtdtn",
        "outputId": "2ab50758-45cd-47ed-c3ed-c6a94e95a3c0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_loser_11 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_11 = d2GPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.97032207 1.31583119]. \t  -659.5505379977991 \t -45.00854175284071\n",
            "init   \t [ 0.59758024 -0.3125739 ]. \t  -45.00854175284071 \t -45.00854175284071\n",
            "init   \t [-1.21933423 -0.03314987]. \t  -235.94289844157058 \t -45.00854175284071\n",
            "init   \t [-1.48046507 -0.19447384]. \t  -575.5719351651762 \t -45.00854175284071\n",
            "init   \t [-1.58214297 -2.0360213 ]. \t  -2067.0989998073865 \t -45.00854175284071\n",
            "1      \t [-0.59782787  0.66030788]. \t  \u001b[92m-11.728483422473923\u001b[0m \t -11.728483422473923\n",
            "2      \t [-0.48099284  0.29072557]. \t  \u001b[92m-2.5458368185197586\u001b[0m \t -2.5458368185197586\n",
            "3      \t [-1.51479317  0.91582766]. \t  -196.42504923820073 \t -2.5458368185197586\n",
            "4      \t [ 1.24763518 -1.08755389]. \t  -699.2128870564127 \t -2.5458368185197586\n",
            "5      \t [-0.84568122  0.65790028]. \t  -3.7345983417829682 \t -2.5458368185197586\n",
            "6      \t [-0.17040653  1.91058643]. \t  -355.39215614461466 \t -2.5458368185197586\n",
            "7      \t [0.86108268 0.02839222]. \t  -50.86634547015357 \t -2.5458368185197586\n",
            "8      \t [-0.78387951  0.47324305]. \t  -5.17664881882583 \t -2.5458368185197586\n",
            "9      \t [-0.45946827 -0.23688737]. \t  -22.200309190469696 \t -2.5458368185197586\n",
            "10     \t [0.32671337 0.01372152]. \t  \u001b[92m-1.3185890249791758\u001b[0m \t -1.3185890249791758\n",
            "11     \t [-0.81760807  1.43209638]. \t  -61.614244889445445 \t -1.3185890249791758\n",
            "12     \t [-0.81485775  0.95757903]. \t  -11.912975214664602 \t -1.3185890249791758\n",
            "13     \t [-0.14923856 -0.0982244 ]. \t  -2.772691186909121 \t -1.3185890249791758\n",
            "14     \t [ 0.20898188 -0.0661775 ]. \t  -1.8324323009447747 \t -1.3185890249791758\n",
            "15     \t [-1.2386734  -1.13925684]. \t  -719.8085800301743 \t -1.3185890249791758\n",
            "16     \t [-0.13568435 -0.03663645]. \t  -1.5927927857074646 \t -1.3185890249791758\n",
            "17     \t [0.24305295 0.09282584]. \t  \u001b[92m-0.6868825250838098\u001b[0m \t -0.6868825250838098\n",
            "18     \t [0.12629817 0.08976554]. \t  -1.3082100659066231 \t -0.6868825250838098\n",
            "19     \t [-0.02526951  0.02834893]. \t  -1.127964100984225 \t -0.6868825250838098\n",
            "20     \t [ 1.0137647  -0.07230145]. \t  -121.00465961248214 \t -0.6868825250838098\n",
            "21     \t [0.59718318 0.3761849 ]. \t  \u001b[92m-0.20050962134737754\u001b[0m \t -0.20050962134737754\n",
            "22     \t [ 0.2436819  -0.27884161]. \t  -12.011461671266463 \t -0.20050962134737754\n",
            "23     \t [ 1.21602431 -0.70188661]. \t  -475.54905943904737 \t -0.20050962134737754\n",
            "24     \t [0.17726688 1.06658703]. \t  -107.8332336585574 \t -0.20050962134737754\n",
            "25     \t [-1.2198352   0.34836495]. \t  -134.80399984543754 \t -0.20050962134737754\n",
            "26     \t [0.55658795 0.34293503]. \t  -0.3064725823616601 \t -0.20050962134737754\n",
            "27     \t [0.54573972 0.32954028]. \t  -0.30689495819183915 \t -0.20050962134737754\n",
            "28     \t [1.27910866 0.64469621]. \t  -98.36981129039162 \t -0.20050962134737754\n",
            "29     \t [-1.47304488 -0.75853993]. \t  -863.669276188972 \t -0.20050962134737754\n",
            "30     \t [ 1.24950157 -1.20455379]. \t  -765.0316215108699 \t -0.20050962134737754\n",
            "31     \t [0.69961256 0.4454616 ]. \t  -0.2837986840930933 \t -0.20050962134737754\n",
            "32     \t [-1.79219421  0.5095534 ]. \t  -738.0965349548324 \t -0.20050962134737754\n",
            "33     \t [-1.52624916  1.97411536]. \t  -19.00724703600006 \t -0.20050962134737754\n",
            "34     \t [-1.28361083  1.38157964]. \t  -12.294581762496986 \t -0.20050962134737754\n",
            "35     \t [0.07628541 0.51436288]. \t  -26.714889428745124 \t -0.20050962134737754\n",
            "36     \t [-0.88702882 -0.79713069]. \t  -254.45089311807683 \t -0.20050962134737754\n",
            "37     \t [-0.18752053 -0.79313575]. \t  -70.01824450778345 \t -0.20050962134737754\n",
            "38     \t [-0.27563309  1.9377205 ]. \t  -348.23739126600947 \t -0.20050962134737754\n",
            "39     \t [ 1.93024108 -0.94086061]. \t  -2178.6660545401273 \t -0.20050962134737754\n",
            "40     \t [-1.98053855 -1.49949419]. \t  -2948.7214246399426 \t -0.20050962134737754\n",
            "41     \t [ 1.22778986 -1.53194722]. \t  -923.8563456372658 \t -0.20050962134737754\n",
            "42     \t [0.53458034 0.30928026]. \t  -0.2718598252129951 \t -0.20050962134737754\n",
            "43     \t [1.71365695 1.0271433 ]. \t  -365.1194820522272 \t -0.20050962134737754\n",
            "44     \t [0.93920879 0.30816011]. \t  -32.94590397346381 \t -0.20050962134737754\n",
            "45     \t [-0.08912908  0.32218779]. \t  -11.061118648132027 \t -0.20050962134737754\n",
            "46     \t [-0.0069477   0.10223989]. \t  -2.058256476849774 \t -0.20050962134737754\n",
            "47     \t [1.92883444 0.33470046]. \t  -1147.160415538827 \t -0.20050962134737754\n",
            "48     \t [-0.57896128 -1.17887706]. \t  -231.73488884239327 \t -0.20050962134737754\n",
            "49     \t [1.20403766 1.03100076]. \t  -17.573096303309917 \t -0.20050962134737754\n",
            "50     \t [-0.69152467  1.25049012]. \t  -62.50347520244296 \t -0.20050962134737754\n",
            "51     \t [0.61794618 1.65841838]. \t  -163.10673822296363 \t -0.20050962134737754\n",
            "52     \t [0.53762205 0.279527  ]. \t  -0.22283827093694472 \t -0.20050962134737754\n",
            "53     \t [0.98115619 0.95299302]. \t  \u001b[92m-0.009714587400323622\u001b[0m \t -0.009714587400323622\n",
            "54     \t [ 0.9735238  -1.26120847]. \t  -487.9498313854103 \t -0.009714587400323622\n",
            "55     \t [-1.33833782  1.90814184]. \t  -6.836576574872194 \t -0.009714587400323622\n",
            "56     \t [-0.04710502 -1.72545809]. \t  -299.5832021814439 \t -0.009714587400323622\n",
            "57     \t [-0.18157075  0.43033067]. \t  -17.18582347230089 \t -0.009714587400323622\n",
            "58     \t [ 0.47994735 -1.40300951]. \t  -267.056607350391 \t -0.009714587400323622\n",
            "59     \t [1.71424921 1.48059655]. \t  -213.10223813541185 \t -0.009714587400323622\n",
            "60     \t [ 1.86779441 -1.8241196 ]. \t  -2823.3114968565624 \t -0.009714587400323622\n",
            "61     \t [2.14398427 2.88377286]. \t  -294.70985965875946 \t -0.009714587400323622\n",
            "62     \t [0.77269269 0.66140664]. \t  -0.4657950413297715 \t -0.009714587400323622\n",
            "63     \t [1.34657198 1.83835826]. \t  -0.18312394593670486 \t -0.009714587400323622\n",
            "64     \t [0.8764226 1.4176972]. \t  -42.210768926149 \t -0.009714587400323622\n",
            "65     \t [-0.03637786  1.38300939]. \t  -191.97971095134145 \t -0.009714587400323622\n",
            "66     \t [0.40434128 0.5194397 ]. \t  -13.024695036665513 \t -0.009714587400323622\n",
            "67     \t [1.29337817 1.53111871]. \t  -2.0941975903930077 \t -0.009714587400323622\n",
            "68     \t [-1.22447442 -1.35309648]. \t  -818.5863114414681 \t -0.009714587400323622\n",
            "69     \t [1.43997596 1.95562646]. \t  -1.5837212193023473 \t -0.009714587400323622\n",
            "70     \t [ 1.96677042 -1.23243807]. \t  -2602.5710986536355 \t -0.009714587400323622\n",
            "71     \t [-1.67254638  0.24145778]. \t  -660.4323974788158 \t -0.009714587400323622\n",
            "72     \t [0.74593363 0.55973245]. \t  -0.06564894616442922 \t -0.009714587400323622\n",
            "73     \t [ 0.47673951 -1.2253542 ]. \t  -211.28857552365994 \t -0.009714587400323622\n",
            "74     \t [0.94928522 0.07194122]. \t  -68.76003629130035 \t -0.009714587400323622\n",
            "75     \t [-0.80793361 -1.44559157]. \t  -443.5751761740207 \t -0.009714587400323622\n",
            "76     \t [-1.30501148  0.16778885]. \t  -241.01728153385264 \t -0.009714587400323622\n",
            "77     \t [ 1.98216118 -0.56707881]. \t  -2022.4037849155825 \t -0.009714587400323622\n",
            "78     \t [ 0.6256004  -1.77182998]. \t  -468.0861277393353 \t -0.009714587400323622\n",
            "79     \t [-0.14297143 -1.50855436]. \t  -235.0890121843255 \t -0.009714587400323622\n",
            "80     \t [1.56533633 0.93464884]. \t  -230.0327265035141 \t -0.009714587400323622\n",
            "81     \t [ 0.28557859 -0.87338187]. \t  -91.7008658277363 \t -0.009714587400323622\n",
            "82     \t [0.56217192 1.41420373]. \t  -120.78865068275417 \t -0.009714587400323622\n",
            "83     \t [1.79282834 0.64597849]. \t  -660.2219442582725 \t -0.009714587400323622\n",
            "84     \t [ 1.70460458 -1.09192798]. \t  -1598.5808533328623 \t -0.009714587400323622\n",
            "85     \t [-1.29975562  1.32315694]. \t  -18.699687040661487 \t -0.009714587400323622\n",
            "86     \t [1.32610079 1.71893042]. \t  -0.26325971782569924 \t -0.009714587400323622\n",
            "87     \t [-2.00994707  0.85300458]. \t  -1024.6818743389156 \t -0.009714587400323622\n",
            "88     \t [-0.88361966 -0.74654485]. \t  -236.82127108162658 \t -0.009714587400323622\n",
            "89     \t [-1.78620936  1.42392272]. \t  -319.8579928979332 \t -0.009714587400323622\n",
            "90     \t [-0.42532623  1.76570948]. \t  -253.19290341592327 \t -0.009714587400323622\n",
            "91     \t [-0.23251562 -0.87987128]. \t  -88.74251487636296 \t -0.009714587400323622\n",
            "92     \t [-0.28023732  0.8644397 ]. \t  -63.40394859145026 \t -0.009714587400323622\n",
            "93     \t [ 0.63138666 -1.65560771]. \t  -422.13298671981994 \t -0.009714587400323622\n",
            "94     \t [1.55955816 0.71295826]. \t  -295.89976870463903 \t -0.009714587400323622\n",
            "95     \t [1.18209282 0.49641175]. \t  -81.20094879602708 \t -0.009714587400323622\n",
            "96     \t [0.11221832 0.17690602]. \t  -3.488034753685005 \t -0.009714587400323622\n",
            "97     \t [ 1.53115004 -0.15748759]. \t  -626.2364977558283 \t -0.009714587400323622\n",
            "98     \t [-0.04377054  1.07377422]. \t  -115.97749058569023 \t -0.009714587400323622\n",
            "99     \t [-1.07275224  1.30988671]. \t  -6.8272437195548115 \t -0.009714587400323622\n",
            "100    \t [ 1.99729274 -0.68967908]. \t  -2190.1652376019097 \t -0.009714587400323622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjKllHqtdwW",
        "outputId": "f0259418-a8f6-46dc-c5ff-030ec44ab1b8"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_loser_12 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_12 = d2GPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.26353633  0.50015753]. \t  -125.32555445527122 \t -19.52113145175031\n",
            "init   \t [-0.25506718  1.16882876]. \t  -123.40590278793995 \t -19.52113145175031\n",
            "init   \t [ 1.14678091 -0.93146069]. \t  -504.72793807789463 \t -19.52113145175031\n",
            "init   \t [-0.91560241  1.23646844]. \t  -19.52113145175031 \t -19.52113145175031\n",
            "init   \t [1.87653879 1.53982007]. \t  -393.43336554061835 \t -19.52113145175031\n",
            "1      \t [-1.5807869   1.84651715]. \t  -49.21913193836258 \t -19.52113145175031\n",
            "2      \t [-5.32092965 -5.3197166 ]. \t  -113151.1568340007 \t -19.52113145175031\n",
            "3      \t [-0.99001754  1.70314883]. \t  -56.23510823872594 \t -19.52113145175031\n",
            "4      \t [-1.41428192  1.35769893]. \t  -47.10866587145217 \t -19.52113145175031\n",
            "5      \t [-1.26900693 -2.01275233]. \t  -1317.8561656117965 \t -19.52113145175031\n",
            "6      \t [-1.19370865 -1.3207978 ]. \t  -758.720153086171 \t -19.52113145175031\n",
            "7      \t [0.49818184 1.06984657]. \t  -67.76457057592293 \t -19.52113145175031\n",
            "8      \t [0.5651675  0.63849688]. \t  \u001b[92m-10.370448682858372\u001b[0m \t -10.370448682858372\n",
            "9      \t [0.76618385 0.84890775]. \t  \u001b[92m-6.9122626838359436\u001b[0m \t -6.9122626838359436\n",
            "10     \t [0.9267065  0.70950159]. \t  \u001b[92m-2.2339237174316997\u001b[0m \t -2.2339237174316997\n",
            "11     \t [0.87996977 0.69771017]. \t  \u001b[92m-0.6017243805315178\u001b[0m \t -0.6017243805315178\n",
            "12     \t [-0.45669532 -0.58668045]. \t  -65.36438753782245 \t -0.6017243805315178\n",
            "13     \t [0.58359327 1.09024576]. \t  -56.37310446471709 \t -0.6017243805315178\n",
            "14     \t [-0.44674014 -0.00499089]. \t  -6.277848991175647 \t -0.6017243805315178\n",
            "15     \t [ 0.13180762 -0.02910733]. \t  -0.969802387030947 \t -0.6017243805315178\n",
            "16     \t [-0.00782162 -0.23369184]. \t  -6.479751595050169 \t -0.6017243805315178\n",
            "17     \t [-0.2427739   0.03344125]. \t  -1.6095013394474955 \t -0.6017243805315178\n",
            "18     \t [0.31574566 0.10316433]. \t  \u001b[92m-0.46940741028878447\u001b[0m \t -0.46940741028878447\n",
            "19     \t [1.37233369 0.60017456]. \t  -164.77966097529503 \t -0.46940741028878447\n",
            "20     \t [-0.16822463  0.21355451]. \t  -4.7966896499975835 \t -0.46940741028878447\n",
            "21     \t [-1.8898996  -1.52770608]. \t  -2608.766667606483 \t -0.46940741028878447\n",
            "22     \t [-0.67528109  0.37605484]. \t  -3.4457624042398445 \t -0.46940741028878447\n",
            "23     \t [-0.19226789  1.6783132 ]. \t  -270.82325797463244 \t -0.46940741028878447\n",
            "24     \t [0.83854704 0.75493754]. \t  \u001b[92m-0.29414667265327027\u001b[0m \t -0.29414667265327027\n",
            "25     \t [-1.90380966  1.58542849]. \t  -424.20978872368084 \t -0.29414667265327027\n",
            "26     \t [0.66567971 0.51961789]. \t  -0.6968178999187397 \t -0.29414667265327027\n",
            "27     \t [0.7011432  0.21686683]. \t  -7.637245058596269 \t -0.29414667265327027\n",
            "28     \t [-1.25298003  1.75584366]. \t  -8.531231134607253 \t -0.29414667265327027\n",
            "29     \t [-0.8380545  -1.56912158]. \t  -519.3301011515124 \t -0.29414667265327027\n",
            "30     \t [-1.63060351  2.03515271]. \t  -45.82212679869235 \t -0.29414667265327027\n",
            "31     \t [-0.25784332 -1.26042859]. \t  -177.651652670226 \t -0.29414667265327027\n",
            "32     \t [1.28092455 1.27436466]. \t  -13.504038134636817 \t -0.29414667265327027\n",
            "33     \t [0.65461909 0.42844223]. \t  \u001b[92m-0.1192886759589269\u001b[0m \t -0.1192886759589269\n",
            "34     \t [1.13676642 1.71329304]. \t  -17.74744904508429 \t -0.1192886759589269\n",
            "35     \t [-1.25457954 -0.90390996]. \t  -619.0719516289206 \t -0.1192886759589269\n",
            "36     \t [1.13648388 1.31268008]. \t  \u001b[92m-0.06308328425918654\u001b[0m \t -0.06308328425918654\n",
            "37     \t [1.16298671 0.3173189 ]. \t  -107.1944404400957 \t -0.06308328425918654\n",
            "38     \t [-0.87070096  1.76451424]. \t  -104.78242689426114 \t -0.06308328425918654\n",
            "39     \t [1.17835732 0.73878845]. \t  -42.24769636950319 \t -0.06308328425918654\n",
            "40     \t [1.03056976 1.15819075]. \t  -0.924777087160749 \t -0.06308328425918654\n",
            "41     \t [-1.60659348  0.49208595]. \t  -443.21010900556564 \t -0.06308328425918654\n",
            "42     \t [1.0078514  1.01446131]. \t  \u001b[92m-0.00023145942915070469\u001b[0m \t -0.00023145942915070469\n",
            "43     \t [1.03757793 1.11179844]. \t  -0.12553074575784126 \t -0.00023145942915070469\n",
            "44     \t [ 1.97086788 -1.38085597]. \t  -2773.1505780850493 \t -0.00023145942915070469\n",
            "45     \t [-1.1721999   1.78049686]. \t  -21.238145501831017 \t -0.00023145942915070469\n",
            "46     \t [ 0.39378869 -1.16170092]. \t  -173.75593578988787 \t -0.00023145942915070469\n",
            "47     \t [ 0.74708299 -1.63196796]. \t  -479.71818826441717 \t -0.00023145942915070469\n",
            "48     \t [1.00275562 1.00025021]. \t  -0.0027834374847643477 \t -0.00023145942915070469\n",
            "49     \t [2.0291388  0.88476863]. \t  -1046.0524466450936 \t -0.00023145942915070469\n",
            "50     \t [3.00496407 3.43016232]. \t  -3139.624253267591 \t -0.00023145942915070469\n",
            "51     \t [0.85234672 0.72202859]. \t  -0.023796306809060305 \t -0.00023145942915070469\n",
            "52     \t [-0.79672646  0.54017111]. \t  -4.1231787832939615 \t -0.00023145942915070469\n",
            "53     \t [1.36862969 1.74698665]. \t  -1.727536775507679 \t -0.00023145942915070469\n",
            "54     \t [-1.97029281 -0.58851854]. \t  -2007.4243074353817 \t -0.00023145942915070469\n",
            "55     \t [ 0.7991025  -1.46293189]. \t  -441.6691954364644 \t -0.00023145942915070469\n",
            "56     \t [ 1.81666571 -1.39798178]. \t  -2208.027975628221 \t -0.00023145942915070469\n",
            "57     \t [-0.38189943 -0.10655757]. \t  -8.280461489813375 \t -0.00023145942915070469\n",
            "58     \t [-0.6690598   0.62857839]. \t  -6.059594112791341 \t -0.00023145942915070469\n",
            "59     \t [-1.05187429  0.9227578 ]. \t  -7.584085831378985 \t -0.00023145942915070469\n",
            "60     \t [1.21477754 1.44272193]. \t  -0.15478220990516706 \t -0.00023145942915070469\n",
            "61     \t [1.17391862 1.40292686]. \t  -0.09195989196567633 \t -0.00023145942915070469\n",
            "62     \t [0.99416961 0.99303535]. \t  -0.00220754470493691 \t -0.00023145942915070469\n",
            "63     \t [ 1.38770185 -0.97071159]. \t  -839.0798349707751 \t -0.00023145942915070469\n",
            "64     \t [1.31559841 1.66975903]. \t  -0.4721923522364755 \t -0.00023145942915070469\n",
            "65     \t [ 1.40147933 -0.0424511 ]. \t  -402.80370032728825 \t -0.00023145942915070469\n",
            "66     \t [ 0.62192199 -1.03710214]. \t  -202.88896043583838 \t -0.00023145942915070469\n",
            "67     \t [-0.99635422  1.42286402]. \t  -22.48766865522903 \t -0.00023145942915070469\n",
            "68     \t [-0.78083258 -1.02298409]. \t  -269.7369418380132 \t -0.00023145942915070469\n",
            "69     \t [0.5688227  0.28067253]. \t  -0.36984110356267497 \t -0.00023145942915070469\n",
            "70     \t [0.67177029 0.74464743]. \t  -8.714454155806434 \t -0.00023145942915070469\n",
            "71     \t [-1.47018171  0.40299074]. \t  -315.3141561232518 \t -0.00023145942915070469\n",
            "72     \t [1.27200548 0.21893352]. \t  -195.81210967711115 \t -0.00023145942915070469\n",
            "73     \t [-0.73846938  1.78295618]. \t  -156.1923914126747 \t -0.00023145942915070469\n",
            "74     \t [1.29200634 1.66625638]. \t  -0.08618215770273947 \t -0.00023145942915070469\n",
            "75     \t [0.51526031 0.24503145]. \t  -0.27684083081721195 \t -0.00023145942915070469\n",
            "76     \t [1.35840484 0.76388173]. \t  -117.06715361980729 \t -0.00023145942915070469\n",
            "77     \t [ 0.41220436 -1.23097664]. \t  -196.59452422588546 \t -0.00023145942915070469\n",
            "78     \t [ 0.94679312 -1.0564852 ]. \t  -381.3856099237539 \t -0.00023145942915070469\n",
            "79     \t [0.3264778  1.96547712]. \t  -346.00059964084403 \t -0.00023145942915070469\n",
            "80     \t [1.28286425 0.77135349]. \t  -76.53531097115662 \t -0.00023145942915070469\n",
            "81     \t [-0.05827082 -1.51731853]. \t  -232.3770506578901 \t -0.00023145942915070469\n",
            "82     \t [-0.0245695   0.50057882]. \t  -26.047258765573183 \t -0.00023145942915070469\n",
            "83     \t [1.52288195 0.32830641]. \t  -396.62696437497647 \t -0.00023145942915070469\n",
            "84     \t [ 0.24020323 -1.74935592]. \t  -327.12153009436537 \t -0.00023145942915070469\n",
            "85     \t [-0.37725487  1.1371482 ]. \t  -100.86489912110035 \t -0.00023145942915070469\n",
            "86     \t [0.59075068 0.3705354 ]. \t  -0.21392106764824198 \t -0.00023145942915070469\n",
            "87     \t [0.95115313 0.90638088]. \t  -0.0026711546260692227 \t -0.00023145942915070469\n",
            "88     \t [-1.95437521 -0.72886382]. \t  -2077.5646770921367 \t -0.00023145942915070469\n",
            "89     \t [ 1.22901738 -1.40143556]. \t  -847.9798365211404 \t -0.00023145942915070469\n",
            "90     \t [-1.56558783  0.93830891]. \t  -235.42541967516422 \t -0.00023145942915070469\n",
            "91     \t [-0.65059558 -0.51476499]. \t  -90.71629548434643 \t -0.00023145942915070469\n",
            "92     \t [ 1.23184003 -0.68497411]. \t  -485.1120682204871 \t -0.00023145942915070469\n",
            "93     \t [-0.5821073  -1.55784162]. \t  -362.24656034397196 \t -0.00023145942915070469\n",
            "94     \t [-1.93926247  0.49980756]. \t  -1072.006603036883 \t -0.00023145942915070469\n",
            "95     \t [-1.40258848  2.01904946]. \t  -6.04070376181843 \t -0.00023145942915070469\n",
            "96     \t [ 0.21458814 -1.62712584]. \t  -280.5679665819224 \t -0.00023145942915070469\n",
            "97     \t [1.00421244 1.90356606]. \t  -80.12461242201168 \t -0.00023145942915070469\n",
            "98     \t [1.28873441 1.66118257]. \t  -0.08337954378610914 \t -0.00023145942915070469\n",
            "99     \t [-1.00773153 -1.93180522]. \t  -872.7052511651193 \t -0.00023145942915070469\n",
            "100    \t [-0.43163006 -0.55778265]. \t  -57.41613508491781 \t -0.00023145942915070469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoONg4VEtdy-",
        "outputId": "3a030953-af46-438d-a13e-33c2ba92c59f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_loser_13 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_13 = d2GPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.65799909 -0.35389173]. \t  -62.0309701572776 \t -62.0309701572776\n",
            "init   \t [ 0.68854808 -1.10615174]. \t  -249.81607110978882 \t -62.0309701572776\n",
            "init   \t [1.26025046 0.56040843]. \t  -105.7097019264073 \t -62.0309701572776\n",
            "init   \t [-1.34269398 -0.98145948]. \t  -780.713409392333 \t -62.0309701572776\n",
            "init   \t [ 1.70115068 -0.15230767]. \t  -928.4380405275942 \t -62.0309701572776\n",
            "1      \t [0.36934199 0.61085505]. \t  \u001b[92m-22.907207745006197\u001b[0m \t -22.907207745006197\n",
            "2      \t [1.58116795 1.67002464]. \t  -69.2389530876244 \t -22.907207745006197\n",
            "3      \t [0.62990312 0.33262501]. \t  \u001b[92m-0.5485315957962275\u001b[0m \t -0.5485315957962275\n",
            "4      \t [1.11519895 1.11681829]. \t  -1.622373386589119 \t -0.5485315957962275\n",
            "5      \t [0.28920962 0.09037692]. \t  \u001b[92m-0.509758614970116\u001b[0m \t -0.509758614970116\n",
            "6      \t [-0.99950196  1.73732785]. \t  -58.5101944451962 \t -0.509758614970116\n",
            "7      \t [ 0.32250907 -0.39110006]. \t  -24.97259877268609 \t -0.509758614970116\n",
            "8      \t [0.42458555 0.10540022]. \t  -0.8916934884883143 \t -0.509758614970116\n",
            "9      \t [-1.00286342  0.61194266]. \t  -19.518705950172787 \t -0.509758614970116\n",
            "10     \t [0.9921364  0.76037074]. \t  -5.016043981191586 \t -0.509758614970116\n",
            "11     \t [-0.57302366  0.56387679]. \t  -8.021402399957688 \t -0.509758614970116\n",
            "12     \t [-2.26772279  1.58335854]. \t  -1277.4742616974606 \t -0.509758614970116\n",
            "13     \t [-0.09970933  2.00673877]. \t  -399.9291134103481 \t -0.509758614970116\n",
            "14     \t [0.46157773 0.36937418]. \t  -2.7334985049068496 \t -0.509758614970116\n",
            "15     \t [-0.29104128  0.85903597]. \t  -61.6256283562939 \t -0.509758614970116\n",
            "16     \t [0.76236929 0.55327104]. \t  \u001b[92m-0.13450980400950044\u001b[0m \t -0.13450980400950044\n",
            "17     \t [-0.64492089  0.2439618 ]. \t  -5.662828626957081 \t -0.13450980400950044\n",
            "18     \t [1.56153311 0.02799735]. \t  -581.3124927720667 \t -0.13450980400950044\n",
            "19     \t [ 1.39317566 -0.10390501]. \t  -418.29305163403814 \t -0.13450980400950044\n",
            "20     \t [0.92805809 0.87413209]. \t  \u001b[92m-0.02166289307486688\u001b[0m \t -0.02166289307486688\n",
            "21     \t [-0.52856026 -1.00599816]. \t  -167.55515641751728 \t -0.02166289307486688\n",
            "22     \t [-0.1877756  -2.11968026]. \t  -465.7874221744523 \t -0.02166289307486688\n",
            "23     \t [1.68411474 0.68570093]. \t  -462.9508923919746 \t -0.02166289307486688\n",
            "24     \t [ 0.07681279 -0.08146121]. \t  -1.615476247913909 \t -0.02166289307486688\n",
            "25     \t [-0.95011608 -1.76342322]. \t  -714.6352160143671 \t -0.02166289307486688\n",
            "26     \t [ 1.74525021 -2.03394314]. \t  -2581.034292136097 \t -0.02166289307486688\n",
            "27     \t [0.18921825 0.16059989]. \t  -2.214779877651205 \t -0.02166289307486688\n",
            "28     \t [-1.41188748  0.3343338 ]. \t  -281.07598340222137 \t -0.02166289307486688\n",
            "29     \t [ 1.97957486 -1.68024005]. \t  -3135.791165885233 \t -0.02166289307486688\n",
            "30     \t [-2.03682352 -1.95950133]. \t  -3740.1736451495244 \t -0.02166289307486688\n",
            "31     \t [-0.73440544  0.50750786]. \t  -3.10956305470775 \t -0.02166289307486688\n",
            "32     \t [-0.72830085  0.70355003]. \t  -5.9843508877020195 \t -0.02166289307486688\n",
            "33     \t [-1.83727401  1.86692274]. \t  -235.65352583664207 \t -0.02166289307486688\n",
            "34     \t [-1.41726462  1.984549  ]. \t  -5.901201106476064 \t -0.02166289307486688\n",
            "35     \t [-0.68082076 -0.94634829]. \t  -201.59714479346763 \t -0.02166289307486688\n",
            "36     \t [-1.29592936  1.71113221]. \t  -5.37177619141482 \t -0.02166289307486688\n",
            "37     \t [-1.54445384 -0.55743512]. \t  -872.4654088084312 \t -0.02166289307486688\n",
            "38     \t [-1.8437376   0.34724672]. \t  -939.6314711821524 \t -0.02166289307486688\n",
            "39     \t [0.96352453 0.9302635 ]. \t  \u001b[92m-0.001685400343915174\u001b[0m \t -0.001685400343915174\n",
            "40     \t [ 1.19278592 -1.8498044 ]. \t  -1070.9907086052215 \t -0.001685400343915174\n",
            "41     \t [1.20384507 1.73687691]. \t  -8.314881768060129 \t -0.001685400343915174\n",
            "42     \t [ 0.4781391  -1.74497403]. \t  -389.7784949090759 \t -0.001685400343915174\n",
            "43     \t [-1.65106047  1.61048312]. \t  -131.46606497698292 \t -0.001685400343915174\n",
            "44     \t [ 1.22585933 -1.0603278 ]. \t  -656.9781034524161 \t -0.001685400343915174\n",
            "45     \t [1.2182756  1.60672611]. \t  -1.5490204913747572 \t -0.001685400343915174\n",
            "46     \t [1.49237443 2.01963374]. \t  -4.55003699976616 \t -0.001685400343915174\n",
            "47     \t [-1.92765507 -0.28267287]. \t  -1607.3929445715162 \t -0.001685400343915174\n",
            "48     \t [ 0.88818791 -1.32839832]. \t  -448.2983037969398 \t -0.001685400343915174\n",
            "49     \t [-0.65987705 -0.13774205]. \t  -35.608697485650005 \t -0.001685400343915174\n",
            "50     \t [1.9362214  1.29276233]. \t  -604.1639204282061 \t -0.001685400343915174\n",
            "51     \t [-0.62056197 -1.76656905]. \t  -465.5929716936383 \t -0.001685400343915174\n",
            "52     \t [-0.21907144 -1.96703807]. \t  -407.5208723607068 \t -0.001685400343915174\n",
            "53     \t [-1.70191383  1.46957766]. \t  -210.91412597964643 \t -0.001685400343915174\n",
            "54     \t [-1.10215362  1.20305473]. \t  -4.432710490178077 \t -0.001685400343915174\n",
            "55     \t [-0.6445735   0.19937118]. \t  -7.374707665560582 \t -0.001685400343915174\n",
            "56     \t [0.8318862  1.35426881]. \t  -43.88367000284221 \t -0.001685400343915174\n",
            "57     \t [-0.49534133 -0.87140828]. \t  -126.95386347910244 \t -0.001685400343915174\n",
            "58     \t [0.9100024  0.85537964]. \t  -0.08249360504754533 \t -0.001685400343915174\n",
            "59     \t [0.46896872 0.98691183]. \t  -59.10785224430413 \t -0.001685400343915174\n",
            "60     \t [1.02667399 1.0636183 ]. \t  -0.009848584950261956 \t -0.001685400343915174\n",
            "61     \t [-2.04602583 -0.00310691]. \t  -1764.3256972698305 \t -0.001685400343915174\n",
            "62     \t [ 0.98149748 -1.26261417]. \t  -495.4863362533916 \t -0.001685400343915174\n",
            "63     \t [0.96238417 0.93918237]. \t  -0.018312564324216435 \t -0.001685400343915174\n",
            "64     \t [-0.83953917  0.4039758 ]. \t  -12.434989412544088 \t -0.001685400343915174\n",
            "65     \t [-0.44755408 -1.96706802]. \t  -471.8458404542322 \t -0.001685400343915174\n",
            "66     \t [1.31828968 1.71317367]. \t  -0.1623865275504108 \t -0.001685400343915174\n",
            "67     \t [0.83759372 0.14908133]. \t  -30.550001785808465 \t -0.001685400343915174\n",
            "68     \t [0.98456094 0.25075815]. \t  -51.63913704506476 \t -0.001685400343915174\n",
            "69     \t [ 0.73246608 -1.77368981]. \t  -533.772297993229 \t -0.001685400343915174\n",
            "70     \t [-1.58563016  1.17593792]. \t  -185.78617810527686 \t -0.001685400343915174\n",
            "71     \t [0.79168256 1.72100923]. \t  -119.78125431659336 \t -0.001685400343915174\n",
            "72     \t [-0.81686469  0.1363013 ]. \t  -31.49355347048604 \t -0.001685400343915174\n",
            "73     \t [1.49237833 1.5409657 ]. \t  -47.333238345706455 \t -0.001685400343915174\n",
            "74     \t [1.14984897 1.26353372]. \t  -0.3660727187338451 \t -0.001685400343915174\n",
            "75     \t [1.18280153 1.37675643]. \t  -0.08298067324099273 \t -0.001685400343915174\n",
            "76     \t [ 1.10650968 -0.76968109]. \t  -397.6327937907719 \t -0.001685400343915174\n",
            "77     \t [1.00261888 1.01176499]. \t  -0.004258387839006215 \t -0.001685400343915174\n",
            "78     \t [1.0710362  1.16274755]. \t  -0.029472704418356095 \t -0.001685400343915174\n",
            "79     \t [ 0.29947585 -1.35759449]. \t  -209.95275381756917 \t -0.001685400343915174\n",
            "80     \t [0.41354938 1.88425379]. \t  -293.8598650763438 \t -0.001685400343915174\n",
            "81     \t [ 1.16142564 -0.78263878]. \t  -454.37587569895425 \t -0.001685400343915174\n",
            "82     \t [1.099743   1.21029986]. \t  -0.010023522509316448 \t -0.001685400343915174\n",
            "83     \t [-1.55106656  1.40530115]. \t  -106.60922967669718 \t -0.001685400343915174\n",
            "84     \t [-0.66643241 -1.82568483]. \t  -517.9839136256584 \t -0.001685400343915174\n",
            "85     \t [-1.30003343  0.94185481]. \t  -61.27528154653576 \t -0.001685400343915174\n",
            "86     \t [-1.77977978  0.80529652]. \t  -565.7825354395953 \t -0.001685400343915174\n",
            "87     \t [-1.17277124 -0.21195015]. \t  -256.686568077995 \t -0.001685400343915174\n",
            "88     \t [-0.37738586  2.03176769]. \t  -358.8606279912727 \t -0.001685400343915174\n",
            "89     \t [ 1.09585673 -0.96871735]. \t  -470.7339851117273 \t -0.001685400343915174\n",
            "90     \t [1.21564778 1.42834496]. \t  -0.29107943655372714 \t -0.001685400343915174\n",
            "91     \t [1.26013997 2.01043038]. \t  -17.91640790402721 \t -0.001685400343915174\n",
            "92     \t [1.06460311 1.12424786]. \t  -0.012512767571803815 \t -0.001685400343915174\n",
            "93     \t [-0.60869851  1.45333029]. \t  -119.83704824326959 \t -0.001685400343915174\n",
            "94     \t [-0.37949809 -1.97403528]. \t  -450.51832415832763 \t -0.001685400343915174\n",
            "95     \t [-0.03532646  0.20508262]. \t  -5.226757806966602 \t -0.001685400343915174\n",
            "96     \t [-1.44592539  0.65227005]. \t  -212.89069487972304 \t -0.001685400343915174\n",
            "97     \t [-0.85519495  1.45638497]. \t  -56.008100437405474 \t -0.001685400343915174\n",
            "98     \t [-1.10398428  1.61088889]. \t  -19.801586554812506 \t -0.001685400343915174\n",
            "99     \t [1.0275188  1.04328329]. \t  -0.016411293445117516 \t -0.001685400343915174\n",
            "100    \t [2.0161086 1.2859591]. \t  -773.1691858866905 \t -0.001685400343915174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQHSe2Dtd1m",
        "outputId": "31496d32-5f25-4e76-9bd8-31d20897287a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_loser_14 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_14 = d2GPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.6043691  0.02928512]. \t  -11.444570460525577 \t -4.306489127802793\n",
            "init   \t [0.11608629 1.6231842 ]. \t  -259.89734211112767 \t -4.306489127802793\n",
            "init   \t [0.81916392 0.87776093]. \t  -4.306489127802793 \t -4.306489127802793\n",
            "init   \t [ 0.89021801 -1.13533148]. \t  -371.66089065122344 \t -4.306489127802793\n",
            "init   \t [-1.33056707 -0.17677726]. \t  -384.58487399689074 \t -4.306489127802793\n",
            "1      \t [1.24800705 0.56011416]. \t  -99.54366772238504 \t -4.306489127802793\n",
            "2      \t [0.49471651 0.39956896]. \t  \u001b[92m-2.652375168352414\u001b[0m \t -2.652375168352414\n",
            "3      \t [1.3279018  1.78259635]. \t  \u001b[92m-0.14466504609048203\u001b[0m \t -0.14466504609048203\n",
            "4      \t [-1.10253403 -0.55489366]. \t  -317.8788037567552 \t -0.14466504609048203\n",
            "5      \t [0.58599126 0.35218709]. \t  -0.1791495767026406 \t -0.14466504609048203\n",
            "6      \t [1.22709133 1.64386416]. \t  -1.9590361316735203 \t -0.14466504609048203\n",
            "7      \t [1.95579534 1.99686802]. \t  -335.1697046308149 \t -0.14466504609048203\n",
            "8      \t [ 0.06241567 -1.28637043]. \t  -167.35773773045298 \t -0.14466504609048203\n",
            "9      \t [1.16868084 1.7865735 ]. \t  -17.7322330154344 \t -0.14466504609048203\n",
            "10     \t [-1.9081828   1.84188939]. \t  -332.1955764905027 \t -0.14466504609048203\n",
            "11     \t [-1.51846679 -1.34386354]. \t  -1338.30428218148 \t -0.14466504609048203\n",
            "12     \t [ 0.28703135 -0.42608967]. \t  -26.363176805245892 \t -0.14466504609048203\n",
            "13     \t [-0.53461138 -0.21466716]. \t  -27.402704124229643 \t -0.14466504609048203\n",
            "14     \t [0.69259573 0.5412449 ]. \t  -0.47341230763589415 \t -0.14466504609048203\n",
            "15     \t [-0.26571277 -0.52794389]. \t  -37.42790049501683 \t -0.14466504609048203\n",
            "16     \t [-0.96563258 -1.46251381]. \t  -577.4470927077354 \t -0.14466504609048203\n",
            "17     \t [ 0.92289904 -1.64478012]. \t  -623.2685286112684 \t -0.14466504609048203\n",
            "18     \t [1.18946073 1.36290644]. \t  -0.30536428209668504 \t -0.14466504609048203\n",
            "19     \t [1.25861787 1.56282609]. \t  \u001b[92m-0.11222176042455242\u001b[0m \t -0.11222176042455242\n",
            "20     \t [1.30616259 1.75172054]. \t  -0.3022174203778006 \t -0.11222176042455242\n",
            "21     \t [1.28496257 1.04363211]. \t  -36.98642785984151 \t -0.11222176042455242\n",
            "22     \t [-0.20172275 -0.83419603]. \t  -77.98705544652253 \t -0.11222176042455242\n",
            "23     \t [-0.18188964  0.04843406]. \t  -1.4204260516855303 \t -0.11222176042455242\n",
            "24     \t [0.80634835 0.69451276]. \t  -0.23388378077715044 \t -0.11222176042455242\n",
            "25     \t [-0.17347511 -0.56901051]. \t  -37.26961907705688 \t -0.11222176042455242\n",
            "26     \t [ 1.64179877 -1.22275857]. \t  -1535.6894516137688 \t -0.11222176042455242\n",
            "27     \t [ 1.84769985 -1.65815949]. \t  -2573.3934633382755 \t -0.11222176042455242\n",
            "28     \t [-0.35713266  0.70617794]. \t  -35.32356255369239 \t -0.11222176042455242\n",
            "29     \t [ 0.08063829 -0.60628709]. \t  -38.39633775902093 \t -0.11222176042455242\n",
            "30     \t [-1.39992158  0.49737864]. \t  -219.62152666592806 \t -0.11222176042455242\n",
            "31     \t [-1.08945402 -0.16533092]. \t  -187.22138818008403 \t -0.11222176042455242\n",
            "32     \t [-0.53111638  0.2664204 ]. \t  -2.3688541172125452 \t -0.11222176042455242\n",
            "33     \t [-1.16085581  0.21366414]. \t  -133.24722419089474 \t -0.11222176042455242\n",
            "34     \t [ 1.06406115 -0.38228808]. \t  -229.37943032512587 \t -0.11222176042455242\n",
            "35     \t [0.7476354  1.17273707]. \t  -37.736077303702444 \t -0.11222176042455242\n",
            "36     \t [ 0.12886668 -0.07487823]. \t  -1.5958210172126552 \t -0.11222176042455242\n",
            "37     \t [-0.73850866 -1.64347618]. \t  -482.13813546138 \t -0.11222176042455242\n",
            "38     \t [ 1.47847777 -1.31991067]. \t  -1229.297341811999 \t -0.11222176042455242\n",
            "39     \t [0.24445474 0.67058581]. \t  -37.88189592221282 \t -0.11222176042455242\n",
            "40     \t [ 0.10463596 -0.106465  ]. \t  -2.180274051730791 \t -0.11222176042455242\n",
            "41     \t [1.2577242 1.6006576]. \t  \u001b[92m-0.10171855870529098\u001b[0m \t -0.10171855870529098\n",
            "42     \t [1.43367697 0.32061112]. \t  -301.14760728437517 \t -0.10171855870529098\n",
            "43     \t [1.3584071  1.94007189]. \t  -1.0271985687812801 \t -0.10171855870529098\n",
            "44     \t [-1.5182108   0.80987152]. \t  -229.87155168550814 \t -0.10171855870529098\n",
            "45     \t [ 0.32514034 -1.28652307]. \t  -194.28846499360978 \t -0.10171855870529098\n",
            "46     \t [-1.13412551  1.57038965]. \t  -12.628555986745358 \t -0.10171855870529098\n",
            "47     \t [0.71410584 0.528128  ]. \t  -0.114789819979297 \t -0.10171855870529098\n",
            "48     \t [-0.96660783  1.04014064]. \t  -4.987120680304127 \t -0.10171855870529098\n",
            "49     \t [ 1.23819025 -0.23669483]. \t  -313.27944944341465 \t -0.10171855870529098\n",
            "50     \t [-0.05142272  2.04169095]. \t  -416.87661347692347 \t -0.10171855870529098\n",
            "51     \t [-2.00496167  0.62367447]. \t  -1162.4450734371903 \t -0.10171855870529098\n",
            "52     \t [-1.22461496  1.46191001]. \t  -5.091582585491981 \t -0.10171855870529098\n",
            "53     \t [-0.8835942   0.72000929]. \t  -3.9167333373327198 \t -0.10171855870529098\n",
            "54     \t [0.05155122 0.37314437]. \t  -14.625605019530663 \t -0.10171855870529098\n",
            "55     \t [1.19272477 1.42495248]. \t  \u001b[92m-0.03769984449128316\u001b[0m \t -0.03769984449128316\n",
            "56     \t [ 0.33061555 -1.7446117 ]. \t  -344.14939823471684 \t -0.03769984449128316\n",
            "57     \t [-1.58088158 -0.22387269]. \t  -748.1661172920212 \t -0.03769984449128316\n",
            "58     \t [-0.87484484  1.09494647]. \t  -14.37819674086421 \t -0.03769984449128316\n",
            "59     \t [1.11981129 2.01294109]. \t  -57.61695416628044 \t -0.03769984449128316\n",
            "60     \t [0.69738463 0.43765622]. \t  -0.32863895080849415 \t -0.03769984449128316\n",
            "61     \t [-0.98387027 -1.93324359]. \t  -845.6575912879017 \t -0.03769984449128316\n",
            "62     \t [-0.83360606 -0.39821277]. \t  -122.85145702179399 \t -0.03769984449128316\n",
            "63     \t [-1.66722611  0.57154307]. \t  -494.6845860274701 \t -0.03769984449128316\n",
            "64     \t [ 1.43432849 -0.71886116]. \t  -770.8947222516093 \t -0.03769984449128316\n",
            "65     \t [1.74145888 0.47871748]. \t  -652.8217275890095 \t -0.03769984449128316\n",
            "66     \t [1.03268597 1.06475767]. \t  \u001b[92m-0.0013515000235966132\u001b[0m \t -0.0013515000235966132\n",
            "67     \t [ 1.33553529 -1.94373514]. \t  -1389.4559372659976 \t -0.0013515000235966132\n",
            "68     \t [ 2.03794533 -1.12526205]. \t  -2787.3158341879744 \t -0.0013515000235966132\n",
            "69     \t [-1.70860006 -0.65519372]. \t  -1285.0471706975995 \t -0.0013515000235966132\n",
            "70     \t [ 1.67728183 -0.22584165]. \t  -924.0813148512133 \t -0.0013515000235966132\n",
            "71     \t [-0.55951318  0.93751851]. \t  -41.427548758187484 \t -0.0013515000235966132\n",
            "72     \t [-1.19208035 -1.95536668]. \t  -1144.827932674559 \t -0.0013515000235966132\n",
            "73     \t [-2.02108705  1.69906427]. \t  -578.2970575781673 \t -0.0013515000235966132\n",
            "74     \t [1.05253757 1.0987825 ]. \t  -0.01095559731997343 \t -0.0013515000235966132\n",
            "75     \t [-1.29688211  1.82849896]. \t  -7.4246990651923275 \t -0.0013515000235966132\n",
            "76     \t [ 1.68856139 -1.69422831]. \t  -2066.601950465006 \t -0.0013515000235966132\n",
            "77     \t [ 0.65265246 -1.88587041]. \t  -534.5744305473129 \t -0.0013515000235966132\n",
            "78     \t [-1.69721438 -1.92572466]. \t  -2317.2897352832306 \t -0.0013515000235966132\n",
            "79     \t [ 1.38138552 -1.7334792 ]. \t  -1326.3470913776823 \t -0.0013515000235966132\n",
            "80     \t [-1.18472947  0.67165453]. \t  -58.34510613785976 \t -0.0013515000235966132\n",
            "81     \t [-1.49436433  0.52273651]. \t  -298.7646462028464 \t -0.0013515000235966132\n",
            "82     \t [-1.06147932 -1.21766938]. \t  -553.8744545919657 \t -0.0013515000235966132\n",
            "83     \t [-0.48247986 -1.08389492]. \t  -175.56282593029763 \t -0.0013515000235966132\n",
            "84     \t [ 1.39245159 -1.35409851]. \t  -1084.5520546572018 \t -0.0013515000235966132\n",
            "85     \t [ 0.68835944 -1.54346285]. \t  -407.0476822785868 \t -0.0013515000235966132\n",
            "86     \t [1.01643062 0.60629059]. \t  -18.21956078873848 \t -0.0013515000235966132\n",
            "87     \t [1.3625676  0.35762958]. \t  -224.81982638961267 \t -0.0013515000235966132\n",
            "88     \t [1.67259036 0.60769277]. \t  -480.00357033406283 \t -0.0013515000235966132\n",
            "89     \t [-0.10675584  1.4505127 ]. \t  -208.3303635018325 \t -0.0013515000235966132\n",
            "90     \t [ 1.66960399 -0.90521397]. \t  -1364.119242511162 \t -0.0013515000235966132\n",
            "91     \t [-1.2511616   1.55097647]. \t  -5.0885478413268865 \t -0.0013515000235966132\n",
            "92     \t [-0.32989008  0.52002008]. \t  -18.676544542455904 \t -0.0013515000235966132\n",
            "93     \t [ 1.00089176 -0.74819103]. \t  -306.24137151272816 \t -0.0013515000235966132\n",
            "94     \t [-0.31594662  0.30611815]. \t  -5.987514954531987 \t -0.0013515000235966132\n",
            "95     \t [0.51232447 0.86448493]. \t  -36.479259130122884 \t -0.0013515000235966132\n",
            "96     \t [ 1.19759156 -0.75635892]. \t  -479.90507167496264 \t -0.0013515000235966132\n",
            "97     \t [ 1.29913071 -1.09676739]. \t  -775.4379568726519 \t -0.0013515000235966132\n",
            "98     \t [ 1.98684688 -0.24356647]. \t  -1757.5284212307308 \t -0.0013515000235966132\n",
            "99     \t [ 1.8417731  -0.26493655]. \t  -1338.1207981665664 \t -0.0013515000235966132\n",
            "100    \t [-0.67578045 -1.95670776]. \t  -585.2519126932482 \t -0.0013515000235966132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq4J_FTytd4O",
        "outputId": "01d1d5cf-398f-4e98-c81a-84401360b958"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_loser_15 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_15 = d2GPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.31900409 -1.00146655]. \t  -122.17543449532224 \t -6.867717811955245\n",
            "init   \t [-1.01364994  0.85976824]. \t  -6.867717811955245 \t -6.867717811955245\n",
            "init   \t [-0.21482552 -1.11844164]. \t  -137.1031718367486 \t -6.867717811955245\n",
            "init   \t [-0.40259762  1.56572768]. \t  -198.98860193376754 \t -6.867717811955245\n",
            "init   \t [-0.25717364  1.55002954]. \t  -221.77381285664336 \t -6.867717811955245\n",
            "1      \t [-1.82151828  0.68960249]. \t  -698.7709184396541 \t -6.867717811955245\n",
            "2      \t [-0.20803264  0.28777365]. \t  -7.4371755439131615 \t -6.867717811955245\n",
            "3      \t [-0.71353464  0.78042254]. \t  -10.29607442865114 \t -6.867717811955245\n",
            "4      \t [1.51801528 1.92878838]. \t  -14.374524553155442 \t -6.867717811955245\n",
            "5      \t [1.08082053 0.08053379]. \t  -118.30244166824869 \t -6.867717811955245\n",
            "6      \t [-0.36335747 -0.03783507]. \t  \u001b[92m-4.74411168658011\u001b[0m \t -4.74411168658011\n",
            "7      \t [-1.07305035  1.20223167]. \t  \u001b[92m-4.555547010369245\u001b[0m \t -4.555547010369245\n",
            "8      \t [-0.01669207 -0.02314013]. \t  \u001b[92m-1.0885065846766693\u001b[0m \t -1.0885065846766693\n",
            "9      \t [ 1.92273073 -1.97728368]. \t  -3220.480076808586 \t -1.0885065846766693\n",
            "10     \t [1.25594009 1.69598967]. \t  -1.4722002702680579 \t -1.0885065846766693\n",
            "11     \t [1.1703513 1.3859841]. \t  \u001b[92m-0.05546459981799511\u001b[0m \t -0.05546459981799511\n",
            "12     \t [-1.27366353  1.91863335]. \t  -13.955705840548255 \t -0.05546459981799511\n",
            "13     \t [-0.780779    0.53159184]. \t  -3.7799482857229743 \t -0.05546459981799511\n",
            "14     \t [1.57883823 1.00418797]. \t  -221.91083339516308 \t -0.05546459981799511\n",
            "15     \t [0.7357525  1.10112061]. \t  -31.406183996579944 \t -0.05546459981799511\n",
            "16     \t [-0.53982021  0.34933963]. \t  -2.706678484288056 \t -0.05546459981799511\n",
            "17     \t [1.32339482 1.60912997]. \t  -2.127916248328541 \t -0.05546459981799511\n",
            "18     \t [1.27359219 1.99899866]. \t  -14.284857578721928 \t -0.05546459981799511\n",
            "19     \t [-1.00960544 -1.69868823]. \t  -742.7862207882696 \t -0.05546459981799511\n",
            "20     \t [-1.9191221   1.39569548]. \t  -531.7110285934393 \t -0.05546459981799511\n",
            "21     \t [1.17976353 1.44148663]. \t  -0.27877406347518974 \t -0.05546459981799511\n",
            "22     \t [0.76357062 0.56575029]. \t  -0.08579258876773765 \t -0.05546459981799511\n",
            "23     \t [-1.28718132  1.3649696 ]. \t  -13.749783661926397 \t -0.05546459981799511\n",
            "24     \t [ 0.01659477 -0.33150539]. \t  -11.97493442602081 \t -0.05546459981799511\n",
            "25     \t [0.60887603 0.41065015]. \t  -0.3123396224364302 \t -0.05546459981799511\n",
            "26     \t [ 0.22401697 -1.35838773]. \t  -199.00946979239325 \t -0.05546459981799511\n",
            "27     \t [ 1.42645813 -1.38249747]. \t  -1167.962308583028 \t -0.05546459981799511\n",
            "28     \t [1.27654895 0.99584516]. \t  -40.2381133304632 \t -0.05546459981799511\n",
            "29     \t [0.90609461 0.92591399]. \t  -1.1093566565605248 \t -0.05546459981799511\n",
            "30     \t [ 0.97642433 -0.04462397]. \t  -99.60663494896336 \t -0.05546459981799511\n",
            "31     \t [0.20243607 1.33620308]. \t  -168.3962972454647 \t -0.05546459981799511\n",
            "32     \t [ 1.86202408 -0.41052955]. \t  -1504.3702930072584 \t -0.05546459981799511\n",
            "33     \t [-1.87844345  1.70426946]. \t  -341.08531238943704 \t -0.05546459981799511\n",
            "34     \t [-0.69688178  0.87300274]. \t  -17.884070497534147 \t -0.05546459981799511\n",
            "35     \t [-0.5765119   0.70890288]. \t  -16.66339463956982 \t -0.05546459981799511\n",
            "36     \t [0.5749547  0.29133758]. \t  -0.33460452191642776 \t -0.05546459981799511\n",
            "37     \t [ 0.94346704 -0.1780317 ]. \t  -114.10015004857898 \t -0.05546459981799511\n",
            "38     \t [0.75464442 0.56752197]. \t  -0.060585966473464356 \t -0.05546459981799511\n",
            "39     \t [0.69851994 0.45008435]. \t  -0.23412040565448666 \t -0.05546459981799511\n",
            "40     \t [1.15381854 1.09155764]. \t  -5.77116687300655 \t -0.05546459981799511\n",
            "41     \t [-1.33649111 -0.36559919]. \t  -468.48682209390284 \t -0.05546459981799511\n",
            "42     \t [-0.45892964  0.92975848]. \t  -53.84500608764505 \t -0.05546459981799511\n",
            "43     \t [0.98128879 0.91133416]. \t  -0.26653920343508974 \t -0.05546459981799511\n",
            "44     \t [0.17414949 0.71603838]. \t  -47.70189533400468 \t -0.05546459981799511\n",
            "45     \t [ 1.68741734 -1.4058258 ]. \t  -1809.4462006025503 \t -0.05546459981799511\n",
            "46     \t [0.87307096 1.35901039]. \t  -35.6280596638206 \t -0.05546459981799511\n",
            "47     \t [1.02623239 1.06208972]. \t  \u001b[92m-0.00867479644395892\u001b[0m \t -0.00867479644395892\n",
            "48     \t [1.35710706 1.88281138]. \t  -0.29621477368128035 \t -0.00867479644395892\n",
            "49     \t [-1.12522902 -1.26760128]. \t  -646.5012649272198 \t -0.00867479644395892\n",
            "50     \t [1.02357673 1.05171782]. \t  \u001b[92m-0.0021626676448831587\u001b[0m \t -0.0021626676448831587\n",
            "51     \t [0.81650675 0.64244951]. \t  -0.09239729910080685 \t -0.0021626676448831587\n",
            "52     \t [-0.97041722 -0.56868483]. \t  -232.01166970174543 \t -0.0021626676448831587\n",
            "53     \t [0.92711275 0.88121177]. \t  -0.05228755474200532 \t -0.0021626676448831587\n",
            "54     \t [0.96021954 0.95337528]. \t  -0.09988798143770664 \t -0.0021626676448831587\n",
            "55     \t [ 0.13604281 -1.96151768]. \t  -392.79644976636814 \t -0.0021626676448831587\n",
            "56     \t [0.49790348 1.06233932]. \t  -66.5819590662643 \t -0.0021626676448831587\n",
            "57     \t [0.80687221 0.63348717]. \t  -0.06811821303302987 \t -0.0021626676448831587\n",
            "58     \t [0.95246595 0.91919823]. \t  -0.01667593589089989 \t -0.0021626676448831587\n",
            "59     \t [0.74736134 0.50395292]. \t  -0.361899100263984 \t -0.0021626676448831587\n",
            "60     \t [ 2.02435328 -0.77141525]. \t  -2372.1758244197367 \t -0.0021626676448831587\n",
            "61     \t [0.49676181 0.3065502 ]. \t  -0.6105884926302754 \t -0.0021626676448831587\n",
            "62     \t [ 0.74638391 -0.94930964]. \t  -226.98799123775052 \t -0.0021626676448831587\n",
            "63     \t [-1.85923427  0.72504786]. \t  -754.3960043276296 \t -0.0021626676448831587\n",
            "64     \t [1.29422912 1.65370397]. \t  -0.13204656406780957 \t -0.0021626676448831587\n",
            "65     \t [-1.0754649   0.43522195]. \t  -56.34975499227283 \t -0.0021626676448831587\n",
            "66     \t [0.68320998 0.47494266]. \t  -0.10702555738314529 \t -0.0021626676448831587\n",
            "67     \t [0.05975501 0.77168105]. \t  -59.883417596274604 \t -0.0021626676448831587\n",
            "68     \t [0.96582491 0.94273216]. \t  -0.010997489813224338 \t -0.0021626676448831587\n",
            "69     \t [0.99943212 1.00428487]. \t  -0.0029382945008131363 \t -0.0021626676448831587\n",
            "70     \t [1.00630484 1.01727835]. \t  -0.0021824393687718793 \t -0.0021626676448831587\n",
            "71     \t [-0.0198923  -0.92816516]. \t  -87.26270799406798 \t -0.0021626676448831587\n",
            "72     \t [1.01021726 0.99828821]. \t  -0.049613736625678376 \t -0.0021626676448831587\n",
            "73     \t [-0.88726298  1.60797499]. \t  -70.92307559057677 \t -0.0021626676448831587\n",
            "74     \t [-1.9100255   0.72643737]. \t  -862.1364214861749 \t -0.0021626676448831587\n",
            "75     \t [ 0.56475636 -1.4631005 ]. \t  -317.7597443470307 \t -0.0021626676448831587\n",
            "76     \t [ 0.82312635 -1.53627469]. \t  -490.1274956390867 \t -0.0021626676448831587\n",
            "77     \t [-0.50112241  0.72246393]. \t  -24.46953284373991 \t -0.0021626676448831587\n",
            "78     \t [ 2.0139713  -0.14158256]. \t  -1763.065573072827 \t -0.0021626676448831587\n",
            "79     \t [0.43465746 0.19616264]. \t  -0.32484748480555864 \t -0.0021626676448831587\n",
            "80     \t [ 0.74454531 -1.06963287]. \t  -263.7965541835455 \t -0.0021626676448831587\n",
            "81     \t [1.58678507 2.0096641 ]. \t  -26.173355388676914 \t -0.0021626676448831587\n",
            "82     \t [0.93562776 0.86152371]. \t  -0.023396993412763373 \t -0.0021626676448831587\n",
            "83     \t [-0.06708015 -1.62779617]. \t  -267.5776570316121 \t -0.0021626676448831587\n",
            "84     \t [0.68517674 0.17783462]. \t  -8.604067714590572 \t -0.0021626676448831587\n",
            "85     \t [0.50152512 0.21268759]. \t  -0.3993306669650516 \t -0.0021626676448831587\n",
            "86     \t [-1.00569849  0.65747462]. \t  -16.551228310140736 \t -0.0021626676448831587\n",
            "87     \t [-1.69201713 -0.00320477]. \t  -828.7152142879839 \t -0.0021626676448831587\n",
            "88     \t [ 0.20134487 -0.00572668]. \t  -0.8519083034298089 \t -0.0021626676448831587\n",
            "89     \t [-0.40366291 -0.83154994]. \t  -100.87203805527996 \t -0.0021626676448831587\n",
            "90     \t [-0.59352026 -0.2073231 ]. \t  -33.85333716244417 \t -0.0021626676448831587\n",
            "91     \t [1.32553206 0.05199988]. \t  -290.8205294941111 \t -0.0021626676448831587\n",
            "92     \t [1.1832348  1.38873823]. \t  -0.04635836716497785 \t -0.0021626676448831587\n",
            "93     \t [-1.85018417  1.29794549]. \t  -459.7863391163911 \t -0.0021626676448831587\n",
            "94     \t [1.53159452 0.88600749]. \t  -213.37668811417538 \t -0.0021626676448831587\n",
            "95     \t [1.06770387 1.46676907]. \t  -10.682938992362306 \t -0.0021626676448831587\n",
            "96     \t [ 1.52335898 -1.74116497]. \t  -1650.0857154121345 \t -0.0021626676448831587\n",
            "97     \t [0.87720362 0.79027476]. \t  -0.05829541102554585 \t -0.0021626676448831587\n",
            "98     \t [ 0.83029518 -0.30582201]. \t  -99.07350907469653 \t -0.0021626676448831587\n",
            "99     \t [-0.87332185 -0.51619905]. \t  -167.06532525331025 \t -0.0021626676448831587\n",
            "100    \t [-1.01396943 -1.98510142]. \t  -912.0148419647545 \t -0.0021626676448831587\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tJ-ta9Gtd7H",
        "outputId": "f856515a-dcbf-4804-bbd2-c377d63d445f"
      },
      "source": [
        "\r\n",
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_loser_16 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_16 = d2GPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.49073237 -0.75216486]. \t  -884.9748202573794 \t -21.690996320546372\n",
            "init   \t [0.70512959 0.03240619]. \t  -21.690996320546372 \t -21.690996320546372\n",
            "init   \t [ 1.15368111 -0.88603985]. \t  -491.54136063574043 \t -21.690996320546372\n",
            "init   \t [-1.09072882  0.26132352]. \t  -90.5574614167091 \t -21.690996320546372\n",
            "init   \t [1.5360998 0.8967902]. \t  -214.26941031258198 \t -21.690996320546372\n",
            "1      \t [ 0.02637858 -1.11294955]. \t  -124.96854236820391 \t -21.690996320546372\n",
            "2      \t [0.24240804 0.47502699]. \t  \u001b[92m-17.901628692275246\u001b[0m \t -17.901628692275246\n",
            "3      \t [ 0.32835375 -0.26393763]. \t  \u001b[92m-14.271198725936454\u001b[0m \t -14.271198725936454\n",
            "4      \t [-2.08679219  0.77173543]. \t  -1293.2929780713318 \t -14.271198725936454\n",
            "5      \t [-1.00309663 -0.18113209]. \t  -144.98882165820356 \t -14.271198725936454\n",
            "6      \t [ 0.39616178 -2.15203159]. \t  -533.5015187890908 \t -14.271198725936454\n",
            "7      \t [-0.61694144  0.88471295]. \t  -28.025799395554486 \t -14.271198725936454\n",
            "8      \t [-0.58007505  0.28662699]. \t  \u001b[92m-2.7452398147394588\u001b[0m \t -2.7452398147394588\n",
            "9      \t [0.52592076 0.53115105]. \t  -6.704749011412503 \t -2.7452398147394588\n",
            "10     \t [-1.58662742 -0.14000879]. \t  -712.8656417917898 \t -2.7452398147394588\n",
            "11     \t [0.41175023 0.90810386]. \t  -54.893952686434055 \t -2.7452398147394588\n",
            "12     \t [0.41482158 0.12694349]. \t  \u001b[92m-0.5461366341189933\u001b[0m \t -0.5461366341189933\n",
            "13     \t [ 1.99132563 -0.5046576 ]. \t  -1999.1043247698622 \t -0.5461366341189933\n",
            "14     \t [1.79656131 1.94136013]. \t  -166.0841831545317 \t -0.5461366341189933\n",
            "15     \t [-0.58362187  0.51050397]. \t  -5.394101909149882 \t -0.5461366341189933\n",
            "16     \t [0.5698142  0.32125695]. \t  \u001b[92m-0.1862371831330674\u001b[0m \t -0.1862371831330674\n",
            "17     \t [0.42811862 0.19662396]. \t  -0.34483962324341405 \t -0.1862371831330674\n",
            "18     \t [1.28959433 1.43567933]. \t  -5.2537672055258255 \t -0.1862371831330674\n",
            "19     \t [-1.83963812  0.09233433]. \t  -1091.7465452041292 \t -0.1862371831330674\n",
            "20     \t [-1.29034655  1.93821483]. \t  -12.710638339021232 \t -0.1862371831330674\n",
            "21     \t [-1.08214435  1.89426662]. \t  -56.641519232196316 \t -0.1862371831330674\n",
            "22     \t [-1.09066884  1.80179203]. \t  -41.85388301514736 \t -0.1862371831330674\n",
            "23     \t [0.86675926 0.73687586]. \t  \u001b[92m-0.038476882343683974\u001b[0m \t -0.038476882343683974\n",
            "24     \t [-0.31372452  0.2296909 ]. \t  -3.44899629234766 \t -0.038476882343683974\n",
            "25     \t [-0.23046947 -0.24219856]. \t  -10.23513447929492 \t -0.038476882343683974\n",
            "26     \t [1.11779956 1.30514399]. \t  -0.32377083283254593 \t -0.038476882343683974\n",
            "27     \t [ 0.67906291 -1.46483459]. \t  -371.0355872844689 \t -0.038476882343683974\n",
            "28     \t [-1.99274381  0.30019054]. \t  -1356.4611941865257 \t -0.038476882343683974\n",
            "29     \t [-1.76176897 -1.25921798]. \t  -1911.2460390105102 \t -0.038476882343683974\n",
            "30     \t [ 0.9332122  -1.70126987]. \t  -661.6025332165561 \t -0.038476882343683974\n",
            "31     \t [ 0.79193639 -1.45821261]. \t  -434.9225345066252 \t -0.038476882343683974\n",
            "32     \t [1.02179283 0.94301733]. \t  -1.0214491533240662 \t -0.038476882343683974\n",
            "33     \t [1.0888245  1.19069713]. \t  \u001b[92m-0.010550632919207846\u001b[0m \t -0.010550632919207846\n",
            "34     \t [0.71847905 0.42270834]. \t  -0.953550266626682 \t -0.010550632919207846\n",
            "35     \t [-1.28105532  1.59582301]. \t  -5.408238774268764 \t -0.010550632919207846\n",
            "36     \t [ 0.23034354 -0.58857712]. \t  -41.761952530407115 \t -0.010550632919207846\n",
            "37     \t [ 0.0871563  -0.03197559]. \t  -0.9898764260466782 \t -0.010550632919207846\n",
            "38     \t [1.21043543 2.00032964]. \t  -28.685588517613564 \t -0.010550632919207846\n",
            "39     \t [-1.69585202 -1.39566601]. \t  -1831.9072668479507 \t -0.010550632919207846\n",
            "40     \t [ 0.07572925 -2.0285615 ]. \t  -414.69046851086 \t -0.010550632919207846\n",
            "41     \t [-0.33921788  2.02645588]. \t  -367.13357261498413 \t -0.010550632919207846\n",
            "42     \t [1.51433978 2.00771194]. \t  -8.416314764337956 \t -0.010550632919207846\n",
            "43     \t [1.05913737 1.12414879]. \t  \u001b[92m-0.0040621530356655545\u001b[0m \t -0.0040621530356655545\n",
            "44     \t [ 1.6790709  -1.97870545]. \t  -2302.5266957759495 \t -0.0040621530356655545\n",
            "45     \t [-0.61286995 -1.65116907]. \t  -413.3845153643779 \t -0.0040621530356655545\n",
            "46     \t [1.04565638 1.10311869]. \t  -0.011535096913577314 \t -0.0040621530356655545\n",
            "47     \t [-0.22066691 -0.38020652]. \t  -19.885583106297887 \t -0.0040621530356655545\n",
            "48     \t [1.35668294 1.8983668 ]. \t  -0.46105478041088677 \t -0.0040621530356655545\n",
            "49     \t [1.35945442 2.01107771]. \t  -2.7848485364500832 \t -0.0040621530356655545\n",
            "50     \t [0.60749349 0.32997843]. \t  -0.30670717787550733 \t -0.0040621530356655545\n",
            "51     \t [1.12478789 1.27913747]. \t  -0.035143115742366546 \t -0.0040621530356655545\n",
            "52     \t [-0.67517123 -1.00998659]. \t  -217.67570332419444 \t -0.0040621530356655545\n",
            "53     \t [-1.27228905 -1.86451717]. \t  -1218.457017748896 \t -0.0040621530356655545\n",
            "54     \t [0.68674822 0.44958093]. \t  -0.14671246635983115 \t -0.0040621530356655545\n",
            "55     \t [-0.64063064 -1.4676662 ]. \t  -355.40779388843447 \t -0.0040621530356655545\n",
            "56     \t [ 1.44253993 -0.83813951]. \t  -852.2875350360233 \t -0.0040621530356655545\n",
            "57     \t [-1.20340356  0.65695278]. \t  -67.45905943685975 \t -0.0040621530356655545\n",
            "58     \t [2.950141   4.49967173]. \t  -1770.8789503240266 \t -0.0040621530356655545\n",
            "59     \t [-0.93991592  0.77228725]. \t  -4.998810234724818 \t -0.0040621530356655545\n",
            "60     \t [0.00200424 1.56567155]. \t  -246.12747811354427 \t -0.0040621530356655545\n",
            "61     \t [-0.78139034  0.53479172]. \t  -3.747599573318423 \t -0.0040621530356655545\n",
            "62     \t [ 1.35482936 -0.40326941]. \t  -501.3627715286027 \t -0.0040621530356655545\n",
            "63     \t [-0.19401555 -0.5657402 ]. \t  -37.83268513239098 \t -0.0040621530356655545\n",
            "64     \t [-1.88164839 -0.66326923]. \t  -1775.5560972745068 \t -0.0040621530356655545\n",
            "65     \t [-0.35956237  1.98789795]. \t  -347.292581304873 \t -0.0040621530356655545\n",
            "66     \t [-1.5604096  -1.72930828]. \t  -1740.6005414601545 \t -0.0040621530356655545\n",
            "67     \t [-0.51937017 -1.56237223]. \t  -337.97397823813054 \t -0.0040621530356655545\n",
            "68     \t [ 1.47032429 -0.11759852]. \t  -519.8113640209032 \t -0.0040621530356655545\n",
            "69     \t [1.29801797 1.061399  ]. \t  -38.95801006777799 \t -0.0040621530356655545\n",
            "70     \t [ 1.27704013 -0.05560941]. \t  -284.48504812707824 \t -0.0040621530356655545\n",
            "71     \t [-0.01455274  1.61486514]. \t  -261.7398640546982 \t -0.0040621530356655545\n",
            "72     \t [-0.20521007 -0.20636756]. \t  -7.626699547511159 \t -0.0040621530356655545\n",
            "73     \t [1.19665449 1.40070136]. \t  -0.1365206689847126 \t -0.0040621530356655545\n",
            "74     \t [1.17001987 1.3447501 ]. \t  -0.08745329084247008 \t -0.0040621530356655545\n",
            "75     \t [-1.33548973  0.16839956]. \t  -266.3200555030319 \t -0.0040621530356655545\n",
            "76     \t [-1.13398124  1.03745487]. \t  -10.727043227510059 \t -0.0040621530356655545\n",
            "77     \t [-1.27488388  0.72116571]. \t  -86.92620525398058 \t -0.0040621530356655545\n",
            "78     \t [-0.0209748   0.02390357]. \t  -1.097443739468415 \t -0.0040621530356655545\n",
            "79     \t [ 1.95609107 -0.37593049]. \t  -1766.7817341777504 \t -0.0040621530356655545\n",
            "80     \t [1.91253661 0.98388592]. \t  -715.8123979341622 \t -0.0040621530356655545\n",
            "81     \t [-1.97445608 -2.00286589]. \t  -3491.4319350150254 \t -0.0040621530356655545\n",
            "82     \t [ 1.40973898 -1.13316608]. \t  -973.9386714775475 \t -0.0040621530356655545\n",
            "83     \t [ 1.67192921 -1.86254559]. \t  -2170.0480973974777 \t -0.0040621530356655545\n",
            "84     \t [ 0.51738178 -0.67274971]. \t  -88.67445975258771 \t -0.0040621530356655545\n",
            "85     \t [-0.72281118 -1.29658803]. \t  -333.8601983413895 \t -0.0040621530356655545\n",
            "86     \t [0.23713683 1.18773581]. \t  -128.61162224204764 \t -0.0040621530356655545\n",
            "87     \t [ 0.52141901 -1.73419679]. \t  -402.66255897447763 \t -0.0040621530356655545\n",
            "88     \t [0.49652896 0.24105326]. \t  -0.256494624176258 \t -0.0040621530356655545\n",
            "89     \t [0.12518397 0.9027846 ]. \t  -79.46235216551702 \t -0.0040621530356655545\n",
            "90     \t [ 0.27307719 -1.38925742]. \t  -214.80782634268718 \t -0.0040621530356655545\n",
            "91     \t [1.04560963 1.57958228]. \t  -23.64917436500178 \t -0.0040621530356655545\n",
            "92     \t [-0.07028959 -1.58607329]. \t  -254.27804647879904 \t -0.0040621530356655545\n",
            "93     \t [-1.5765528  -1.26771701]. \t  -1415.3164776664685 \t -0.0040621530356655545\n",
            "94     \t [-0.93985372  1.36091761]. \t  -26.572501212386467 \t -0.0040621530356655545\n",
            "95     \t [-0.6762613   1.75609806]. \t  -171.48987023474416 \t -0.0040621530356655545\n",
            "96     \t [ 0.61478593 -0.26500832]. \t  -41.48944069562792 \t -0.0040621530356655545\n",
            "97     \t [-1.16529491  1.41904107]. \t  -5.062175677754481 \t -0.0040621530356655545\n",
            "98     \t [1.03446245 1.09305449]. \t  -0.05382082520092298 \t -0.0040621530356655545\n",
            "99     \t [-1.45038161  1.64952399]. \t  -26.623491401680013 \t -0.0040621530356655545\n",
            "100    \t [-1.31115942  1.94872672]. \t  -10.612508665163395 \t -0.0040621530356655545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqjDLo0otd9-",
        "outputId": "c6642135-f40f-4c61-dae6-3f019ccc294d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_loser_17 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_17 = d2GPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.62187122 0.94420838]. \t  -31.22188590191926 \t -31.22188590191926\n",
            "init   \t [-1.25596728  0.08059797]. \t  -229.14713198618372 \t -31.22188590191926\n",
            "init   \t [0.72246586 1.2470062 ]. \t  -52.64667111223406 \t -31.22188590191926\n",
            "init   \t [0.13726869 1.671308  ]. \t  -273.80846401311214 \t -31.22188590191926\n",
            "init   \t [-0.2315013  -0.99953783]. \t  -112.42501801714917 \t -31.22188590191926\n",
            "1      \t [1.38985203 0.67340752]. \t  -158.47912633425426 \t -31.22188590191926\n",
            "2      \t [-5.29158242 -5.02533356]. \t  -109112.42796991288 \t -31.22188590191926\n",
            "3      \t [0.48520605 0.70400813]. \t  \u001b[92m-22.22203554670755\u001b[0m \t -22.22203554670755\n",
            "4      \t [-0.07633927 -4.00669486]. \t  -1611.1922189553645 \t -22.22203554670755\n",
            "5      \t [0.4329683  0.42971303]. \t  \u001b[92m-6.19010294534742\u001b[0m \t -6.19010294534742\n",
            "6      \t [0.29225687 0.02326178]. \t  \u001b[92m-0.8871911182600547\u001b[0m \t -0.8871911182600547\n",
            "7      \t [ 0.70483753 -0.42345124]. \t  -84.77260802886116 \t -0.8871911182600547\n",
            "8      \t [ 0.68301883 -1.76299303]. \t  -497.17095596989674 \t -0.8871911182600547\n",
            "9      \t [-0.66906229 -0.37411569]. \t  -70.31472418109783 \t -0.8871911182600547\n",
            "10     \t [-2.01216355 -1.52072272]. \t  -3111.033869252535 \t -0.8871911182600547\n",
            "11     \t [-0.14818959 -0.3550115 ]. \t  -15.52910245168684 \t -0.8871911182600547\n",
            "12     \t [ 1.93938365 -0.78303923]. \t  -2065.9015925754293 \t -0.8871911182600547\n",
            "13     \t [1.61708335 1.87446851]. \t  -55.21334351374121 \t -0.8871911182600547\n",
            "14     \t [0.61239826 0.10969719]. \t  -7.190471894848907 \t -0.8871911182600547\n",
            "15     \t [0.479831   0.10979671]. \t  -1.721181168110944 \t -0.8871911182600547\n",
            "16     \t [-0.56000551  0.6035436 ]. \t  -10.83998864340854 \t -0.8871911182600547\n",
            "17     \t [-0.39009421 -0.03039096]. \t  -5.265339739847079 \t -0.8871911182600547\n",
            "18     \t [-0.34730843  0.29308076]. \t  -4.7894026438676995 \t -0.8871911182600547\n",
            "19     \t [-1.44454305  1.80559901]. \t  -13.877827823955053 \t -0.8871911182600547\n",
            "20     \t [-1.15356099  1.1116049 ]. \t  -9.438220799873484 \t -0.8871911182600547\n",
            "21     \t [0.86012806 0.73355477]. \t  \u001b[92m-0.02348982399751418\u001b[0m \t -0.02348982399751418\n",
            "22     \t [1.02556505 1.11276889]. \t  -0.37257322174233404 \t -0.02348982399751418\n",
            "23     \t [-1.29787085  1.47510234]. \t  -9.663639196735883 \t -0.02348982399751418\n",
            "24     \t [-1.91905569  2.00351113]. \t  -290.51350979565063 \t -0.02348982399751418\n",
            "25     \t [-1.08176025  2.03616127]. \t  -79.32171178315438 \t -0.02348982399751418\n",
            "26     \t [-1.0962071   0.44784178]. \t  -61.219782983337005 \t -0.02348982399751418\n",
            "27     \t [-0.52252074 -0.35167814]. \t  -41.34383566617724 \t -0.02348982399751418\n",
            "28     \t [-0.58181021  0.43737629]. \t  -3.4797139114987603 \t -0.02348982399751418\n",
            "29     \t [0.70659798 0.51008932]. \t  -0.09776736396818252 \t -0.02348982399751418\n",
            "30     \t [1.09449733 1.1749249 ]. \t  -0.06182741284910654 \t -0.02348982399751418\n",
            "31     \t [-0.61373384 -0.38423968]. \t  -60.50237428910913 \t -0.02348982399751418\n",
            "32     \t [-1.40875573  1.6289175 ]. \t  -18.452590617769218 \t -0.02348982399751418\n",
            "33     \t [1.52138779 0.29329189]. \t  -408.84890386898417 \t -0.02348982399751418\n",
            "34     \t [1.74080353 1.54362102]. \t  -221.59905021424652 \t -0.02348982399751418\n",
            "35     \t [1.38712502 1.91055307]. \t  -0.16826063219756351 \t -0.02348982399751418\n",
            "36     \t [-0.63748997 -1.38065512]. \t  -322.03563613815305 \t -0.02348982399751418\n",
            "37     \t [1.21769661 1.51443398]. \t  -0.14755748320459522 \t -0.02348982399751418\n",
            "38     \t [-0.35326329 -0.03337889]. \t  -4.333218142660941 \t -0.02348982399751418\n",
            "39     \t [0.82190017 0.70090886]. \t  -0.09617957494150906 \t -0.02348982399751418\n",
            "40     \t [0.77914003 0.58874034]. \t  -0.08233711756889409 \t -0.02348982399751418\n",
            "41     \t [-0.53458742 -0.72927028]. \t  -105.38841833205716 \t -0.02348982399751418\n",
            "42     \t [-1.44728885  0.73940159]. \t  -189.65769942715474 \t -0.02348982399751418\n",
            "43     \t [0.88019095 0.78475451]. \t  -0.024391034129808135 \t -0.02348982399751418\n",
            "44     \t [-0.99610818  0.19881197]. \t  -66.93590511449064 \t -0.02348982399751418\n",
            "45     \t [ 0.36166798 -0.62458116]. \t  -57.46810040818697 \t -0.02348982399751418\n",
            "46     \t [1.75456992 1.83954575]. \t  -154.07400851032006 \t -0.02348982399751418\n",
            "47     \t [ 0.15483644 -1.32420254]. \t  -182.47238793527598 \t -0.02348982399751418\n",
            "48     \t [ 1.47443898 -1.28631293]. \t  -1197.5811107866384 \t -0.02348982399751418\n",
            "49     \t [0.179539   0.02316406]. \t  -0.6813830883331408 \t -0.02348982399751418\n",
            "50     \t [ 0.80102667 -0.25698711]. \t  -80.79332915429475 \t -0.02348982399751418\n",
            "51     \t [ 0.0130077  -0.00385923]. \t  -0.975776633735372 \t -0.02348982399751418\n",
            "52     \t [-1.81682755 -1.25327162]. \t  -2081.9481213711797 \t -0.02348982399751418\n",
            "53     \t [1.40239586 2.01874439]. \t  -0.43263714254082325 \t -0.02348982399751418\n",
            "54     \t [-0.5832036  -0.39844562]. \t  -57.055402148986836 \t -0.02348982399751418\n",
            "55     \t [-1.9677794  0.3980393]. \t  -1215.756244906381 \t -0.02348982399751418\n",
            "56     \t [ 0.3935953  -1.31480281]. \t  -216.3754355014879 \t -0.02348982399751418\n",
            "57     \t [-1.9069261   0.97514391]. \t  -716.6611417020337 \t -0.02348982399751418\n",
            "58     \t [-1.39981897  1.24181038]. \t  -57.265984360090215 \t -0.02348982399751418\n",
            "59     \t [ 0.00784076 -0.97890301]. \t  -96.821526355202 \t -0.02348982399751418\n",
            "60     \t [-1.12769857  1.00116836]. \t  -11.846057670026436 \t -0.02348982399751418\n",
            "61     \t [0.73263739 0.51349456]. \t  -0.12559940352862 \t -0.02348982399751418\n",
            "62     \t [-1.8409285   0.12421796]. \t  -1073.9626300129419 \t -0.02348982399751418\n",
            "63     \t [0.95663178 1.56151027]. \t  -41.78076889114262 \t -0.02348982399751418\n",
            "64     \t [ 0.50275513 -1.72798608]. \t  -392.5838341975225 \t -0.02348982399751418\n",
            "65     \t [0.605327   0.36611034]. \t  -0.15577641270288978 \t -0.02348982399751418\n",
            "66     \t [1.21206539 1.48801823]. \t  -0.08075217498249235 \t -0.02348982399751418\n",
            "67     \t [-1.90526878  1.91562327]. \t  -302.36618322749564 \t -0.02348982399751418\n",
            "68     \t [-1.8590975  -0.03748674]. \t  -1228.789541338643 \t -0.02348982399751418\n",
            "69     \t [1.50847337 0.24772284]. \t  -411.4432843590957 \t -0.02348982399751418\n",
            "70     \t [0.78560143 0.61215899]. \t  -0.04847736861870728 \t -0.02348982399751418\n",
            "71     \t [1.16794486 1.37444498]. \t  -0.0389172856546883 \t -0.02348982399751418\n",
            "72     \t [1.65108357 1.7624896 ]. \t  -93.27396602248619 \t -0.02348982399751418\n",
            "73     \t [-0.8309511   0.38541964]. \t  -12.65854777429919 \t -0.02348982399751418\n",
            "74     \t [-0.62039625 -0.28322156]. \t  -47.26318975428057 \t -0.02348982399751418\n",
            "75     \t [1.58093254 0.269114  ]. \t  -497.73172264831635 \t -0.02348982399751418\n",
            "76     \t [-1.88671546  1.04551085]. \t  -640.4454340967029 \t -0.02348982399751418\n",
            "77     \t [-1.10915018  0.20551825]. \t  -109.44867834043086 \t -0.02348982399751418\n",
            "78     \t [1.31707692 1.67360377]. \t  -0.47371031095915356 \t -0.02348982399751418\n",
            "79     \t [1.92040529 0.19413672]. \t  -1221.52480784025 \t -0.02348982399751418\n",
            "80     \t [0.59269678 0.35491972]. \t  -0.16721378205504453 \t -0.02348982399751418\n",
            "81     \t [0.3186804  1.09882346]. \t  -99.9181963257552 \t -0.02348982399751418\n",
            "82     \t [1.94725409 1.95571794]. \t  -338.01646995656824 \t -0.02348982399751418\n",
            "83     \t [ 0.75218183 -1.40652874]. \t  -389.0606101266476 \t -0.02348982399751418\n",
            "84     \t [1.03799912 1.0927165 ]. \t  -0.02477441638079696 \t -0.02348982399751418\n",
            "85     \t [-1.42493603  2.0188988 ]. \t  -5.893640873074455 \t -0.02348982399751418\n",
            "86     \t [-0.74083596 -1.65416337]. \t  -488.351976952918 \t -0.02348982399751418\n",
            "87     \t [1.01939695 1.07068557]. \t  -0.09969846428570008 \t -0.02348982399751418\n",
            "88     \t [-0.56896611 -0.73121945]. \t  -113.7518930266233 \t -0.02348982399751418\n",
            "89     \t [1.27427815 0.38180957]. \t  -154.3254793068761 \t -0.02348982399751418\n",
            "90     \t [0.46906583 0.20677824]. \t  -0.2994328043313553 \t -0.02348982399751418\n",
            "91     \t [ 0.6677092  -1.23963205]. \t  -284.19052973424647 \t -0.02348982399751418\n",
            "92     \t [-1.00700917 -1.26368828]. \t  -522.8452122917694 \t -0.02348982399751418\n",
            "93     \t [0.16015336 0.24262124]. \t  -5.413033502014389 \t -0.02348982399751418\n",
            "94     \t [0.41485172 0.16677226]. \t  -0.3452390673676354 \t -0.02348982399751418\n",
            "95     \t [ 0.76057997 -0.8169352 ]. \t  -194.77620593589174 \t -0.02348982399751418\n",
            "96     \t [-1.91187252 -1.00609437]. \t  -2181.2982218772822 \t -0.02348982399751418\n",
            "97     \t [-0.32671966 -0.20386439]. \t  -11.40804985655089 \t -0.02348982399751418\n",
            "98     \t [1.39696587 0.04097297]. \t  -365.1741489099554 \t -0.02348982399751418\n",
            "99     \t [1.676243   1.94336126]. \t  -75.52728652939464 \t -0.02348982399751418\n",
            "100    \t [0.53826131 1.57679642]. \t  -165.8684252955217 \t -0.02348982399751418\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOqNmgSIteAw",
        "outputId": "06dce769-565a-4588-e5af-1466237d91c6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_loser_18 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_18 = d2GPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.52956683 1.91914255]. \t  -17.95675982372887 \t -1.7663579664225912\n",
            "init   \t [1.51222084 0.12638491]. \t  -467.0068292530542 \t -1.7663579664225912\n",
            "init   \t [-1.09474477 -2.0013105 ]. \t  -1028.244988519411 \t -1.7663579664225912\n",
            "init   \t [-0.28479972 -0.39996883]. \t  -24.79447905426233 \t -1.7663579664225912\n",
            "init   \t [ 0.09287545 -0.0885072 ]. \t  -1.7663579664225912 \t -1.7663579664225912\n",
            "1      \t [-0.37518604  0.79486843]. \t  -44.676323273026185 \t -1.7663579664225912\n",
            "2      \t [-0.41546317  0.04330888]. \t  -3.6754048035136746 \t -1.7663579664225912\n",
            "3      \t [-0.15089224 -0.0726361 ]. \t  -2.234756131066808 \t -1.7663579664225912\n",
            "4      \t [-1.51830957  0.50994365]. \t  -328.65938500459714 \t -1.7663579664225912\n",
            "5      \t [ 0.93728722 -1.35473636]. \t  -498.74167689764977 \t -1.7663579664225912\n",
            "6      \t [0.09069548 1.89692593]. \t  -357.54570199924825 \t -1.7663579664225912\n",
            "7      \t [ 1.28302434 -1.42060701]. \t  -940.5808480748063 \t -1.7663579664225912\n",
            "8      \t [1.99593318 1.96939376]. \t  -406.75468239982354 \t -1.7663579664225912\n",
            "9      \t [1.08171939 1.5128207 ]. \t  -11.751271790933831 \t -1.7663579664225912\n",
            "10     \t [1.16074401 1.88212976]. \t  -28.62727440689314 \t -1.7663579664225912\n",
            "11     \t [1.24913155 1.6313656 ]. \t  \u001b[92m-0.5666772699851954\u001b[0m \t -0.5666772699851954\n",
            "12     \t [0.69244018 0.79060027]. \t  -9.774586281572839 \t -0.5666772699851954\n",
            "13     \t [ 0.19796648 -0.79911642]. \t  -70.9191451645731 \t -0.5666772699851954\n",
            "14     \t [1.40299179 1.99208881]. \t  \u001b[92m-0.21858483939607354\u001b[0m \t -0.21858483939607354\n",
            "15     \t [0.19952188 0.85664956]. \t  -67.36361743010498 \t -0.21858483939607354\n",
            "16     \t [0.14905594 1.43408062]. \t  -200.0598027000524 \t -0.21858483939607354\n",
            "17     \t [-1.21930197  1.68034636]. \t  -8.675297591556301 \t -0.21858483939607354\n",
            "18     \t [0.00842168 1.21627265]. \t  -148.89789227870298 \t -0.21858483939607354\n",
            "19     \t [-1.54842812  2.04257632]. \t  -19.100772316288136 \t -0.21858483939607354\n",
            "20     \t [-1.51317726  0.21494298]. \t  -436.7799826043828 \t -0.21858483939607354\n",
            "21     \t [1.36465468 1.79040632]. \t  -0.6495901307152054 \t -0.21858483939607354\n",
            "22     \t [-1.61960448  1.48643539]. \t  -136.06721290494295 \t -0.21858483939607354\n",
            "23     \t [-1.01589236  1.01236841]. \t  -4.102508464377995 \t -0.21858483939607354\n",
            "24     \t [-2.00786669 -1.57681342]. \t  -3154.39735885571 \t -0.21858483939607354\n",
            "25     \t [ 0.05111387 -2.67054575]. \t  -715.4779555205749 \t -0.21858483939607354\n",
            "26     \t [-1.02894882 -0.67536524]. \t  -304.8272344430442 \t -0.21858483939607354\n",
            "27     \t [0.41751648 0.19062986]. \t  -0.36588817112287786 \t -0.21858483939607354\n",
            "28     \t [-1.7750829  0.9792554]. \t  -479.3134904673417 \t -0.21858483939607354\n",
            "29     \t [0.30163337 0.08477067]. \t  -0.4915748729219045 \t -0.21858483939607354\n",
            "30     \t [0.9571332  0.89175438]. \t  \u001b[92m-0.061127750733312475\u001b[0m \t -0.061127750733312475\n",
            "31     \t [1.32346668 1.7576601 ]. \t  -0.10834685842696845 \t -0.061127750733312475\n",
            "32     \t [-0.51379973 -1.63690855]. \t  -363.6331805481531 \t -0.061127750733312475\n",
            "33     \t [0.07922411 0.02791721]. \t  -0.8946604496552162 \t -0.061127750733312475\n",
            "34     \t [1.33718159 1.80549516]. \t  -0.14410867860741455 \t -0.061127750733312475\n",
            "35     \t [0.98487825 1.038207  ]. \t  -0.4656506212169315 \t -0.061127750733312475\n",
            "36     \t [-0.74895875  1.57222134]. \t  -105.32801185836587 \t -0.061127750733312475\n",
            "37     \t [-1.83696341 -1.25620347]. \t  -2152.32922596957 \t -0.061127750733312475\n",
            "38     \t [1.8550285  0.74680791]. \t  -726.6686225385389 \t -0.061127750733312475\n",
            "39     \t [-1.88536862  2.00180992]. \t  -249.44566312940998 \t -0.061127750733312475\n",
            "40     \t [-0.69850149  0.40041293]. \t  -3.6503818879308128 \t -0.061127750733312475\n",
            "41     \t [1.068591   0.91240867]. \t  -5.270722615947526 \t -0.061127750733312475\n",
            "42     \t [-1.26947753  2.02696633]. \t  -22.40567283384248 \t -0.061127750733312475\n",
            "43     \t [0.51472498 0.82805609]. \t  -31.945262563754127 \t -0.061127750733312475\n",
            "44     \t [-0.51307061  0.77787495]. \t  -28.774146598562776 \t -0.061127750733312475\n",
            "45     \t [ 0.34533537 -0.98658915]. \t  -122.71805110077798 \t -0.061127750733312475\n",
            "46     \t [ 0.93458502 -0.79773117]. \t  -279.28864832088374 \t -0.061127750733312475\n",
            "47     \t [0.86577071 0.70171215]. \t  -0.2469488746458518 \t -0.061127750733312475\n",
            "48     \t [-0.33344666  1.70336935]. \t  -255.28264766624903 \t -0.061127750733312475\n",
            "49     \t [1.39236348 1.95887501]. \t  -0.19474890023561603 \t -0.061127750733312475\n",
            "50     \t [0.97561873 0.96841336]. \t  \u001b[92m-0.02808890586959634\u001b[0m \t -0.02808890586959634\n",
            "51     \t [ 0.22431893 -0.01304156]. \t  -1.0031369957016563 \t -0.02808890586959634\n",
            "52     \t [1.61404428 1.32600028]. \t  -163.99662342852554 \t -0.02808890586959634\n",
            "53     \t [-1.94029947 -0.86847316]. \t  -2155.3322056729285 \t -0.02808890586959634\n",
            "54     \t [0.07464172 1.83765711]. \t  -336.51009897136794 \t -0.02808890586959634\n",
            "55     \t [0.51311545 0.0998633 ]. \t  -2.9078022004534447 \t -0.02808890586959634\n",
            "56     \t [-0.9724487  -0.53938542]. \t  -224.42549533519733 \t -0.02808890586959634\n",
            "57     \t [1.03697633 1.0319875 ]. \t  -0.18913702011945493 \t -0.02808890586959634\n",
            "58     \t [1.00713096 0.99768186]. \t  \u001b[92m-0.02770957987774812\u001b[0m \t -0.02770957987774812\n",
            "59     \t [0.7566178  0.52569115]. \t  -0.27806554457169613 \t -0.02770957987774812\n",
            "60     \t [ 0.66227292 -0.70944906]. \t  -131.9169686429098 \t -0.02770957987774812\n",
            "61     \t [ 1.91918506 -0.9411309 ]. \t  -2139.354445757398 \t -0.02770957987774812\n",
            "62     \t [ 0.61530074 -0.04220243]. \t  -17.855040930280474 \t -0.02770957987774812\n",
            "63     \t [-1.26278418  1.72912074]. \t  -6.929132761748157 \t -0.02770957987774812\n",
            "64     \t [1.11349614 1.2473693 ]. \t  \u001b[92m-0.01849984238540816\u001b[0m \t -0.01849984238540816\n",
            "65     \t [-1.3258731  -1.69927707]. \t  -1200.6443181943291 \t -0.01849984238540816\n",
            "66     \t [ 1.18926945 -0.81746395]. \t  -498.14045549179895 \t -0.01849984238540816\n",
            "67     \t [ 1.94377297 -1.45285214]. \t  -2737.3371698894134 \t -0.01849984238540816\n",
            "68     \t [1.25395039 1.49810155]. \t  -0.6163916927328872 \t -0.01849984238540816\n",
            "69     \t [-1.26602996  1.28388001]. \t  -15.30791956683049 \t -0.01849984238540816\n",
            "70     \t [-1.92633866 -2.01001904]. \t  -3281.3183646001717 \t -0.01849984238540816\n",
            "71     \t [-1.66391243 -0.01085516]. \t  -779.6360682164172 \t -0.01849984238540816\n",
            "72     \t [-0.84373096  1.94458006]. \t  -155.35381258533093 \t -0.01849984238540816\n",
            "73     \t [-1.84375537 -1.86727236]. \t  -2781.9064022820107 \t -0.01849984238540816\n",
            "74     \t [-0.78666941 -1.10787842]. \t  -301.3508649798184 \t -0.01849984238540816\n",
            "75     \t [0.33148238 0.90152338]. \t  -63.11674969328898 \t -0.01849984238540816\n",
            "76     \t [1.6702811  1.70236723]. \t  -118.70875420772408 \t -0.01849984238540816\n",
            "77     \t [ 0.11668496 -1.96729657]. \t  -393.1814597329686 \t -0.01849984238540816\n",
            "78     \t [-0.88247338  2.04742146]. \t  -164.49408042071573 \t -0.01849984238540816\n",
            "79     \t [-0.43549238 -2.01846151]. \t  -489.6378795741759 \t -0.01849984238540816\n",
            "80     \t [-0.01091106  0.37740414]. \t  -15.256345137726763 \t -0.01849984238540816\n",
            "81     \t [-0.1208149   1.48729322]. \t  -218.1398643921702 \t -0.01849984238540816\n",
            "82     \t [ 1.26996063 -1.6710591 ]. \t  -1078.4459340400506 \t -0.01849984238540816\n",
            "83     \t [0.03857045 1.98165671]. \t  -393.0312840335908 \t -0.01849984238540816\n",
            "84     \t [1.25627707 1.55911194]. \t  -0.1022358848862952 \t -0.01849984238540816\n",
            "85     \t [ 0.38269592 -1.23938671]. \t  -192.4371118418537 \t -0.01849984238540816\n",
            "86     \t [ 0.61123068 -0.5270834 ]. \t  -81.27473123256354 \t -0.01849984238540816\n",
            "87     \t [-0.11353625  0.93397012]. \t  -86.07873293752077 \t -0.01849984238540816\n",
            "88     \t [-0.92679023 -1.14150193]. \t  -403.8893634877983 \t -0.01849984238540816\n",
            "89     \t [1.04779559 1.08256261]. \t  -0.025733207977007115 \t -0.01849984238540816\n",
            "90     \t [1.38648903 1.95227109]. \t  -0.23888990096381357 \t -0.01849984238540816\n",
            "91     \t [-0.2896775 -0.7808088]. \t  -76.43765662694076 \t -0.01849984238540816\n",
            "92     \t [-1.14294969 -0.48032313]. \t  -323.8066030564311 \t -0.01849984238540816\n",
            "93     \t [-0.87056293 -0.71276528]. \t  -219.7787060538371 \t -0.01849984238540816\n",
            "94     \t [1.58263691 1.77828318]. \t  -53.11335976600227 \t -0.01849984238540816\n",
            "95     \t [-0.3000354 -0.4367719]. \t  -29.441193255529072 \t -0.01849984238540816\n",
            "96     \t [ 1.6314697  -1.10691842]. \t  -1420.642250098064 \t -0.01849984238540816\n",
            "97     \t [-0.58438823 -2.01372533]. \t  -557.2234469458028 \t -0.01849984238540816\n",
            "98     \t [-0.97394672  1.79359404]. \t  -75.30265313975646 \t -0.01849984238540816\n",
            "99     \t [ 2.00177263 -1.26549273]. \t  -2781.0202770904502 \t -0.01849984238540816\n",
            "100    \t [-0.20113591  1.12016858]. \t  -118.02072811771028 \t -0.01849984238540816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4HnKuoqteDo",
        "outputId": "87fc9bcc-1c98-4312-b2c9-0dcf1e39b39e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_loser_19 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_19 = d2GPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.24348908 1.07107119]. \t  -102.94304782477323 \t -4.219752052396591\n",
            "init   \t [-1.04639875  1.11284007]. \t  -4.219752052396591 \t -4.219752052396591\n",
            "init   \t [-0.59608235  1.21527255]. \t  -76.50031976976825 \t -4.219752052396591\n",
            "init   \t [-0.63078391 -0.22989031]. \t  -42.07005958957044 \t -4.219752052396591\n",
            "init   \t [-0.8056685  -0.23710138]. \t  -81.79603430667063 \t -4.219752052396591\n",
            "1      \t [ 0.51626155 -0.56228938]. \t  -68.92749304221446 \t -4.219752052396591\n",
            "2      \t [-2.01974514  1.53547989]. \t  -656.2567615793643 \t -4.219752052396591\n",
            "3      \t [-0.2348982   0.43181231]. \t  -15.710376934861005 \t -4.219752052396591\n",
            "4      \t [1.4674717  1.42866499]. \t  -52.75322074972573 \t -4.219752052396591\n",
            "5      \t [-0.8101096   0.90290421]. \t  -9.358966735498882 \t -4.219752052396591\n",
            "6      \t [ 0.25517628 -0.09768572]. \t  \u001b[92m-3.205167585453764\u001b[0m \t -3.205167585453764\n",
            "7      \t [-0.11292085 -0.00022941]. \t  \u001b[92m-1.2554422302544317\u001b[0m \t -1.2554422302544317\n",
            "8      \t [0.95568556 0.67455006]. \t  -5.703783578614714 \t -1.2554422302544317\n",
            "9      \t [-0.4873257  -1.64043458]. \t  -354.87083187620107 \t -1.2554422302544317\n",
            "10     \t [0.74722887 0.14699418]. \t  -16.985335493250744 \t -1.2554422302544317\n",
            "11     \t [0.2655869  0.28459397]. \t  -5.121427107891311 \t -1.2554422302544317\n",
            "12     \t [ 0.11804599 -0.10148523]. \t  -2.1100225795518504 \t -1.2554422302544317\n",
            "13     \t [-0.73934761  1.83511187]. \t  -169.04262503651273 \t -1.2554422302544317\n",
            "14     \t [-1.02326689  0.13551371]. \t  -87.18803054460024 \t -1.2554422302544317\n",
            "15     \t [-0.91779786  0.09679596]. \t  -59.26346567837915 \t -1.2554422302544317\n",
            "16     \t [-0.22515032  0.07446674]. \t  -1.5575139454934113 \t -1.2554422302544317\n",
            "17     \t [-1.29377738 -1.90212913]. \t  -1284.0311715010898 \t -1.2554422302544317\n",
            "18     \t [ 1.02583277 -1.1838309 ]. \t  -500.0435106849533 \t -1.2554422302544317\n",
            "19     \t [-0.76618822 -0.19049423]. \t  -63.576049891706525 \t -1.2554422302544317\n",
            "20     \t [-1.58996028  0.63482111]. \t  -365.1105597023613 \t -1.2554422302544317\n",
            "21     \t [-1.00134346  1.20627918]. \t  -8.15028274373621 \t -1.2554422302544317\n",
            "22     \t [1.77546441 0.92724041]. \t  -495.67873283468606 \t -1.2554422302544317\n",
            "23     \t [0.93063246 1.87785381]. \t  -102.37408850145867 \t -1.2554422302544317\n",
            "24     \t [0.74527564 0.62032589]. \t  \u001b[92m-0.4859571951465588\u001b[0m \t -0.4859571951465588\n",
            "25     \t [1.09263626 1.50073143]. \t  -9.425957182389265 \t -0.4859571951465588\n",
            "26     \t [0.95176939 0.90164791]. \t  \u001b[92m-0.004104546783159123\u001b[0m \t -0.004104546783159123\n",
            "27     \t [3.41718573 3.21998366]. \t  -7158.223134770461 \t -0.004104546783159123\n",
            "28     \t [-1.63856669  0.69331682]. \t  -403.60271179563733 \t -0.004104546783159123\n",
            "29     \t [1.19168629 1.33257778]. \t  -0.8030414293488007 \t -0.004104546783159123\n",
            "30     \t [-0.20962842  0.03663851]. \t  -1.468538038697957 \t -0.004104546783159123\n",
            "31     \t [1.32928849 1.66700354]. \t  -1.1085181387460854 \t -0.004104546783159123\n",
            "32     \t [-0.62571956  1.4256426 ]. \t  -109.58289262533435 \t -0.004104546783159123\n",
            "33     \t [-0.61375131  0.89657007]. \t  -29.631652197295683 \t -0.004104546783159123\n",
            "34     \t [1.27735388 1.64448156]. \t  -0.09343392134855734 \t -0.004104546783159123\n",
            "35     \t [1.29720394 1.63638728]. \t  -0.3031697035913069 \t -0.004104546783159123\n",
            "36     \t [0.58700767 0.31186101]. \t  -0.27760283593720764 \t -0.004104546783159123\n",
            "37     \t [ 1.33652762 -1.41097198]. \t  -1022.3719497808307 \t -0.004104546783159123\n",
            "38     \t [ 0.322592   -1.38684796]. \t  -222.74120686815738 \t -0.004104546783159123\n",
            "39     \t [1.17092552 1.57273923]. \t  -4.096401225250451 \t -0.004104546783159123\n",
            "40     \t [0.759251   0.60408869]. \t  -0.13428305632045917 \t -0.004104546783159123\n",
            "41     \t [1.16751281 1.41421715]. \t  -0.289498310485199 \t -0.004104546783159123\n",
            "42     \t [-0.24680134 -1.78126617]. \t  -340.9161515362546 \t -0.004104546783159123\n",
            "43     \t [-0.58202633 -0.52425901]. \t  -76.98206422017245 \t -0.004104546783159123\n",
            "44     \t [-0.93957858 -0.85981068]. \t  -307.43391881515123 \t -0.004104546783159123\n",
            "45     \t [0.28585056 0.05142585]. \t  -0.601725672423762 \t -0.004104546783159123\n",
            "46     \t [ 1.79677526 -1.32817717]. \t  -2076.875614620295 \t -0.004104546783159123\n",
            "47     \t [0.79281754 0.70307785]. \t  -0.5982207655329881 \t -0.004104546783159123\n",
            "48     \t [-0.65978578 -1.76653712]. \t  -487.5711645016507 \t -0.004104546783159123\n",
            "49     \t [-0.24266514 -0.76140332]. \t  -68.8317344229052 \t -0.004104546783159123\n",
            "50     \t [1.16977644 1.43925212]. \t  -0.5311533515503688 \t -0.004104546783159123\n",
            "51     \t [1.4252979  0.46022507]. \t  -247.06323074062726 \t -0.004104546783159123\n",
            "52     \t [-0.99195143 -0.35693566]. \t  -183.77003696304735 \t -0.004104546783159123\n",
            "53     \t [1.42875409 0.43741702]. \t  -257.4401579992734 \t -0.004104546783159123\n",
            "54     \t [1.50216865 0.12571883]. \t  -454.2795524743432 \t -0.004104546783159123\n",
            "55     \t [-0.18601702 -1.73004712]. \t  -312.8054036447344 \t -0.004104546783159123\n",
            "56     \t [-2.02404919  0.73184435]. \t  -1141.4207830370153 \t -0.004104546783159123\n",
            "57     \t [1.20068517 1.39851087]. \t  -0.22632874131712996 \t -0.004104546783159123\n",
            "58     \t [-0.31411906  0.91034596]. \t  -67.60856780287071 \t -0.004104546783159123\n",
            "59     \t [1.28775011 1.59566325]. \t  -0.475140674306122 \t -0.004104546783159123\n",
            "60     \t [-1.14287721 -1.84974167]. \t  -1000.568697410595 \t -0.004104546783159123\n",
            "61     \t [ 0.60120811 -0.72900613]. \t  -119.0687515408353 \t -0.004104546783159123\n",
            "62     \t [-1.0803744   0.18974846]. \t  -99.8708387620765 \t -0.004104546783159123\n",
            "63     \t [-1.07883665  0.83854152]. \t  -14.906629231210427 \t -0.004104546783159123\n",
            "64     \t [-1.78051982  0.0386201 ]. \t  -988.4423965795083 \t -0.004104546783159123\n",
            "65     \t [-1.74403802 -1.30179646]. \t  -1894.098633935269 \t -0.004104546783159123\n",
            "66     \t [-0.02586991 -1.69586234]. \t  -288.8743529540324 \t -0.004104546783159123\n",
            "67     \t [ 1.84141782 -0.56448018]. \t  -1565.1476106907987 \t -0.004104546783159123\n",
            "68     \t [0.29070172 0.83551013]. \t  -56.90360093054751 \t -0.004104546783159123\n",
            "69     \t [ 0.77466228 -0.26976528]. \t  -75.71762526666582 \t -0.004104546783159123\n",
            "70     \t [-1.77269767 -1.13558288]. \t  -1837.8503994612217 \t -0.004104546783159123\n",
            "71     \t [1.20456699 1.49484794]. \t  -0.23427284808354526 \t -0.004104546783159123\n",
            "72     \t [0.97955475 0.94901259]. \t  -0.011474348161723396 \t -0.004104546783159123\n",
            "73     \t [-0.41998961 -1.70756351]. \t  -356.9449335463213 \t -0.004104546783159123\n",
            "74     \t [0.84458681 0.70516282]. \t  -0.030818444821309066 \t -0.004104546783159123\n",
            "75     \t [ 1.54651295 -0.94071756]. \t  -1110.8008886824762 \t -0.004104546783159123\n",
            "76     \t [1.22026773 1.09116792]. \t  -15.879797912924316 \t -0.004104546783159123\n",
            "77     \t [1.24743735 1.51468879]. \t  -0.23271366493838885 \t -0.004104546783159123\n",
            "78     \t [-0.25579785 -1.91213514]. \t  -392.65441879290506 \t -0.004104546783159123\n",
            "79     \t [-1.01193613  1.27793013]. \t  -10.4951895211278 \t -0.004104546783159123\n",
            "80     \t [-0.04833811  0.43081048]. \t  -19.4580015708361 \t -0.004104546783159123\n",
            "81     \t [0.14672588 0.87963436]. \t  -74.36264551014955 \t -0.004104546783159123\n",
            "82     \t [ 1.59712897 -0.86236332]. \t  -1165.3392525757588 \t -0.004104546783159123\n",
            "83     \t [-0.98486741  0.68231356]. \t  -12.213965271784032 \t -0.004104546783159123\n",
            "84     \t [-1.81862779  1.1758283 ]. \t  -462.3074627004391 \t -0.004104546783159123\n",
            "85     \t [-0.24366068  1.84452526]. \t  -320.2244345913622 \t -0.004104546783159123\n",
            "86     \t [-1.22222237  1.39634206]. \t  -5.88861363497911 \t -0.004104546783159123\n",
            "87     \t [0.87499096 0.74409058]. \t  -0.0619322451656779 \t -0.004104546783159123\n",
            "88     \t [-0.14002292 -1.35041328]. \t  -188.99504911871696 \t -0.004104546783159123\n",
            "89     \t [-0.57263001  0.90454645]. \t  -35.72468675050905 \t -0.004104546783159123\n",
            "90     \t [-0.52357218  0.25964062]. \t  -2.342260135583863 \t -0.004104546783159123\n",
            "91     \t [0.01824425 1.73847894]. \t  -303.07902757254914 \t -0.004104546783159123\n",
            "92     \t [1.81914489 1.51624507]. \t  -322.17134307118727 \t -0.004104546783159123\n",
            "93     \t [ 0.88411369 -0.31217853]. \t  -119.66104849291484 \t -0.004104546783159123\n",
            "94     \t [0.88798542 0.19356752]. \t  -35.40916619948873 \t -0.004104546783159123\n",
            "95     \t [-0.93516627 -0.76413114]. \t  -272.2678550483519 \t -0.004104546783159123\n",
            "96     \t [0.99785923 1.02203016]. \t  -0.06921107633250193 \t -0.004104546783159123\n",
            "97     \t [1.22201364 0.65736029]. \t  -69.93170963058354 \t -0.004104546783159123\n",
            "98     \t [-1.51314065  0.48297648]. \t  -332.70279227581244 \t -0.004104546783159123\n",
            "99     \t [-2.02472409  1.55710344]. \t  -655.530863928991 \t -0.004104546783159123\n",
            "100    \t [-0.20069779  1.79982045]. \t  -311.04007298950785 \t -0.004104546783159123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfiDmH_bteGg",
        "outputId": "e2ba14ca-2ccd-4a15-a340-c189a2603a49"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_loser_20 = dGaussianProcess(cov_func)\r\n",
        "\r\n",
        "loser_20 = d2GPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.2888388  0.26430978]. \t  -3.777577453542735 \t -3.777577453542735\n",
            "init   \t [-0.04734225 -0.66978712]. \t  -46.25914429040042 \t -3.777577453542735\n",
            "init   \t [-0.50844394  0.13121892]. \t  -3.895838569726608 \t -3.777577453542735\n",
            "init   \t [-1.76903664  0.34623103]. \t  -782.3209705533411 \t -3.777577453542735\n",
            "init   \t [-1.07357076 -1.38954104]. \t  -650.524506936468 \t -3.777577453542735\n",
            "1      \t [-0.04243452  0.03655484]. \t  \u001b[92m-1.2074547980027717\u001b[0m \t -1.2074547980027717\n",
            "2      \t [-0.17521923  1.07986149]. \t  -111.45475044450576 \t -1.2074547980027717\n",
            "3      \t [1.52145555 0.45818482]. \t  -344.9839327128462 \t -1.2074547980027717\n",
            "4      \t [ 0.64617018 -1.58680744]. \t  -401.86441762239986 \t -1.2074547980027717\n",
            "5      \t [0.44599456 0.72907409]. \t  -28.414196275862338 \t -1.2074547980027717\n",
            "6      \t [-0.23993779  0.25579499]. \t  -5.4667548036756735 \t -1.2074547980027717\n",
            "7      \t [-0.33309491  0.13382094]. \t  -1.8294398994854826 \t -1.2074547980027717\n",
            "8      \t [ 0.20066455 -0.22599007]. \t  -7.728180677839326 \t -1.2074547980027717\n",
            "9      \t [1.3771472 1.6997598]. \t  -4.0142649271370585 \t -1.2074547980027717\n",
            "10     \t [1.63284947 2.02301574]. \t  -41.76876094589841 \t -1.2074547980027717\n",
            "11     \t [1.01261893 1.61594319]. \t  -34.874628198103295 \t -1.2074547980027717\n",
            "12     \t [0.09161453 0.02906792]. \t  \u001b[92m-0.8679084958481715\u001b[0m \t -0.8679084958481715\n",
            "13     \t [-1.29855054 -0.41876535]. \t  -448.3853529987201 \t -0.8679084958481715\n",
            "14     \t [0.12188557 0.01894414]. \t  \u001b[92m-0.7727561604554114\u001b[0m \t -0.7727561604554114\n",
            "15     \t [-0.3983729  -1.64194288]. \t  -326.1872740695626 \t -0.7727561604554114\n",
            "16     \t [ 0.44378849 -1.53929824]. \t  -301.76455004759686 \t -0.7727561604554114\n",
            "17     \t [-0.39119219  0.81005883]. \t  -45.10392970029373 \t -0.7727561604554114\n",
            "18     \t [ 0.25799003 -0.24867382]. \t  -10.4877429368878 \t -0.7727561604554114\n",
            "19     \t [ 0.20646555 -1.89909653]. \t  -377.6591207061545 \t -0.7727561604554114\n",
            "20     \t [-1.07807572  1.97281578]. \t  -70.02053109356922 \t -0.7727561604554114\n",
            "21     \t [-0.48316149  0.04533448]. \t  -5.738325805710129 \t -0.7727561604554114\n",
            "22     \t [-0.19580929  0.08150855]. \t  -1.616301184436927 \t -0.7727561604554114\n",
            "23     \t [ 1.66764852 -1.71888722]. \t  -2025.3906879916842 \t -0.7727561604554114\n",
            "24     \t [-1.69537024 -1.39278119]. \t  -1828.0463414122016 \t -0.7727561604554114\n",
            "25     \t [1.247297   1.24464098]. \t  -9.740025499798 \t -0.7727561604554114\n",
            "26     \t [ 0.8501451  -0.35546572]. \t  -116.27665665103598 \t -0.7727561604554114\n",
            "27     \t [1.24670793 1.5108281 ]. \t  \u001b[92m-0.24967738654749078\u001b[0m \t -0.24967738654749078\n",
            "28     \t [1.37897128 1.57306518]. \t  -10.934621544910028 \t -0.24967738654749078\n",
            "29     \t [0.96902863 1.02695859]. \t  -0.7743405299600068 \t -0.24967738654749078\n",
            "30     \t [1.05286345 1.11177597]. \t  \u001b[92m-0.0038537395242922994\u001b[0m \t -0.0038537395242922994\n",
            "31     \t [0.43348374 0.24789285]. \t  -0.6807571136465769 \t -0.0038537395242922994\n",
            "32     \t [1.26229901 1.94894157]. \t  -12.709867897991318 \t -0.0038537395242922994\n",
            "33     \t [-0.47830041 -1.52576977]. \t  -310.02680065959424 \t -0.0038537395242922994\n",
            "34     \t [-1.20550988 -0.05889056]. \t  -233.52241158111184 \t -0.0038537395242922994\n",
            "35     \t [-0.52674853  0.54617333]. \t  -9.55143065929903 \t -0.0038537395242922994\n",
            "36     \t [0.69527276 0.55560523]. \t  -0.6141573411794163 \t -0.0038537395242922994\n",
            "37     \t [1.32115648 1.18152797]. \t  -31.904447673878682 \t -0.0038537395242922994\n",
            "38     \t [-0.3040362   0.76544083]. \t  -46.9937903703847 \t -0.0038537395242922994\n",
            "39     \t [-0.14460198  1.34721041]. \t  -177.21746209505548 \t -0.0038537395242922994\n",
            "40     \t [1.0990445 1.2158096]. \t  -0.016067846202957457 \t -0.0038537395242922994\n",
            "41     \t [ 0.53483739 -1.11396349]. \t  -196.22044177322152 \t -0.0038537395242922994\n",
            "42     \t [-0.4068628  -1.28073464]. \t  -211.14952512344544 \t -0.0038537395242922994\n",
            "43     \t [0.17529204 1.15916279]. \t  -128.0168083834562 \t -0.0038537395242922994\n",
            "44     \t [0.60187974 0.34939177]. \t  -0.1750568811306341 \t -0.0038537395242922994\n",
            "45     \t [0.47964708 1.26569985]. \t  -107.52548316444184 \t -0.0038537395242922994\n",
            "46     \t [-1.50941603 -1.10529415]. \t  -1151.1929674840562 \t -0.0038537395242922994\n",
            "47     \t [0.54817781 0.27627233]. \t  -0.26283604334147936 \t -0.0038537395242922994\n",
            "48     \t [1.51949793 0.87348176]. \t  -206.30495661161856 \t -0.0038537395242922994\n",
            "49     \t [-1.86573362  0.37119973]. \t  -975.2745320400953 \t -0.0038537395242922994\n",
            "50     \t [-1.12620705  1.25801629]. \t  -4.531419063477136 \t -0.0038537395242922994\n",
            "51     \t [-0.98283438  0.91426446]. \t  -4.198910359406088 \t -0.0038537395242922994\n",
            "52     \t [-1.18641791  1.72449582]. \t  -14.823514401821361 \t -0.0038537395242922994\n",
            "53     \t [ 1.71219045 -1.9877464 ]. \t  -2420.500308270193 \t -0.0038537395242922994\n",
            "54     \t [0.46888773 1.61398374]. \t  -194.64137800239578 \t -0.0038537395242922994\n",
            "55     \t [ 1.15212581 -1.08914448]. \t  -583.9889106648615 \t -0.0038537395242922994\n",
            "56     \t [1.10529464 0.8664695 ]. \t  -12.628270272037758 \t -0.0038537395242922994\n",
            "57     \t [0.97825923 0.93755616]. \t  -0.03824441295259288 \t -0.0038537395242922994\n",
            "58     \t [-1.04905137  1.31822097]. \t  -8.93847154467447 \t -0.0038537395242922994\n",
            "59     \t [-0.99523387  0.24252604]. \t  -59.926034914967154 \t -0.0038537395242922994\n",
            "60     \t [-0.64931251 -0.54001115]. \t  -95.19112707327969 \t -0.0038537395242922994\n",
            "61     \t [ 1.9065061  -0.75279122]. \t  -1925.8871723897323 \t -0.0038537395242922994\n",
            "62     \t [0.58319821 0.28775523]. \t  -0.4479322263932979 \t -0.0038537395242922994\n",
            "63     \t [-1.39710539  1.85303705]. \t  -6.723570994645275 \t -0.0038537395242922994\n",
            "64     \t [0.44374421 1.12386373]. \t  -86.23394152062177 \t -0.0038537395242922994\n",
            "65     \t [1.18674958 1.09267993]. \t  -10.00118618641607 \t -0.0038537395242922994\n",
            "66     \t [-0.91082315 -1.26232069]. \t  -441.26396375882774 \t -0.0038537395242922994\n",
            "67     \t [0.0821836  0.58251462]. \t  -33.99239946621883 \t -0.0038537395242922994\n",
            "68     \t [1.15112364 0.41629861]. \t  -82.61222519369333 \t -0.0038537395242922994\n",
            "69     \t [0.46285741 0.20378809]. \t  -0.2994400890802518 \t -0.0038537395242922994\n",
            "70     \t [0.00420307 0.2388898 ]. \t  -6.697601156350904 \t -0.0038537395242922994\n",
            "71     \t [1.11036593 1.21933424]. \t  -0.03061754712837163 \t -0.0038537395242922994\n",
            "72     \t [2.02236401 0.80552465]. \t  -1079.7942702821092 \t -0.0038537395242922994\n",
            "73     \t [-0.62886151 -1.17512401]. \t  -249.32873851716164 \t -0.0038537395242922994\n",
            "74     \t [1.52137197 0.73026457]. \t  -251.27504800338738 \t -0.0038537395242922994\n",
            "75     \t [0.38581887 1.73789702]. \t  -252.88229059851358 \t -0.0038537395242922994\n",
            "76     \t [-1.96944417 -0.5409523 ]. \t  -1962.1593894476853 \t -0.0038537395242922994\n",
            "77     \t [ 1.154773   -0.95401997]. \t  -523.2990278949036 \t -0.0038537395242922994\n",
            "78     \t [-0.51051861  1.68444812]. \t  -205.0076833315466 \t -0.0038537395242922994\n",
            "79     \t [-1.06095289 -0.39755859]. \t  -236.25514667039812 \t -0.0038537395242922994\n",
            "80     \t [ 0.36031694 -0.00897675]. \t  -2.335878478975027 \t -0.0038537395242922994\n",
            "81     \t [-1.75119222 -1.53266927]. \t  -2122.965094391892 \t -0.0038537395242922994\n",
            "82     \t [-0.82949027  1.47402342]. \t  -65.12181013587157 \t -0.0038537395242922994\n",
            "83     \t [ 0.10348957 -1.23911708]. \t  -157.01052777221707 \t -0.0038537395242922994\n",
            "84     \t [1.78402586 0.47746595]. \t  -732.4699348470266 \t -0.0038537395242922994\n",
            "85     \t [-1.05457877  1.81289119]. \t  -53.32702467490017 \t -0.0038537395242922994\n",
            "86     \t [-0.85440648 -1.43697053]. \t  -473.019468218265 \t -0.0038537395242922994\n",
            "87     \t [1.30286057 1.66690612]. \t  -0.18499100652013073 \t -0.0038537395242922994\n",
            "88     \t [-1.42980548  1.22539506]. \t  -72.97164514750482 \t -0.0038537395242922994\n",
            "89     \t [-1.88312686  1.82949245]. \t  -303.00949927111895 \t -0.0038537395242922994\n",
            "90     \t [-1.4660533   1.66046065]. \t  -29.979010274014193 \t -0.0038537395242922994\n",
            "91     \t [-0.0655655   0.64578957]. \t  -42.28646587945757 \t -0.0038537395242922994\n",
            "92     \t [1.59988677 0.83088525]. \t  -299.2183568062418 \t -0.0038537395242922994\n",
            "93     \t [-0.18625368  0.67477663]. \t  -42.37823194204345 \t -0.0038537395242922994\n",
            "94     \t [1.19295637 0.88925704]. \t  -28.540856130884436 \t -0.0038537395242922994\n",
            "95     \t [ 1.25493264 -0.61556837]. \t  -479.8608501814786 \t -0.0038537395242922994\n",
            "96     \t [1.5768618  1.87960322]. \t  -37.1643059079143 \t -0.0038537395242922994\n",
            "97     \t [0.6027097  1.82522772]. \t  -213.89309756821692 \t -0.0038537395242922994\n",
            "98     \t [-1.99286282 -1.79067893]. \t  -3329.230399486352 \t -0.0038537395242922994\n",
            "99     \t [ 1.5240691  -0.14424668]. \t  -608.8999803784616 \t -0.0038537395242922994\n",
            "100    \t [-0.71917525  1.86097463]. \t  -183.5250858961645 \t -0.0038537395242922994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kM4bcwSteJO",
        "outputId": "cdf66efc-bf9d-4bd7-8536-f28861d3da51"
      },
      "source": [
        "end_lose = time.time()\r\n",
        "end_lose\r\n",
        "\r\n",
        "time_lose = end_lose - start_lose\r\n",
        "time_lose\r\n",
        "\r\n",
        "start_win = time.time()\r\n",
        "start_win"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1616064186.6723"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NMzLM7nteQH",
        "outputId": "33b7463f-99a6-46f4-fe1d-c2248c520550"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_winner_1 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_1 = dGPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.45944904 -1.35549029]. \t  -245.71064611316496 \t -108.5713485785257\n",
            "init   \t [-0.26190226  1.10289909]. \t  -108.5713485785257 \t -108.5713485785257\n",
            "init   \t [-0.83834755 -1.43702853]. \t  -461.2775269355244 \t -108.5713485785257\n",
            "init   \t [-1.95592878 -0.32676048]. \t  -1732.9949421003257 \t -108.5713485785257\n",
            "init   \t [-1.07035795 -0.66496024]. \t  -332.12317010404104 \t -108.5713485785257\n",
            "1      \t [-0.45795185 -0.30236194]. \t  \u001b[92m-28.3484050352548\u001b[0m \t -28.3484050352548\n",
            "2      \t [ 1.77122139 -1.9380348 ]. \t  -2576.421199974247 \t -28.3484050352548\n",
            "3      \t [-0.13796132 -0.76251713]. \t  -62.37706656915885 \t -28.3484050352548\n",
            "4      \t [1.88493603 1.44424789]. \t  -445.45983372037045 \t -28.3484050352548\n",
            "5      \t [-0.50551879  1.86210364]. \t  -260.36828725301234 \t -28.3484050352548\n",
            "6      \t [ 1.77169217 -0.26658693]. \t  -1160.3249553860608 \t -28.3484050352548\n",
            "7      \t [ 1.78803192 -0.60648493]. \t  -1447.3149830671293 \t -28.3484050352548\n",
            "8      \t [-0.54544849 -0.38504307]. \t  -48.97683286981722 \t -28.3484050352548\n",
            "9      \t [-0.28693588  0.06998856]. \t  \u001b[92m-1.671440517146343\u001b[0m \t -1.671440517146343\n",
            "10     \t [-0.20789894  0.20341802]. \t  -4.0252972538534815 \t -1.671440517146343\n",
            "11     \t [ 0.36142632 -1.18833087]. \t  -174.3732864880468 \t -1.671440517146343\n",
            "12     \t [-0.29238286  0.16890347]. \t  -2.3660718696245593 \t -1.671440517146343\n",
            "13     \t [-0.94458782 -1.75105902]. \t  -702.4876461284873 \t -1.671440517146343\n",
            "14     \t [0.9684561 1.7270646]. \t  -62.27793206271763 \t -1.671440517146343\n",
            "15     \t [-0.13798151  1.2120092 ]. \t  -143.61281532895606 \t -1.671440517146343\n",
            "16     \t [0.67147821 2.03864898]. \t  -252.20801317965538 \t -1.671440517146343\n",
            "17     \t [-0.27647387 -0.70282806]. \t  -62.35491409208667 \t -1.671440517146343\n",
            "18     \t [ 0.74775298 -0.63960254]. \t  -143.7606812354837 \t -1.671440517146343\n",
            "19     \t [ 0.38714027 -0.06508762]. \t  -4.996601182775563 \t -1.671440517146343\n",
            "20     \t [ 0.3496504  -0.43226619]. \t  -31.172373957437184 \t -1.671440517146343\n",
            "21     \t [-0.37843791  1.36814964]. \t  -151.94651586396787 \t -1.671440517146343\n",
            "22     \t [0.47457207 0.20381443]. \t  \u001b[92m-0.32188857216549255\u001b[0m \t -0.32188857216549255\n",
            "23     \t [1.06089113 1.82174536]. \t  -48.480860367576014 \t -0.32188857216549255\n",
            "24     \t [-1.92372447 -0.91331464]. \t  -2137.475875853538 \t -0.32188857216549255\n",
            "25     \t [-1.49960569 -1.47265252]. \t  -1391.181726837824 \t -0.32188857216549255\n",
            "26     \t [0.77364958 0.56590163]. \t  \u001b[92m-0.15771949233931992\u001b[0m \t -0.15771949233931992\n",
            "27     \t [0.62932335 0.53982993]. \t  -2.204729073113254 \t -0.15771949233931992\n",
            "28     \t [-1.02080879 -0.13940014]. \t  -143.66625118755718 \t -0.15771949233931992\n",
            "29     \t [0.63714925 0.36514124]. \t  -0.2982709405119086 \t -0.15771949233931992\n",
            "30     \t [1.51037191 0.4337747 ]. \t  -341.56711141881 \t -0.15771949233931992\n",
            "31     \t [0.64266371 0.38026332]. \t  -0.2349672530070994 \t -0.15771949233931992\n",
            "32     \t [-0.01023269  0.03677599]. \t  -1.155048346445624 \t -0.15771949233931992\n",
            "33     \t [-1.39253331 -0.59085225]. \t  -645.8148494698042 \t -0.15771949233931992\n",
            "34     \t [0.7830366  0.62493331]. \t  \u001b[92m-0.06096641394710938\u001b[0m \t -0.06096641394710938\n",
            "35     \t [ 0.81411133 -0.95859836]. \t  -262.92044163522456 \t -0.06096641394710938\n",
            "36     \t [-1.23462122  0.38938055]. \t  -133.79537733532314 \t -0.06096641394710938\n",
            "37     \t [-1.58775183  1.54279751]. \t  -102.37583880116446 \t -0.06096641394710938\n",
            "38     \t [0.72147045 0.53528981]. \t  -0.09939460109848386 \t -0.06096641394710938\n",
            "39     \t [-1.6856219   0.18648594]. \t  -712.0275913623143 \t -0.06096641394710938\n",
            "40     \t [-1.53740048  1.2231408 ]. \t  -136.50317180697115 \t -0.06096641394710938\n",
            "41     \t [0.68795008 0.48252832]. \t  -0.10593698333281998 \t -0.06096641394710938\n",
            "42     \t [0.08183052 1.23046565]. \t  -150.60419279831945 \t -0.06096641394710938\n",
            "43     \t [-1.16223016 -1.42849377]. \t  -777.1109183233424 \t -0.06096641394710938\n",
            "44     \t [-1.04316404  1.15873988]. \t  -4.672230813198803 \t -0.06096641394710938\n",
            "45     \t [0.74327672 1.96714473]. \t  -200.19911334585981 \t -0.06096641394710938\n",
            "46     \t [-2.01047749 -0.74297611]. \t  -2298.681505002316 \t -0.06096641394710938\n",
            "47     \t [-0.65523244  0.41880051]. \t  -2.750880489418862 \t -0.06096641394710938\n",
            "48     \t [-1.33437787  0.68815167]. \t  -124.78585917988336 \t -0.06096641394710938\n",
            "49     \t [-0.23094754 -1.61276938]. \t  -279.10620252253784 \t -0.06096641394710938\n",
            "50     \t [-0.96282751  1.0077401 ]. \t  -4.503993810368809 \t -0.06096641394710938\n",
            "51     \t [0.93165312 0.86168907]. \t  \u001b[92m-0.008625782998406396\u001b[0m \t -0.008625782998406396\n",
            "52     \t [1.22158747 1.08415614]. \t  -16.70527922550893 \t -0.008625782998406396\n",
            "53     \t [ 0.16555488 -0.53612718]. \t  -32.4535352858786 \t -0.008625782998406396\n",
            "54     \t [ 0.42173736 -1.26611057]. \t  -208.84018173094518 \t -0.008625782998406396\n",
            "55     \t [-0.5189422  -1.68782909]. \t  -385.34301020541506 \t -0.008625782998406396\n",
            "56     \t [-0.95070309  1.48236624]. \t  -37.27492459488175 \t -0.008625782998406396\n",
            "57     \t [ 0.54384183 -0.89280069]. \t  -141.4766651669655 \t -0.008625782998406396\n",
            "58     \t [0.8051323  0.49947153]. \t  -2.2511201711580875 \t -0.008625782998406396\n",
            "59     \t [0.40937437 1.18707475]. \t  -104.28428965658428 \t -0.008625782998406396\n",
            "60     \t [1.02250523 1.72285278]. \t  -45.878888676329034 \t -0.008625782998406396\n",
            "61     \t [-0.54492688 -0.24007982]. \t  -31.226397664342578 \t -0.008625782998406396\n",
            "62     \t [0.98075065 1.02514641]. \t  -0.4007377904572951 \t -0.008625782998406396\n",
            "63     \t [-1.12854391  1.29562523]. \t  -4.57916022324821 \t -0.008625782998406396\n",
            "64     \t [-1.00705448  1.16548517]. \t  -6.318237027567069 \t -0.008625782998406396\n",
            "65     \t [-0.21515472  1.99157176]. \t  -379.8881065908075 \t -0.008625782998406396\n",
            "66     \t [0.97254944 0.91457515]. \t  -0.0985801857821006 \t -0.008625782998406396\n",
            "67     \t [ 1.46348283 -1.48988797]. \t  -1319.1174901038878 \t -0.008625782998406396\n",
            "68     \t [0.58030806 0.31242197]. \t  -0.2353628420447037 \t -0.008625782998406396\n",
            "69     \t [0.61277162 0.34978262]. \t  -0.2160279219698361 \t -0.008625782998406396\n",
            "70     \t [-1.55333206 -1.38547729]. \t  -1449.24130480916 \t -0.008625782998406396\n",
            "71     \t [1.27468998 0.27710306]. \t  -181.71346766613016 \t -0.008625782998406396\n",
            "72     \t [-0.28009762  1.23351318]. \t  -135.0546635396757 \t -0.008625782998406396\n",
            "73     \t [0.48728597 1.70008284]. \t  -214.19305277710876 \t -0.008625782998406396\n",
            "74     \t [-0.27488168 -1.9321867 ]. \t  -404.7299793637241 \t -0.008625782998406396\n",
            "75     \t [-0.24921281  0.39865629]. \t  -12.887073322359871 \t -0.008625782998406396\n",
            "76     \t [-0.52180996 -0.0714736 ]. \t  -14.132946809213406 \t -0.008625782998406396\n",
            "77     \t [1.10267427 1.26710472]. \t  -0.27283114831775146 \t -0.008625782998406396\n",
            "78     \t [1.02999374 2.00653251]. \t  -89.42542473466041 \t -0.008625782998406396\n",
            "79     \t [-0.4332399 -0.1926707]. \t  -16.522120495331716 \t -0.008625782998406396\n",
            "80     \t [1.0573334  1.06879388]. \t  -0.2449581118713727 \t -0.008625782998406396\n",
            "81     \t [ 0.66521893 -0.23347934]. \t  -45.80907842624932 \t -0.008625782998406396\n",
            "82     \t [1.06649139 1.1029999 ]. \t  -0.12278460417748603 \t -0.008625782998406396\n",
            "83     \t [ 0.74413541 -1.9884645 ]. \t  -646.3445759354339 \t -0.008625782998406396\n",
            "84     \t [1.18993284 1.65277456]. \t  -5.6451275735506075 \t -0.008625782998406396\n",
            "85     \t [-1.82086888  0.66929264]. \t  -708.232246310807 \t -0.008625782998406396\n",
            "86     \t [ 0.74271154 -1.83825245]. \t  -571.2154314512803 \t -0.008625782998406396\n",
            "87     \t [-0.39927098  1.30131624]. \t  -132.3512741366938 \t -0.008625782998406396\n",
            "88     \t [-1.08176543  0.14264375]. \t  -109.92430978318076 \t -0.008625782998406396\n",
            "89     \t [0.88647147 0.72135748]. \t  -0.42858072349503895 \t -0.008625782998406396\n",
            "90     \t [ 1.21960029 -0.11987661]. \t  -258.3900302193461 \t -0.008625782998406396\n",
            "91     \t [0.94712867 0.91261806]. \t  -0.027023372759658458 \t -0.008625782998406396\n",
            "92     \t [ 1.46806628 -0.6833924 ]. \t  -805.9903261107626 \t -0.008625782998406396\n",
            "93     \t [1.10093847 1.23767273]. \t  -0.07576147281820803 \t -0.008625782998406396\n",
            "94     \t [0.98562259 0.92921117]. \t  -0.17863457512455758 \t -0.008625782998406396\n",
            "95     \t [ 1.65361628 -1.69356049]. \t  -1961.1520765663745 \t -0.008625782998406396\n",
            "96     \t [-1.65372101 -1.45526157]. \t  -1762.698113551249 \t -0.008625782998406396\n",
            "97     \t [1.05498416 1.12829335]. \t  -0.026437674680498402 \t -0.008625782998406396\n",
            "98     \t [ 1.72114382 -0.85529876]. \t  -1457.9536047382583 \t -0.008625782998406396\n",
            "99     \t [0.34268958 0.38226986]. \t  -7.445746616971624 \t -0.008625782998406396\n",
            "100    \t [-1.0954881   0.33887054]. \t  -78.56168510444935 \t -0.008625782998406396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBLcW6tlteS2",
        "outputId": "c2d30474-9660-4fde-c7ae-89ec6c36af66"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_winner_2 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_2 = dGPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.44173258 -1.74529086]. \t  -1462.4037722675753 \t -1.3013277264983028\n",
            "init   \t [ 1.6176405  -0.26012243]. \t  -828.0271838688934 \t -1.3013277264983028\n",
            "init   \t [-1.525032    0.31071385]. \t  -412.40181251788414 \t -1.3013277264983028\n",
            "init   \t [ 1.39456889 -0.26574622]. \t  -488.8170397507174 \t -1.3013277264983028\n",
            "init   \t [0.80244966 0.75627765]. \t  -1.3013277264983028 \t -1.3013277264983028\n",
            "1      \t [ 0.33906095 -0.26492231]. \t  -14.868074192108132 \t -1.3013277264983028\n",
            "2      \t [ 0.57066633 -0.01674805]. \t  -11.90865821837115 \t -1.3013277264983028\n",
            "3      \t [0.92586135 1.81593596]. \t  -91.91927181046066 \t -1.3013277264983028\n",
            "4      \t [ 0.46909359 -0.52982229]. \t  -56.51252683278345 \t -1.3013277264983028\n",
            "5      \t [-0.91273848 -0.90921594]. \t  -307.222102538232 \t -1.3013277264983028\n",
            "6      \t [0.92300805 0.98102343]. \t  -1.6720809692187144 \t -1.3013277264983028\n",
            "7      \t [0.48312025 1.27821445]. \t  -109.42980591548745 \t -1.3013277264983028\n",
            "8      \t [ 0.96743136 -0.15673713]. \t  -119.39177174495406 \t -1.3013277264983028\n",
            "9      \t [0.07526988 0.23183411]. \t  -5.970347195745959 \t -1.3013277264983028\n",
            "10     \t [0.37222026 0.29128458]. \t  -2.726956178357351 \t -1.3013277264983028\n",
            "11     \t [1.95207255 1.92746704]. \t  -355.5206160216748 \t -1.3013277264983028\n",
            "12     \t [-0.10199791  0.05210515]. \t  -1.3883015200589874 \t -1.3013277264983028\n",
            "13     \t [0.81860128 0.89318377]. \t  -5.009183440238817 \t -1.3013277264983028\n",
            "14     \t [-0.61573294  1.91029004]. \t  -237.05660256142815 \t -1.3013277264983028\n",
            "15     \t [0.12331075 0.01264854]. \t  \u001b[92m-0.7692378731611671\u001b[0m \t -0.7692378731611671\n",
            "16     \t [0.41750835 0.230651  ]. \t  \u001b[92m-0.656691039051414\u001b[0m \t -0.656691039051414\n",
            "17     \t [-0.61156446  0.86155167]. \t  -26.366721503073624 \t -0.656691039051414\n",
            "18     \t [0.65031677 0.49562128]. \t  \u001b[92m-0.6509438342997204\u001b[0m \t -0.6509438342997204\n",
            "19     \t [2.00013178 1.35546904]. \t  -700.6334995177767 \t -0.6509438342997204\n",
            "20     \t [-0.3413371  -0.26077155]. \t  -16.033398685166116 \t -0.6509438342997204\n",
            "21     \t [ 1.78173395 -1.38890908]. \t  -2083.1506130796793 \t -0.6509438342997204\n",
            "22     \t [-1.8893095  -0.46960809]. \t  -1639.7797752822478 \t -0.6509438342997204\n",
            "23     \t [-0.25321018 -2.02663948]. \t  -438.69613207630664 \t -0.6509438342997204\n",
            "24     \t [-0.80199266  0.57659333]. \t  -3.690718889652942 \t -0.6509438342997204\n",
            "25     \t [0.65016024 0.40897501]. \t  \u001b[92m-0.14124829137644457\u001b[0m \t -0.14124829137644457\n",
            "26     \t [-0.66739128  0.56954444]. \t  -4.32110180327635 \t -0.14124829137644457\n",
            "27     \t [-1.73899525  1.26629116]. \t  -316.4928675688702 \t -0.14124829137644457\n",
            "28     \t [0.15102971 1.10510574]. \t  -117.8571634189896 \t -0.14124829137644457\n",
            "29     \t [2.03504526 0.92232824]. \t  -1037.3195379050223 \t -0.14124829137644457\n",
            "30     \t [-0.46905516 -0.16980238]. \t  -17.353706206974486 \t -0.14124829137644457\n",
            "31     \t [0.35834341 0.11670329]. \t  -0.42542789130019637 \t -0.14124829137644457\n",
            "32     \t [0.44325224 0.573012  ]. \t  -14.48816406142393 \t -0.14124829137644457\n",
            "33     \t [-0.24541195 -1.78732495]. \t  -342.8958826553927 \t -0.14124829137644457\n",
            "34     \t [-0.08853253 -0.47931179]. \t  -24.91639580189656 \t -0.14124829137644457\n",
            "35     \t [-0.68739673 -0.15970921]. \t  -42.81796004380665 \t -0.14124829137644457\n",
            "36     \t [ 1.74293882 -0.80536341]. \t  -1477.5699245440094 \t -0.14124829137644457\n",
            "37     \t [1.19934537 0.68577621]. \t  -56.6884084518539 \t -0.14124829137644457\n",
            "38     \t [0.98309222 0.91418125]. \t  -0.27370051266393847 \t -0.14124829137644457\n",
            "39     \t [0.84065146 0.69970027]. \t  \u001b[92m-0.030284404489958974\u001b[0m \t -0.030284404489958974\n",
            "40     \t [-0.80967392 -0.64047878]. \t  -171.24964427357978 \t -0.030284404489958974\n",
            "41     \t [1.03165866 0.66417548]. \t  -16.01253240286429 \t -0.030284404489958974\n",
            "42     \t [0.90591466 0.84510522]. \t  -0.06850446654765592 \t -0.030284404489958974\n",
            "43     \t [-0.75738889  0.35083364]. \t  -8.05259127383539 \t -0.030284404489958974\n",
            "44     \t [-1.55338422  1.07956766]. \t  -184.32462558936453 \t -0.030284404489958974\n",
            "45     \t [ 1.99501367 -1.7679637 ]. \t  -3304.990185125095 \t -0.030284404489958974\n",
            "46     \t [-1.75491246  0.43709123]. \t  -705.9370292132911 \t -0.030284404489958974\n",
            "47     \t [-1.92446685  2.01946897]. \t  -292.1730256583349 \t -0.030284404489958974\n",
            "48     \t [-1.16608873  1.36943305]. \t  -4.7012915131738024 \t -0.030284404489958974\n",
            "49     \t [-0.57036757 -1.33478685]. \t  -278.0612514402123 \t -0.030284404489958974\n",
            "50     \t [-1.69779039 -0.43012228]. \t  -1104.6195465731287 \t -0.030284404489958974\n",
            "51     \t [1.05914411 1.12345495]. \t  \u001b[92m-0.0037764840180860245\u001b[0m \t -0.0037764840180860245\n",
            "52     \t [-1.51848669  1.61216493]. \t  -54.45599160830821 \t -0.0037764840180860245\n",
            "53     \t [-1.28014554  1.69347956]. \t  -5.498348840884845 \t -0.0037764840180860245\n",
            "54     \t [-0.57362638  0.37193407]. \t  -2.6602281661743064 \t -0.0037764840180860245\n",
            "55     \t [-0.84109744  0.58123478]. \t  -4.982539243900186 \t -0.0037764840180860245\n",
            "56     \t [1.31403595 0.46630098]. \t  -158.95678900240353 \t -0.0037764840180860245\n",
            "57     \t [1.96646943 0.26738163]. \t  -1296.6607706834602 \t -0.0037764840180860245\n",
            "58     \t [-1.93116689 -0.85024414]. \t  -2105.910875856383 \t -0.0037764840180860245\n",
            "59     \t [-1.32427512  0.34521736]. \t  -203.78588019736992 \t -0.0037764840180860245\n",
            "60     \t [0.28483277 1.61744199]. \t  -236.53700601606423 \t -0.0037764840180860245\n",
            "61     \t [-1.39996957  1.92260048]. \t  -5.899089727361261 \t -0.0037764840180860245\n",
            "62     \t [1.85709005 1.58285552]. \t  -348.9033138942158 \t -0.0037764840180860245\n",
            "63     \t [1.38029812 1.78076422]. \t  -1.6936229687432374 \t -0.0037764840180860245\n",
            "64     \t [1.11360539 1.2707486 ]. \t  -0.10673584069019053 \t -0.0037764840180860245\n",
            "65     \t [-0.11136909  1.32217248]. \t  -172.78473047588355 \t -0.0037764840180860245\n",
            "66     \t [0.68156913 0.41441561]. \t  -0.35260827284530105 \t -0.0037764840180860245\n",
            "67     \t [ 0.60840368 -1.00349936]. \t  -188.84598804941893 \t -0.0037764840180860245\n",
            "68     \t [-0.04636727 -0.07615456]. \t  -1.7080436883233414 \t -0.0037764840180860245\n",
            "69     \t [-1.14585149 -1.04130742]. \t  -558.8695561563712 \t -0.0037764840180860245\n",
            "70     \t [ 0.27362341 -0.21059029]. \t  -8.676367778288038 \t -0.0037764840180860245\n",
            "71     \t [0.74365306 0.54086756]. \t  -0.08048163691127136 \t -0.0037764840180860245\n",
            "72     \t [1.14839252 1.33771885]. \t  -0.05779227199757152 \t -0.0037764840180860245\n",
            "73     \t [0.95314975 0.73193607]. \t  -3.1194807181052044 \t -0.0037764840180860245\n",
            "74     \t [-0.06472406 -0.11432975]. \t  -2.5383114700253273 \t -0.0037764840180860245\n",
            "75     \t [-1.9517589  -0.69069591]. \t  -2033.76572776341 \t -0.0037764840180860245\n",
            "76     \t [1.76927873 1.12586904]. \t  -402.38507314741815 \t -0.0037764840180860245\n",
            "77     \t [-1.18605588  1.00865149]. \t  -20.62537567388703 \t -0.0037764840180860245\n",
            "78     \t [1.13720816 1.18231401]. \t  -1.2493368666492157 \t -0.0037764840180860245\n",
            "79     \t [-1.03351079  1.23317937]. \t  -6.858815070908965 \t -0.0037764840180860245\n",
            "80     \t [-1.03425542  0.62109077]. \t  -24.261808152218876 \t -0.0037764840180860245\n",
            "81     \t [0.54678719 1.70335149]. \t  -197.43238918791414 \t -0.0037764840180860245\n",
            "82     \t [-1.06598285  1.8813477 ]. \t  -59.77499610171581 \t -0.0037764840180860245\n",
            "83     \t [2.010771   1.52827972]. \t  -633.504066675221 \t -0.0037764840180860245\n",
            "84     \t [ 1.63095484 -0.77756562]. \t  -1182.0932557353174 \t -0.0037764840180860245\n",
            "85     \t [-0.08660451 -0.70069306]. \t  -51.33449912311258 \t -0.0037764840180860245\n",
            "86     \t [1.46932108 1.16443248]. \t  -99.11770622554333 \t -0.0037764840180860245\n",
            "87     \t [1.889115   0.21074836]. \t  -1128.411706159457 \t -0.0037764840180860245\n",
            "88     \t [-0.34497038 -1.43593559]. \t  -243.5928354559929 \t -0.0037764840180860245\n",
            "89     \t [0.74091969 0.55119627]. \t  -0.06762180313179203 \t -0.0037764840180860245\n",
            "90     \t [0.87053935 0.76388366]. \t  -0.020414150695507532 \t -0.0037764840180860245\n",
            "91     \t [-0.98007518  1.8084631 ]. \t  -75.81680806136724 \t -0.0037764840180860245\n",
            "92     \t [-0.28495107 -1.45635923]. \t  -238.05904995480884 \t -0.0037764840180860245\n",
            "93     \t [-0.35843956  0.40250036]. \t  -9.35413291051952 \t -0.0037764840180860245\n",
            "94     \t [-1.60692968  1.75495421]. \t  -75.2334457526766 \t -0.0037764840180860245\n",
            "95     \t [1.31428135 1.6857859 ]. \t  -0.2714094132373471 \t -0.0037764840180860245\n",
            "96     \t [1.17085781 0.86407361]. \t  -25.717303713322334 \t -0.0037764840180860245\n",
            "97     \t [-0.54378274  1.59506367]. \t  -171.2179454991696 \t -0.0037764840180860245\n",
            "98     \t [-1.20891657  0.77681768]. \t  -51.75546165606691 \t -0.0037764840180860245\n",
            "99     \t [ 1.04183685 -0.83097723]. \t  -367.26112762398185 \t -0.0037764840180860245\n",
            "100    \t [-0.67481668  0.98829967]. \t  -31.20560964966402 \t -0.0037764840180860245\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emgjXwfeuvRY",
        "outputId": "300c0414-0731-4573-b838-b02e6abced7f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_winner_3 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_3 = dGPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.03045369 -1.60050678]. \t  -708.8071948417274 \t -1.118465165857483\n",
            "init   \t [-0.05684724 -0.0006915 ]. \t  -1.118465165857483 \t -1.118465165857483\n",
            "init   \t [ 0.64352256 -1.08181622]. \t  -223.90997740226942 \t -1.118465165857483\n",
            "init   \t [ 0.46200787 -1.55790376]. \t  -314.05929962166397 \t -1.118465165857483\n",
            "init   \t [ 0.86918061 -0.52199202]. \t  -163.20929387450548 \t -1.118465165857483\n",
            "1      \t [-0.56212963 -1.0124551 ]. \t  -178.9168151053094 \t -1.118465165857483\n",
            "2      \t [ 0.05299415 -0.17242805]. \t  -3.967600882071375 \t -1.118465165857483\n",
            "3      \t [-0.41910369  1.35120083]. \t  -140.20632370766666 \t -1.118465165857483\n",
            "4      \t [-0.17804899  0.27999122]. \t  -7.55258091557359 \t -1.118465165857483\n",
            "5      \t [-0.32780197  0.14453739]. \t  -1.900574874717073 \t -1.118465165857483\n",
            "6      \t [1.78622843 1.94317708]. \t  -156.22753952620175 \t -1.118465165857483\n",
            "7      \t [1.01799814 0.68687832]. \t  -12.21128808354645 \t -1.118465165857483\n",
            "8      \t [-1.66456866  0.53875491]. \t  -505.29746991579844 \t -1.118465165857483\n",
            "9      \t [1.60324335 0.58687722]. \t  -393.79589440060465 \t -1.118465165857483\n",
            "10     \t [0.68191297 1.32546995]. \t  -74.14112269548784 \t -1.118465165857483\n",
            "11     \t [0.64144349 0.36046263]. \t  \u001b[92m-0.38853145802709754\u001b[0m \t -0.38853145802709754\n",
            "12     \t [0.81911456 0.61174432]. \t  \u001b[92m-0.38323494620650933\u001b[0m \t -0.38323494620650933\n",
            "13     \t [0.99574087 1.00147807]. \t  \u001b[92m-0.009974592319267696\u001b[0m \t -0.009974592319267696\n",
            "14     \t [0.38583385 0.00835099]. \t  -2.3516963893086014 \t -0.009974592319267696\n",
            "15     \t [1.10273788 1.47407595]. \t  -6.669283309424699 \t -0.009974592319267696\n",
            "16     \t [ 0.8520427 -0.3307208]. \t  -111.6828633500786 \t -0.009974592319267696\n",
            "17     \t [0.8575366  1.97026682]. \t  -152.51755456713286 \t -0.009974592319267696\n",
            "18     \t [0.63329295 0.29079694]. \t  -1.3502675054359043 \t -0.009974592319267696\n",
            "19     \t [1.08480008 1.21772343]. \t  -0.17473580214477888 \t -0.009974592319267696\n",
            "20     \t [-0.38415023 -0.51128198]. \t  -45.324649641757965 \t -0.009974592319267696\n",
            "21     \t [-1.93971307  0.59809255]. \t  -1009.9810140546972 \t -0.009974592319267696\n",
            "22     \t [-0.85894999  0.52296713]. \t  -8.070800207643725 \t -0.009974592319267696\n",
            "23     \t [-1.40045097 -0.19027628]. \t  -468.6742569340373 \t -0.009974592319267696\n",
            "24     \t [-0.86774288  0.63332426]. \t  -4.920158121310172 \t -0.009974592319267696\n",
            "25     \t [-1.1079368   1.20285759]. \t  -4.504240524603761 \t -0.009974592319267696\n",
            "26     \t [ 0.86474998 -1.06134866]. \t  -327.31747625263705 \t -0.009974592319267696\n",
            "27     \t [-0.17477802  1.48873115]. \t  -214.0101020827603 \t -0.009974592319267696\n",
            "28     \t [1.38753524 1.99555813]. \t  -0.6444500539681968 \t -0.009974592319267696\n",
            "29     \t [1.83979201 0.74160758]. \t  -699.3701751977119 \t -0.009974592319267696\n",
            "30     \t [1.03511617 1.14922769]. \t  -0.60592919313856 \t -0.009974592319267696\n",
            "31     \t [ 1.13060795 -1.47916085]. \t  -760.3619397240727 \t -0.009974592319267696\n",
            "32     \t [-0.98261309  0.04318056]. \t  -89.00332324256519 \t -0.009974592319267696\n",
            "33     \t [-0.67499788  1.75886749]. \t  -172.6504632335947 \t -0.009974592319267696\n",
            "34     \t [-1.28509462  0.25780746]. \t  -199.45068044631728 \t -0.009974592319267696\n",
            "35     \t [-0.39069957 -0.26605664]. \t  -19.465248567313655 \t -0.009974592319267696\n",
            "36     \t [ 1.16486684 -0.53976381]. \t  -359.7661432565623 \t -0.009974592319267696\n",
            "37     \t [-0.42003048 -1.28318347]. \t  -215.0623511402162 \t -0.009974592319267696\n",
            "38     \t [-1.94580847 -0.71166838]. \t  -2031.7333252118815 \t -0.009974592319267696\n",
            "39     \t [-1.03861437 -1.56074171]. \t  -700.8316600995099 \t -0.009974592319267696\n",
            "40     \t [1.63843289 1.36021904]. \t  -175.76962581466148 \t -0.009974592319267696\n",
            "41     \t [ 0.78733669 -2.04448324]. \t  -709.9385288558885 \t -0.009974592319267696\n",
            "42     \t [1.21614859 1.47042643]. \t  -0.05410066927809473 \t -0.009974592319267696\n",
            "43     \t [-0.3711829 -0.3645665]. \t  -27.115015768712222 \t -0.009974592319267696\n",
            "44     \t [0.8911853  0.76385037]. \t  -0.1040188783536322 \t -0.009974592319267696\n",
            "45     \t [-1.83494523 -0.90244545]. \t  -1830.873844658998 \t -0.009974592319267696\n",
            "46     \t [ 0.34057347 -0.16994831]. \t  -8.610931318744916 \t -0.009974592319267696\n",
            "47     \t [0.7946948  0.64826948]. \t  -0.07013838891524227 \t -0.009974592319267696\n",
            "48     \t [1.62675981 0.68831337]. \t  -383.7825862242523 \t -0.009974592319267696\n",
            "49     \t [-1.92677455  0.16661211]. \t  -1265.869850209115 \t -0.009974592319267696\n",
            "50     \t [-1.43196859  1.68221133]. \t  -19.48063398134607 \t -0.009974592319267696\n",
            "51     \t [-1.45985298  2.03008325]. \t  -7.0727447641879655 \t -0.009974592319267696\n",
            "52     \t [ 0.85535277 -0.34463053]. \t  -115.85424275582528 \t -0.009974592319267696\n",
            "53     \t [1.13269135 1.26759145]. \t  -0.04131755720586214 \t -0.009974592319267696\n",
            "54     \t [1.38281945 0.84828167]. \t  -113.33656656331196 \t -0.009974592319267696\n",
            "55     \t [0.85238635 0.74968644]. \t  -0.07526150509776647 \t -0.009974592319267696\n",
            "56     \t [-0.93135105 -0.28698016]. \t  -136.99288509973206 \t -0.009974592319267696\n",
            "57     \t [-1.39071243  0.62388476]. \t  -177.3769387689481 \t -0.009974592319267696\n",
            "58     \t [0.89558766 0.78814397]. \t  -0.03031559161773625 \t -0.009974592319267696\n",
            "59     \t [-1.1281658   0.19057678]. \t  -121.6407261095112 \t -0.009974592319267696\n",
            "60     \t [-0.94243415  0.15022679]. \t  -58.23085882279816 \t -0.009974592319267696\n",
            "61     \t [0.81456657 0.65549692]. \t  -0.04082045478636227 \t -0.009974592319267696\n",
            "62     \t [-0.40346979 -0.06644562]. \t  -7.22452655935794 \t -0.009974592319267696\n",
            "63     \t [-1.72920729  0.0166434 ]. \t  -891.627389019488 \t -0.009974592319267696\n",
            "64     \t [ 0.62106185 -0.95154766]. \t  -178.97149180482535 \t -0.009974592319267696\n",
            "65     \t [-1.21261842 -1.9313326 ]. \t  -1162.1036949545671 \t -0.009974592319267696\n",
            "66     \t [1.66604392 0.39622405]. \t  -566.6353090913572 \t -0.009974592319267696\n",
            "67     \t [1.16085906 1.35068609]. \t  -0.02683189292412615 \t -0.009974592319267696\n",
            "68     \t [0.60207405 0.38762294]. \t  -0.22149561162241863 \t -0.009974592319267696\n",
            "69     \t [0.97054361 0.76680745]. \t  -3.0685303410528384 \t -0.009974592319267696\n",
            "70     \t [1.76609451 0.29053984]. \t  -800.6563950055495 \t -0.009974592319267696\n",
            "71     \t [1.26308978 1.63452918]. \t  -0.22235843424364615 \t -0.009974592319267696\n",
            "72     \t [ 1.73459313 -1.33251473]. \t  -1885.252561455859 \t -0.009974592319267696\n",
            "73     \t [ 1.07843284 -1.26873019]. \t  -591.3457791951057 \t -0.009974592319267696\n",
            "74     \t [ 0.05185248 -0.61927705]. \t  -39.58312041762906 \t -0.009974592319267696\n",
            "75     \t [ 1.39075807 -1.89625902]. \t  -1467.4004652258593 \t -0.009974592319267696\n",
            "76     \t [ 0.65582879 -0.27149012]. \t  -49.342923630423144 \t -0.009974592319267696\n",
            "77     \t [-1.13297187 -0.62810777]. \t  -370.021887695414 \t -0.009974592319267696\n",
            "78     \t [-0.16329517 -0.12542853]. \t  -3.6665093913833675 \t -0.009974592319267696\n",
            "79     \t [1.12645294 0.53893632]. \t  -53.30013520835561 \t -0.009974592319267696\n",
            "80     \t [ 1.59495137 -0.17072792]. \t  -737.2580887986887 \t -0.009974592319267696\n",
            "81     \t [0.50527116 0.25516119]. \t  -0.24475852382951466 \t -0.009974592319267696\n",
            "82     \t [0.68399128 0.74996957]. \t  -8.059340982336861 \t -0.009974592319267696\n",
            "83     \t [-1.37415569  1.40566405]. \t  -28.9307325057905 \t -0.009974592319267696\n",
            "84     \t [-0.71861415  0.91425582]. \t  -18.78205873832965 \t -0.009974592319267696\n",
            "85     \t [-1.7717308   2.02961981]. \t  -130.761591605953 \t -0.009974592319267696\n",
            "86     \t [0.37447845 0.96941186]. \t  -69.14485198697025 \t -0.009974592319267696\n",
            "87     \t [ 0.13406096 -0.30988621]. \t  -11.49897327350787 \t -0.009974592319267696\n",
            "88     \t [-1.97757266  0.61735958]. \t  -1093.5367051180658 \t -0.009974592319267696\n",
            "89     \t [-0.04372883 -0.81460824]. \t  -67.75993521587327 \t -0.009974592319267696\n",
            "90     \t [-0.9563154  -0.45374215]. \t  -191.04654219354157 \t -0.009974592319267696\n",
            "91     \t [0.51069402 0.31521316]. \t  -0.5354083256716863 \t -0.009974592319267696\n",
            "92     \t [ 0.56971754 -0.26063979]. \t  -34.43313749931704 \t -0.009974592319267696\n",
            "93     \t [ 0.71186019 -0.97858099]. \t  -220.70233264226943 \t -0.009974592319267696\n",
            "94     \t [-0.68390632 -0.72930392]. \t  -146.12404581908285 \t -0.009974592319267696\n",
            "95     \t [0.76579775 1.19033201]. \t  -36.5226583551558 \t -0.009974592319267696\n",
            "96     \t [ 1.7242307  -0.10450261]. \t  -947.6092010408778 \t -0.009974592319267696\n",
            "97     \t [-1.25274348  1.20739645]. \t  -18.17706518631196 \t -0.009974592319267696\n",
            "98     \t [-1.8188858  -2.01500327]. \t  -2841.750389796985 \t -0.009974592319267696\n",
            "99     \t [0.28598232 1.17276022]. \t  -119.53232007701105 \t -0.009974592319267696\n",
            "100    \t [ 1.94508247 -0.03896131]. \t  -1461.8963614633797 \t -0.009974592319267696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8riJpBBKuvT4",
        "outputId": "33406306-9763-4295-81b5-adb86cf39dee"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_winner_4 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_4 = dGPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.39002285 1.31602418]. \t  -38.11488737154776 \t -12.122423820878506\n",
            "init   \t [0.58248055 0.68494384]. \t  -12.122423820878506 \t -12.122423820878506\n",
            "init   \t [-1.88768295 -1.7790059 ]. \t  -2862.4120613667637 \t -12.122423820878506\n",
            "init   \t [-0.91545338 -0.75218738]. \t  -256.5560079635026 \t -12.122423820878506\n",
            "init   \t [-1.25504548 -1.24070756]. \t  -797.98450090518 \t -12.122423820878506\n",
            "1      \t [ 0.0784139  -1.99165818]. \t  -399.9725697606536 \t -12.122423820878506\n",
            "2      \t [0.36848897 1.24803051]. \t  -124.10800925982912 \t -12.122423820878506\n",
            "3      \t [1.30511175 0.39567731]. \t  -171.08516983793774 \t -12.122423820878506\n",
            "4      \t [2.04167073 2.00984239]. \t  -467.03053841253114 \t -12.122423820878506\n",
            "5      \t [0.79593688 0.97927214]. \t  \u001b[92m-11.996405831797803\u001b[0m \t -11.996405831797803\n",
            "6      \t [1.06751992 0.99522106]. \t  \u001b[92m-2.0890515799102998\u001b[0m \t -2.0890515799102998\n",
            "7      \t [ 1.73111495 -0.26146777]. \t  -1062.138680699351 \t -2.0890515799102998\n",
            "8      \t [0.91633332 0.64286051]. \t  -3.8802697106202237 \t -2.0890515799102998\n",
            "9      \t [ 1.56120513 -1.00487715]. \t  -1185.2156248763604 \t -2.0890515799102998\n",
            "10     \t [0.97912462 0.94454136]. \t  \u001b[92m-0.020440100377350882\u001b[0m \t -0.020440100377350882\n",
            "11     \t [-0.68497996  0.16259641]. \t  -12.239583504391522 \t -0.020440100377350882\n",
            "12     \t [-1.55488042  0.60867409]. \t  -333.76793250443313 \t -0.020440100377350882\n",
            "13     \t [0.99252095 2.00458484]. \t  -103.93543206467135 \t -0.020440100377350882\n",
            "14     \t [-0.78974143  0.27651144]. \t  -15.256575425689658 \t -0.020440100377350882\n",
            "15     \t [-0.17325487  1.62640431]. \t  -256.22169244829337 \t -0.020440100377350882\n",
            "16     \t [-0.18308617 -1.34190572]. \t  -190.579434904392 \t -0.020440100377350882\n",
            "17     \t [-0.61716653  0.41143276]. \t  -2.70848594564955 \t -0.020440100377350882\n",
            "18     \t [1.71579144 1.97745606]. \t  -93.92152626136267 \t -0.020440100377350882\n",
            "19     \t [ 1.12210002 -1.56110415]. \t  -795.3748184470774 \t -0.020440100377350882\n",
            "20     \t [1.30842938 1.52198791]. \t  -3.7051108310266336 \t -0.020440100377350882\n",
            "21     \t [0.98143203 0.80758683]. \t  -2.422165147209792 \t -0.020440100377350882\n",
            "22     \t [1.35328057 1.97031429]. \t  -2.0554055417217794 \t -0.020440100377350882\n",
            "23     \t [-0.29426598  0.27064566]. \t  -5.062682049140375 \t -0.020440100377350882\n",
            "24     \t [1.83420092 1.92805264]. \t  -206.97453394502486 \t -0.020440100377350882\n",
            "25     \t [1.36793403 0.01419889]. \t  -344.99684787182787 \t -0.020440100377350882\n",
            "26     \t [-1.47557584  0.22933391]. \t  -385.5950403975736 \t -0.020440100377350882\n",
            "27     \t [1.42433292 1.98162195]. \t  -0.4019213199553636 \t -0.020440100377350882\n",
            "28     \t [0.81522261 0.09355927]. \t  -32.64151210850692 \t -0.020440100377350882\n",
            "29     \t [1.3944129  1.81608946]. \t  -1.8015959202528349 \t -0.020440100377350882\n",
            "30     \t [-1.92875429 -0.65237303]. \t  -1920.4236213733727 \t -0.020440100377350882\n",
            "31     \t [-0.54320563  0.35996153]. \t  -2.802544136069724 \t -0.020440100377350882\n",
            "32     \t [-0.55885019 -1.36792651]. \t  -284.7506778298577 \t -0.020440100377350882\n",
            "33     \t [0.24363327 1.74337258]. \t  -284.16287928828496 \t -0.020440100377350882\n",
            "34     \t [-0.01587209 -0.58220972]. \t  -34.958152717907716 \t -0.020440100377350882\n",
            "35     \t [-1.70714828 -1.19774021]. \t  -1698.2615538946 \t -0.020440100377350882\n",
            "36     \t [-1.37723669  1.28158256]. \t  -43.498152342951585 \t -0.020440100377350882\n",
            "37     \t [-1.57101865  2.03105274]. \t  -25.711132868522846 \t -0.020440100377350882\n",
            "38     \t [ 0.37926631 -1.66568356]. \t  -327.8239238447716 \t -0.020440100377350882\n",
            "39     \t [-1.1348636   1.49902886]. \t  -9.014532014255431 \t -0.020440100377350882\n",
            "40     \t [ 0.82992569 -0.41506505]. \t  -121.87557525235378 \t -0.020440100377350882\n",
            "41     \t [-0.25533525 -0.95263609]. \t  -105.17410174748937 \t -0.020440100377350882\n",
            "42     \t [1.19079437 0.37061068]. \t  -109.7370036372121 \t -0.020440100377350882\n",
            "43     \t [-1.82438208  1.90720576]. \t  -209.94790309091658 \t -0.020440100377350882\n",
            "44     \t [-0.00051807 -0.49995919]. \t  -25.996982315917172 \t -0.020440100377350882\n",
            "45     \t [-1.37429839  1.73310574]. \t  -8.058127826750235 \t -0.020440100377350882\n",
            "46     \t [-0.23860944 -1.69672867]. \t  -309.06759363010303 \t -0.020440100377350882\n",
            "47     \t [0.1361602  0.91973458]. \t  -81.9614576900326 \t -0.020440100377350882\n",
            "48     \t [-0.13525437 -0.01839927]. \t  -1.4234402205912833 \t -0.020440100377350882\n",
            "49     \t [-0.85724841 -1.36641014]. \t  -444.9892310789567 \t -0.020440100377350882\n",
            "50     \t [0.88651513 0.84004552]. \t  -0.30595420640886656 \t -0.020440100377350882\n",
            "51     \t [1.80553109 0.05877185]. \t  -1025.398245789344 \t -0.020440100377350882\n",
            "52     \t [1.96827608 0.51385253]. \t  -1130.0710754655745 \t -0.020440100377350882\n",
            "53     \t [1.88164587 0.89465385]. \t  -700.875734542644 \t -0.020440100377350882\n",
            "54     \t [ 1.32955488 -1.4778569 ]. \t  -1053.4830680164557 \t -0.020440100377350882\n",
            "55     \t [0.12204083 0.02399996]. \t  -0.7791042207725108 \t -0.020440100377350882\n",
            "56     \t [-1.32313472  1.93226442]. \t  -8.694046079110318 \t -0.020440100377350882\n",
            "57     \t [0.0025451  1.09913131]. \t  -121.80245700293212 \t -0.020440100377350882\n",
            "58     \t [-1.13628881 -0.58089985]. \t  -355.02164384040896 \t -0.020440100377350882\n",
            "59     \t [-1.90671123  0.45571025]. \t  -1019.5855922763678 \t -0.020440100377350882\n",
            "60     \t [ 1.69350482 -0.70147587]. \t  -1274.5671653469997 \t -0.020440100377350882\n",
            "61     \t [ 1.58903548 -0.02424579]. \t  -650.2295841149264 \t -0.020440100377350882\n",
            "62     \t [0.43221933 0.15398189]. \t  -0.4301667242898687 \t -0.020440100377350882\n",
            "63     \t [-0.68900583 -0.93015724]. \t  -200.2232850113495 \t -0.020440100377350882\n",
            "64     \t [-1.9511115   1.59976625]. \t  -495.82478171662814 \t -0.020440100377350882\n",
            "65     \t [0.1433744  0.57671401]. \t  -31.66495620399632 \t -0.020440100377350882\n",
            "66     \t [-0.75099975 -0.36212237]. \t  -88.83637904884068 \t -0.020440100377350882\n",
            "67     \t [0.71197017 0.50526466]. \t  -0.083229114579198 \t -0.020440100377350882\n",
            "68     \t [0.1155818 0.9244811]. \t  -83.79651624949977 \t -0.020440100377350882\n",
            "69     \t [0.9302396  0.89922877]. \t  -0.11967264793471892 \t -0.020440100377350882\n",
            "70     \t [-1.47252835  0.50797933]. \t  -281.7930650445806 \t -0.020440100377350882\n",
            "71     \t [-0.60496585  0.88407464]. \t  -29.41773966484951 \t -0.020440100377350882\n",
            "72     \t [0.13545758 1.00238521]. \t  -97.58020718432583 \t -0.020440100377350882\n",
            "73     \t [0.81149774 0.67475577]. \t  -0.06186525973497762 \t -0.020440100377350882\n",
            "74     \t [-0.84245666 -1.64991733]. \t  -560.1897177737782 \t -0.020440100377350882\n",
            "75     \t [ 1.60969256 -1.00479884]. \t  -1293.4278517665257 \t -0.020440100377350882\n",
            "76     \t [0.33941695 0.41635123]. \t  -9.50534310900239 \t -0.020440100377350882\n",
            "77     \t [1.43984719 1.69352848]. \t  -14.605468984402297 \t -0.020440100377350882\n",
            "78     \t [1.44351854 2.00896715]. \t  -0.7558928709733288 \t -0.020440100377350882\n",
            "79     \t [0.59607989 0.36541864]. \t  -0.17336742519789472 \t -0.020440100377350882\n",
            "80     \t [0.68103099 0.44398436]. \t  -0.14101994206855661 \t -0.020440100377350882\n",
            "81     \t [0.77102632 0.77151141]. \t  -3.1863844894260773 \t -0.020440100377350882\n",
            "82     \t [-1.88207283  1.49017923]. \t  -429.3845006603239 \t -0.020440100377350882\n",
            "83     \t [ 0.15937131 -1.33503194]. \t  -185.78394798982714 \t -0.020440100377350882\n",
            "84     \t [1.01586111 1.01434984]. \t  -0.03131192845511335 \t -0.020440100377350882\n",
            "85     \t [1.78889777 0.96399803]. \t  -500.6622668173832 \t -0.020440100377350882\n",
            "86     \t [0.40301274 0.14838457]. \t  -0.3760910721400262 \t -0.020440100377350882\n",
            "87     \t [-1.99508571  1.16711341]. \t  -800.410112024754 \t -0.020440100377350882\n",
            "88     \t [ 2.00197735 -0.98261503]. \t  -2491.5412798331467 \t -0.020440100377350882\n",
            "89     \t [-0.75829718 -1.78029594]. \t  -557.8403897443384 \t -0.020440100377350882\n",
            "90     \t [ 1.26053213 -0.50401093]. \t  -438.1127623760935 \t -0.020440100377350882\n",
            "91     \t [0.50783036 0.26695439]. \t  -0.2504442461451423 \t -0.020440100377350882\n",
            "92     \t [ 1.1015313  -0.52844185]. \t  -303.40158362456754 \t -0.020440100377350882\n",
            "93     \t [1.11703424 1.1815487 ]. \t  -0.4521633044595778 \t -0.020440100377350882\n",
            "94     \t [0.97817863 0.96346351]. \t  \u001b[92m-0.0048719716429606475\u001b[0m \t -0.0048719716429606475\n",
            "95     \t [-0.26471549 -0.51119417]. \t  -35.38680766315234 \t -0.0048719716429606475\n",
            "96     \t [ 1.35705638 -0.0890798 ]. \t  -372.880721250297 \t -0.0048719716429606475\n",
            "97     \t [0.79248782 0.00551502]. \t  -38.79641589678887 \t -0.0048719716429606475\n",
            "98     \t [-1.22520577  1.49539825]. \t  -4.954825042362605 \t -0.0048719716429606475\n",
            "99     \t [-0.26276442  1.13854958]. \t  -115.97854910800217 \t -0.0048719716429606475\n",
            "100    \t [ 1.28981407 -0.97803313]. \t  -697.9172874987738 \t -0.0048719716429606475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrvkDXP0uvWU",
        "outputId": "a77f20d9-d95f-4666-8340-f33f9fef4fab"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_winner_5 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_5 = dGPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.23491901 0.17105153]. \t  -1.9278091788796494 \t -1.9278091788796494\n",
            "init   \t [ 0.24749582 -0.48313101]. \t  -30.201785940713883 \t -1.9278091788796494\n",
            "init   \t [-1.56019024  1.41721238]. \t  -109.97965348801837 \t -1.9278091788796494\n",
            "init   \t [0.98198164 1.77093658]. \t  -65.06852657321838 \t -1.9278091788796494\n",
            "init   \t [1.47180752 0.85239655]. \t  -172.83511538815276 \t -1.9278091788796494\n",
            "1      \t [-0.29310344  0.48195416]. \t  -17.357244120121994 \t -1.9278091788796494\n",
            "2      \t [-2.0143131  -1.31251332]. \t  -2892.7444960501225 \t -1.9278091788796494\n",
            "3      \t [ 0.99390592 -1.25402384]. \t  -502.5994088777894 \t -1.9278091788796494\n",
            "4      \t [-0.07196585  0.07270944]. \t  \u001b[92m-1.6051456622453808\u001b[0m \t -1.6051456622453808\n",
            "5      \t [-0.39446258  1.16212925]. \t  -103.25449275171425 \t -1.6051456622453808\n",
            "6      \t [ 0.19547418 -0.20209885]. \t  -6.422103437492146 \t -1.6051456622453808\n",
            "7      \t [-0.36629948 -0.2342708 ]. \t  -15.44202785243899 \t -1.6051456622453808\n",
            "8      \t [0.5995695  0.59140943]. \t  -5.53930419155847 \t -1.6051456622453808\n",
            "9      \t [ 0.04377549 -0.06238293]. \t  \u001b[92m-1.3278043346732593\u001b[0m \t -1.3278043346732593\n",
            "10     \t [0.60266064 0.23445843]. \t  -1.8153139404587588 \t -1.3278043346732593\n",
            "11     \t [-0.34021369  0.21351898]. \t  -2.752140999584523 \t -1.3278043346732593\n",
            "12     \t [-0.19467408 -0.03492486]. \t  -1.957563002429 \t -1.3278043346732593\n",
            "13     \t [-1.07397932  2.01020652]. \t  -77.70771821226292 \t -1.3278043346732593\n",
            "14     \t [0.5134028  0.26269689]. \t  \u001b[92m-0.2368552504730209\u001b[0m \t -0.2368552504730209\n",
            "15     \t [-2.0223997  -1.18506995]. \t  -2791.877273939035 \t -0.2368552504730209\n",
            "16     \t [-1.20400563  1.61743151]. \t  -7.673390676574341 \t -0.2368552504730209\n",
            "17     \t [-1.17769894  1.3811026 ]. \t  -4.745820924115678 \t -0.2368552504730209\n",
            "18     \t [-1.56321837  0.0803017 ]. \t  -565.1123929380714 \t -0.2368552504730209\n",
            "19     \t [-0.17517263  0.69141474]. \t  -45.03734983140083 \t -0.2368552504730209\n",
            "20     \t [-0.73274214 -0.48359735]. \t  -107.14613451292593 \t -0.2368552504730209\n",
            "21     \t [ 0.43346255 -0.75667882]. \t  -89.54194982904822 \t -0.2368552504730209\n",
            "22     \t [ 0.91814057 -0.34450911]. \t  -141.02024019634268 \t -0.2368552504730209\n",
            "23     \t [-0.34012153  0.38623137]. \t  -9.115586071104392 \t -0.2368552504730209\n",
            "24     \t [0.39632798 0.15340558]. \t  -0.365767010649447 \t -0.2368552504730209\n",
            "25     \t [-1.33177649 -1.0812734 ]. \t  -820.4837430337147 \t -0.2368552504730209\n",
            "26     \t [-0.47734203 -1.85325129]. \t  -435.28305167595016 \t -0.2368552504730209\n",
            "27     \t [0.8013301  0.17053804]. \t  -22.279359836832548 \t -0.2368552504730209\n",
            "28     \t [ 0.87329433 -0.30051235]. \t  -113.04598083597291 \t -0.2368552504730209\n",
            "29     \t [-0.64761208 -0.63941521]. \t  -114.82388800391946 \t -0.2368552504730209\n",
            "30     \t [-1.47691244 -1.57602495]. \t  -1417.861890443338 \t -0.2368552504730209\n",
            "31     \t [ 0.4327339  -1.35970691]. \t  -239.6320259130426 \t -0.2368552504730209\n",
            "32     \t [-1.18032486 -1.07964242]. \t  -616.2323504683599 \t -0.2368552504730209\n",
            "33     \t [2.04191633 1.94083269]. \t  -497.7467544769756 \t -0.2368552504730209\n",
            "34     \t [1.09830144 1.56154684]. \t  -12.632106730370626 \t -0.2368552504730209\n",
            "35     \t [-1.67606772 -0.89015035]. \t  -1375.682863433497 \t -0.2368552504730209\n",
            "36     \t [0.98688608 1.12168522]. \t  -2.1829149470670677 \t -0.2368552504730209\n",
            "37     \t [0.94597083 1.29963657]. \t  -16.387260962488128 \t -0.2368552504730209\n",
            "38     \t [-0.75365745 -1.90976445]. \t  -617.0067622668932 \t -0.2368552504730209\n",
            "39     \t [-0.82794695 -1.66106782]. \t  -553.9776385316743 \t -0.2368552504730209\n",
            "40     \t [ 1.09158889 -1.85527902]. \t  -928.3350305129967 \t -0.2368552504730209\n",
            "41     \t [-1.90053733 -1.91620189]. \t  -3064.5613349294117 \t -0.2368552504730209\n",
            "42     \t [-1.62986014  0.30449881]. \t  -560.0808238757978 \t -0.2368552504730209\n",
            "43     \t [-1.06064946  0.22065185]. \t  -86.02672492343893 \t -0.2368552504730209\n",
            "44     \t [-1.4610998   1.76453335]. \t  -19.767685852850942 \t -0.2368552504730209\n",
            "45     \t [-0.25640987  0.21172911]. \t  -3.7096718553017576 \t -0.2368552504730209\n",
            "46     \t [ 0.15915991 -0.78903105]. \t  -67.02570961865136 \t -0.2368552504730209\n",
            "47     \t [-0.79190428  0.75327598]. \t  -4.802645845664083 \t -0.2368552504730209\n",
            "48     \t [0.47897968 0.23845816]. \t  -0.27962824284233395 \t -0.2368552504730209\n",
            "49     \t [-0.12274723  1.13054738]. \t  -125.69023654816054 \t -0.2368552504730209\n",
            "50     \t [1.12805075 1.41851326]. \t  -2.1484280874112036 \t -0.2368552504730209\n",
            "51     \t [-0.15643341 -0.63785765]. \t  -45.20531701437525 \t -0.2368552504730209\n",
            "52     \t [-1.26879899 -0.26761964]. \t  -357.63700430421267 \t -0.2368552504730209\n",
            "53     \t [-0.57842583 -1.50280606]. \t  -340.0888766860873 \t -0.2368552504730209\n",
            "54     \t [0.49963123 0.23086254]. \t  -0.2855958144241081 \t -0.2368552504730209\n",
            "55     \t [ 0.82248618 -1.04625839]. \t  -296.8154796675672 \t -0.2368552504730209\n",
            "56     \t [-0.64689927  1.63209789]. \t  -149.9994387123553 \t -0.2368552504730209\n",
            "57     \t [0.93196297 0.90382006]. \t  \u001b[92m-0.1289916174950372\u001b[0m \t -0.1289916174950372\n",
            "58     \t [-1.45316867 -0.38311089]. \t  -628.4257640568197 \t -0.1289916174950372\n",
            "59     \t [1.58170114 0.41906204]. \t  -434.1091590560216 \t -0.1289916174950372\n",
            "60     \t [ 1.54664472 -0.85143791]. \t  -1052.3590489943044 \t -0.1289916174950372\n",
            "61     \t [-0.81535936  0.58043406]. \t  -4.007474413946899 \t -0.1289916174950372\n",
            "62     \t [-0.97376053 -1.4571699 ]. \t  -582.480767503922 \t -0.1289916174950372\n",
            "63     \t [-0.61116527 -1.07885359]. \t  -213.53562625344438 \t -0.1289916174950372\n",
            "64     \t [0.83871275 0.71732964]. \t  \u001b[92m-0.045308336030899815\u001b[0m \t -0.045308336030899815\n",
            "65     \t [1.39449448 0.02403067]. \t  -369.01998822142326 \t -0.045308336030899815\n",
            "66     \t [0.99036749 0.33968709]. \t  -41.10622914377682 \t -0.045308336030899815\n",
            "67     \t [-0.41301572  0.94702602]. \t  -62.28314763695713 \t -0.045308336030899815\n",
            "68     \t [1.18565431 0.24203744]. \t  -135.46324582697912 \t -0.045308336030899815\n",
            "69     \t [1.10271346 1.1649134 ]. \t  -0.27129901041038773 \t -0.045308336030899815\n",
            "70     \t [-1.63445622 -1.84554485]. \t  -2047.2620122828723 \t -0.045308336030899815\n",
            "71     \t [ 1.47354817 -0.45096802]. \t  -687.8763878924361 \t -0.045308336030899815\n",
            "72     \t [ 0.51006207 -0.81553677]. \t  -115.9531049663765 \t -0.045308336030899815\n",
            "73     \t [-1.70649306  0.19121674]. \t  -747.6557790250947 \t -0.045308336030899815\n",
            "74     \t [1.13872968 1.32783194]. \t  -0.11613274787740212 \t -0.045308336030899815\n",
            "75     \t [-1.52326452 -0.02977024]. \t  -558.6662390049898 \t -0.045308336030899815\n",
            "76     \t [ 0.25685491 -1.73171831]. \t  -323.72218821205206 \t -0.045308336030899815\n",
            "77     \t [1.95511732 1.8879273 ]. \t  -375.16310905375974 \t -0.045308336030899815\n",
            "78     \t [-0.80366534  0.34169506]. \t  -12.505933083898942 \t -0.045308336030899815\n",
            "79     \t [-1.16481664  1.87316636]. \t  -31.35007917044794 \t -0.045308336030899815\n",
            "80     \t [ 0.20382741 -0.87757499]. \t  -85.11215914353261 \t -0.045308336030899815\n",
            "81     \t [ 0.92531161 -0.60711948]. \t  -214.1364263200481 \t -0.045308336030899815\n",
            "82     \t [-1.48490288  1.04258879]. \t  -141.27997684480601 \t -0.045308336030899815\n",
            "83     \t [0.39968827 1.86533819]. \t  -291.263238083348 \t -0.045308336030899815\n",
            "84     \t [-0.14592709  1.77647586]. \t  -309.37923450719995 \t -0.045308336030899815\n",
            "85     \t [-0.93470523  0.42585779]. \t  -23.79700799426778 \t -0.045308336030899815\n",
            "86     \t [0.525904   0.88949121]. \t  -37.79139283682128 \t -0.045308336030899815\n",
            "87     \t [-1.57565981  0.39355628]. \t  -443.08777565153105 \t -0.045308336030899815\n",
            "88     \t [ 0.32296706 -1.50958795]. \t  -260.9242971549489 \t -0.045308336030899815\n",
            "89     \t [ 1.48541484 -0.86606207]. \t  -944.273132645884 \t -0.045308336030899815\n",
            "90     \t [-1.46338831 -1.42839435]. \t  -1280.4866590917438 \t -0.045308336030899815\n",
            "91     \t [ 1.87252561 -0.1611803 ]. \t  -1345.8407490325253 \t -0.045308336030899815\n",
            "92     \t [1.3327794  1.84126194]. \t  -0.5327353884558459 \t -0.045308336030899815\n",
            "93     \t [1.28066055 1.67255676]. \t  -0.184170020423924 \t -0.045308336030899815\n",
            "94     \t [-1.09556612  1.43234486]. \t  -9.777497992412677 \t -0.045308336030899815\n",
            "95     \t [ 1.7290815  -1.47455581]. \t  -1993.509957799454 \t -0.045308336030899815\n",
            "96     \t [1.62807101 0.40241991]. \t  -505.8326830375496 \t -0.045308336030899815\n",
            "97     \t [ 1.8800602  -0.87843687]. \t  -1948.2872100578213 \t -0.045308336030899815\n",
            "98     \t [-1.81468614  1.5420402 ]. \t  -314.5385193068393 \t -0.045308336030899815\n",
            "99     \t [-0.74573766  0.83727171]. \t  -10.951966768460675 \t -0.045308336030899815\n",
            "100    \t [-0.72720043 -1.80539374]. \t  -547.8388160624896 \t -0.045308336030899815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO3I_9cbuvY2",
        "outputId": "2cde2000-bcc3-4867-a2f7-87f42963581b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_winner_6 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_6 = dGPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.60915518 -0.68821072]. \t  -1074.6314195925434 \t -3.0269049669752817\n",
            "init   \t [ 1.31575449 -1.87721062]. \t  -1302.169546932896 \t -3.0269049669752817\n",
            "init   \t [-1.60703824  0.38933325]. \t  -487.8262244570432 \t -3.0269049669752817\n",
            "init   \t [ 0.12213192 -0.33256477]. \t  -12.844955340992902 \t -3.0269049669752817\n",
            "init   \t [-0.67416945  0.50183959]. \t  -3.0269049669752817 \t -3.0269049669752817\n",
            "1      \t [-0.06756934  0.5044222 ]. \t  -26.125364719936726 \t -3.0269049669752817\n",
            "2      \t [-0.59148658 -0.75888088]. \t  -125.46266108053989 \t -3.0269049669752817\n",
            "3      \t [-0.27694202 -0.00472458]. \t  \u001b[92m-2.293526420261754\u001b[0m \t -2.293526420261754\n",
            "4      \t [-0.82039928  1.48818721]. \t  -69.75790944848586 \t -2.293526420261754\n",
            "5      \t [-0.55904175  0.64835235]. \t  -13.708432315939397 \t -2.293526420261754\n",
            "6      \t [0.23059478 0.12812777]. \t  \u001b[92m-1.153791820817171\u001b[0m \t -1.153791820817171\n",
            "7      \t [1.31707116 1.94887381]. \t  -4.688585606627337 \t -1.153791820817171\n",
            "8      \t [-0.59466418  0.41894207]. \t  -2.9695794548312695 \t -1.153791820817171\n",
            "9      \t [0.98559259 1.17637765]. \t  -4.202088546340913 \t -1.153791820817171\n",
            "10     \t [ 1.62127626 -1.6671004 ]. \t  -1845.635789466035 \t -1.153791820817171\n",
            "11     \t [-0.38391992 -1.82390802]. \t  -390.518599727533 \t -1.153791820817171\n",
            "12     \t [0.78703365 0.66657844]. \t  \u001b[92m-0.26772787976063084\u001b[0m \t -0.26772787976063084\n",
            "13     \t [1.20294084 1.96302686]. \t  -26.66267840871849 \t -0.26772787976063084\n",
            "14     \t [2.048 2.048]. \t  -461.7603900415999 \t -0.26772787976063084\n",
            "15     \t [-1.36308453  1.85427806]. \t  -5.585553360764396 \t -0.26772787976063084\n",
            "16     \t [1.31326373 1.36354527]. \t  -13.138635449068879 \t -0.26772787976063084\n",
            "17     \t [-0.28683888 -1.56337834]. \t  -272.47395206470316 \t -0.26772787976063084\n",
            "18     \t [1.29695812 1.8334941 ]. \t  -2.3801903329381235 \t -0.26772787976063084\n",
            "19     \t [-0.06538267  0.06909628]. \t  -1.5552214530707542 \t -0.26772787976063084\n",
            "20     \t [0.55836207 0.61296696]. \t  -9.267113287796171 \t -0.26772787976063084\n",
            "21     \t [-1.29304087  1.52031126]. \t  -7.557609392178444 \t -0.26772787976063084\n",
            "22     \t [1.37411082 0.79543521]. \t  -119.54919310897355 \t -0.26772787976063084\n",
            "23     \t [0.01844167 0.44839901]. \t  -21.039136270192785 \t -0.26772787976063084\n",
            "24     \t [1.15642092 1.31820278]. \t  \u001b[92m-0.06097358714231962\u001b[0m \t -0.06097358714231962\n",
            "25     \t [1.45693244 1.91501627]. \t  -4.5200529916615935 \t -0.06097358714231962\n",
            "26     \t [0.65709915 0.42449169]. \t  -0.12289190937165485 \t -0.06097358714231962\n",
            "27     \t [-1.64046227  1.09604527]. \t  -261.3972480403634 \t -0.06097358714231962\n",
            "28     \t [0.10568514 0.11910562]. \t  -1.9648230673134135 \t -0.06097358714231962\n",
            "29     \t [-1.16042126 -0.48243   ]. \t  -339.1942622308875 \t -0.06097358714231962\n",
            "30     \t [ 0.70648111 -1.04731814]. \t  -239.23187114732363 \t -0.06097358714231962\n",
            "31     \t [ 0.48119433 -0.05490514]. \t  -8.47469889811514 \t -0.06097358714231962\n",
            "32     \t [-1.74169083  1.27475939]. \t  -316.8291374869721 \t -0.06097358714231962\n",
            "33     \t [-0.91588355  0.93509797]. \t  -4.597117806678276 \t -0.06097358714231962\n",
            "34     \t [ 1.01819213 -0.91463989]. \t  -380.7790056557882 \t -0.06097358714231962\n",
            "35     \t [ 1.72965811 -0.3611323 ]. \t  -1124.692361971671 \t -0.06097358714231962\n",
            "36     \t [0.94326674 0.84868926]. \t  -0.17183476954902127 \t -0.06097358714231962\n",
            "37     \t [-1.95473651 -1.41815147]. \t  -2753.5958428807457 \t -0.06097358714231962\n",
            "38     \t [-1.15564314  1.83845432]. \t  -29.94198916774869 \t -0.06097358714231962\n",
            "39     \t [ 1.21764807 -1.67881643]. \t  -999.5450123676623 \t -0.06097358714231962\n",
            "40     \t [0.40598237 0.15515586]. \t  -0.3621997604851612 \t -0.06097358714231962\n",
            "41     \t [0.24322351 0.69817329]. \t  -41.40680608386912 \t -0.06097358714231962\n",
            "42     \t [2.01611511 0.40900302]. \t  -1337.4592511891349 \t -0.06097358714231962\n",
            "43     \t [0.77768147 0.60165079]. \t  \u001b[92m-0.05041002920584206\u001b[0m \t -0.05041002920584206\n",
            "44     \t [ 1.58689748 -1.98885517]. \t  -2031.7383830266047 \t -0.05041002920584206\n",
            "45     \t [-0.06960103  0.39329284]. \t  -16.233272792355127 \t -0.05041002920584206\n",
            "46     \t [ 1.48293339 -1.80569008]. \t  -1604.0607230964827 \t -0.05041002920584206\n",
            "47     \t [0.884062   0.80621283]. \t  -0.07419009954315937 \t -0.05041002920584206\n",
            "48     \t [ 1.09019811 -0.85902057]. \t  -419.2552535068078 \t -0.05041002920584206\n",
            "49     \t [0.07221105 0.65215451]. \t  -42.71393786596149 \t -0.05041002920584206\n",
            "50     \t [0.60534978 0.36004684]. \t  -0.15984674139762628 \t -0.05041002920584206\n",
            "51     \t [-0.77698697  1.56001318]. \t  -94.60949982977071 \t -0.05041002920584206\n",
            "52     \t [-0.93002234 -0.66266491]. \t  -237.0831376175313 \t -0.05041002920584206\n",
            "53     \t [-0.14395639  0.83083395]. \t  -66.93653951463152 \t -0.05041002920584206\n",
            "54     \t [ 1.64283333 -0.54736037]. \t  -1054.234750181094 \t -0.05041002920584206\n",
            "55     \t [ 1.26456752 -1.16271586]. \t  -762.8498168433456 \t -0.05041002920584206\n",
            "56     \t [-1.31227773  2.03619975]. \t  -15.214200315819033 \t -0.05041002920584206\n",
            "57     \t [-0.80713641 -0.74818458]. \t  -199.16880468917404 \t -0.05041002920584206\n",
            "58     \t [0.65318575 1.34301448]. \t  -84.09236747648043 \t -0.05041002920584206\n",
            "59     \t [ 1.18645898 -0.35980089]. \t  -312.43536793777577 \t -0.05041002920584206\n",
            "60     \t [-1.34258878 -1.30591657]. \t  -971.7408229441094 \t -0.05041002920584206\n",
            "61     \t [1.09661291 1.10736505]. \t  -0.9155395800710626 \t -0.05041002920584206\n",
            "62     \t [-0.00653096  1.4474278 ]. \t  -210.50548033462462 \t -0.05041002920584206\n",
            "63     \t [ 0.89735443 -0.11846241]. \t  -85.33406945143894 \t -0.05041002920584206\n",
            "64     \t [-1.99042143  0.62034692]. \t  -1125.458420324439 \t -0.05041002920584206\n",
            "65     \t [-1.91902188 -1.03299139]. \t  -2232.243308095427 \t -0.05041002920584206\n",
            "66     \t [1.47166778 0.25534753]. \t  -365.20765281558084 \t -0.05041002920584206\n",
            "67     \t [0.48556553 0.58101052]. \t  -12.183476168587521 \t -0.05041002920584206\n",
            "68     \t [-0.55155062  0.27051137]. \t  -2.520856175085368 \t -0.05041002920584206\n",
            "69     \t [-1.28137897 -1.19756858]. \t  -811.4810884152644 \t -0.05041002920584206\n",
            "70     \t [1.78871846 1.50721248]. \t  -287.01042744204386 \t -0.05041002920584206\n",
            "71     \t [-1.52580274 -1.09844367]. \t  -1180.4820266939364 \t -0.05041002920584206\n",
            "72     \t [1.74924623 1.41644118]. \t  -270.64469116755384 \t -0.05041002920584206\n",
            "73     \t [ 0.26996934 -0.00881265]. \t  -1.2003699574098554 \t -0.05041002920584206\n",
            "74     \t [ 0.39624454 -1.5080447 ]. \t  -277.60514967002473 \t -0.05041002920584206\n",
            "75     \t [1.32339511 1.68487457]. \t  -0.5468099390640744 \t -0.05041002920584206\n",
            "76     \t [1.59467551 0.79854923]. \t  -304.6609988132639 \t -0.05041002920584206\n",
            "77     \t [1.31389352 1.69902415]. \t  -0.17301461096386167 \t -0.05041002920584206\n",
            "78     \t [ 0.45875963 -1.83421845]. \t  -418.36409948972675 \t -0.05041002920584206\n",
            "79     \t [-0.54559481 -2.02115196]. \t  -540.0841043685225 \t -0.05041002920584206\n",
            "80     \t [-0.14442587  1.30848992]. \t  -167.10909260384756 \t -0.05041002920584206\n",
            "81     \t [1.16813263 0.97525035]. \t  -15.182431900590537 \t -0.05041002920584206\n",
            "82     \t [-1.66291297  0.60205752]. \t  -475.0440655718367 \t -0.05041002920584206\n",
            "83     \t [-1.72488578  0.24382631]. \t  -753.4821381841246 \t -0.05041002920584206\n",
            "84     \t [-1.77559241 -0.23396533]. \t  -1154.673348529511 \t -0.05041002920584206\n",
            "85     \t [-0.40389518 -1.25094207]. \t  -201.93127424599834 \t -0.05041002920584206\n",
            "86     \t [1.49181292 1.1552911 ]. \t  -114.77783114989622 \t -0.05041002920584206\n",
            "87     \t [1.82363278 1.78019387]. \t  -239.51767098091653 \t -0.05041002920584206\n",
            "88     \t [1.0304445  1.05936354]. \t  \u001b[92m-0.0015282576420380743\u001b[0m \t -0.0015282576420380743\n",
            "89     \t [1.38559925 1.91310283]. \t  -0.1532869501122807 \t -0.0015282576420380743\n",
            "90     \t [-0.80526177 -0.21591912]. \t  -77.97176485445083 \t -0.0015282576420380743\n",
            "91     \t [-1.68156932 -1.93210104]. \t  -2272.737961281629 \t -0.0015282576420380743\n",
            "92     \t [ 1.55930984 -1.22208842]. \t  -1335.1450532415909 \t -0.0015282576420380743\n",
            "93     \t [-0.73903477  0.13573331]. \t  -19.87026636959404 \t -0.0015282576420380743\n",
            "94     \t [-0.0111504  1.0522277]. \t  -111.71457518049557 \t -0.0015282576420380743\n",
            "95     \t [1.34555748 1.79746949]. \t  -0.13645445664675504 \t -0.0015282576420380743\n",
            "96     \t [-1.80615082 -1.30792707]. \t  -2096.4630705644536 \t -0.0015282576420380743\n",
            "97     \t [-1.51158698 -1.05923967]. \t  -1124.6318739826365 \t -0.0015282576420380743\n",
            "98     \t [-1.21562782 -1.17840815]. \t  -710.4271513891267 \t -0.0015282576420380743\n",
            "99     \t [0.94405384 0.92628073]. \t  -0.1259317692654256 \t -0.0015282576420380743\n",
            "100    \t [-1.45895949 -1.65195926]. \t  -1435.2811721917542 \t -0.0015282576420380743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rQbLZD8uvbI",
        "outputId": "2544589f-c780-47cd-9346-f44a762ea4dd"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_winner_7 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_7 = dGPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.42268934 -0.80954733]. \t  -808.7939502616841 \t -2.0077595729598063\n",
            "init   \t [-1.79389885 -0.16441204]. \t  -1151.9264213711547 \t -2.0077595729598063\n",
            "init   \t [1.37319786 1.74897991]. \t  -2.0077595729598063 \t -2.0077595729598063\n",
            "init   \t [0.92974688 1.09976052]. \t  -5.543015908052858 \t -2.0077595729598063\n",
            "init   \t [-0.94533605  0.58994398]. \t  -13.008689168416776 \t -2.0077595729598063\n",
            "1      \t [-0.59759223  0.9308147 ]. \t  -35.46526561710063 \t -2.0077595729598063\n",
            "2      \t [-0.11927001 -0.06911553]. \t  \u001b[92m-1.9473353252776673\u001b[0m \t -1.9473353252776673\n",
            "3      \t [1.75260192 0.5041669 ]. \t  -659.7446123697583 \t -1.9473353252776673\n",
            "4      \t [0.72751807 1.41909438]. \t  -79.25075688681243 \t -1.9473353252776673\n",
            "5      \t [-0.3732473   0.39432367]. \t  -8.388824774966118 \t -1.9473353252776673\n",
            "6      \t [-1.48301994  1.41562513]. \t  -67.58756520073175 \t -1.9473353252776673\n",
            "7      \t [ 0.03101556 -1.63765317]. \t  -269.4448880480768 \t -1.9473353252776673\n",
            "8      \t [1.15803313 1.42924387]. \t  \u001b[92m-0.8029537454168005\u001b[0m \t -0.8029537454168005\n",
            "9      \t [-0.97982182  0.91393292]. \t  -4.132380304885428 \t -0.8029537454168005\n",
            "10     \t [-0.66300988  0.4649159 ]. \t  -2.8297820076230247 \t -0.8029537454168005\n",
            "11     \t [1.5834199  1.81371894]. \t  -48.43455216349437 \t -0.8029537454168005\n",
            "12     \t [ 0.35268615 -0.12502998]. \t  -6.639924319869301 \t -0.8029537454168005\n",
            "13     \t [-2.02876466  2.03982622]. \t  -440.1758591245962 \t -0.8029537454168005\n",
            "14     \t [0.21077468 0.21365386]. \t  -3.4866846804280436 \t -0.8029537454168005\n",
            "15     \t [1.23803088 1.64253822]. \t  -1.262652709411629 \t -0.8029537454168005\n",
            "16     \t [0.01018566 0.05884235]. \t  -1.3247547810661318 \t -0.8029537454168005\n",
            "17     \t [ 0.06068768 -0.12184518]. \t  -2.458039931679046 \t -0.8029537454168005\n",
            "18     \t [1.42863203 1.91312377]. \t  -1.8186894093825778 \t -0.8029537454168005\n",
            "19     \t [-1.69743641 -1.22823506]. \t  -1696.0960855088406 \t -0.8029537454168005\n",
            "20     \t [ 1.15396095 -1.49427819]. \t  -798.5970803684866 \t -0.8029537454168005\n",
            "21     \t [-0.38719379 -0.21978603]. \t  -15.592489681865912 \t -0.8029537454168005\n",
            "22     \t [0.53034447 0.22254209]. \t  \u001b[92m-0.5654173536225957\u001b[0m \t -0.5654173536225957\n",
            "23     \t [0.42247463 0.1024085 ]. \t  -0.9122961668477317 \t -0.5654173536225957\n",
            "24     \t [1.30589016 0.52928269]. \t  -138.40678783944495 \t -0.5654173536225957\n",
            "25     \t [-0.23527315  1.89308588]. \t  -339.25194620375197 \t -0.5654173536225957\n",
            "26     \t [1.24775957 1.63437628]. \t  -0.6615810608294141 \t -0.5654173536225957\n",
            "27     \t [1.02306199 0.98865114]. \t  \u001b[92m-0.33698639076405223\u001b[0m \t -0.33698639076405223\n",
            "28     \t [0.8160386  0.57112302]. \t  -0.9324694177394204 \t -0.33698639076405223\n",
            "29     \t [-0.82434682 -0.60356239]. \t  -167.96538417843664 \t -0.33698639076405223\n",
            "30     \t [1.38228676 2.03828843]. \t  -1.773598184653324 \t -0.33698639076405223\n",
            "31     \t [-1.2491973   1.36576703]. \t  -8.850743886580085 \t -0.33698639076405223\n",
            "32     \t [ 1.54002649 -1.83787344]. \t  -1772.3269971951645 \t -0.33698639076405223\n",
            "33     \t [-1.8431759  -1.77951983]. \t  -2688.0272945478355 \t -0.33698639076405223\n",
            "34     \t [1.31530313 1.74904088]. \t  \u001b[92m-0.13558661457542254\u001b[0m \t -0.13558661457542254\n",
            "35     \t [1.07090779 1.13641703]. \t  \u001b[92m-0.01589904692265757\u001b[0m \t -0.01589904692265757\n",
            "36     \t [-0.63864392  1.52983507]. \t  -128.5665997916627 \t -0.01589904692265757\n",
            "37     \t [-0.73283867  0.87632897]. \t  -14.513580664318035 \t -0.01589904692265757\n",
            "38     \t [-1.76261228  1.06688063]. \t  -423.75995997652166 \t -0.01589904692265757\n",
            "39     \t [-1.72517692 -0.55026939]. \t  -1251.0501918294053 \t -0.01589904692265757\n",
            "40     \t [0.05564954 0.60649055]. \t  -37.300190512549484 \t -0.01589904692265757\n",
            "41     \t [-1.82254402  1.35506658]. \t  -394.71835903777065 \t -0.01589904692265757\n",
            "42     \t [-1.237254    1.59257486]. \t  -5.386950267359938 \t -0.01589904692265757\n",
            "43     \t [0.8199031  1.16600161]. \t  -24.41237951303199 \t -0.01589904692265757\n",
            "44     \t [-0.86486021  0.97471617]. \t  -8.61848851357872 \t -0.01589904692265757\n",
            "45     \t [1.34803695 0.03901215]. \t  -316.3176151563217 \t -0.01589904692265757\n",
            "46     \t [ 1.50666073 -0.10246566]. \t  -563.1286359744842 \t -0.01589904692265757\n",
            "47     \t [ 0.83852599 -0.34052251]. \t  -108.9462605973516 \t -0.01589904692265757\n",
            "48     \t [ 1.28403975 -1.80099872]. \t  -1190.162876097512 \t -0.01589904692265757\n",
            "49     \t [1.35790219 1.8516503 ]. \t  -0.13410324273290322 \t -0.01589904692265757\n",
            "50     \t [-0.34422207  1.24450668]. \t  -128.59855232461769 \t -0.01589904692265757\n",
            "51     \t [-1.01553322 -0.07176961]. \t  -125.74033204142796 \t -0.01589904692265757\n",
            "52     \t [ 0.58352362 -0.23479548]. \t  -33.2699195494349 \t -0.01589904692265757\n",
            "53     \t [1.27684505 0.33886227]. \t  -166.86638213510824 \t -0.01589904692265757\n",
            "54     \t [ 1.95242307 -1.48669605]. \t  -2808.478305233238 \t -0.01589904692265757\n",
            "55     \t [ 0.40544191 -1.72069599]. \t  -355.7058359961106 \t -0.01589904692265757\n",
            "56     \t [ 0.96780316 -0.78044245]. \t  -294.8392690826092 \t -0.01589904692265757\n",
            "57     \t [-0.46836524  0.05583377]. \t  -4.830375481641471 \t -0.01589904692265757\n",
            "58     \t [ 0.99548805 -1.06068493]. \t  -420.93966873039165 \t -0.01589904692265757\n",
            "59     \t [-1.23998148 -0.56124367]. \t  -445.5127150577988 \t -0.01589904692265757\n",
            "60     \t [-0.48624993  1.60285367]. \t  -188.91784469060016 \t -0.01589904692265757\n",
            "61     \t [ 1.7078212  -0.23497913]. \t  -993.779676556309 \t -0.01589904692265757\n",
            "62     \t [1.93673741 0.97984521]. \t  -768.7806515093562 \t -0.01589904692265757\n",
            "63     \t [0.81115052 0.70978983]. \t  -0.3042436276168672 \t -0.01589904692265757\n",
            "64     \t [0.9422807  0.87247312]. \t  -0.02710853180809069 \t -0.01589904692265757\n",
            "65     \t [0.97108911 0.96030652]. \t  -0.030738761196914842 \t -0.01589904692265757\n",
            "66     \t [0.9893409 0.9655526]. \t  -0.017650821030332026 \t -0.01589904692265757\n",
            "67     \t [ 1.29866214 -0.78551704]. \t  -611.1875774241657 \t -0.01589904692265757\n",
            "68     \t [-0.35571567 -0.50568623]. \t  -41.80816166414239 \t -0.01589904692265757\n",
            "69     \t [1.38254386 1.91179959]. \t  -0.14635364566026624 \t -0.01589904692265757\n",
            "70     \t [-1.453335    0.70777001]. \t  -203.2563341481985 \t -0.01589904692265757\n",
            "71     \t [0.92628316 0.87905167]. \t  -0.0497493904416827 \t -0.01589904692265757\n",
            "72     \t [-0.12450811 -1.3949929 ]. \t  -200.21418176685694 \t -0.01589904692265757\n",
            "73     \t [-0.93108572  1.6070963 ]. \t  -58.51509767547627 \t -0.01589904692265757\n",
            "74     \t [-1.41877428 -0.24425245]. \t  -515.3334162173542 \t -0.01589904692265757\n",
            "75     \t [-0.62409191 -0.74202643]. \t  -130.6707793379426 \t -0.01589904692265757\n",
            "76     \t [ 1.84142169 -0.68915833]. \t  -1665.3415997891302 \t -0.01589904692265757\n",
            "77     \t [-1.11398796 -0.24648974]. \t  -225.72234792939003 \t -0.01589904692265757\n",
            "78     \t [-0.09797941  0.996038  ]. \t  -98.51155747480169 \t -0.01589904692265757\n",
            "79     \t [1.1508533 0.387372 ]. \t  -87.83676818002708 \t -0.01589904692265757\n",
            "80     \t [ 1.05092279 -1.43042672]. \t  -642.5568651619753 \t -0.01589904692265757\n",
            "81     \t [0.20377698 0.76973986]. \t  -53.66365112401365 \t -0.01589904692265757\n",
            "82     \t [1.0043492  1.01149819]. \t  \u001b[92m-0.0007922403871347229\u001b[0m \t -0.0007922403871347229\n",
            "83     \t [1.84719157 0.07493504]. \t  -1114.3958721029478 \t -0.0007922403871347229\n",
            "84     \t [ 1.10464633 -1.45254347]. \t  -714.3899783273297 \t -0.0007922403871347229\n",
            "85     \t [1.12337412 1.26949402]. \t  -0.020883145044259768 \t -0.0007922403871347229\n",
            "86     \t [-1.15538548 -0.32404392]. \t  -279.8603611900302 \t -0.0007922403871347229\n",
            "87     \t [-0.43085748 -1.0387392 ]. \t  -151.95734865395278 \t -0.0007922403871347229\n",
            "88     \t [-0.33939423  0.24049692]. \t  -3.364198389531944 \t -0.0007922403871347229\n",
            "89     \t [1.35936773 0.81886257]. \t  -106.01696195155297 \t -0.0007922403871347229\n",
            "90     \t [-1.9380262   0.13610953]. \t  -1318.9532876497697 \t -0.0007922403871347229\n",
            "91     \t [0.61813169 0.35402239]. \t  -0.22458442920762428 \t -0.0007922403871347229\n",
            "92     \t [0.7427851  0.55471353]. \t  -0.067049831427108 \t -0.0007922403871347229\n",
            "93     \t [0.01802314 2.0015417 ]. \t  -401.4511721215828 \t -0.0007922403871347229\n",
            "94     \t [ 0.15498551 -0.88449689]. \t  -83.25443505500101 \t -0.0007922403871347229\n",
            "95     \t [0.63363781 0.41420535]. \t  -0.15037179969733092 \t -0.0007922403871347229\n",
            "96     \t [-1.20940169 -1.15883084]. \t  -692.0989167941194 \t -0.0007922403871347229\n",
            "97     \t [ 0.28093551 -0.28203391]. \t  -13.546170105063041 \t -0.0007922403871347229\n",
            "98     \t [0.63313443 0.41442596]. \t  -0.15299603144374702 \t -0.0007922403871347229\n",
            "99     \t [-1.29675083  1.09136695]. \t  -40.10816935674119 \t -0.0007922403871347229\n",
            "100    \t [-0.54071186  1.67082572]. \t  -192.38799757088222 \t -0.0007922403871347229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YRio_skuvd2",
        "outputId": "93ea6640-2fb6-46f2-e506-074926a27459"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_winner_8 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_8 = dGPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.59948851 -1.8750873 ]. \t  -1972.3059423071145 \t -7.238275380208551\n",
            "init   \t [ 0.40834083 -0.81972959]. \t  -97.66272647293047 \t -7.238275380208551\n",
            "init   \t [ 1.21492186 -0.54806067]. \t  -409.7425700197761 \t -7.238275380208551\n",
            "init   \t [ 1.55897178 -0.82804067]. \t  -1062.051446884513 \t -7.238275380208551\n",
            "init   \t [-0.69999303  0.28146449]. \t  -7.238275380208551 \t -7.238275380208551\n",
            "1      \t [0.61360238 0.32771785]. \t  \u001b[92m-0.38734981524274686\u001b[0m \t -0.38734981524274686\n",
            "2      \t [ 0.44413641 -0.13898414]. \t  -11.61480534400125 \t -0.38734981524274686\n",
            "3      \t [-1.38236625  0.9213737 ]. \t  -103.59911305494524 \t -0.38734981524274686\n",
            "4      \t [1.32471165 0.75318888]. \t  -100.44013040885407 \t -0.38734981524274686\n",
            "5      \t [-0.73590555  0.66630259]. \t  -4.569514982496509 \t -0.38734981524274686\n",
            "6      \t [0.68197488 0.16676674]. \t  -9.000801151785016 \t -0.38734981524274686\n",
            "7      \t [0.36330576 1.59476576]. \t  -214.37635880756577 \t -0.38734981524274686\n",
            "8      \t [-0.03623755  0.54476206]. \t  -30.60745868366779 \t -0.38734981524274686\n",
            "9      \t [-1.0689437   0.42664547]. \t  -55.54543512742329 \t -0.38734981524274686\n",
            "10     \t [0.79518667 0.77956588]. \t  -2.2100291781119217 \t -0.38734981524274686\n",
            "11     \t [ 0.64467069 -0.79401276]. \t  -146.44263235022598 \t -0.38734981524274686\n",
            "12     \t [0.59105258 0.43472463]. \t  -0.8962377307812004 \t -0.38734981524274686\n",
            "13     \t [0.67440918 0.48081637]. \t  \u001b[92m-0.1735502508261092\u001b[0m \t -0.1735502508261092\n",
            "14     \t [1.32525949 1.9548935 ]. \t  -4.049226774586152 \t -0.1735502508261092\n",
            "15     \t [-1.39359154  0.69519013]. \t  -161.20705136277712 \t -0.1735502508261092\n",
            "16     \t [-1.28953266  1.92406673]. \t  -12.063054241298978 \t -0.1735502508261092\n",
            "17     \t [-1.34348464  1.60665084]. \t  -9.424214808744573 \t -0.1735502508261092\n",
            "18     \t [-1.54848356 -1.82686842]. \t  -1791.2782240225968 \t -0.1735502508261092\n",
            "19     \t [-0.94197154 -0.81995239]. \t  -295.24586773137753 \t -0.1735502508261092\n",
            "20     \t [-1.686961    1.99091964]. \t  -80.30819905910431 \t -0.1735502508261092\n",
            "21     \t [-0.67700663  0.47290868]. \t  -2.833581773349782 \t -0.1735502508261092\n",
            "22     \t [-0.21558547 -1.89326419]. \t  -377.7372734311474 \t -0.1735502508261092\n",
            "23     \t [1.57358202 2.01107741]. \t  -21.959212897737203 \t -0.1735502508261092\n",
            "24     \t [-1.06826397  1.22002792]. \t  -4.899290715749482 \t -0.1735502508261092\n",
            "25     \t [0.47475314 1.75949646]. \t  -235.62398067075662 \t -0.1735502508261092\n",
            "26     \t [-1.13686556  0.57091361]. \t  -56.629591495046235 \t -0.1735502508261092\n",
            "27     \t [-0.48949062  0.73595592]. \t  -26.85539619748126 \t -0.1735502508261092\n",
            "28     \t [0.2767406  0.46772662]. \t  -15.82225228302519 \t -0.1735502508261092\n",
            "29     \t [-1.49964567  1.78875104]. \t  -27.425351780715324 \t -0.1735502508261092\n",
            "30     \t [1.99728149 1.79335453]. \t  -483.1390356175105 \t -0.1735502508261092\n",
            "31     \t [-0.45283819  1.95181393]. \t  -307.22482120864316 \t -0.1735502508261092\n",
            "32     \t [ 1.98022044 -0.46929244]. \t  -1928.6673011391758 \t -0.1735502508261092\n",
            "33     \t [-1.25330541  1.95501977]. \t  -19.841831903727883 \t -0.1735502508261092\n",
            "34     \t [0.76966649 0.51349111]. \t  -0.6755017685872692 \t -0.1735502508261092\n",
            "35     \t [-1.65658863  0.95732344]. \t  -326.3809421815248 \t -0.1735502508261092\n",
            "36     \t [-1.80825474  0.3658329 ]. \t  -851.1801983666131 \t -0.1735502508261092\n",
            "37     \t [-2.04007732  1.73210541]. \t  -599.6397734779823 \t -0.1735502508261092\n",
            "38     \t [-0.69666159  2.01108644]. \t  -235.66968390762443 \t -0.1735502508261092\n",
            "39     \t [-0.11215858  1.13031619]. \t  -126.17041680603957 \t -0.1735502508261092\n",
            "40     \t [-1.85366589 -1.66299848]. \t  -2608.200710103936 \t -0.1735502508261092\n",
            "41     \t [2.02060323 1.4606836 ]. \t  -688.6106859946037 \t -0.1735502508261092\n",
            "42     \t [-0.32245275 -0.91934482]. \t  -106.46738525737548 \t -0.1735502508261092\n",
            "43     \t [0.89327409 0.75745118]. \t  -0.17531350759187325 \t -0.1735502508261092\n",
            "44     \t [-0.66904423  1.19340861]. \t  -58.40574725562008 \t -0.1735502508261092\n",
            "45     \t [1.25805159 0.19123451]. \t  -193.68248995741166 \t -0.1735502508261092\n",
            "46     \t [-1.07582552  1.90269457]. \t  -59.855369374280805 \t -0.1735502508261092\n",
            "47     \t [-0.51424044 -0.06635342]. \t  -13.235566372013212 \t -0.1735502508261092\n",
            "48     \t [-1.11227828  0.93402843]. \t  -13.650774336140383 \t -0.1735502508261092\n",
            "49     \t [0.85892397 0.34161258]. \t  -15.7124183297375 \t -0.1735502508261092\n",
            "50     \t [-0.39428439  1.1020433 ]. \t  -91.54598937155194 \t -0.1735502508261092\n",
            "51     \t [1.03799728 1.22229839]. \t  -2.0998868809733406 \t -0.1735502508261092\n",
            "52     \t [-0.56482597 -1.12583464]. \t  -211.21159278875356 \t -0.1735502508261092\n",
            "53     \t [-1.92850496  0.52318824]. \t  -1029.9814077744916 \t -0.1735502508261092\n",
            "54     \t [0.13029166 0.06837489]. \t  -1.0205779934953298 \t -0.1735502508261092\n",
            "55     \t [ 1.25588263 -1.705961  ]. \t  -1078.0071343372058 \t -0.1735502508261092\n",
            "56     \t [-1.25725437 -0.32600639]. \t  -368.64375370499255 \t -0.1735502508261092\n",
            "57     \t [-0.19041351 -0.0273844 ]. \t  -1.8221109614319444 \t -0.1735502508261092\n",
            "58     \t [0.88169297 1.8141501 ]. \t  -107.50270556616614 \t -0.1735502508261092\n",
            "59     \t [0.89389391 0.81594664]. \t  \u001b[92m-0.039820613055831275\u001b[0m \t -0.039820613055831275\n",
            "60     \t [-1.07729279  0.11680071]. \t  -113.25844116075896 \t -0.039820613055831275\n",
            "61     \t [-0.26935638 -0.24773242]. \t  -11.86953142243642 \t -0.039820613055831275\n",
            "62     \t [-1.43699318  0.90086473]. \t  -141.448244037027 \t -0.039820613055831275\n",
            "63     \t [-0.04835609  0.47621891]. \t  -23.555332262915186 \t -0.039820613055831275\n",
            "64     \t [ 1.58178491 -1.94189443]. \t  -1975.1968996050994 \t -0.039820613055831275\n",
            "65     \t [1.85635975 1.58021445]. \t  -348.87561045171907 \t -0.039820613055831275\n",
            "66     \t [1.34135491 1.66929147]. \t  -1.8050030413681906 \t -0.039820613055831275\n",
            "67     \t [-1.37121146  0.53976074]. \t  -185.30597907635834 \t -0.039820613055831275\n",
            "68     \t [-1.10591708 -1.40193983]. \t  -693.4934082066238 \t -0.039820613055831275\n",
            "69     \t [0.88375916 0.93238652]. \t  -2.3043837843615584 \t -0.039820613055831275\n",
            "70     \t [ 0.20673218 -0.15698146]. \t  -4.618067879582819 \t -0.039820613055831275\n",
            "71     \t [ 1.92973463 -0.0818242 ]. \t  -1449.1996114104088 \t -0.039820613055831275\n",
            "72     \t [-0.45455404  0.97947863]. \t  -61.84687062939467 \t -0.039820613055831275\n",
            "73     \t [ 0.57074712 -0.03149577]. \t  -12.946874619150488 \t -0.039820613055831275\n",
            "74     \t [-1.81192679 -1.07318148]. \t  -1905.6072111759756 \t -0.039820613055831275\n",
            "75     \t [-0.83864115 -0.48317136]. \t  -144.15653311972133 \t -0.039820613055831275\n",
            "76     \t [ 1.81540232 -0.88185794]. \t  -1745.8518633501558 \t -0.039820613055831275\n",
            "77     \t [1.21315618 1.39356664]. \t  -0.6566667038755232 \t -0.039820613055831275\n",
            "78     \t [ 2.02210299 -1.73191346]. \t  -3389.2322236048617 \t -0.039820613055831275\n",
            "79     \t [-0.12581057 -1.97756492]. \t  -398.6291010451451 \t -0.039820613055831275\n",
            "80     \t [1.40754666 2.04755551]. \t  -0.606564402231802 \t -0.039820613055831275\n",
            "81     \t [ 1.26257603 -0.50415807]. \t  -440.33689059340634 \t -0.039820613055831275\n",
            "82     \t [-1.08263569  1.50197853]. \t  -15.219353636984508 \t -0.039820613055831275\n",
            "83     \t [ 1.45351392 -0.62312236]. \t  -748.6795639055398 \t -0.039820613055831275\n",
            "84     \t [-1.10985455  1.86904196]. \t  -45.0621325530082 \t -0.039820613055831275\n",
            "85     \t [-0.31437303 -1.03790435]. \t  -130.94416630874125 \t -0.039820613055831275\n",
            "86     \t [ 0.190941   -0.04878897]. \t  -1.3812889957609409 \t -0.039820613055831275\n",
            "87     \t [0.26139067 0.5017304 ]. \t  -19.329560711259465 \t -0.039820613055831275\n",
            "88     \t [-0.93060338  0.17375074]. \t  -51.651269276420294 \t -0.039820613055831275\n",
            "89     \t [ 0.31133267 -0.3654709 ]. \t  -21.855539664990918 \t -0.039820613055831275\n",
            "90     \t [ 0.13619448 -0.65511577]. \t  -46.12857340817518 \t -0.039820613055831275\n",
            "91     \t [-1.33951156  1.72886835]. \t  -5.901329482203336 \t -0.039820613055831275\n",
            "92     \t [-0.08108414  1.84567641]. \t  -339.39827472078963 \t -0.039820613055831275\n",
            "93     \t [0.78207583 0.61807271]. \t  -0.051625580150817384 \t -0.039820613055831275\n",
            "94     \t [0.65952851 1.43503437]. \t  -100.12722412902917 \t -0.039820613055831275\n",
            "95     \t [-0.8577131   1.03479741]. \t  -12.398713195129323 \t -0.039820613055831275\n",
            "96     \t [-2.01567355 -0.66988734]. \t  -2249.0596095799283 \t -0.039820613055831275\n",
            "97     \t [ 2.02019527 -1.57878686]. \t  -3204.5733881370375 \t -0.039820613055831275\n",
            "98     \t [-0.03460129  1.98045929]. \t  -392.81822179015063 \t -0.039820613055831275\n",
            "99     \t [-0.05600739 -1.1276132 ]. \t  -128.97471474266496 \t -0.039820613055831275\n",
            "100    \t [-2.04035406 -0.23039551]. \t  -1939.4754235779617 \t -0.039820613055831275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejw6v-Ihuvf_",
        "outputId": "59c680da-7c97-4f52-ca74-0b46563a98f6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_winner_9 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_9 = dGPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.70565298 -0.04883088]. \t  -29.9831488845538 \t -29.9831488845538\n",
            "init   \t [ 1.33322823 -1.9191956 ]. \t  -1366.6650412963722 \t -29.9831488845538\n",
            "init   \t [1.26177265 0.26876895]. \t  -175.18114988457984 \t -29.9831488845538\n",
            "init   \t [-0.82893825 -1.85673433]. \t  -650.4739702903224 \t -29.9831488845538\n",
            "init   \t [ 2.00960983 -2.0200418 ]. \t  -3671.650548034952 \t -29.9831488845538\n",
            "1      \t [ 0.43023396 -1.68219722]. \t  -349.00499321479344 \t -29.9831488845538\n",
            "2      \t [-0.30159707  0.76110224]. \t  -46.60311065191736 \t -29.9831488845538\n",
            "3      \t [-1.8510361   2.03742245]. \t  -201.03611268756 \t -29.9831488845538\n",
            "4      \t [0.23613418 0.32638547]. \t  \u001b[92m-7.907340824478641\u001b[0m \t -7.907340824478641\n",
            "5      \t [0.51685865 0.32921605]. \t  \u001b[92m-0.6187336669171021\u001b[0m \t -0.6187336669171021\n",
            "6      \t [-1.19704929  0.13798041]. \t  -172.5156939449146 \t -0.6187336669171021\n",
            "7      \t [0.1403732  0.80691645]. \t  -62.70920268612352 \t -0.6187336669171021\n",
            "8      \t [-0.15828987  1.24297839]. \t  -149.67520799598637 \t -0.6187336669171021\n",
            "9      \t [0.44456852 0.30375372]. \t  -1.434491465730187 \t -0.6187336669171021\n",
            "10     \t [-2.01346326 -1.00148458]. \t  -2564.9080532207845 \t -0.6187336669171021\n",
            "11     \t [-1.11897659  0.66248313]. \t  -39.25588068318474 \t -0.6187336669171021\n",
            "12     \t [-0.5994319   0.27029269]. \t  -3.3507435590954286 \t -0.6187336669171021\n",
            "13     \t [ 0.83279428 -1.82803982]. \t  -635.8676218890135 \t -0.6187336669171021\n",
            "14     \t [-0.10878582 -1.02179017]. \t  -108.06737242246588 \t -0.6187336669171021\n",
            "15     \t [-0.64534156  0.40636648]. \t  -2.7173483085035364 \t -0.6187336669171021\n",
            "16     \t [-1.81926685  0.12418451]. \t  -1022.7194650397062 \t -0.6187336669171021\n",
            "17     \t [ 1.63480771 -0.56566643]. \t  -1049.0374887651844 \t -0.6187336669171021\n",
            "18     \t [-1.80931692 -1.27237471]. \t  -2074.5060733234022 \t -0.6187336669171021\n",
            "19     \t [1.83310149 1.59724977]. \t  -311.51493996050334 \t -0.6187336669171021\n",
            "20     \t [-0.47478963 -0.54557743]. \t  -61.61950824247563 \t -0.6187336669171021\n",
            "21     \t [-0.73422898  1.20642268]. \t  -47.54054729250897 \t -0.6187336669171021\n",
            "22     \t [1.24071924 1.92822824]. \t  -15.177911978567126 \t -0.6187336669171021\n",
            "23     \t [0.78179429 1.80037644]. \t  -141.4611271589353 \t -0.6187336669171021\n",
            "24     \t [ 0.69400485 -2.01494389]. \t  -623.3881102390728 \t -0.6187336669171021\n",
            "25     \t [-1.41959775 -1.09305594]. \t  -972.0158623952941 \t -0.6187336669171021\n",
            "26     \t [-1.19298832  1.39107125]. \t  -4.912559220868705 \t -0.6187336669171021\n",
            "27     \t [0.48802176 0.29590318]. \t  \u001b[92m-0.5954887436610876\u001b[0m \t -0.5954887436610876\n",
            "28     \t [-0.0795336  -1.23139253]. \t  -154.36000755826518 \t -0.5954887436610876\n",
            "29     \t [-1.40918017  1.68587243]. \t  -14.79912853253838 \t -0.5954887436610876\n",
            "30     \t [-0.83988106  2.01909162]. \t  -175.96367733433914 \t -0.5954887436610876\n",
            "31     \t [ 0.29366346 -0.21595003]. \t  -9.630685702077225 \t -0.5954887436610876\n",
            "32     \t [1.24554173 1.59958294]. \t  \u001b[92m-0.29269901063787895\u001b[0m \t -0.29269901063787895\n",
            "33     \t [0.99126108 1.07593749]. \t  -0.8712923147084861 \t -0.29269901063787895\n",
            "34     \t [1.82219672 1.80090893]. \t  -231.56158444838957 \t -0.29269901063787895\n",
            "35     \t [-0.37583209 -1.61138224]. \t  -309.064807839783 \t -0.29269901063787895\n",
            "36     \t [1.17368832 1.63098366]. \t  -6.453319994282974 \t -0.29269901063787895\n",
            "37     \t [1.0838456  1.10318992]. \t  -0.5187035856541257 \t -0.29269901063787895\n",
            "38     \t [0.93633137 0.8670299 ]. \t  \u001b[92m-0.013436592041505463\u001b[0m \t -0.013436592041505463\n",
            "39     \t [0.61217472 0.43199786]. \t  -0.4780498396167564 \t -0.013436592041505463\n",
            "40     \t [-1.56920135 -0.12707265]. \t  -677.1339649108769 \t -0.013436592041505463\n",
            "41     \t [0.65123123 0.46456381]. \t  -0.2853545507890848 \t -0.013436592041505463\n",
            "42     \t [-0.15119024 -1.71306133]. \t  -302.6670012948018 \t -0.013436592041505463\n",
            "43     \t [-0.961376    0.81298437]. \t  -5.084862338385937 \t -0.013436592041505463\n",
            "44     \t [0.99127909 1.00634457]. \t  -0.05629401970937036 \t -0.013436592041505463\n",
            "45     \t [1.14937007 1.3279947 ]. \t  -0.02713214540937328 \t -0.013436592041505463\n",
            "46     \t [-1.87945054 -0.53187245]. \t  -1660.0689019354684 \t -0.013436592041505463\n",
            "47     \t [-0.94126862 -1.11145889]. \t  -402.7473815286371 \t -0.013436592041505463\n",
            "48     \t [1.02245441 1.03376632]. \t  -0.01406873990130508 \t -0.013436592041505463\n",
            "49     \t [-0.06763778 -0.20081956]. \t  -5.358537573523489 \t -0.013436592041505463\n",
            "50     \t [0.79821937 0.66418828]. \t  -0.11379970586295174 \t -0.013436592041505463\n",
            "51     \t [ 0.88625641 -1.55731416]. \t  -548.8675271403135 \t -0.013436592041505463\n",
            "52     \t [-0.25987559 -0.36331888]. \t  -20.150820827709044 \t -0.013436592041505463\n",
            "53     \t [1.11598235 1.20889884]. \t  -0.1468066015464473 \t -0.013436592041505463\n",
            "54     \t [-1.03742352  0.77025954]. \t  -13.513961478754117 \t -0.013436592041505463\n",
            "55     \t [1.12612472 1.24937664]. \t  -0.051177227773190184 \t -0.013436592041505463\n",
            "56     \t [ 1.63554429 -0.09981155]. \t  -770.3646643128745 \t -0.013436592041505463\n",
            "57     \t [-0.54231303  0.00288144]. \t  -10.859753918600669 \t -0.013436592041505463\n",
            "58     \t [-1.49595357 -0.91197093]. \t  -998.3840314171645 \t -0.013436592041505463\n",
            "59     \t [0.65487724 0.95429416]. \t  -27.726773753029832 \t -0.013436592041505463\n",
            "60     \t [0.73099845 0.47728471]. \t  -0.3981063127539933 \t -0.013436592041505463\n",
            "61     \t [ 1.846403  -1.6901432]. \t  -2601.050623124933 \t -0.013436592041505463\n",
            "62     \t [1.15282556 1.38072306]. \t  -0.2908130442126818 \t -0.013436592041505463\n",
            "63     \t [1.06211672 1.1484952 ]. \t  -0.045487836985732354 \t -0.013436592041505463\n",
            "64     \t [ 0.27674062 -1.32846936]. \t  -197.9409839920002 \t -0.013436592041505463\n",
            "65     \t [0.72963728 0.53897645]. \t  -0.07745978597048989 \t -0.013436592041505463\n",
            "66     \t [1.03882213 1.09451969]. \t  -0.025125548040681935 \t -0.013436592041505463\n",
            "67     \t [0.47213099 1.05764644]. \t  -69.95752629871546 \t -0.013436592041505463\n",
            "68     \t [-1.79204133 -1.16368841]. \t  -1921.9459548040177 \t -0.013436592041505463\n",
            "69     \t [-0.55761127 -1.33865854]. \t  -274.54049687577555 \t -0.013436592041505463\n",
            "70     \t [0.4719424  0.16193564]. \t  -0.6484358096632062 \t -0.013436592041505463\n",
            "71     \t [-1.84667548 -0.8660456 ]. \t  -1836.7400488056433 \t -0.013436592041505463\n",
            "72     \t [0.35377083 0.55626988]. \t  -19.00371957437456 \t -0.013436592041505463\n",
            "73     \t [ 0.23754772 -0.61027856]. \t  -45.03121945417927 \t -0.013436592041505463\n",
            "74     \t [-1.66305092 -0.65620777]. \t  -1178.0633699721684 \t -0.013436592041505463\n",
            "75     \t [ 0.13847559 -0.18332592]. \t  -4.842906597165351 \t -0.013436592041505463\n",
            "76     \t [-0.88307779  0.82963019]. \t  -3.7940238790094662 \t -0.013436592041505463\n",
            "77     \t [0.81461897 0.64916221]. \t  -0.05522282730712497 \t -0.013436592041505463\n",
            "78     \t [-0.69654673 -1.22160209]. \t  -294.18787606883797 \t -0.013436592041505463\n",
            "79     \t [-0.46708971 -1.31363253]. \t  -236.7951087892674 \t -0.013436592041505463\n",
            "80     \t [ 0.05482189 -1.28791705]. \t  -167.54144919181485 \t -0.013436592041505463\n",
            "81     \t [0.79924822 0.21251998]. \t  -18.211571521285908 \t -0.013436592041505463\n",
            "82     \t [-1.85316016  0.25770396]. \t  -1017.154878475344 \t -0.013436592041505463\n",
            "83     \t [ 1.69336897 -0.61239054]. \t  -1211.4435039774137 \t -0.013436592041505463\n",
            "84     \t [0.9512592  0.89009216]. \t  -0.024285276923990406 \t -0.013436592041505463\n",
            "85     \t [-0.5255028   0.31883678]. \t  -2.509347690056986 \t -0.013436592041505463\n",
            "86     \t [1.78829037 0.77377538]. \t  -588.2993861148012 \t -0.013436592041505463\n",
            "87     \t [-0.2330249  -0.80892747]. \t  -76.03662216236455 \t -0.013436592041505463\n",
            "88     \t [-1.43296824  0.75859945]. \t  -173.569656950802 \t -0.013436592041505463\n",
            "89     \t [-0.93161299 -0.72401088]. \t  -257.1500312220242 \t -0.013436592041505463\n",
            "90     \t [1.14179758 1.48818296]. \t  -3.423439666435232 \t -0.013436592041505463\n",
            "91     \t [-1.10498356 -0.99634577]. \t  -496.0881529291035 \t -0.013436592041505463\n",
            "92     \t [-1.09491229 -1.51316321]. \t  -739.8809613632862 \t -0.013436592041505463\n",
            "93     \t [ 0.88961009 -0.45274637]. \t  -154.8037243800122 \t -0.013436592041505463\n",
            "94     \t [-1.25086189 -0.9100251 ]. \t  -617.4707731834394 \t -0.013436592041505463\n",
            "95     \t [-1.19440196 -1.04886507]. \t  -617.6061689884486 \t -0.013436592041505463\n",
            "96     \t [ 1.98571538 -1.49840793]. \t  -2961.9350309063184 \t -0.013436592041505463\n",
            "97     \t [0.94988028 0.92605334]. \t  -0.05906458166288756 \t -0.013436592041505463\n",
            "98     \t [-0.65541128  0.80886722]. \t  -17.12748433920213 \t -0.013436592041505463\n",
            "99     \t [-0.45261262  0.90300121]. \t  -50.85045173697143 \t -0.013436592041505463\n",
            "100    \t [ 1.19856704 -0.13210183]. \t  -246.11034989199715 \t -0.013436592041505463\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4xG5dbuuvig",
        "outputId": "ba8b9473-0f61-4959-ca24-8d5c70e09d0d"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_winner_10 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_10 = dGPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.62910294 -1.57693156]. \t  -389.29291138113445 \t -8.580376531587937\n",
            "init   \t [ 1.84435861 -0.07294402]. \t  -1207.999341247548 \t -8.580376531587937\n",
            "init   \t [ 1.5256557  -1.17828534]. \t  -1229.4172568797705 \t -8.580376531587937\n",
            "init   \t [-1.88125338 -0.42109149]. \t  -1576.6245828972787 \t -8.580376531587937\n",
            "init   \t [-1.09309052  1.39977001]. \t  -8.580376531587937 \t -8.580376531587937\n",
            "1      \t [-0.86019812  2.03750805]. \t  -171.82841453589126 \t -8.580376531587937\n",
            "2      \t [-0.5337917   1.15217037]. \t  -77.5624817749036 \t -8.580376531587937\n",
            "3      \t [-1.92938007  1.76150785]. \t  -393.13321643334365 \t -8.580376531587937\n",
            "4      \t [-0.09127565 -1.98421081]. \t  -398.21326857120295 \t -8.580376531587937\n",
            "5      \t [-0.45198894  1.79927878]. \t  -256.50591736736254 \t -8.580376531587937\n",
            "6      \t [-0.93600563  1.20728058]. \t  -14.715742726647553 \t -8.580376531587937\n",
            "7      \t [0.76191163 0.20393692]. \t  -14.237364089085258 \t -8.580376531587937\n",
            "8      \t [0.27844298 0.21308311]. \t  \u001b[92m-2.3580957786728742\u001b[0m \t -2.3580957786728742\n",
            "9      \t [0.38472129 0.55035441]. \t  -16.56663204242878 \t -2.3580957786728742\n",
            "10     \t [0.50945855 0.24847209]. \t  \u001b[92m-0.25289852782539335\u001b[0m \t -0.25289852782539335\n",
            "11     \t [1.27585704 1.32993405]. \t  -8.949176187910732 \t -0.25289852782539335\n",
            "12     \t [0.95718966 0.86162227]. \t  -0.2998370307700867 \t -0.25289852782539335\n",
            "13     \t [2.048 2.048]. \t  -461.76039004157815 \t -0.25289852782539335\n",
            "14     \t [1.11667806 1.38114594]. \t  -1.8139351734906792 \t -0.25289852782539335\n",
            "15     \t [-0.28481356  0.46366225]. \t  -16.28469832257008 \t -0.25289852782539335\n",
            "16     \t [0.98226372 1.2697215 ]. \t  -9.295464012565606 \t -0.25289852782539335\n",
            "17     \t [1.11675039 1.24314888]. \t  \u001b[92m-0.015216720567815251\u001b[0m \t -0.015216720567815251\n",
            "18     \t [0.77641821 0.41262801]. \t  -3.6674874101316375 \t -0.015216720567815251\n",
            "19     \t [-1.27490501  1.75603797]. \t  -6.882270592583296 \t -0.015216720567815251\n",
            "20     \t [0.8928771  1.92779926]. \t  -127.8302676649817 \t -0.015216720567815251\n",
            "21     \t [ 0.30304065 -0.16771128]. \t  -7.222108706871566 \t -0.015216720567815251\n",
            "22     \t [1.12982801 1.3142464 ]. \t  -0.15924883110648017 \t -0.015216720567815251\n",
            "23     \t [0.87420463 0.76134876]. \t  -0.016656783455400573 \t -0.015216720567815251\n",
            "24     \t [-0.08262942  1.92456551]. \t  -368.94394876270246 \t -0.015216720567815251\n",
            "25     \t [-1.42911955 -1.23177359]. \t  -1077.9105586898488 \t -0.015216720567815251\n",
            "26     \t [1.45733524 0.6290517 ]. \t  -223.64417807412963 \t -0.015216720567815251\n",
            "27     \t [0.45360384 0.17719118]. \t  -0.38014620926712106 \t -0.015216720567815251\n",
            "28     \t [-1.66538701 -1.53751163]. \t  -1865.5984088211412 \t -0.015216720567815251\n",
            "29     \t [-0.42226106 -0.3698526 ]. \t  -32.07043669928008 \t -0.015216720567815251\n",
            "30     \t [0.99194882 0.10029902]. \t  -78.08617211182937 \t -0.015216720567815251\n",
            "31     \t [ 1.11567264 -0.90387046]. \t  -461.65981006904394 \t -0.015216720567815251\n",
            "32     \t [-0.31793576  0.82467337]. \t  -54.09523653176086 \t -0.015216720567815251\n",
            "33     \t [1.2574234  0.81459335]. \t  -58.82159645758195 \t -0.015216720567815251\n",
            "34     \t [-0.18459878 -0.99157824]. \t  -106.60008293430585 \t -0.015216720567815251\n",
            "35     \t [ 2.04545002 -1.72379955]. \t  -3491.1439590465025 \t -0.015216720567815251\n",
            "36     \t [-1.08562463 -1.84139658]. \t  -916.3761885599844 \t -0.015216720567815251\n",
            "37     \t [0.19972562 0.07154198]. \t  -0.7406218419174726 \t -0.015216720567815251\n",
            "38     \t [-1.9731415   1.30257183]. \t  -680.0202743820246 \t -0.015216720567815251\n",
            "39     \t [ 0.55732215 -1.45954397]. \t  -313.5397560590302 \t -0.015216720567815251\n",
            "40     \t [-0.71931561  1.74910406]. \t  -154.66185238171147 \t -0.015216720567815251\n",
            "41     \t [-0.56491971  0.28220789]. \t  -2.585329471689296 \t -0.015216720567815251\n",
            "42     \t [-1.17017422 -0.53387238]. \t  -366.9190964188499 \t -0.015216720567815251\n",
            "43     \t [ 1.09423273 -1.03254202]. \t  -497.2486072284271 \t -0.015216720567815251\n",
            "44     \t [-1.87395402  0.49278705]. \t  -919.6453670223667 \t -0.015216720567815251\n",
            "45     \t [1.82708159 0.67678285]. \t  -709.0126391146559 \t -0.015216720567815251\n",
            "46     \t [ 1.89366666 -1.78216789]. \t  -2882.4927331825857 \t -0.015216720567815251\n",
            "47     \t [ 1.1378844  -0.65381764]. \t  -379.7226447456352 \t -0.015216720567815251\n",
            "48     \t [0.34835018 1.21836145]. \t  -120.76853083741302 \t -0.015216720567815251\n",
            "49     \t [-0.46297728 -1.73230771]. \t  -381.0871345807821 \t -0.015216720567815251\n",
            "50     \t [1.1227541  0.79666545]. \t  -21.536438712554748 \t -0.015216720567815251\n",
            "51     \t [0.77148732 0.58393711]. \t  -0.06488684571752519 \t -0.015216720567815251\n",
            "52     \t [0.41637339 0.15656252]. \t  -0.3688584008621456 \t -0.015216720567815251\n",
            "53     \t [0.84464057 1.90600766]. \t  -142.25122111186738 \t -0.015216720567815251\n",
            "54     \t [1.90861306 1.11266196]. \t  -640.9873500056517 \t -0.015216720567815251\n",
            "55     \t [-1.49630139  1.46255007]. \t  -66.50621539864875 \t -0.015216720567815251\n",
            "56     \t [-0.77159888  0.08083053]. \t  -29.613117616473627 \t -0.015216720567815251\n",
            "57     \t [ 1.38455217 -0.42026351]. \t  -546.4208092931456 \t -0.015216720567815251\n",
            "58     \t [1.18092966 1.41135911]. \t  -0.060839586759303836 \t -0.015216720567815251\n",
            "59     \t [1.31417954 1.98169538]. \t  -6.582225985731442 \t -0.015216720567815251\n",
            "60     \t [1.31944636 1.69603905]. \t  -0.3036438353721587 \t -0.015216720567815251\n",
            "61     \t [-1.66270093 -1.55400635]. \t  -1872.1039474825202 \t -0.015216720567815251\n",
            "62     \t [ 1.8351678  -0.07827489]. \t  -1188.26887670644 \t -0.015216720567815251\n",
            "63     \t [-1.81823939  1.2990678 ]. \t  -410.71794193666307 \t -0.015216720567815251\n",
            "64     \t [0.47241056 1.13269949]. \t  -83.00242487243595 \t -0.015216720567815251\n",
            "65     \t [ 1.2739886  -1.98423576]. \t  -1301.3239220270361 \t -0.015216720567815251\n",
            "66     \t [-1.4064444  -0.22099504]. \t  -489.3866516394009 \t -0.015216720567815251\n",
            "67     \t [1.23787765 1.47346854]. \t  -0.4031832918887638 \t -0.015216720567815251\n",
            "68     \t [-0.95567247  0.83860872]. \t  -4.382681071476705 \t -0.015216720567815251\n",
            "69     \t [-0.22185091  0.06742732]. \t  -1.5260782015117649 \t -0.015216720567815251\n",
            "70     \t [-1.40558332 -1.91251359]. \t  -1517.579699438475 \t -0.015216720567815251\n",
            "71     \t [ 1.40517301 -0.85657528]. \t  -801.669225988052 \t -0.015216720567815251\n",
            "72     \t [-1.5112475   2.02381614]. \t  -13.069112565943167 \t -0.015216720567815251\n",
            "73     \t [ 1.63415265 -1.63483115]. \t  -1853.9509454567055 \t -0.015216720567815251\n",
            "74     \t [2.03618554 0.66993058]. \t  -1209.415371467293 \t -0.015216720567815251\n",
            "75     \t [0.70215515 0.54250982]. \t  -0.3336173872927583 \t -0.015216720567815251\n",
            "76     \t [ 0.66620021 -1.06959045]. \t  -229.15336766252585 \t -0.015216720567815251\n",
            "77     \t [0.62080722 0.40307486]. \t  -0.17502154320294153 \t -0.015216720567815251\n",
            "78     \t [ 0.29383177 -1.78799774]. \t  -351.8117852221128 \t -0.015216720567815251\n",
            "79     \t [0.35188336 2.00297952]. \t  -353.54339364049775 \t -0.015216720567815251\n",
            "80     \t [-1.91804923  0.41488613]. \t  -1073.902053974256 \t -0.015216720567815251\n",
            "81     \t [0.59590602 0.32792707]. \t  -0.2371504419933906 \t -0.015216720567815251\n",
            "82     \t [1.01320442 1.0250001 ]. \t  \u001b[92m-0.00042497435016257694\u001b[0m \t -0.00042497435016257694\n",
            "83     \t [-1.16956642  1.40716474]. \t  -4.861303510122351 \t -0.00042497435016257694\n",
            "84     \t [-2.01561484 -0.16401453]. \t  -1795.6082127740456 \t -0.00042497435016257694\n",
            "85     \t [ 1.66561392 -1.2937774 ]. \t  -1655.3437923316073 \t -0.00042497435016257694\n",
            "86     \t [ 0.22527611 -1.7089182 ]. \t  -310.24317612133376 \t -0.00042497435016257694\n",
            "87     \t [1.31844919 1.77939599]. \t  -0.2702299989797614 \t -0.00042497435016257694\n",
            "88     \t [1.09940539 1.22074925]. \t  -0.024418635368187384 \t -0.00042497435016257694\n",
            "89     \t [-1.67514889 -0.19090665]. \t  -905.3755728724991 \t -0.00042497435016257694\n",
            "90     \t [0.54829616 0.29275421]. \t  -0.21023708943098843 \t -0.00042497435016257694\n",
            "91     \t [1.22762998 1.53356027]. \t  -0.12196035733472323 \t -0.00042497435016257694\n",
            "92     \t [0.78705217 0.06078357]. \t  -31.25628888559174 \t -0.00042497435016257694\n",
            "93     \t [1.19064195 1.63726879]. \t  -4.860540991330615 \t -0.00042497435016257694\n",
            "94     \t [ 0.45046954 -1.93188098]. \t  -456.04070283784597 \t -0.00042497435016257694\n",
            "95     \t [-0.29073196 -0.0322581 ]. \t  -3.0298199791996803 \t -0.00042497435016257694\n",
            "96     \t [-0.81057464  1.07467568]. \t  -20.720868440719848 \t -0.00042497435016257694\n",
            "97     \t [-0.79374399  1.80956559]. \t  -142.348050764057 \t -0.00042497435016257694\n",
            "98     \t [-0.65314311 -0.35198627]. \t  -63.351906424072624 \t -0.00042497435016257694\n",
            "99     \t [ 1.97285981 -1.2448696 ]. \t  -2639.870017628281 \t -0.00042497435016257694\n",
            "100    \t [2.01579377 0.71686644]. \t  -1120.9769394817645 \t -0.00042497435016257694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSj_CQIAuvk3",
        "outputId": "a947419c-a04c-427c-a758-de82d35c816f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_winner_11 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_11 = dGPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.97032207 1.31583119]. \t  -659.5505379977991 \t -45.00854175284071\n",
            "init   \t [ 0.59758024 -0.3125739 ]. \t  -45.00854175284071 \t -45.00854175284071\n",
            "init   \t [-1.21933423 -0.03314987]. \t  -235.94289844157058 \t -45.00854175284071\n",
            "init   \t [-1.48046507 -0.19447384]. \t  -575.5719351651762 \t -45.00854175284071\n",
            "init   \t [-1.58214297 -2.0360213 ]. \t  -2067.0989998073865 \t -45.00854175284071\n",
            "1      \t [-0.59782787  0.66030788]. \t  \u001b[92m-11.728483422473923\u001b[0m \t -11.728483422473923\n",
            "2      \t [-0.41458909 -0.01756331]. \t  \u001b[92m-5.590095057336445\u001b[0m \t -5.590095057336445\n",
            "3      \t [-0.73360589  0.27454655]. \t  -9.955522490248732 \t -5.590095057336445\n",
            "4      \t [-1.11779274  1.61911442]. \t  -18.149439844949185 \t -5.590095057336445\n",
            "5      \t [-0.1525002   1.70927563]. \t  -285.59437057782685 \t -5.590095057336445\n",
            "6      \t [-1.30270534  0.99399956]. \t  -54.72920699978776 \t -5.590095057336445\n",
            "7      \t [ 0.13940602 -0.73146617]. \t  -57.125734461906596 \t -5.590095057336445\n",
            "8      \t [-0.76470598  0.89417169]. \t  -12.686803690241558 \t -5.590095057336445\n",
            "9      \t [-1.31173887  1.8210921 ]. \t  -6.352820147310787 \t -5.590095057336445\n",
            "10     \t [-1.6423388   1.83955474]. \t  -80.55065729055872 \t -5.590095057336445\n",
            "11     \t [-0.19560139 -0.01476939]. \t  \u001b[92m-1.7106732761592038\u001b[0m \t -1.7106732761592038\n",
            "12     \t [-0.01741941 -0.11849502]. \t  -2.446449583940668 \t -1.7106732761592038\n",
            "13     \t [-1.28735147  1.9300587 ]. \t  -12.673135720153995 \t -1.7106732761592038\n",
            "14     \t [0.35089792 0.55263684]. \t  -18.869001454076848 \t -1.7106732761592038\n",
            "15     \t [ 1.95238384 -1.58459963]. \t  -2913.02279344303 \t -1.7106732761592038\n",
            "16     \t [0.27993279 0.09223316]. \t  \u001b[92m-0.5377366769027837\u001b[0m \t -0.5377366769027837\n",
            "17     \t [0.04378846 0.01447252]. \t  -0.9301035546185972 \t -0.5377366769027837\n",
            "18     \t [0.12629817 0.08976554]. \t  -1.3082100659066231 \t -0.5377366769027837\n",
            "19     \t [ 1.98042259 -0.98573901]. \t  -2409.6237150913776 \t -0.5377366769027837\n",
            "20     \t [ 1.0137647  -0.07230145]. \t  -121.00465961248214 \t -0.5377366769027837\n",
            "21     \t [ 0.25820023 -0.05162852]. \t  -1.9496583244928545 \t -0.5377366769027837\n",
            "22     \t [ 0.2436819  -0.27884161]. \t  -12.011461671266463 \t -0.5377366769027837\n",
            "23     \t [ 1.21602431 -0.70188661]. \t  -475.54905943904737 \t -0.5377366769027837\n",
            "24     \t [0.17726688 1.06658703]. \t  -107.8332336585574 \t -0.5377366769027837\n",
            "25     \t [-1.2198352   0.34836495]. \t  -134.80399984543754 \t -0.5377366769027837\n",
            "26     \t [ 1.29236218 -0.97473783]. \t  -699.6550875729368 \t -0.5377366769027837\n",
            "27     \t [-1.70805982 -0.79601716]. \t  -1386.3310485804243 \t -0.5377366769027837\n",
            "28     \t [1.27910866 0.64469621]. \t  -98.36981129039162 \t -0.5377366769027837\n",
            "29     \t [0.5037907  0.32391209]. \t  -0.7377231154778684 \t -0.5377366769027837\n",
            "30     \t [ 1.24950157 -1.20455379]. \t  -765.0316215108699 \t -0.5377366769027837\n",
            "31     \t [-1.09782657 -0.40581759]. \t  -263.94610895647355 \t -0.5377366769027837\n",
            "32     \t [-1.79219421  0.5095534 ]. \t  -738.0965349548324 \t -0.5377366769027837\n",
            "33     \t [-0.8175968  0.6193056]. \t  -3.545318179338563 \t -0.5377366769027837\n",
            "34     \t [0.60323062 0.27754313]. \t  -0.9029553151165571 \t -0.5377366769027837\n",
            "35     \t [0.07628541 0.51436288]. \t  -26.714889428745124 \t -0.5377366769027837\n",
            "36     \t [0.7285731  0.47843388]. \t  \u001b[92m-0.34809010145187547\u001b[0m \t -0.34809010145187547\n",
            "37     \t [0.60003205 0.40261172]. \t  \u001b[92m-0.3412225970799834\u001b[0m \t -0.3412225970799834\n",
            "38     \t [-0.27563309  1.9377205 ]. \t  -348.23739126600947 \t -0.3412225970799834\n",
            "39     \t [ 1.93024108 -0.94086061]. \t  -2178.6660545401273 \t -0.3412225970799834\n",
            "40     \t [-1.13490584  1.19348733]. \t  -5.451300415022138 \t -0.3412225970799834\n",
            "41     \t [ 1.22778986 -1.53194722]. \t  -923.8563456372658 \t -0.3412225970799834\n",
            "42     \t [0.53458034 0.30928026]. \t  \u001b[92m-0.2718598252129951\u001b[0m \t -0.2718598252129951\n",
            "43     \t [1.71365695 1.0271433 ]. \t  -365.1194820522272 \t -0.2718598252129951\n",
            "44     \t [0.93920879 0.30816011]. \t  -32.94590397346381 \t -0.2718598252129951\n",
            "45     \t [-0.08912908  0.32218779]. \t  -11.061118648132027 \t -0.2718598252129951\n",
            "46     \t [-0.0069477   0.10223989]. \t  -2.058256476849774 \t -0.2718598252129951\n",
            "47     \t [0.58221996 0.38095616]. \t  -0.35073928361197143 \t -0.2718598252129951\n",
            "48     \t [-0.57896128 -1.17887706]. \t  -231.73488884239327 \t -0.2718598252129951\n",
            "49     \t [0.46990238 0.1582271 ]. \t  -0.6726435191673077 \t -0.2718598252129951\n",
            "50     \t [-0.69152467  1.25049012]. \t  -62.50347520244296 \t -0.2718598252129951\n",
            "51     \t [0.50730424 0.211766  ]. \t  -0.45060837674650267 \t -0.2718598252129951\n",
            "52     \t [0.53762205 0.279527  ]. \t  \u001b[92m-0.22283827093694472\u001b[0m \t -0.22283827093694472\n",
            "53     \t [ 1.77670957 -0.01402205]. \t  -1005.9491539234891 \t -0.22283827093694472\n",
            "54     \t [0.89798956 0.82313406]. \t  \u001b[92m-0.03845841432218488\u001b[0m \t -0.03845841432218488\n",
            "55     \t [ 1.13312633 -1.69041578]. \t  -884.7179340438174 \t -0.03845841432218488\n",
            "56     \t [-0.04710502 -1.72545809]. \t  -299.5832021814439 \t -0.03845841432218488\n",
            "57     \t [-0.18157075  0.43033067]. \t  -17.18582347230089 \t -0.03845841432218488\n",
            "58     \t [ 0.47994735 -1.40300951]. \t  -267.056607350391 \t -0.03845841432218488\n",
            "59     \t [1.71424921 1.48059655]. \t  -213.10223813541185 \t -0.03845841432218488\n",
            "60     \t [0.92761219 0.88466484]. \t  -0.06380622718153994 \t -0.03845841432218488\n",
            "61     \t [1.222663   1.32404809]. \t  -2.9687808818086343 \t -0.03845841432218488\n",
            "62     \t [-1.64117344 -1.89599003]. \t  -2113.2720115420593 \t -0.03845841432218488\n",
            "63     \t [ 1.31580711 -1.11887593]. \t  -812.477579666318 \t -0.03845841432218488\n",
            "64     \t [1.09399802 1.00446893]. \t  -3.709177709540974 \t -0.03845841432218488\n",
            "65     \t [1.38530645 1.88614241]. \t  -0.2569098273430356 \t -0.03845841432218488\n",
            "66     \t [0.40434128 0.5194397 ]. \t  -13.024695036665513 \t -0.03845841432218488\n",
            "67     \t [-0.59792941 -0.37098712]. \t  -55.62557857024103 \t -0.03845841432218488\n",
            "68     \t [-1.22447442 -1.35309648]. \t  -818.5863114414681 \t -0.03845841432218488\n",
            "69     \t [-0.04818242  0.19123297]. \t  -4.667439202146498 \t -0.03845841432218488\n",
            "70     \t [0.69073045 0.50141074]. \t  -0.1547072577710833 \t -0.03845841432218488\n",
            "71     \t [0.73479928 0.49096264]. \t  -0.3101115022995303 \t -0.03845841432218488\n",
            "72     \t [0.74593363 0.55973245]. \t  -0.06564894616442922 \t -0.03845841432218488\n",
            "73     \t [ 0.47673951 -1.2253542 ]. \t  -211.28857552365994 \t -0.03845841432218488\n",
            "74     \t [0.94928522 0.07194122]. \t  -68.76003629130035 \t -0.03845841432218488\n",
            "75     \t [-0.80793361 -1.44559157]. \t  -443.5751761740207 \t -0.03845841432218488\n",
            "76     \t [-1.30501148  0.16778885]. \t  -241.01728153385264 \t -0.03845841432218488\n",
            "77     \t [0.72790346 0.49913675]. \t  -0.1683266293839114 \t -0.03845841432218488\n",
            "78     \t [0.75680475 0.53715269]. \t  -0.1858852036671069 \t -0.03845841432218488\n",
            "79     \t [-0.14297143 -1.50855436]. \t  -235.0890121843255 \t -0.03845841432218488\n",
            "80     \t [1.56533633 0.93464884]. \t  -230.0327265035141 \t -0.03845841432218488\n",
            "81     \t [ 0.28557859 -0.87338187]. \t  -91.7008658277363 \t -0.03845841432218488\n",
            "82     \t [0.56217192 1.41420373]. \t  -120.78865068275417 \t -0.03845841432218488\n",
            "83     \t [1.79282834 0.64597849]. \t  -660.2219442582725 \t -0.03845841432218488\n",
            "84     \t [ 1.70460458 -1.09192798]. \t  -1598.5808533328623 \t -0.03845841432218488\n",
            "85     \t [0.70858878 0.52242116]. \t  -0.12622330354170108 \t -0.03845841432218488\n",
            "86     \t [1.32610079 1.71893042]. \t  -0.26325971782569924 \t -0.03845841432218488\n",
            "87     \t [-2.00994707  0.85300458]. \t  -1024.6818743389156 \t -0.03845841432218488\n",
            "88     \t [-0.88361966 -0.74654485]. \t  -236.82127108162658 \t -0.03845841432218488\n",
            "89     \t [-1.78620936  1.42392272]. \t  -319.8579928979332 \t -0.03845841432218488\n",
            "90     \t [-0.42532623  1.76570948]. \t  -253.19290341592327 \t -0.03845841432218488\n",
            "91     \t [-0.23251562 -0.87987128]. \t  -88.74251487636296 \t -0.03845841432218488\n",
            "92     \t [-0.28023732  0.8644397 ]. \t  -63.40394859145026 \t -0.03845841432218488\n",
            "93     \t [ 0.63138666 -1.65560771]. \t  -422.13298671981994 \t -0.03845841432218488\n",
            "94     \t [1.55955816 0.71295826]. \t  -295.89976870463903 \t -0.03845841432218488\n",
            "95     \t [1.18209282 0.49641175]. \t  -81.20094879602708 \t -0.03845841432218488\n",
            "96     \t [0.11221832 0.17690602]. \t  -3.488034753685005 \t -0.03845841432218488\n",
            "97     \t [ 1.53115004 -0.15748759]. \t  -626.2364977558283 \t -0.03845841432218488\n",
            "98     \t [-0.04377054  1.07377422]. \t  -115.97749058569023 \t -0.03845841432218488\n",
            "99     \t [-1.07275224  1.30988671]. \t  -6.8272437195548115 \t -0.03845841432218488\n",
            "100    \t [ 1.99729274 -0.68967908]. \t  -2190.1652376019097 \t -0.03845841432218488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l98Nt7Tguvna",
        "outputId": "0ec0fe45-4242-4a79-b6c4-966aa11571fb"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_winner_12 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_12 = dGPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.26353633  0.50015753]. \t  -125.32555445527122 \t -19.52113145175031\n",
            "init   \t [-0.25506718  1.16882876]. \t  -123.40590278793995 \t -19.52113145175031\n",
            "init   \t [ 1.14678091 -0.93146069]. \t  -504.72793807789463 \t -19.52113145175031\n",
            "init   \t [-0.91560241  1.23646844]. \t  -19.52113145175031 \t -19.52113145175031\n",
            "init   \t [1.87653879 1.53982007]. \t  -393.43336554061835 \t -19.52113145175031\n",
            "1      \t [-1.5807869   1.84651715]. \t  -49.21913193836258 \t -19.52113145175031\n",
            "2      \t [-0.98157718  1.63290916]. \t  -48.73834486257445 \t -19.52113145175031\n",
            "3      \t [-1.3757202   1.32098032]. \t  -38.31964590832042 \t -19.52113145175031\n",
            "4      \t [-1.67837885 -1.78584282]. \t  -2125.7490199053195 \t -19.52113145175031\n",
            "5      \t [-0.8308906   0.89287063]. \t  \u001b[92m-7.452438960329609\u001b[0m \t -7.452438960329609\n",
            "6      \t [-1.19370865 -1.3207978 ]. \t  -758.720153086171 \t -7.452438960329609\n",
            "7      \t [-0.54663708 -0.5935088 ]. \t  -82.01574529953533 \t -7.452438960329609\n",
            "8      \t [ 0.00770506 -0.6820323 ]. \t  -47.50955294104664 \t -7.452438960329609\n",
            "9      \t [-0.14959779 -1.39281181]. \t  -201.59821768750695 \t -7.452438960329609\n",
            "10     \t [-1.77437392 -0.11040737]. \t  -1069.6815217935523 \t -7.452438960329609\n",
            "11     \t [ 1.05910733 -0.24097216]. \t  -185.69330959029529 \t -7.452438960329609\n",
            "12     \t [-0.87157477  0.88924539]. \t  \u001b[92m-5.182480853135878\u001b[0m \t -5.182480853135878\n",
            "13     \t [ 0.27754597 -0.19002756]. \t  -7.654008245842556 \t -5.182480853135878\n",
            "14     \t [-0.9113006  0.9358446]. \t  \u001b[92m-4.763476321423984\u001b[0m \t -4.763476321423984\n",
            "15     \t [ 0.51522622 -0.87850331]. \t  -131.09976624440213 \t -4.763476321423984\n",
            "16     \t [0.64452823 0.53406155]. \t  \u001b[92m-1.5340216517716425\u001b[0m \t -1.5340216517716425\n",
            "17     \t [0.55189266 0.16439394]. \t  -2.1661678407839924 \t -1.5340216517716425\n",
            "18     \t [0.67682079 0.4419798 ]. \t  \u001b[92m-0.1303869815487635\u001b[0m \t -0.1303869815487635\n",
            "19     \t [1.37233369 0.60017456]. \t  -164.77966097529503 \t -0.1303869815487635\n",
            "20     \t [-0.16822463  0.21355451]. \t  -4.7966896499975835 \t -0.1303869815487635\n",
            "21     \t [0.48226185 0.3871917 ]. \t  -2.658638762533256 \t -0.1303869815487635\n",
            "22     \t [-0.63429467  0.40121825]. \t  -2.671042623106346 \t -0.1303869815487635\n",
            "23     \t [-0.19226789  1.6783132 ]. \t  -270.82325797463244 \t -0.1303869815487635\n",
            "24     \t [ 0.19522319 -1.99656291]. \t  -414.63790159038905 \t -0.1303869815487635\n",
            "25     \t [-1.90380966  1.58542849]. \t  -424.20978872368084 \t -0.1303869815487635\n",
            "26     \t [-1.62219888 -0.9583691 ]. \t  -1295.6129145410832 \t -0.1303869815487635\n",
            "27     \t [0.7011432  0.21686683]. \t  -7.637245058596269 \t -0.1303869815487635\n",
            "28     \t [-1.28207097  1.61665549]. \t  -5.281020743393033 \t -0.1303869815487635\n",
            "29     \t [-0.8380545  -1.56912158]. \t  -519.3301011515124 \t -0.1303869815487635\n",
            "30     \t [0.57828186 0.36707547]. \t  -0.2845500470533481 \t -0.1303869815487635\n",
            "31     \t [-1.32023351  1.88336187]. \t  -7.353165291906504 \t -0.1303869815487635\n",
            "32     \t [1.28092455 1.27436466]. \t  -13.504038134636817 \t -0.1303869815487635\n",
            "33     \t [1.08502183 1.88885703]. \t  -50.64250265416173 \t -0.1303869815487635\n",
            "34     \t [0.9669263  0.83057366]. \t  -1.0904623110643736 \t -0.1303869815487635\n",
            "35     \t [1.0578971  1.02419907]. \t  -0.9048490879040845 \t -0.1303869815487635\n",
            "36     \t [1.13648388 1.31268008]. \t  \u001b[92m-0.06308328425918654\u001b[0m \t -0.06308328425918654\n",
            "37     \t [1.16298671 0.3173189 ]. \t  -107.1944404400957 \t -0.06308328425918654\n",
            "38     \t [-0.87070096  1.76451424]. \t  -104.78242689426114 \t -0.06308328425918654\n",
            "39     \t [1.17835732 0.73878845]. \t  -42.24769636950319 \t -0.06308328425918654\n",
            "40     \t [-1.67712848 -0.15291956]. \t  -886.6925160063921 \t -0.06308328425918654\n",
            "41     \t [-1.60659348  0.49208595]. \t  -443.21010900556564 \t -0.06308328425918654\n",
            "42     \t [1.9716731  0.02544809]. \t  -1492.4846375196191 \t -0.06308328425918654\n",
            "43     \t [1.08442781 1.15520696]. \t  \u001b[92m-0.05029524899465926\u001b[0m \t -0.05029524899465926\n",
            "44     \t [ 1.97086788 -1.38085597]. \t  -2773.1505780850493 \t -0.05029524899465926\n",
            "45     \t [-1.1721999   1.78049686]. \t  -21.238145501831017 \t -0.05029524899465926\n",
            "46     \t [ 0.03298517 -0.06661061]. \t  -1.3934281580334764 \t -0.05029524899465926\n",
            "47     \t [ 0.74708299 -1.63196796]. \t  -479.71818826441717 \t -0.05029524899465926\n",
            "48     \t [-1.59011409 -1.17205592]. \t  -1376.0925958070266 \t -0.05029524899465926\n",
            "49     \t [2.0291388  0.88476863]. \t  -1046.0524466450936 \t -0.05029524899465926\n",
            "50     \t [1.44500644 1.98654748]. \t  -1.2281773013661643 \t -0.05029524899465926\n",
            "51     \t [ 0.20679112 -1.91326379]. \t  -383.23308876674696 \t -0.05029524899465926\n",
            "52     \t [-0.79672646  0.54017111]. \t  -4.1231787832939615 \t -0.05029524899465926\n",
            "53     \t [ 0.21272891 -0.7209774 ]. \t  -59.330788597808315 \t -0.05029524899465926\n",
            "54     \t [-1.97029281 -0.58851854]. \t  -2007.4243074353817 \t -0.05029524899465926\n",
            "55     \t [ 0.7991025  -1.46293189]. \t  -441.6691954364644 \t -0.05029524899465926\n",
            "56     \t [ 1.81666571 -1.39798178]. \t  -2208.027975628221 \t -0.05029524899465926\n",
            "57     \t [-0.38189943 -0.10655757]. \t  -8.280461489813375 \t -0.05029524899465926\n",
            "58     \t [-0.6690598   0.62857839]. \t  -6.059594112791341 \t -0.05029524899465926\n",
            "59     \t [-1.05187429  0.9227578 ]. \t  -7.584085831378985 \t -0.05029524899465926\n",
            "60     \t [1.14782618 1.37577057]. \t  -0.3613411085898156 \t -0.05029524899465926\n",
            "61     \t [1.17391862 1.40292686]. \t  -0.09195989196567633 \t -0.05029524899465926\n",
            "62     \t [0.68437748 1.71040291]. \t  -154.3635645981404 \t -0.05029524899465926\n",
            "63     \t [ 1.38770185 -0.97071159]. \t  -839.0798349707751 \t -0.05029524899465926\n",
            "64     \t [1.01970208 1.04122363]. \t  \u001b[92m-0.0005930351006262985\u001b[0m \t -0.0005930351006262985\n",
            "65     \t [ 1.40147933 -0.0424511 ]. \t  -402.80370032728825 \t -0.0005930351006262985\n",
            "66     \t [ 0.62192199 -1.03710214]. \t  -202.88896043583838 \t -0.0005930351006262985\n",
            "67     \t [-0.99635422  1.42286402]. \t  -22.48766865522903 \t -0.0005930351006262985\n",
            "68     \t [-0.78083258 -1.02298409]. \t  -269.7369418380132 \t -0.0005930351006262985\n",
            "69     \t [-1.20195423 -2.00630038]. \t  -1195.7848075551187 \t -0.0005930351006262985\n",
            "70     \t [0.67177029 0.74464743]. \t  -8.714454155806434 \t -0.0005930351006262985\n",
            "71     \t [-1.47018171  0.40299074]. \t  -315.3141561232518 \t -0.0005930351006262985\n",
            "72     \t [1.27200548 0.21893352]. \t  -195.81210967711115 \t -0.0005930351006262985\n",
            "73     \t [-0.73846938  1.78295618]. \t  -156.1923914126747 \t -0.0005930351006262985\n",
            "74     \t [0.39623881 0.17467933]. \t  -0.3957650996603667 \t -0.0005930351006262985\n",
            "75     \t [0.7026122  1.41582394]. \t  -85.12635272420111 \t -0.0005930351006262985\n",
            "76     \t [1.35840484 0.76388173]. \t  -117.06715361980729 \t -0.0005930351006262985\n",
            "77     \t [ 0.41220436 -1.23097664]. \t  -196.59452422588546 \t -0.0005930351006262985\n",
            "78     \t [ 0.94679312 -1.0564852 ]. \t  -381.3856099237539 \t -0.0005930351006262985\n",
            "79     \t [0.3264778  1.96547712]. \t  -346.00059964084403 \t -0.0005930351006262985\n",
            "80     \t [1.28286425 0.77135349]. \t  -76.53531097115662 \t -0.0005930351006262985\n",
            "81     \t [-0.05827082 -1.51731853]. \t  -232.3770506578901 \t -0.0005930351006262985\n",
            "82     \t [-0.0245695   0.50057882]. \t  -26.047258765573183 \t -0.0005930351006262985\n",
            "83     \t [1.52288195 0.32830641]. \t  -396.62696437497647 \t -0.0005930351006262985\n",
            "84     \t [ 0.24020323 -1.74935592]. \t  -327.12153009436537 \t -0.0005930351006262985\n",
            "85     \t [-0.37725487  1.1371482 ]. \t  -100.86489912110035 \t -0.0005930351006262985\n",
            "86     \t [1.89917358 1.78193519]. \t  -333.84367620804505 \t -0.0005930351006262985\n",
            "87     \t [0.81180189 1.42532162]. \t  -58.756882950286126 \t -0.0005930351006262985\n",
            "88     \t [-1.95437521 -0.72886382]. \t  -2077.5646770921367 \t -0.0005930351006262985\n",
            "89     \t [ 1.22901738 -1.40143556]. \t  -847.9798365211404 \t -0.0005930351006262985\n",
            "90     \t [-1.56558783  0.93830891]. \t  -235.42541967516422 \t -0.0005930351006262985\n",
            "91     \t [-0.65059558 -0.51476499]. \t  -90.71629548434643 \t -0.0005930351006262985\n",
            "92     \t [ 1.23184003 -0.68497411]. \t  -485.1120682204871 \t -0.0005930351006262985\n",
            "93     \t [-0.5821073  -1.55784162]. \t  -362.24656034397196 \t -0.0005930351006262985\n",
            "94     \t [-1.93926247  0.49980756]. \t  -1072.006603036883 \t -0.0005930351006262985\n",
            "95     \t [1.38918894 1.95261974]. \t  -0.2033328123713055 \t -0.0005930351006262985\n",
            "96     \t [ 0.21458814 -1.62712584]. \t  -280.5679665819224 \t -0.0005930351006262985\n",
            "97     \t [1.00421244 1.90356606]. \t  -80.12461242201168 \t -0.0005930351006262985\n",
            "98     \t [-1.25873717  1.22698148]. \t  -17.87807061042809 \t -0.0005930351006262985\n",
            "99     \t [-1.00773153 -1.93180522]. \t  -872.7052511651193 \t -0.0005930351006262985\n",
            "100    \t [-0.43163006 -0.55778265]. \t  -57.41613508491781 \t -0.0005930351006262985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Bpn-kmNuvqC",
        "outputId": "629d4610-678c-44b5-e018-936c4bb278e6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_winner_13 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_13 = dGPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.65799909 -0.35389173]. \t  -62.0309701572776 \t -62.0309701572776\n",
            "init   \t [ 0.68854808 -1.10615174]. \t  -249.81607110978882 \t -62.0309701572776\n",
            "init   \t [1.26025046 0.56040843]. \t  -105.7097019264073 \t -62.0309701572776\n",
            "init   \t [-1.34269398 -0.98145948]. \t  -780.713409392333 \t -62.0309701572776\n",
            "init   \t [ 1.70115068 -0.15230767]. \t  -928.4380405275942 \t -62.0309701572776\n",
            "1      \t [0.4293342  1.08165506]. \t  -80.84526980526817 \t -62.0309701572776\n",
            "2      \t [0.36363636 0.13941193]. \t  \u001b[92m-0.41011468997669837\u001b[0m \t -0.41011468997669837\n",
            "3      \t [1.50072597 1.55400012]. \t  -48.996024661620744 \t -0.41011468997669837\n",
            "4      \t [0.74544411 0.45034568]. \t  -1.1744764387900246 \t -0.41011468997669837\n",
            "5      \t [1.07173738 0.87290468]. \t  -7.6070951610508155 \t -0.41011468997669837\n",
            "6      \t [-0.65396955  1.91789583]. \t  -224.81107909910284 \t -0.41011468997669837\n",
            "7      \t [ 0.32250907 -0.39110006]. \t  -24.97259877268609 \t -0.41011468997669837\n",
            "8      \t [0.42458555 0.10540022]. \t  -0.8916934884883143 \t -0.41011468997669837\n",
            "9      \t [-0.49515144  0.60460547]. \t  -15.154507539917688 \t -0.41011468997669837\n",
            "10     \t [2.04441277 1.99913053]. \t  -476.5457840404299 \t -0.41011468997669837\n",
            "11     \t [0.42389776 0.43866745]. \t  -7.038861775182548 \t -0.41011468997669837\n",
            "12     \t [0.74227073 1.9266892 ]. \t  -189.32789938861438 \t -0.41011468997669837\n",
            "13     \t [-0.3874052   0.39258806]. \t  -7.8057734636918426 \t -0.41011468997669837\n",
            "14     \t [ 0.20226545 -1.04869366]. \t  -119.36027946747926 \t -0.41011468997669837\n",
            "15     \t [-0.29104128  0.85903597]. \t  -61.6256283562939 \t -0.41011468997669837\n",
            "16     \t [-2.00869485  1.12833381]. \t  -853.8387845342978 \t -0.41011468997669837\n",
            "17     \t [-1.79116532  0.39368635]. \t  -799.9805187663303 \t -0.41011468997669837\n",
            "18     \t [1.56153311 0.02799735]. \t  -581.3124927720667 \t -0.41011468997669837\n",
            "19     \t [1.08793653 1.27063972]. \t  -0.7652216852619245 \t -0.41011468997669837\n",
            "20     \t [0.92805809 0.87413209]. \t  \u001b[92m-0.02166289307486688\u001b[0m \t -0.02166289307486688\n",
            "21     \t [-0.52856026 -1.00599816]. \t  -167.55515641751728 \t -0.02166289307486688\n",
            "22     \t [0.59745798 0.32677181]. \t  -0.25314889233396876 \t -0.02166289307486688\n",
            "23     \t [1.68411474 0.68570093]. \t  -462.9508923919746 \t -0.02166289307486688\n",
            "24     \t [-0.56137467 -0.71804184]. \t  -109.18467518192139 \t -0.02166289307486688\n",
            "25     \t [-0.02583589 -0.21975066]. \t  -5.910755537584871 \t -0.02166289307486688\n",
            "26     \t [ 1.74525021 -2.03394314]. \t  -2581.034292136097 \t -0.02166289307486688\n",
            "27     \t [-1.21394464  0.86163408]. \t  -42.359319193002385 \t -0.02166289307486688\n",
            "28     \t [-0.97463709  0.94328009]. \t  -3.903597105799265 \t -0.02166289307486688\n",
            "29     \t [1.10624673 1.29382561]. \t  -0.5019015497775359 \t -0.02166289307486688\n",
            "30     \t [-2.03682352 -1.95950133]. \t  -3740.1736451495244 \t -0.02166289307486688\n",
            "31     \t [ 0.59201426 -1.62687485]. \t  -391.16002009217027 \t -0.02166289307486688\n",
            "32     \t [-0.79503399  0.6368169 ]. \t  -3.224391749169635 \t -0.02166289307486688\n",
            "33     \t [-1.83727401  1.86692274]. \t  -235.65352583664207 \t -0.02166289307486688\n",
            "34     \t [1.15720654 1.28059855]. \t  -0.36727173760664067 \t -0.02166289307486688\n",
            "35     \t [-0.68082076 -0.94634829]. \t  -201.59714479346763 \t -0.02166289307486688\n",
            "36     \t [-0.14916624 -1.03404296]. \t  -112.89618517418847 \t -0.02166289307486688\n",
            "37     \t [-1.54445384 -0.55743512]. \t  -872.4654088084312 \t -0.02166289307486688\n",
            "38     \t [1.21933868 1.40765359]. \t  -0.6743160546485591 \t -0.02166289307486688\n",
            "39     \t [0.96352453 0.9302635 ]. \t  \u001b[92m-0.001685400343915174\u001b[0m \t -0.001685400343915174\n",
            "40     \t [ 1.19278592 -1.8498044 ]. \t  -1070.9907086052215 \t -0.001685400343915174\n",
            "41     \t [ 0.13353934 -0.00260272]. \t  -0.792514946262475 \t -0.001685400343915174\n",
            "42     \t [-1.36586415  2.04363529]. \t  -8.767508002243801 \t -0.001685400343915174\n",
            "43     \t [-1.65106047  1.61048312]. \t  -131.46606497698292 \t -0.001685400343915174\n",
            "44     \t [-1.31649063  1.73682065]. \t  -5.367477985454733 \t -0.001685400343915174\n",
            "45     \t [0.05342914 0.77434006]. \t  -60.41496633178125 \t -0.001685400343915174\n",
            "46     \t [-1.54062542  0.92282972]. \t  -216.90694503923066 \t -0.001685400343915174\n",
            "47     \t [-1.92765507 -0.28267287]. \t  -1607.3929445715162 \t -0.001685400343915174\n",
            "48     \t [ 0.88818791 -1.32839832]. \t  -448.2983037969398 \t -0.001685400343915174\n",
            "49     \t [-0.65987705 -0.13774205]. \t  -35.608697485650005 \t -0.001685400343915174\n",
            "50     \t [1.9362214  1.29276233]. \t  -604.1639204282061 \t -0.001685400343915174\n",
            "51     \t [-0.62056197 -1.76656905]. \t  -465.5929716936383 \t -0.001685400343915174\n",
            "52     \t [1.06509099 1.14804164]. \t  -0.022794954441637434 \t -0.001685400343915174\n",
            "53     \t [-1.70191383  1.46957766]. \t  -210.91412597964643 \t -0.001685400343915174\n",
            "54     \t [-1.15543014 -0.80314546]. \t  -461.8205277732385 \t -0.001685400343915174\n",
            "55     \t [-0.6445735   0.19937118]. \t  -7.374707665560582 \t -0.001685400343915174\n",
            "56     \t [0.8318862  1.35426881]. \t  -43.88367000284221 \t -0.001685400343915174\n",
            "57     \t [1.36450479 1.81176751]. \t  -0.38392299268875807 \t -0.001685400343915174\n",
            "58     \t [0.74548014 1.42810719]. \t  -76.16712000504089 \t -0.001685400343915174\n",
            "59     \t [0.46896872 0.98691183]. \t  -59.10785224430413 \t -0.001685400343915174\n",
            "60     \t [1.02667399 1.0636183 ]. \t  -0.009848584950261956 \t -0.001685400343915174\n",
            "61     \t [-2.04602583 -0.00310691]. \t  -1764.3256972698305 \t -0.001685400343915174\n",
            "62     \t [ 0.98149748 -1.26261417]. \t  -495.4863362533916 \t -0.001685400343915174\n",
            "63     \t [1.25730682 1.55638921]. \t  -0.12589534378322112 \t -0.001685400343915174\n",
            "64     \t [-0.83953917  0.4039758 ]. \t  -12.434989412544088 \t -0.001685400343915174\n",
            "65     \t [-0.44755408 -1.96706802]. \t  -471.8458404542322 \t -0.001685400343915174\n",
            "66     \t [1.31828968 1.71317367]. \t  -0.1623865275504108 \t -0.001685400343915174\n",
            "67     \t [0.83759372 0.14908133]. \t  -30.550001785808465 \t -0.001685400343915174\n",
            "68     \t [0.98456094 0.25075815]. \t  -51.63913704506476 \t -0.001685400343915174\n",
            "69     \t [ 0.73246608 -1.77368981]. \t  -533.772297993229 \t -0.001685400343915174\n",
            "70     \t [-1.58563016  1.17593792]. \t  -185.78617810527686 \t -0.001685400343915174\n",
            "71     \t [0.79168256 1.72100923]. \t  -119.78125431659336 \t -0.001685400343915174\n",
            "72     \t [-0.81686469  0.1363013 ]. \t  -31.49355347048604 \t -0.001685400343915174\n",
            "73     \t [1.49237833 1.5409657 ]. \t  -47.333238345706455 \t -0.001685400343915174\n",
            "74     \t [0.77181157 0.30847436]. \t  -8.301530181457267 \t -0.001685400343915174\n",
            "75     \t [1.18280153 1.37675643]. \t  -0.08298067324099273 \t -0.001685400343915174\n",
            "76     \t [ 1.10650968 -0.76968109]. \t  -397.6327937907719 \t -0.001685400343915174\n",
            "77     \t [0.95945875 0.92152027]. \t  -0.0017355964225492944 \t -0.001685400343915174\n",
            "78     \t [ 0.82063841 -0.86387592]. \t  -236.36846866714586 \t -0.001685400343915174\n",
            "79     \t [ 0.29947585 -1.35759449]. \t  -209.95275381756917 \t -0.001685400343915174\n",
            "80     \t [0.41354938 1.88425379]. \t  -293.8598650763438 \t -0.001685400343915174\n",
            "81     \t [ 1.16142564 -0.78263878]. \t  -454.37587569895425 \t -0.001685400343915174\n",
            "82     \t [1.099743   1.21029986]. \t  -0.010023522509316448 \t -0.001685400343915174\n",
            "83     \t [-1.55106656  1.40530115]. \t  -106.60922967669718 \t -0.001685400343915174\n",
            "84     \t [-0.66643241 -1.82568483]. \t  -517.9839136256584 \t -0.001685400343915174\n",
            "85     \t [-1.30003343  0.94185481]. \t  -61.27528154653576 \t -0.001685400343915174\n",
            "86     \t [-1.77977978  0.80529652]. \t  -565.7825354395953 \t -0.001685400343915174\n",
            "87     \t [-1.17277124 -0.21195015]. \t  -256.686568077995 \t -0.001685400343915174\n",
            "88     \t [-0.37738586  2.03176769]. \t  -358.8606279912727 \t -0.001685400343915174\n",
            "89     \t [ 1.09585673 -0.96871735]. \t  -470.7339851117273 \t -0.001685400343915174\n",
            "90     \t [0.90442299 0.82706784]. \t  -0.017392143187383573 \t -0.001685400343915174\n",
            "91     \t [1.26013997 2.01043038]. \t  -17.91640790402721 \t -0.001685400343915174\n",
            "92     \t [1.06460311 1.12424786]. \t  -0.012512767571803815 \t -0.001685400343915174\n",
            "93     \t [-0.60869851  1.45333029]. \t  -119.83704824326959 \t -0.001685400343915174\n",
            "94     \t [-0.37949809 -1.97403528]. \t  -450.51832415832763 \t -0.001685400343915174\n",
            "95     \t [-0.03532646  0.20508262]. \t  -5.226757806966602 \t -0.001685400343915174\n",
            "96     \t [-1.44592539  0.65227005]. \t  -212.89069487972304 \t -0.001685400343915174\n",
            "97     \t [-0.85519495  1.45638497]. \t  -56.008100437405474 \t -0.001685400343915174\n",
            "98     \t [-1.10398428  1.61088889]. \t  -19.801586554812506 \t -0.001685400343915174\n",
            "99     \t [1.0275188  1.04328329]. \t  -0.016411293445117516 \t -0.001685400343915174\n",
            "100    \t [2.0161086 1.2859591]. \t  -773.1691858866905 \t -0.001685400343915174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NdFRXtPuvsP",
        "outputId": "d74610ce-fd05-4fac-dd09-d1ea75945c37"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_winner_14 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_14 = dGPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.6043691  0.02928512]. \t  -11.444570460525577 \t -4.306489127802793\n",
            "init   \t [0.11608629 1.6231842 ]. \t  -259.89734211112767 \t -4.306489127802793\n",
            "init   \t [0.81916392 0.87776093]. \t  -4.306489127802793 \t -4.306489127802793\n",
            "init   \t [ 0.89021801 -1.13533148]. \t  -371.66089065122344 \t -4.306489127802793\n",
            "init   \t [-1.33056707 -0.17677726]. \t  -384.58487399689074 \t -4.306489127802793\n",
            "1      \t [1.20014634 0.52224222]. \t  -84.33247643037394 \t -4.306489127802793\n",
            "2      \t [0.44738171 0.35400542]. \t  \u001b[92m-2.6725236989882903\u001b[0m \t -2.6725236989882903\n",
            "3      \t [1.71705062 2.02592882]. \t  -85.58416441979601 \t -2.6725236989882903\n",
            "4      \t [0.48895989 0.31860394]. \t  \u001b[92m-0.8935395323165084\u001b[0m \t -0.8935395323165084\n",
            "5      \t [0.33648752 0.08941757]. \t  \u001b[92m-0.4969227313965301\u001b[0m \t -0.4969227313965301\n",
            "6      \t [1.04319355 1.15819176]. \t  \u001b[92m-0.4910117331202918\u001b[0m \t -0.4910117331202918\n",
            "7      \t [0.74112232 1.67535394]. \t  -126.87525800509205 \t -0.4910117331202918\n",
            "8      \t [ 0.06241567 -1.28637043]. \t  -167.35773773045298 \t -0.4910117331202918\n",
            "9      \t [-1.32848133  1.73144046]. \t  -5.533529495668711 \t -0.4910117331202918\n",
            "10     \t [-1.9081828   1.84188939]. \t  -332.1955764905027 \t -0.4910117331202918\n",
            "11     \t [-0.97812153  1.44214776]. \t  -27.47680731826417 \t -0.4910117331202918\n",
            "12     \t [-1.06876846  1.87322166]. \t  -57.709418125013414 \t -0.4910117331202918\n",
            "13     \t [-1.38723753  1.4016089 ]. \t  -33.032880943384455 \t -0.4910117331202918\n",
            "14     \t [-0.95781488 -1.9929902 ]. \t  -850.8755911581851 \t -0.4910117331202918\n",
            "15     \t [-1.7998584   1.26172869]. \t  -398.993292706719 \t -0.4910117331202918\n",
            "16     \t [1.83037942 1.27120532]. \t  -432.9483560122253 \t -0.4910117331202918\n",
            "17     \t [-1.25298166  1.54527056]. \t  -5.136898230623584 \t -0.4910117331202918\n",
            "18     \t [-0.76660688  0.62754857]. \t  -3.2798014432442266 \t -0.4910117331202918\n",
            "19     \t [-1.09045904  1.03175955]. \t  -6.8456494650526 \t -0.4910117331202918\n",
            "20     \t [0.45051441 0.44648882]. \t  -6.232405635222794 \t -0.4910117331202918\n",
            "21     \t [-0.68192954  0.71192221]. \t  -8.92456694115358 \t -0.4910117331202918\n",
            "22     \t [-0.49485632  0.05227629]. \t  -5.944321235487564 \t -0.4910117331202918\n",
            "23     \t [1.05690505 1.35696456]. \t  -5.759220097716466 \t -0.4910117331202918\n",
            "24     \t [-1.71316048  0.26234352]. \t  -721.6271162631949 \t -0.4910117331202918\n",
            "25     \t [-0.17347511 -0.56901051]. \t  -37.26961907705688 \t -0.4910117331202918\n",
            "26     \t [1.35949135 1.98432447]. \t  -1.981765697419812 \t -0.4910117331202918\n",
            "27     \t [-1.3910269   2.00484248]. \t  -6.205423946792847 \t -0.4910117331202918\n",
            "28     \t [0.08710804 1.04892057]. \t  -109.27076301525734 \t -0.4910117331202918\n",
            "29     \t [ 0.08063829 -0.60628709]. \t  -38.39633775902093 \t -0.4910117331202918\n",
            "30     \t [-1.39992158  0.49737864]. \t  -219.62152666592806 \t -0.4910117331202918\n",
            "31     \t [-1.08945402 -0.16533092]. \t  -187.22138818008403 \t -0.4910117331202918\n",
            "32     \t [0.40066223 0.19502322]. \t  \u001b[92m-0.4781824374086416\u001b[0m \t -0.4781824374086416\n",
            "33     \t [-1.16085581  0.21366414]. \t  -133.24722419089474 \t -0.4781824374086416\n",
            "34     \t [0.77143026 0.59486869]. \t  \u001b[92m-0.052249695023907375\u001b[0m \t -0.052249695023907375\n",
            "35     \t [0.7476354  1.17273707]. \t  -37.736077303702444 \t -0.052249695023907375\n",
            "36     \t [ 0.77205549 -1.24169292]. \t  -337.7890947457284 \t -0.052249695023907375\n",
            "37     \t [-0.73850866 -1.64347618]. \t  -482.13813546138 \t -0.052249695023907375\n",
            "38     \t [ 1.47847777 -1.31991067]. \t  -1229.297341811999 \t -0.052249695023907375\n",
            "39     \t [0.24445474 0.67058581]. \t  -37.88189592221282 \t -0.052249695023907375\n",
            "40     \t [0.97102375 0.88663794]. \t  -0.31723667702273917 \t -0.052249695023907375\n",
            "41     \t [-0.26347808  0.12501395]. \t  -1.905437828416229 \t -0.052249695023907375\n",
            "42     \t [1.43367697 0.32061112]. \t  -301.14760728437517 \t -0.052249695023907375\n",
            "43     \t [-0.01326109 -0.49478701]. \t  -25.525522378701645 \t -0.052249695023907375\n",
            "44     \t [-1.5182108   0.80987152]. \t  -229.87155168550814 \t -0.052249695023907375\n",
            "45     \t [0.8778477  0.72821064]. \t  -0.19474750245166178 \t -0.052249695023907375\n",
            "46     \t [-1.9703253   1.59067058]. \t  -533.9251830288297 \t -0.052249695023907375\n",
            "47     \t [ 1.91370402 -1.69217447]. \t  -2867.835007825963 \t -0.052249695023907375\n",
            "48     \t [-1.73379435 -0.73116528]. \t  -1404.1460948150188 \t -0.052249695023907375\n",
            "49     \t [ 1.23819025 -0.23669483]. \t  -313.27944944341465 \t -0.052249695023907375\n",
            "50     \t [-0.05142272  2.04169095]. \t  -416.87661347692347 \t -0.052249695023907375\n",
            "51     \t [-2.00496167  0.62367447]. \t  -1162.4450734371903 \t -0.052249695023907375\n",
            "52     \t [ 0.29888104 -1.46818963]. \t  -243.07826902833955 \t -0.052249695023907375\n",
            "53     \t [-1.65210435 -0.0990571 ]. \t  -807.0782056667098 \t -0.052249695023907375\n",
            "54     \t [0.05155122 0.37314437]. \t  -14.625605019530663 \t -0.052249695023907375\n",
            "55     \t [-1.6060395  -1.08628769]. \t  -1350.4908402004257 \t -0.052249695023907375\n",
            "56     \t [ 0.33061555 -1.7446117 ]. \t  -344.14939823471684 \t -0.052249695023907375\n",
            "57     \t [0.58250368 0.30259678]. \t  -0.3090931506733943 \t -0.052249695023907375\n",
            "58     \t [-0.87484484  1.09494647]. \t  -14.37819674086421 \t -0.052249695023907375\n",
            "59     \t [1.11981129 2.01294109]. \t  -57.61695416628044 \t -0.052249695023907375\n",
            "60     \t [0.65208095 0.39877619]. \t  -0.1909199973104507 \t -0.052249695023907375\n",
            "61     \t [-0.98387027 -1.93324359]. \t  -845.6575912879017 \t -0.052249695023907375\n",
            "62     \t [-0.83360606 -0.39821277]. \t  -122.85145702179399 \t -0.052249695023907375\n",
            "63     \t [-1.66722611  0.57154307]. \t  -494.6845860274701 \t -0.052249695023907375\n",
            "64     \t [ 1.43432849 -0.71886116]. \t  -770.8947222516093 \t -0.052249695023907375\n",
            "65     \t [1.74145888 0.47871748]. \t  -652.8217275890095 \t -0.052249695023907375\n",
            "66     \t [-1.38319376 -0.62574996]. \t  -650.3189806290884 \t -0.052249695023907375\n",
            "67     \t [0.80136755 0.61666005]. \t  -0.10463241327296494 \t -0.052249695023907375\n",
            "68     \t [ 2.03794533 -1.12526205]. \t  -2787.3158341879744 \t -0.052249695023907375\n",
            "69     \t [-1.70860006 -0.65519372]. \t  -1285.0471706975995 \t -0.052249695023907375\n",
            "70     \t [1.20379689 1.53521035]. \t  -0.7825680639339133 \t -0.052249695023907375\n",
            "71     \t [-0.55951318  0.93751851]. \t  -41.427548758187484 \t -0.052249695023907375\n",
            "72     \t [-1.19208035 -1.95536668]. \t  -1144.827932674559 \t -0.052249695023907375\n",
            "73     \t [1.28830482 1.68699392]. \t  -0.15745550837657923 \t -0.052249695023907375\n",
            "74     \t [0.84384095 0.0379231 ]. \t  -45.471459668446435 \t -0.052249695023907375\n",
            "75     \t [-0.67274238 -1.56222325]. \t  -408.7422158525488 \t -0.052249695023907375\n",
            "76     \t [ 1.68856139 -1.69422831]. \t  -2066.601950465006 \t -0.052249695023907375\n",
            "77     \t [ 0.65265246 -1.88587041]. \t  -534.5744305473129 \t -0.052249695023907375\n",
            "78     \t [0.77240592 0.59831718]. \t  \u001b[92m-0.052090206417226136\u001b[0m \t -0.052090206417226136\n",
            "79     \t [ 1.38138552 -1.7334792 ]. \t  -1326.3470913776823 \t -0.052090206417226136\n",
            "80     \t [0.71699368 0.52207064]. \t  -0.0864777190918296 \t -0.052090206417226136\n",
            "81     \t [-1.49436433  0.52273651]. \t  -298.7646462028464 \t -0.052090206417226136\n",
            "82     \t [-1.06147932 -1.21766938]. \t  -553.8744545919657 \t -0.052090206417226136\n",
            "83     \t [-0.48247986 -1.08389492]. \t  -175.56282593029763 \t -0.052090206417226136\n",
            "84     \t [ 1.39245159 -1.35409851]. \t  -1084.5520546572018 \t -0.052090206417226136\n",
            "85     \t [ 0.68835944 -1.54346285]. \t  -407.0476822785868 \t -0.052090206417226136\n",
            "86     \t [1.01643062 0.60629059]. \t  -18.21956078873848 \t -0.052090206417226136\n",
            "87     \t [1.3625676  0.35762958]. \t  -224.81982638961267 \t -0.052090206417226136\n",
            "88     \t [1.67259036 0.60769277]. \t  -480.00357033406283 \t -0.052090206417226136\n",
            "89     \t [-0.10675584  1.4505127 ]. \t  -208.3303635018325 \t -0.052090206417226136\n",
            "90     \t [ 1.66960399 -0.90521397]. \t  -1364.119242511162 \t -0.052090206417226136\n",
            "91     \t [1.00698279 1.00630881]. \t  \u001b[92m-0.00598628680283637\u001b[0m \t -0.00598628680283637\n",
            "92     \t [-0.32989008  0.52002008]. \t  -18.676544542455904 \t -0.00598628680283637\n",
            "93     \t [ 1.00089176 -0.74819103]. \t  -306.24137151272816 \t -0.00598628680283637\n",
            "94     \t [-0.31594662  0.30611815]. \t  -5.987514954531987 \t -0.00598628680283637\n",
            "95     \t [0.51232447 0.86448493]. \t  -36.479259130122884 \t -0.00598628680283637\n",
            "96     \t [ 1.19759156 -0.75635892]. \t  -479.90507167496264 \t -0.00598628680283637\n",
            "97     \t [1.06233966 1.11046425]. \t  -0.0366519316149771 \t -0.00598628680283637\n",
            "98     \t [ 1.98684688 -0.24356647]. \t  -1757.5284212307308 \t -0.00598628680283637\n",
            "99     \t [ 1.8417731  -0.26493655]. \t  -1338.1207981665664 \t -0.00598628680283637\n",
            "100    \t [-0.67578045 -1.95670776]. \t  -585.2519126932482 \t -0.00598628680283637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d86panpOuvum",
        "outputId": "3fefe197-53af-40ff-ee7d-c88fd223488c"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_winner_15 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_15 = dGPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.31900409 -1.00146655]. \t  -122.17543449532224 \t -6.867717811955245\n",
            "init   \t [-1.01364994  0.85976824]. \t  -6.867717811955245 \t -6.867717811955245\n",
            "init   \t [-0.21482552 -1.11844164]. \t  -137.1031718367486 \t -6.867717811955245\n",
            "init   \t [-0.40259762  1.56572768]. \t  -198.98860193376754 \t -6.867717811955245\n",
            "init   \t [-0.25717364  1.55002954]. \t  -221.77381285664336 \t -6.867717811955245\n",
            "1      \t [-1.86469748  0.67575432]. \t  -792.9583928281373 \t -6.867717811955245\n",
            "2      \t [-0.51814632  0.48813463]. \t  -7.129776748982136 \t -6.867717811955245\n",
            "3      \t [-0.71353464  0.78042254]. \t  -10.29607442865114 \t -6.867717811955245\n",
            "4      \t [1.51801528 1.92878838]. \t  -14.374524553155442 \t -6.867717811955245\n",
            "5      \t [-0.54608447 -0.29218422]. \t  -37.24670394632657 \t -6.867717811955245\n",
            "6      \t [1.34426055 0.72031768]. \t  -118.21427997193312 \t -6.867717811955245\n",
            "7      \t [-0.59556705  0.03853118]. \t  -12.542113299222674 \t -6.867717811955245\n",
            "8      \t [-0.05786415 -0.06418152]. \t  \u001b[92m-1.575103663427614\u001b[0m \t -1.575103663427614\n",
            "9      \t [-1.24269704  1.55373584]. \t  -5.038601192705563 \t -1.575103663427614\n",
            "10     \t [-1.05731717  1.16063658]. \t  -4.415028060297689 \t -1.575103663427614\n",
            "11     \t [-0.96564753  0.94906782]. \t  -3.8913018922749254 \t -1.575103663427614\n",
            "12     \t [2.048 2.048]. \t  -461.7603900415999 \t -1.575103663427614\n",
            "13     \t [1.14556788 1.72766672]. \t  -17.272000227756642 \t -1.575103663427614\n",
            "14     \t [1.06036192 2.0155369 ]. \t  -79.42195084306921 \t -1.575103663427614\n",
            "15     \t [1.28646128 1.5532717 ]. \t  \u001b[92m-1.1165712433640984\u001b[0m \t -1.1165712433640984\n",
            "16     \t [ 0.53463366 -0.15110661]. \t  -19.30820074735102 \t -1.1165712433640984\n",
            "17     \t [1.32339482 1.60912997]. \t  -2.127916248328541 \t -1.1165712433640984\n",
            "18     \t [0.27966566 0.66426218]. \t  -34.864259202966075 \t -1.1165712433640984\n",
            "19     \t [1.34213985 1.83536003]. \t  \u001b[92m-0.23280017655559596\u001b[0m \t -0.23280017655559596\n",
            "20     \t [-0.7240658   0.49077095]. \t  -3.0846301349948604 \t -0.23280017655559596\n",
            "21     \t [-1.37756223  1.94666541]. \t  -5.892781783557843 \t -0.23280017655559596\n",
            "22     \t [-1.17578673  1.77742605]. \t  -20.332725025732124 \t -0.23280017655559596\n",
            "23     \t [ 1.79628836 -0.7947857 ]. \t  -1617.8300919144265 \t -0.23280017655559596\n",
            "24     \t [0.27724689 0.06005841]. \t  -0.5506210298191572 \t -0.23280017655559596\n",
            "25     \t [1.05855339 1.36656411]. \t  -6.056446722363809 \t -0.23280017655559596\n",
            "26     \t [ 0.22401697 -1.35838773]. \t  -199.00946979239325 \t -0.23280017655559596\n",
            "27     \t [ 1.42645813 -1.38249747]. \t  -1167.962308583028 \t -0.23280017655559596\n",
            "28     \t [1.27654895 0.99584516]. \t  -40.2381133304632 \t -0.23280017655559596\n",
            "29     \t [ 0.06503334 -0.06789705]. \t  -1.3943842559033524 \t -0.23280017655559596\n",
            "30     \t [ 0.97642433 -0.04462397]. \t  -99.60663494896336 \t -0.23280017655559596\n",
            "31     \t [0.77924309 0.44676741]. \t  -2.62323013821001 \t -0.23280017655559596\n",
            "32     \t [1.33051633 1.85119414]. \t  -0.7640525679081125 \t -0.23280017655559596\n",
            "33     \t [0.56549671 0.27539096]. \t  -0.3858896774196756 \t -0.23280017655559596\n",
            "34     \t [-0.13237313  0.08421537]. \t  -1.7270608596479986 \t -0.23280017655559596\n",
            "35     \t [-0.5765119   0.70890288]. \t  -16.66339463956982 \t -0.23280017655559596\n",
            "36     \t [0.68769956 0.08772452]. \t  -14.935911093023686 \t -0.23280017655559596\n",
            "37     \t [ 0.94346704 -0.1780317 ]. \t  -114.10015004857898 \t -0.23280017655559596\n",
            "38     \t [0.75464442 0.56752197]. \t  \u001b[92m-0.060585966473464356\u001b[0m \t -0.060585966473464356\n",
            "39     \t [-0.9926861   0.51774171]. \t  -25.843628897374533 \t -0.060585966473464356\n",
            "40     \t [1.15381854 1.09155764]. \t  -5.77116687300655 \t -0.060585966473464356\n",
            "41     \t [-1.33649111 -0.36559919]. \t  -468.48682209390284 \t -0.060585966473464356\n",
            "42     \t [-0.45892964  0.92975848]. \t  -53.84500608764505 \t -0.060585966473464356\n",
            "43     \t [1.79425229 1.17518273]. \t  -418.4892525149357 \t -0.060585966473464356\n",
            "44     \t [1.17214011 1.30913854]. \t  -0.4491980158191778 \t -0.060585966473464356\n",
            "45     \t [ 1.68741734 -1.4058258 ]. \t  -1809.4462006025503 \t -0.060585966473464356\n",
            "46     \t [0.87307096 1.35901039]. \t  -35.6280596638206 \t -0.060585966473464356\n",
            "47     \t [ 0.07770192 -0.08699998]. \t  -1.7162325778799612 \t -0.060585966473464356\n",
            "48     \t [1.35710706 1.88281138]. \t  -0.29621477368128035 \t -0.060585966473464356\n",
            "49     \t [-1.12522902 -1.26760128]. \t  -646.5012649272198 \t -0.060585966473464356\n",
            "50     \t [1.04734176 1.08157023]. \t  \u001b[92m-0.02581740205057448\u001b[0m \t -0.02581740205057448\n",
            "51     \t [1.82117592 0.02005112]. \t  -1087.4516611715662 \t -0.02581740205057448\n",
            "52     \t [-0.97041722 -0.56868483]. \t  -232.01166970174543 \t -0.02581740205057448\n",
            "53     \t [0.25881451 0.0782059 ]. \t  -0.5619469014730655 \t -0.02581740205057448\n",
            "54     \t [ 0.05025156 -1.881929  ]. \t  -356.01879130785295 \t -0.02581740205057448\n",
            "55     \t [ 0.13604281 -1.96151768]. \t  -392.79644976636814 \t -0.02581740205057448\n",
            "56     \t [0.49790348 1.06233932]. \t  -66.5819590662643 \t -0.02581740205057448\n",
            "57     \t [-1.72252026 -1.78299641]. \t  -2263.730965109315 \t -0.02581740205057448\n",
            "58     \t [-0.42475128  1.02612634]. \t  -73.55291025568293 \t -0.02581740205057448\n",
            "59     \t [-0.89795156 -0.02881667]. \t  -73.34704401720438 \t -0.02581740205057448\n",
            "60     \t [ 2.02435328 -0.77141525]. \t  -2372.1758244197367 \t -0.02581740205057448\n",
            "61     \t [1.41089793 2.01566152]. \t  -0.23147987421871186 \t -0.02581740205057448\n",
            "62     \t [ 0.74638391 -0.94930964]. \t  -226.98799123775052 \t -0.02581740205057448\n",
            "63     \t [-1.85923427  0.72504786]. \t  -754.3960043276296 \t -0.02581740205057448\n",
            "64     \t [1.29422912 1.65370397]. \t  -0.13204656406780957 \t -0.02581740205057448\n",
            "65     \t [-1.0754649   0.43522195]. \t  -56.34975499227283 \t -0.02581740205057448\n",
            "66     \t [0.68320998 0.47494266]. \t  -0.10702555738314529 \t -0.02581740205057448\n",
            "67     \t [0.05975501 0.77168105]. \t  -59.883417596274604 \t -0.02581740205057448\n",
            "68     \t [1.92654531 0.58094617]. \t  -980.9433089825492 \t -0.02581740205057448\n",
            "69     \t [ 0.02631352 -1.58886752]. \t  -253.61814079181158 \t -0.02581740205057448\n",
            "70     \t [ 0.45056277 -1.30412006]. \t  -227.44502188994414 \t -0.02581740205057448\n",
            "71     \t [-0.0198923  -0.92816516]. \t  -87.26270799406798 \t -0.02581740205057448\n",
            "72     \t [1.01021726 0.99828821]. \t  -0.049613736625678376 \t -0.02581740205057448\n",
            "73     \t [-0.88726298  1.60797499]. \t  -70.92307559057677 \t -0.02581740205057448\n",
            "74     \t [1.41104204 1.9901646 ]. \t  -0.1690321238893527 \t -0.02581740205057448\n",
            "75     \t [ 0.56475636 -1.4631005 ]. \t  -317.7597443470307 \t -0.02581740205057448\n",
            "76     \t [ 0.82312635 -1.53627469]. \t  -490.1274956390867 \t -0.02581740205057448\n",
            "77     \t [-0.50112241  0.72246393]. \t  -24.46953284373991 \t -0.02581740205057448\n",
            "78     \t [ 2.0139713  -0.14158256]. \t  -1763.065573072827 \t -0.02581740205057448\n",
            "79     \t [0.43465746 0.19616264]. \t  -0.32484748480555864 \t -0.02581740205057448\n",
            "80     \t [0.6547521  0.44081045]. \t  -0.1338616449430302 \t -0.02581740205057448\n",
            "81     \t [1.58678507 2.0096641 ]. \t  -26.173355388676914 \t -0.02581740205057448\n",
            "82     \t [0.93562776 0.86152371]. \t  \u001b[92m-0.023396993412763373\u001b[0m \t -0.023396993412763373\n",
            "83     \t [-0.06708015 -1.62779617]. \t  -267.5776570316121 \t -0.023396993412763373\n",
            "84     \t [0.68517674 0.17783462]. \t  -8.604067714590572 \t -0.023396993412763373\n",
            "85     \t [0.39495926 1.89434764]. \t  -302.55382616645153 \t -0.023396993412763373\n",
            "86     \t [-1.00569849  0.65747462]. \t  -16.551228310140736 \t -0.023396993412763373\n",
            "87     \t [-1.69201713 -0.00320477]. \t  -828.7152142879839 \t -0.023396993412763373\n",
            "88     \t [0.6276886  0.41282151]. \t  -0.17406713455960235 \t -0.023396993412763373\n",
            "89     \t [-0.40366291 -0.83154994]. \t  -100.87203805527996 \t -0.023396993412763373\n",
            "90     \t [-0.59352026 -0.2073231 ]. \t  -33.85333716244417 \t -0.023396993412763373\n",
            "91     \t [1.32553206 0.05199988]. \t  -290.8205294941111 \t -0.023396993412763373\n",
            "92     \t [-0.83237225  0.07587352]. \t  -41.422790641693126 \t -0.023396993412763373\n",
            "93     \t [-1.85018417  1.29794549]. \t  -459.7863391163911 \t -0.023396993412763373\n",
            "94     \t [1.53159452 0.88600749]. \t  -213.37668811417538 \t -0.023396993412763373\n",
            "95     \t [1.06770387 1.46676907]. \t  -10.682938992362306 \t -0.023396993412763373\n",
            "96     \t [ 1.52335898 -1.74116497]. \t  -1650.0857154121345 \t -0.023396993412763373\n",
            "97     \t [1.10675821 0.2211871 ]. \t  -100.75811175632232 \t -0.023396993412763373\n",
            "98     \t [ 0.83029518 -0.30582201]. \t  -99.07350907469653 \t -0.023396993412763373\n",
            "99     \t [-0.87332185 -0.51619905]. \t  -167.06532525331025 \t -0.023396993412763373\n",
            "100    \t [-1.01396943 -1.98510142]. \t  -912.0148419647545 \t -0.023396993412763373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "any0xrgYuvxA",
        "outputId": "50403527-3628-46e7-de90-58886939a0a2"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_winner_16 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_16 = dGPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.49073237 -0.75216486]. \t  -884.9748202573794 \t -21.690996320546372\n",
            "init   \t [0.70512959 0.03240619]. \t  -21.690996320546372 \t -21.690996320546372\n",
            "init   \t [ 1.15368111 -0.88603985]. \t  -491.54136063574043 \t -21.690996320546372\n",
            "init   \t [-1.09072882  0.26132352]. \t  -90.5574614167091 \t -21.690996320546372\n",
            "init   \t [1.5360998 0.8967902]. \t  -214.26941031258198 \t -21.690996320546372\n",
            "1      \t [ 0.0221755  -1.04001206]. \t  -109.2209581414894 \t -21.690996320546372\n",
            "2      \t [0.28166219 0.5277335 ]. \t  \u001b[92m-20.622257333786692\u001b[0m \t -20.622257333786692\n",
            "3      \t [ 0.27555307 -0.25605044]. \t  \u001b[92m-11.545891184964141\u001b[0m \t -11.545891184964141\n",
            "4      \t [-1.78845141  0.51764158]. \t  -726.5069893944108 \t -11.545891184964141\n",
            "5      \t [0.32507987 0.15261681]. \t  \u001b[92m-0.6758524923354431\u001b[0m \t -0.6758524923354431\n",
            "6      \t [-0.82614884 -0.14332823]. \t  -71.53766591248872 \t -0.6758524923354431\n",
            "7      \t [-0.61523221  0.69745535]. \t  -12.781545967515331 \t -0.6758524923354431\n",
            "8      \t [-0.58007505  0.28662699]. \t  -2.7452398147394588 \t -0.6758524923354431\n",
            "9      \t [0.46104797 0.16300705]. \t  \u001b[92m-0.5360706947107305\u001b[0m \t -0.5360706947107305\n",
            "10     \t [ 0.18654973 -1.88936772]. \t  -370.9041517205095 \t -0.5360706947107305\n",
            "11     \t [ 0.4539389  -0.06316739]. \t  -7.546549610101558 \t -0.5360706947107305\n",
            "12     \t [-0.39117843  0.41473628]. \t  -8.784889160820187 \t -0.5360706947107305\n",
            "13     \t [-0.60624986  1.95217234]. \t  -253.68635664484853 \t -0.5360706947107305\n",
            "14     \t [1.21711595 0.46453086]. \t  -103.44357277888115 \t -0.5360706947107305\n",
            "15     \t [0.44758652 1.78406243]. \t  -251.12483395628925 \t -0.5360706947107305\n",
            "16     \t [-1.52930951 -1.01038496]. \t  -1128.0930833336422 \t -0.5360706947107305\n",
            "17     \t [0.42811862 0.19662396]. \t  \u001b[92m-0.34483962324341405\u001b[0m \t -0.34483962324341405\n",
            "18     \t [1.28959433 1.43567933]. \t  -5.2537672055258255 \t -0.34483962324341405\n",
            "19     \t [1.27826058 1.87512765]. \t  -5.894089219687638 \t -0.34483962324341405\n",
            "20     \t [1.08800789 1.40419291]. \t  -4.866760439273172 \t -0.34483962324341405\n",
            "21     \t [0.68986958 1.53523052]. \t  -112.31005072817484 \t -0.34483962324341405\n",
            "22     \t [1.30662607 1.51424832]. \t  -3.8198218274145526 \t -0.34483962324341405\n",
            "23     \t [1.42280002 1.98239679]. \t  -0.3548500653818869 \t -0.34483962324341405\n",
            "24     \t [1.27997698 1.61664425]. \t  \u001b[92m-0.12546229776455867\u001b[0m \t -0.12546229776455867\n",
            "25     \t [0.2693997  1.07008029]. \t  -100.03521822356028 \t -0.12546229776455867\n",
            "26     \t [-0.28520894  1.99131112]. \t  -366.44914700758426 \t -0.12546229776455867\n",
            "27     \t [1.25982139 1.62680629]. \t  -0.22476985482796866 \t -0.12546229776455867\n",
            "28     \t [-1.99274381  0.30019054]. \t  -1356.4611941865257 \t -0.12546229776455867\n",
            "29     \t [-1.59732332  1.62182361]. \t  -93.16508290623544 \t -0.12546229776455867\n",
            "30     \t [-0.73576266  0.5700587 ]. \t  -3.095309977743792 \t -0.12546229776455867\n",
            "31     \t [-1.07096146  1.12416355]. \t  -4.340842151201662 \t -0.12546229776455867\n",
            "32     \t [-0.40342255 -1.92282347]. \t  -436.9311602155533 \t -0.12546229776455867\n",
            "33     \t [1.0818662  1.24192418]. \t  -0.5177798378237812 \t -0.12546229776455867\n",
            "34     \t [0.71847905 0.42270834]. \t  -0.953550266626682 \t -0.12546229776455867\n",
            "35     \t [-0.63146643 -1.64020932]. \t  -418.3971345771591 \t -0.12546229776455867\n",
            "36     \t [ 0.23034354 -0.58857712]. \t  -41.761952530407115 \t -0.12546229776455867\n",
            "37     \t [ 0.0871563  -0.03197559]. \t  -0.9898764260466782 \t -0.12546229776455867\n",
            "38     \t [-1.12743965  1.91984752]. \t  -46.610718111637404 \t -0.12546229776455867\n",
            "39     \t [-1.69585202 -1.39566601]. \t  -1831.9072668479507 \t -0.12546229776455867\n",
            "40     \t [ 0.07572925 -2.0285615 ]. \t  -414.69046851086 \t -0.12546229776455867\n",
            "41     \t [-0.33921788  2.02645588]. \t  -367.13357261498413 \t -0.12546229776455867\n",
            "42     \t [-1.52470154  1.98994895]. \t  -17.580934212208746 \t -0.12546229776455867\n",
            "43     \t [ 1.27618886 -1.3273556 ]. \t  -873.8779230470094 \t -0.12546229776455867\n",
            "44     \t [0.94399131 0.82716499]. \t  -0.4121562792267474 \t -0.12546229776455867\n",
            "45     \t [0.85333392 0.7578542 ]. \t  \u001b[92m-0.10957400319243078\u001b[0m \t -0.10957400319243078\n",
            "46     \t [0.90102783 0.7714231 ]. \t  -0.17323828615135878 \t -0.10957400319243078\n",
            "47     \t [-0.22066691 -0.38020652]. \t  -19.885583106297887 \t -0.10957400319243078\n",
            "48     \t [0.87601409 0.71501875]. \t  -0.28975918126381606 \t -0.10957400319243078\n",
            "49     \t [0.85233837 0.74226991]. \t  \u001b[92m-0.046733866659147756\u001b[0m \t -0.046733866659147756\n",
            "50     \t [0.83676945 0.64671287]. \t  -0.3125509317796125 \t -0.046733866659147756\n",
            "51     \t [-1.64505925  0.84496156]. \t  -353.4246094702246 \t -0.046733866659147756\n",
            "52     \t [-0.67517123 -1.00998659]. \t  -217.67570332419444 \t -0.046733866659147756\n",
            "53     \t [-1.27228905 -1.86451717]. \t  -1218.457017748896 \t -0.046733866659147756\n",
            "54     \t [ 0.82517819 -1.33677215]. \t  -407.1383404308359 \t -0.046733866659147756\n",
            "55     \t [-0.64063064 -1.4676662 ]. \t  -355.40779388843447 \t -0.046733866659147756\n",
            "56     \t [ 0.04506434 -0.00781415]. \t  -0.9215944074532564 \t -0.046733866659147756\n",
            "57     \t [-1.20340356  0.65695278]. \t  -67.45905943685975 \t -0.046733866659147756\n",
            "58     \t [ 1.74567009 -1.89656171]. \t  -2444.796244157968 \t -0.046733866659147756\n",
            "59     \t [0.02346288 1.9066815 ]. \t  -364.2871597262516 \t -0.046733866659147756\n",
            "60     \t [0.00200424 1.56567155]. \t  -246.12747811354427 \t -0.046733866659147756\n",
            "61     \t [-0.78139034  0.53479172]. \t  -3.747599573318423 \t -0.046733866659147756\n",
            "62     \t [ 1.35482936 -0.40326941]. \t  -501.3627715286027 \t -0.046733866659147756\n",
            "63     \t [-0.19401555 -0.5657402 ]. \t  -37.83268513239098 \t -0.046733866659147756\n",
            "64     \t [0.83828017 0.67834968]. \t  -0.08551354649332034 \t -0.046733866659147756\n",
            "65     \t [-0.35956237  1.98789795]. \t  -347.292581304873 \t -0.046733866659147756\n",
            "66     \t [-1.5604096  -1.72930828]. \t  -1740.6005414601545 \t -0.046733866659147756\n",
            "67     \t [-0.51937017 -1.56237223]. \t  -337.97397823813054 \t -0.046733866659147756\n",
            "68     \t [ 1.47032429 -0.11759852]. \t  -519.8113640209032 \t -0.046733866659147756\n",
            "69     \t [1.29801797 1.061399  ]. \t  -38.95801006777799 \t -0.046733866659147756\n",
            "70     \t [ 1.27704013 -0.05560941]. \t  -284.48504812707824 \t -0.046733866659147756\n",
            "71     \t [-0.01455274  1.61486514]. \t  -261.7398640546982 \t -0.046733866659147756\n",
            "72     \t [-0.20521007 -0.20636756]. \t  -7.626699547511159 \t -0.046733866659147756\n",
            "73     \t [1.25918494 1.5972388 ]. \t  -0.08084734349879602 \t -0.046733866659147756\n",
            "74     \t [1.17001987 1.3447501 ]. \t  -0.08745329084247008 \t -0.046733866659147756\n",
            "75     \t [-1.33548973  0.16839956]. \t  -266.3200555030319 \t -0.046733866659147756\n",
            "76     \t [-1.13398124  1.03745487]. \t  -10.727043227510059 \t -0.046733866659147756\n",
            "77     \t [-1.27488388  0.72116571]. \t  -86.92620525398058 \t -0.046733866659147756\n",
            "78     \t [-0.0209748   0.02390357]. \t  -1.097443739468415 \t -0.046733866659147756\n",
            "79     \t [ 1.95609107 -0.37593049]. \t  -1766.7817341777504 \t -0.046733866659147756\n",
            "80     \t [1.91253661 0.98388592]. \t  -715.8123979341622 \t -0.046733866659147756\n",
            "81     \t [-1.97445608 -2.00286589]. \t  -3491.4319350150254 \t -0.046733866659147756\n",
            "82     \t [ 1.40973898 -1.13316608]. \t  -973.9386714775475 \t -0.046733866659147756\n",
            "83     \t [0.6419498  0.45756176]. \t  -0.3348812845835578 \t -0.046733866659147756\n",
            "84     \t [ 0.51738178 -0.67274971]. \t  -88.67445975258771 \t -0.046733866659147756\n",
            "85     \t [-0.72281118 -1.29658803]. \t  -333.8601983413895 \t -0.046733866659147756\n",
            "86     \t [0.56832542 0.35812604]. \t  -0.3097705252017462 \t -0.046733866659147756\n",
            "87     \t [ 0.52141901 -1.73419679]. \t  -402.66255897447763 \t -0.046733866659147756\n",
            "88     \t [0.49652896 0.24105326]. \t  -0.256494624176258 \t -0.046733866659147756\n",
            "89     \t [0.12518397 0.9027846 ]. \t  -79.46235216551702 \t -0.046733866659147756\n",
            "90     \t [ 0.27307719 -1.38925742]. \t  -214.80782634268718 \t -0.046733866659147756\n",
            "91     \t [1.04560963 1.57958228]. \t  -23.64917436500178 \t -0.046733866659147756\n",
            "92     \t [-0.07028959 -1.58607329]. \t  -254.27804647879904 \t -0.046733866659147756\n",
            "93     \t [-1.5765528  -1.26771701]. \t  -1415.3164776664685 \t -0.046733866659147756\n",
            "94     \t [-0.93985372  1.36091761]. \t  -26.572501212386467 \t -0.046733866659147756\n",
            "95     \t [0.92239243 0.85728956]. \t  \u001b[92m-0.010224262640372973\u001b[0m \t -0.010224262640372973\n",
            "96     \t [ 0.61478593 -0.26500832]. \t  -41.48944069562792 \t -0.010224262640372973\n",
            "97     \t [-1.16529491  1.41904107]. \t  -5.062175677754481 \t -0.010224262640372973\n",
            "98     \t [1.03446245 1.09305449]. \t  -0.05382082520092298 \t -0.010224262640372973\n",
            "99     \t [-1.45038161  1.64952399]. \t  -26.623491401680013 \t -0.010224262640372973\n",
            "100    \t [-1.31115942  1.94872672]. \t  -10.612508665163395 \t -0.010224262640372973\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reLyKt6Quvzx",
        "outputId": "cbda14dd-c07b-4b52-f043-f51de393013b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_winner_17 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_17 = dGPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.62187122 0.94420838]. \t  -31.22188590191926 \t -31.22188590191926\n",
            "init   \t [-1.25596728  0.08059797]. \t  -229.14713198618372 \t -31.22188590191926\n",
            "init   \t [0.72246586 1.2470062 ]. \t  -52.64667111223406 \t -31.22188590191926\n",
            "init   \t [0.13726869 1.671308  ]. \t  -273.80846401311214 \t -31.22188590191926\n",
            "init   \t [-0.2315013  -0.99953783]. \t  -112.42501801714917 \t -31.22188590191926\n",
            "1      \t [1.59286487 0.71471414]. \t  -332.50370461464615 \t -31.22188590191926\n",
            "2      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -31.22188590191926\n",
            "3      \t [ 0.21355149 -0.51660098]. \t  -32.22597260970069 \t -31.22188590191926\n",
            "4      \t [ 0.64825441 -1.81190541]. \t  -498.3682616283546 \t -31.22188590191926\n",
            "5      \t [-0.38794126 -0.39011942]. \t  \u001b[92m-31.153145252836968\u001b[0m \t -31.153145252836968\n",
            "6      \t [-0.17812229 -0.48890719]. \t  \u001b[92m-28.494024926001096\u001b[0m \t -28.494024926001096\n",
            "7      \t [ 0.70483753 -0.42345124]. \t  -84.77260802886116 \t -28.494024926001096\n",
            "8      \t [-0.36019575  0.00688765]. \t  \u001b[92m-3.359426631916929\u001b[0m \t -3.359426631916929\n",
            "9      \t [-0.29466783  0.15910547]. \t  \u001b[92m-2.1985517902925427\u001b[0m \t -2.1985517902925427\n",
            "10     \t [-0.69124827  1.83963183]. \t  -188.31232989194442 \t -2.1985517902925427\n",
            "11     \t [-1.6839319   1.98420797]. \t  -79.69486468267328 \t -2.1985517902925427\n",
            "12     \t [-0.42964963  0.16360892]. \t  \u001b[92m-2.0879556211317873\u001b[0m \t -2.0879556211317873\n",
            "13     \t [-0.37678693  0.07143212]. \t  -2.3930788482672045 \t -2.0879556211317873\n",
            "14     \t [-0.39346024  0.1290425 ]. \t  \u001b[92m-2.008132752890706\u001b[0m \t -2.008132752890706\n",
            "15     \t [-0.20917042  1.10710212]. \t  -114.53338518007739 \t -2.008132752890706\n",
            "16     \t [-1.64824302 -0.21314914]. \t  -865.4177536465476 \t -2.008132752890706\n",
            "17     \t [1.96537777 1.92832177]. \t  -375.1176465859105 \t -2.008132752890706\n",
            "18     \t [0.04751241 0.075279  ]. \t  \u001b[92m-1.4404475611620164\u001b[0m \t -1.4404475611620164\n",
            "19     \t [ 0.59840127 -1.07659765]. \t  -205.99245030304718 \t -1.4404475611620164\n",
            "20     \t [-1.41432614  1.892902  ]. \t  -6.982799998635764 \t -1.4404475611620164\n",
            "21     \t [-0.83945271  0.63726742]. \t  -3.8380433099295144 \t -1.4404475611620164\n",
            "22     \t [-1.46623226  1.26290266]. \t  -84.74755909735492 \t -1.4404475611620164\n",
            "23     \t [-1.98012465  2.04178599]. \t  -361.9857001577987 \t -1.4404475611620164\n",
            "24     \t [-1.43832731  0.14715611]. \t  -375.21137132953044 \t -1.4404475611620164\n",
            "25     \t [0.85774365 0.02614855]. \t  -50.36999313130531 \t -1.4404475611620164\n",
            "26     \t [0.3504058  0.11049111]. \t  \u001b[92m-0.43708468919607324\u001b[0m \t -0.43708468919607324\n",
            "27     \t [-0.52252074 -0.35167814]. \t  -41.34383566617724 \t -0.43708468919607324\n",
            "28     \t [1.68058629 1.72362185]. \t  -121.62790411263364 \t -0.43708468919607324\n",
            "29     \t [1.23312571 1.57305317]. \t  \u001b[92m-0.3294913902273593\u001b[0m \t -0.3294913902273593\n",
            "30     \t [-1.31910352  1.21186882]. \t  -33.27409734432689 \t -0.3294913902273593\n",
            "31     \t [-1.36261297  2.03097554]. \t  -8.618645234195423 \t -0.3294913902273593\n",
            "32     \t [1.12694487 1.22814728]. \t  \u001b[92m-0.19131962242396847\u001b[0m \t -0.19131962242396847\n",
            "33     \t [1.52138779 0.29329189]. \t  -408.84890386898417 \t -0.19131962242396847\n",
            "34     \t [1.74080353 1.54362102]. \t  -221.59905021424652 \t -0.19131962242396847\n",
            "35     \t [1.38712502 1.91055307]. \t  \u001b[92m-0.16826063219756351\u001b[0m \t -0.16826063219756351\n",
            "36     \t [-0.63748997 -1.38065512]. \t  -322.03563613815305 \t -0.16826063219756351\n",
            "37     \t [0.86274726 0.65902026]. \t  -0.7466619355230654 \t -0.16826063219756351\n",
            "38     \t [-0.35326329 -0.03337889]. \t  -4.333218142660941 \t -0.16826063219756351\n",
            "39     \t [-0.93743739 -1.51691736]. \t  -577.6944968849152 \t -0.16826063219756351\n",
            "40     \t [0.77914003 0.58874034]. \t  \u001b[92m-0.08233711756889409\u001b[0m \t -0.08233711756889409\n",
            "41     \t [1.3742869  1.77914753]. \t  -1.33948708109105 \t -0.08233711756889409\n",
            "42     \t [-1.44728885  0.73940159]. \t  -189.65769942715474 \t -0.08233711756889409\n",
            "43     \t [0.76938663 0.51280121]. \t  -0.6797270951710976 \t -0.08233711756889409\n",
            "44     \t [-0.99610818  0.19881197]. \t  -66.93590511449064 \t -0.08233711756889409\n",
            "45     \t [0.96628079 1.0211674 ]. \t  -0.7662166409161495 \t -0.08233711756889409\n",
            "46     \t [0.70464882 0.38703218]. \t  -1.2862089160735926 \t -0.08233711756889409\n",
            "47     \t [ 0.15483644 -1.32420254]. \t  -182.47238793527598 \t -0.08233711756889409\n",
            "48     \t [ 1.47443898 -1.28631293]. \t  -1197.5811107866384 \t -0.08233711756889409\n",
            "49     \t [0.59499947 0.2787695 ]. \t  -0.7303549216802554 \t -0.08233711756889409\n",
            "50     \t [ 0.80102667 -0.25698711]. \t  -80.79332915429475 \t -0.08233711756889409\n",
            "51     \t [1.73420509 1.46014498]. \t  -239.95969685251026 \t -0.08233711756889409\n",
            "52     \t [-1.81682755 -1.25327162]. \t  -2081.9481213711797 \t -0.08233711756889409\n",
            "53     \t [-0.16982153  1.3650376 ]. \t  -179.91105836582113 \t -0.08233711756889409\n",
            "54     \t [-0.5832036  -0.39844562]. \t  -57.055402148986836 \t -0.08233711756889409\n",
            "55     \t [-1.9677794  0.3980393]. \t  -1215.756244906381 \t -0.08233711756889409\n",
            "56     \t [ 0.3935953  -1.31480281]. \t  -216.3754355014879 \t -0.08233711756889409\n",
            "57     \t [-1.9069261   0.97514391]. \t  -716.6611417020337 \t -0.08233711756889409\n",
            "58     \t [-1.39981897  1.24181038]. \t  -57.265984360090215 \t -0.08233711756889409\n",
            "59     \t [ 0.00784076 -0.97890301]. \t  -96.821526355202 \t -0.08233711756889409\n",
            "60     \t [-1.12769857  1.00116836]. \t  -11.846057670026436 \t -0.08233711756889409\n",
            "61     \t [1.7954402  0.87939316]. \t  -550.1658746391516 \t -0.08233711756889409\n",
            "62     \t [-1.8409285   0.12421796]. \t  -1073.9626300129419 \t -0.08233711756889409\n",
            "63     \t [0.95663178 1.56151027]. \t  -41.78076889114262 \t -0.08233711756889409\n",
            "64     \t [0.95460469 0.95076865]. \t  -0.15807421651599768 \t -0.08233711756889409\n",
            "65     \t [-1.34141841 -0.96070529]. \t  -767.3022143375099 \t -0.08233711756889409\n",
            "66     \t [1.13036921 1.22851349]. \t  -0.259267473751217 \t -0.08233711756889409\n",
            "67     \t [-1.90526878  1.91562327]. \t  -302.36618322749564 \t -0.08233711756889409\n",
            "68     \t [-1.8590975  -0.03748674]. \t  -1228.789541338643 \t -0.08233711756889409\n",
            "69     \t [1.50847337 0.24772284]. \t  -411.4432843590957 \t -0.08233711756889409\n",
            "70     \t [-0.03744522  1.13452397]. \t  -129.47279988809532 \t -0.08233711756889409\n",
            "71     \t [0.25774702 1.44265543]. \t  -189.94961304576694 \t -0.08233711756889409\n",
            "72     \t [1.65108357 1.7624896 ]. \t  -93.27396602248619 \t -0.08233711756889409\n",
            "73     \t [-0.8309511   0.38541964]. \t  -12.65854777429919 \t -0.08233711756889409\n",
            "74     \t [-0.62039625 -0.28322156]. \t  -47.26318975428057 \t -0.08233711756889409\n",
            "75     \t [1.58093254 0.269114  ]. \t  -497.73172264831635 \t -0.08233711756889409\n",
            "76     \t [-1.88671546  1.04551085]. \t  -640.4454340967029 \t -0.08233711756889409\n",
            "77     \t [-1.10915018  0.20551825]. \t  -109.44867834043086 \t -0.08233711756889409\n",
            "78     \t [1.31707692 1.67360377]. \t  -0.47371031095915356 \t -0.08233711756889409\n",
            "79     \t [0.58698548 0.34859448]. \t  -0.1722152018197744 \t -0.08233711756889409\n",
            "80     \t [1.37573708 1.87876385]. \t  -0.16046785456753926 \t -0.08233711756889409\n",
            "81     \t [0.3186804  1.09882346]. \t  -99.9181963257552 \t -0.08233711756889409\n",
            "82     \t [1.94725409 1.95571794]. \t  -338.01646995656824 \t -0.08233711756889409\n",
            "83     \t [ 0.75218183 -1.40652874]. \t  -389.0606101266476 \t -0.08233711756889409\n",
            "84     \t [1.03799912 1.0927165 ]. \t  \u001b[92m-0.02477441638079696\u001b[0m \t -0.02477441638079696\n",
            "85     \t [1.942115   1.69760421]. \t  -431.12082590076807 \t -0.02477441638079696\n",
            "86     \t [-0.74083596 -1.65416337]. \t  -488.351976952918 \t -0.02477441638079696\n",
            "87     \t [-0.45545945  1.64796546]. \t  -209.62876938362476 \t -0.02477441638079696\n",
            "88     \t [-0.56896611 -0.73121945]. \t  -113.7518930266233 \t -0.02477441638079696\n",
            "89     \t [1.27427815 0.38180957]. \t  -154.3254793068761 \t -0.02477441638079696\n",
            "90     \t [0.46906583 0.20677824]. \t  -0.2994328043313553 \t -0.02477441638079696\n",
            "91     \t [ 0.6677092  -1.23963205]. \t  -284.19052973424647 \t -0.02477441638079696\n",
            "92     \t [-1.00700917 -1.26368828]. \t  -522.8452122917694 \t -0.02477441638079696\n",
            "93     \t [0.16015336 0.24262124]. \t  -5.413033502014389 \t -0.02477441638079696\n",
            "94     \t [0.41485172 0.16677226]. \t  -0.3452390673676354 \t -0.02477441638079696\n",
            "95     \t [1.23858829 1.51162788]. \t  -0.1074281933194568 \t -0.02477441638079696\n",
            "96     \t [-1.91187252 -1.00609437]. \t  -2181.2982218772822 \t -0.02477441638079696\n",
            "97     \t [-0.32671966 -0.20386439]. \t  -11.40804985655089 \t -0.02477441638079696\n",
            "98     \t [1.39696587 0.04097297]. \t  -365.1741489099554 \t -0.02477441638079696\n",
            "99     \t [1.676243   1.94336126]. \t  -75.52728652939464 \t -0.02477441638079696\n",
            "100    \t [0.53826131 1.57679642]. \t  -165.8684252955217 \t -0.02477441638079696\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI3FtfYSuv2u",
        "outputId": "03482d16-f949-4180-a490-cf08f55a7668"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_winner_18 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_18 = dGPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.52956683 1.91914255]. \t  -17.95675982372887 \t -1.7663579664225912\n",
            "init   \t [1.51222084 0.12638491]. \t  -467.0068292530542 \t -1.7663579664225912\n",
            "init   \t [-1.09474477 -2.0013105 ]. \t  -1028.244988519411 \t -1.7663579664225912\n",
            "init   \t [-0.28479972 -0.39996883]. \t  -24.79447905426233 \t -1.7663579664225912\n",
            "init   \t [ 0.09287545 -0.0885072 ]. \t  -1.7663579664225912 \t -1.7663579664225912\n",
            "1      \t [-0.56561635  0.34366894]. \t  -2.5075469529388807 \t -1.7663579664225912\n",
            "2      \t [-0.41546317  0.04330888]. \t  -3.6754048035136746 \t -1.7663579664225912\n",
            "3      \t [-0.85672851  1.72981318]. \t  -102.61506778668074 \t -1.7663579664225912\n",
            "4      \t [-0.10405602 -0.13747987]. \t  -3.4184517976836686 \t -1.7663579664225912\n",
            "5      \t [-1.73237864  0.20984619]. \t  -786.5956329758978 \t -1.7663579664225912\n",
            "6      \t [0.11389811 1.10943107]. \t  -121.00725473566958 \t -1.7663579664225912\n",
            "7      \t [ 0.6947227  -1.07184342]. \t  -241.73495012602763 \t -1.7663579664225912\n",
            "8      \t [2.048 2.048]. \t  -461.7603900415999 \t -1.7663579664225912\n",
            "9      \t [0.53309287 2.01087946]. \t  -298.3643391813583 \t -1.7663579664225912\n",
            "10     \t [0.96519081 1.26743424]. \t  -11.28012472960955 \t -1.7663579664225912\n",
            "11     \t [1.27538046 1.64250163]. \t  \u001b[92m-0.10113550929231815\u001b[0m \t -0.10113550929231815\n",
            "12     \t [1.1546634  2.00632967]. \t  -45.327871966625 \t -0.10113550929231815\n",
            "13     \t [0.75905672 0.54230317]. \t  -0.17273030588447563 \t -0.10113550929231815\n",
            "14     \t [0.78081075 0.70259922]. \t  -0.9117128000395501 \t -0.10113550929231815\n",
            "15     \t [0.46666461 0.20274557]. \t  -0.3070376017666364 \t -0.10113550929231815\n",
            "16     \t [ 0.25679727 -0.18639741]. \t  -6.920011424501563 \t -0.10113550929231815\n",
            "17     \t [1.32219484 1.95393164]. \t  -4.336393749286235 \t -0.10113550929231815\n",
            "18     \t [1.10074719 0.92325376]. \t  -8.327064757364456 \t -0.10113550929231815\n",
            "19     \t [1.37434932 1.73314903]. \t  -2.563982707406325 \t -0.10113550929231815\n",
            "20     \t [1.127355   1.16897293]. \t  -1.0557294613777106 \t -0.10113550929231815\n",
            "21     \t [-0.62410421  0.5476889 ]. \t  -5.139895165473294 \t -0.10113550929231815\n",
            "22     \t [-2.02930292  1.98301413]. \t  -465.02316825817167 \t -0.10113550929231815\n",
            "23     \t [ 1.78433219 -1.76828436]. \t  -2452.970093434316 \t -0.10113550929231815\n",
            "24     \t [ 0.07986278 -1.15280063]. \t  -135.2161764329294 \t -0.10113550929231815\n",
            "25     \t [-0.31433063  0.28998958]. \t  -5.38266739351212 \t -0.10113550929231815\n",
            "26     \t [-1.02894882 -0.67536524]. \t  -304.8272344430442 \t -0.10113550929231815\n",
            "27     \t [0.48191401 0.2528516 ]. \t  -0.31089232357555585 \t -0.10113550929231815\n",
            "28     \t [0.46668987 0.24522742]. \t  -0.3596491578126802 \t -0.10113550929231815\n",
            "29     \t [ 1.30212754 -1.04704589]. \t  -752.2668928051232 \t -0.10113550929231815\n",
            "30     \t [0.76064417 0.59052837]. \t  \u001b[92m-0.07156861919117774\u001b[0m \t -0.07156861919117774\n",
            "31     \t [0.8055815  0.65344732]. \t  \u001b[92m-0.039810758805343634\u001b[0m \t -0.039810758805343634\n",
            "32     \t [0.69288025 0.48135216]. \t  -0.09448360964924454 \t -0.039810758805343634\n",
            "33     \t [-0.3086102 -1.5223783]. \t  -263.3814376804697 \t -0.039810758805343634\n",
            "34     \t [ 0.70410932 -2.03075123]. \t  -638.4184743565811 \t -0.039810758805343634\n",
            "35     \t [0.54120142 1.47148362]. \t  -139.1166704926515 \t -0.039810758805343634\n",
            "36     \t [-0.74895875  1.57222134]. \t  -105.32801185836587 \t -0.039810758805343634\n",
            "37     \t [0.90857371 0.80292142]. \t  -0.059365931880634784 \t -0.039810758805343634\n",
            "38     \t [1.8550285  0.74680791]. \t  -726.6686225385389 \t -0.039810758805343634\n",
            "39     \t [-1.88536862  2.00180992]. \t  -249.44566312940998 \t -0.039810758805343634\n",
            "40     \t [0.79611724 0.59266256]. \t  -0.2108189530356469 \t -0.039810758805343634\n",
            "41     \t [-1.21483941  1.74215671]. \t  -11.998250232318203 \t -0.039810758805343634\n",
            "42     \t [-1.26947753  2.02696633]. \t  -22.40567283384248 \t -0.039810758805343634\n",
            "43     \t [0.51472498 0.82805609]. \t  -31.945262563754127 \t -0.039810758805343634\n",
            "44     \t [-0.51307061  0.77787495]. \t  -28.774146598562776 \t -0.039810758805343634\n",
            "45     \t [ 0.34533537 -0.98658915]. \t  -122.71805110077798 \t -0.039810758805343634\n",
            "46     \t [1.09059446 1.13807512]. \t  -0.2715933930879733 \t -0.039810758805343634\n",
            "47     \t [0.86577071 0.70171215]. \t  -0.2469488746458518 \t -0.039810758805343634\n",
            "48     \t [-0.33344666  1.70336935]. \t  -255.28264766624903 \t -0.039810758805343634\n",
            "49     \t [1.39236348 1.95887501]. \t  -0.19474890023561603 \t -0.039810758805343634\n",
            "50     \t [0.97561873 0.96841336]. \t  \u001b[92m-0.02808890586959634\u001b[0m \t -0.02808890586959634\n",
            "51     \t [ 0.22431893 -0.01304156]. \t  -1.0031369957016563 \t -0.02808890586959634\n",
            "52     \t [1.61404428 1.32600028]. \t  -163.99662342852554 \t -0.02808890586959634\n",
            "53     \t [-1.94029947 -0.86847316]. \t  -2155.3322056729285 \t -0.02808890586959634\n",
            "54     \t [0.07464172 1.83765711]. \t  -336.51009897136794 \t -0.02808890586959634\n",
            "55     \t [0.51311545 0.0998633 ]. \t  -2.9078022004534447 \t -0.02808890586959634\n",
            "56     \t [-0.9724487  -0.53938542]. \t  -224.42549533519733 \t -0.02808890586959634\n",
            "57     \t [1.03697633 1.0319875 ]. \t  -0.18913702011945493 \t -0.02808890586959634\n",
            "58     \t [1.00713096 0.99768186]. \t  \u001b[92m-0.02770957987774812\u001b[0m \t -0.02770957987774812\n",
            "59     \t [0.32940244 0.11433151]. \t  -0.4530947858990708 \t -0.02770957987774812\n",
            "60     \t [ 0.66227292 -0.70944906]. \t  -131.9169686429098 \t -0.02770957987774812\n",
            "61     \t [ 1.91918506 -0.9411309 ]. \t  -2139.354445757398 \t -0.02770957987774812\n",
            "62     \t [ 0.61530074 -0.04220243]. \t  -17.855040930280474 \t -0.02770957987774812\n",
            "63     \t [-1.26278418  1.72912074]. \t  -6.929132761748157 \t -0.02770957987774812\n",
            "64     \t [1.08652946 1.22040262]. \t  -0.16634022798350892 \t -0.02770957987774812\n",
            "65     \t [-1.3258731  -1.69927707]. \t  -1200.6443181943291 \t -0.02770957987774812\n",
            "66     \t [-1.41201004  1.98746144]. \t  -5.8217751542311165 \t -0.02770957987774812\n",
            "67     \t [-1.22702788  1.40008233]. \t  -6.072996592598146 \t -0.02770957987774812\n",
            "68     \t [1.25395039 1.49810155]. \t  -0.6163916927328872 \t -0.02770957987774812\n",
            "69     \t [-1.26602996  1.28388001]. \t  -15.30791956683049 \t -0.02770957987774812\n",
            "70     \t [-1.92633866 -2.01001904]. \t  -3281.3183646001717 \t -0.02770957987774812\n",
            "71     \t [-1.66391243 -0.01085516]. \t  -779.6360682164172 \t -0.02770957987774812\n",
            "72     \t [-0.84373096  1.94458006]. \t  -155.35381258533093 \t -0.02770957987774812\n",
            "73     \t [-1.84375537 -1.86727236]. \t  -2781.9064022820107 \t -0.02770957987774812\n",
            "74     \t [-0.78666941 -1.10787842]. \t  -301.3508649798184 \t -0.02770957987774812\n",
            "75     \t [0.33148238 0.90152338]. \t  -63.11674969328898 \t -0.02770957987774812\n",
            "76     \t [1.6702811  1.70236723]. \t  -118.70875420772408 \t -0.02770957987774812\n",
            "77     \t [ 0.11668496 -1.96729657]. \t  -393.1814597329686 \t -0.02770957987774812\n",
            "78     \t [-0.88247338  2.04742146]. \t  -164.49408042071573 \t -0.02770957987774812\n",
            "79     \t [-0.43549238 -2.01846151]. \t  -489.6378795741759 \t -0.02770957987774812\n",
            "80     \t [-0.01091106  0.37740414]. \t  -15.256345137726763 \t -0.02770957987774812\n",
            "81     \t [-0.1208149   1.48729322]. \t  -218.1398643921702 \t -0.02770957987774812\n",
            "82     \t [1.05681225 1.10630803]. \t  \u001b[92m-0.014345462707764456\u001b[0m \t -0.014345462707764456\n",
            "83     \t [0.03857045 1.98165671]. \t  -393.0312840335908 \t -0.014345462707764456\n",
            "84     \t [1.20764837 0.79814724]. \t  -43.63841506633272 \t -0.014345462707764456\n",
            "85     \t [1.14429545 1.27412903]. \t  -0.14531061234750747 \t -0.014345462707764456\n",
            "86     \t [ 0.61123068 -0.5270834 ]. \t  -81.27473123256354 \t -0.014345462707764456\n",
            "87     \t [-0.11353625  0.93397012]. \t  -86.07873293752077 \t -0.014345462707764456\n",
            "88     \t [1.37470161 1.92493544]. \t  -0.2638195169756034 \t -0.014345462707764456\n",
            "89     \t [1.04779559 1.08256261]. \t  -0.025733207977007115 \t -0.014345462707764456\n",
            "90     \t [1.38648903 1.95227109]. \t  -0.23888990096381357 \t -0.014345462707764456\n",
            "91     \t [1.12024044 1.27597448]. \t  -0.058708410729165106 \t -0.014345462707764456\n",
            "92     \t [-1.14294969 -0.48032313]. \t  -323.8066030564311 \t -0.014345462707764456\n",
            "93     \t [-0.87056293 -0.71276528]. \t  -219.7787060538371 \t -0.014345462707764456\n",
            "94     \t [1.58263691 1.77828318]. \t  -53.11335976600227 \t -0.014345462707764456\n",
            "95     \t [-0.3000354 -0.4367719]. \t  -29.441193255529072 \t -0.014345462707764456\n",
            "96     \t [ 1.6314697  -1.10691842]. \t  -1420.642250098064 \t -0.014345462707764456\n",
            "97     \t [1.03110902 1.07090098]. \t  \u001b[92m-0.0069201721416625875\u001b[0m \t -0.0069201721416625875\n",
            "98     \t [-0.97394672  1.79359404]. \t  -75.30265313975646 \t -0.0069201721416625875\n",
            "99     \t [ 2.00177263 -1.26549273]. \t  -2781.0202770904502 \t -0.0069201721416625875\n",
            "100    \t [-0.20113591  1.12016858]. \t  -118.02072811771028 \t -0.0069201721416625875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YT-CgKvuv4q",
        "outputId": "854a0be7-8cf1-4371-ec12-cedf1640ea97"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_winner_19 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_19 = dGPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.24348908 1.07107119]. \t  -102.94304782477323 \t -4.219752052396591\n",
            "init   \t [-1.04639875  1.11284007]. \t  -4.219752052396591 \t -4.219752052396591\n",
            "init   \t [-0.59608235  1.21527255]. \t  -76.50031976976825 \t -4.219752052396591\n",
            "init   \t [-0.63078391 -0.22989031]. \t  -42.07005958957044 \t -4.219752052396591\n",
            "init   \t [-0.8056685  -0.23710138]. \t  -81.79603430667063 \t -4.219752052396591\n",
            "1      \t [ 0.45286922 -0.60514617]. \t  -65.94770306515855 \t -4.219752052396591\n",
            "2      \t [-1.93213315  1.53183724]. \t  -493.17013433001347 \t -4.219752052396591\n",
            "3      \t [-0.99390962  0.63967956]. \t  -16.09838191871728 \t -4.219752052396591\n",
            "4      \t [1.39459744 1.35659518]. \t  -34.76620155291126 \t -4.219752052396591\n",
            "5      \t [-0.04916283 -0.21086558]. \t  -5.649687854516544 \t -4.219752052396591\n",
            "6      \t [-0.32412277 -1.46499727]. \t  -248.25989174797965 \t -4.219752052396591\n",
            "7      \t [1.01843798 0.8694239 ]. \t  \u001b[92m-2.8157559759541897\u001b[0m \t -2.8157559759541897\n",
            "8      \t [ 1.95586974 -1.35063865]. \t  -2680.078677029333 \t -2.8157559759541897\n",
            "9      \t [-0.11692609 -0.70341338]. \t  -52.66862695251201 \t -2.8157559759541897\n",
            "10     \t [0.75163017 1.26420933]. \t  -48.95834110843408 \t -2.8157559759541897\n",
            "11     \t [ 0.41181398 -0.11105879]. \t  -8.222379011561728 \t -2.8157559759541897\n",
            "12     \t [0.85073193 0.70907048]. \t  \u001b[92m-0.04381458802715288\u001b[0m \t -0.04381458802715288\n",
            "13     \t [-0.73934761  1.83511187]. \t  -169.04262503651273 \t -0.04381458802715288\n",
            "14     \t [-0.93491307  0.81555679]. \t  -4.086179869446733 \t -0.04381458802715288\n",
            "15     \t [-0.91779786  0.09679596]. \t  -59.26346567837915 \t -0.04381458802715288\n",
            "16     \t [ 1.22568239 -1.1696075 ]. \t  -713.9584663977281 \t -0.04381458802715288\n",
            "17     \t [ 0.53930203 -2.01031829]. \t  -529.748263488144 \t -0.04381458802715288\n",
            "18     \t [-0.04692476  0.09030717]. \t  -1.8723047250279936 \t -0.04381458802715288\n",
            "19     \t [0.86718777 0.38448612]. \t  -13.525359581074524 \t -0.04381458802715288\n",
            "20     \t [0.50911227 0.30819803]. \t  -0.48109753188399884 \t -0.04381458802715288\n",
            "21     \t [1.82696294 0.65763578]. \t  -719.0084573138294 \t -0.04381458802715288\n",
            "22     \t [-1.41872827 -1.02009759]. \t  -925.6909054350544 \t -0.04381458802715288\n",
            "23     \t [1.45933798 1.02393232]. \t  -122.47598556447716 \t -0.04381458802715288\n",
            "24     \t [-1.31971342 -1.08305348]. \t  -803.2723765145327 \t -0.04381458802715288\n",
            "25     \t [ 1.38242446 -0.1308308 ]. \t  -417.0933217978137 \t -0.04381458802715288\n",
            "26     \t [0.83889826 0.78763991]. \t  -0.7297005765149895 \t -0.04381458802715288\n",
            "27     \t [0.02151169 1.4571429 ]. \t  -213.1491434270894 \t -0.04381458802715288\n",
            "28     \t [-1.63856669  0.69331682]. \t  -403.60271179563733 \t -0.04381458802715288\n",
            "29     \t [1.37146333 1.6154494 ]. \t  -7.185006225482654 \t -0.04381458802715288\n",
            "30     \t [-0.20962842  0.03663851]. \t  -1.468538038697957 \t -0.04381458802715288\n",
            "31     \t [1.34120393 1.67891897]. \t  -1.554237016719226 \t -0.04381458802715288\n",
            "32     \t [1.37188993 1.95295716]. \t  -0.6406312629392612 \t -0.04381458802715288\n",
            "33     \t [-0.61375131  0.89657007]. \t  -29.631652197295683 \t -0.04381458802715288\n",
            "34     \t [1.31415594 1.90520542]. \t  -3.274203095798377 \t -0.04381458802715288\n",
            "35     \t [2.0381793 0.72216  ]. \t  -1178.9504173057226 \t -0.04381458802715288\n",
            "36     \t [0.55869531 0.78936277]. \t  -22.968863699515406 \t -0.04381458802715288\n",
            "37     \t [ 1.33652762 -1.41097198]. \t  -1022.3719497808307 \t -0.04381458802715288\n",
            "38     \t [-1.08552051  1.34509139]. \t  -7.129505654932904 \t -0.04381458802715288\n",
            "39     \t [0.26369973 0.12909139]. \t  -0.8968041217137773 \t -0.04381458802715288\n",
            "40     \t [ 0.959215  -1.3861733]. \t  -531.888274532088 \t -0.04381458802715288\n",
            "41     \t [ 0.963913   -1.67233212]. \t  -676.7609160489471 \t -0.04381458802715288\n",
            "42     \t [-0.24680134 -1.78126617]. \t  -340.9161515362546 \t -0.04381458802715288\n",
            "43     \t [-0.58202633 -0.52425901]. \t  -76.98206422017245 \t -0.04381458802715288\n",
            "44     \t [-0.93957858 -0.85981068]. \t  -307.43391881515123 \t -0.04381458802715288\n",
            "45     \t [-0.5335615   0.29151753]. \t  -2.3564752923496344 \t -0.04381458802715288\n",
            "46     \t [ 1.79677526 -1.32817717]. \t  -2076.875614620295 \t -0.04381458802715288\n",
            "47     \t [0.79281754 0.70307785]. \t  -0.5982207655329881 \t -0.04381458802715288\n",
            "48     \t [ 0.13267479 -0.04279656]. \t  -1.117058877927627 \t -0.04381458802715288\n",
            "49     \t [-0.24266514 -0.76140332]. \t  -68.8317344229052 \t -0.04381458802715288\n",
            "50     \t [ 1.02841419 -1.36419327]. \t  -586.5263844771054 \t -0.04381458802715288\n",
            "51     \t [1.4252979  0.46022507]. \t  -247.06323074062726 \t -0.04381458802715288\n",
            "52     \t [0.43844124 0.17957495]. \t  -0.3313650810216695 \t -0.04381458802715288\n",
            "53     \t [1.42875409 0.43741702]. \t  -257.4401579992734 \t -0.04381458802715288\n",
            "54     \t [1.50216865 0.12571883]. \t  -454.2795524743432 \t -0.04381458802715288\n",
            "55     \t [1.0164898  1.00177992]. \t  -0.09931801295078485 \t -0.04381458802715288\n",
            "56     \t [-2.02404919  0.73184435]. \t  -1141.4207830370153 \t -0.04381458802715288\n",
            "57     \t [1.20068517 1.39851087]. \t  -0.22632874131712996 \t -0.04381458802715288\n",
            "58     \t [-1.47234921  2.01440936]. \t  -8.465754086896485 \t -0.04381458802715288\n",
            "59     \t [0.76101456 0.58943797]. \t  -0.06771235104415185 \t -0.04381458802715288\n",
            "60     \t [-1.14287721 -1.84974167]. \t  -1000.568697410595 \t -0.04381458802715288\n",
            "61     \t [ 0.60120811 -0.72900613]. \t  -119.0687515408353 \t -0.04381458802715288\n",
            "62     \t [-1.0803744   0.18974846]. \t  -99.8708387620765 \t -0.04381458802715288\n",
            "63     \t [-1.07883665  0.83854152]. \t  -14.906629231210427 \t -0.04381458802715288\n",
            "64     \t [1.36805399 1.84749585]. \t  -0.193428504375239 \t -0.04381458802715288\n",
            "65     \t [0.6981431  0.44858296]. \t  -0.2418232592397735 \t -0.04381458802715288\n",
            "66     \t [-0.02586991 -1.69586234]. \t  -288.8743529540324 \t -0.04381458802715288\n",
            "67     \t [ 1.84141782 -0.56448018]. \t  -1565.1476106907987 \t -0.04381458802715288\n",
            "68     \t [0.29070172 0.83551013]. \t  -56.90360093054751 \t -0.04381458802715288\n",
            "69     \t [ 0.77466228 -0.26976528]. \t  -75.71762526666582 \t -0.04381458802715288\n",
            "70     \t [0.51623138 0.23845978]. \t  -0.3126285146309222 \t -0.04381458802715288\n",
            "71     \t [0.65205829 0.43189422]. \t  -0.12557148555833916 \t -0.04381458802715288\n",
            "72     \t [0.97955475 0.94901259]. \t  \u001b[92m-0.011474348161723396\u001b[0m \t -0.011474348161723396\n",
            "73     \t [-0.41998961 -1.70756351]. \t  -356.9449335463213 \t -0.011474348161723396\n",
            "74     \t [0.56254087 0.32709241]. \t  -0.2026918237776151 \t -0.011474348161723396\n",
            "75     \t [ 1.54651295 -0.94071756]. \t  -1110.8008886824762 \t -0.011474348161723396\n",
            "76     \t [1.22026773 1.09116792]. \t  -15.879797912924316 \t -0.011474348161723396\n",
            "77     \t [-1.2738401   0.34097693]. \t  -169.44370171654418 \t -0.011474348161723396\n",
            "78     \t [-0.25579785 -1.91213514]. \t  -392.65441879290506 \t -0.011474348161723396\n",
            "79     \t [-1.01193613  1.27793013]. \t  -10.4951895211278 \t -0.011474348161723396\n",
            "80     \t [-0.04833811  0.43081048]. \t  -19.4580015708361 \t -0.011474348161723396\n",
            "81     \t [0.58747614 0.31057717]. \t  -0.28955336596200976 \t -0.011474348161723396\n",
            "82     \t [ 1.59712897 -0.86236332]. \t  -1165.3392525757588 \t -0.011474348161723396\n",
            "83     \t [-0.98486741  0.68231356]. \t  -12.213965271784032 \t -0.011474348161723396\n",
            "84     \t [-1.81862779  1.1758283 ]. \t  -462.3074627004391 \t -0.011474348161723396\n",
            "85     \t [-0.24366068  1.84452526]. \t  -320.2244345913622 \t -0.011474348161723396\n",
            "86     \t [-1.22222237  1.39634206]. \t  -5.88861363497911 \t -0.011474348161723396\n",
            "87     \t [-0.92145058 -1.64561721]. \t  -626.0389838876489 \t -0.011474348161723396\n",
            "88     \t [-0.14002292 -1.35041328]. \t  -188.99504911871696 \t -0.011474348161723396\n",
            "89     \t [-0.57263001  0.90454645]. \t  -35.72468675050905 \t -0.011474348161723396\n",
            "90     \t [-0.52357218  0.25964062]. \t  -2.342260135583863 \t -0.011474348161723396\n",
            "91     \t [0.72896395 0.54696825]. \t  -0.09773360590918789 \t -0.011474348161723396\n",
            "92     \t [1.81914489 1.51624507]. \t  -322.17134307118727 \t -0.011474348161723396\n",
            "93     \t [ 0.88411369 -0.31217853]. \t  -119.66104849291484 \t -0.011474348161723396\n",
            "94     \t [0.88798542 0.19356752]. \t  -35.40916619948873 \t -0.011474348161723396\n",
            "95     \t [-0.93516627 -0.76413114]. \t  -272.2678550483519 \t -0.011474348161723396\n",
            "96     \t [-0.87974617 -1.97633035]. \t  -759.9394729222546 \t -0.011474348161723396\n",
            "97     \t [1.22201364 0.65736029]. \t  -69.93170963058354 \t -0.011474348161723396\n",
            "98     \t [-1.51314065  0.48297648]. \t  -332.70279227581244 \t -0.011474348161723396\n",
            "99     \t [-2.02472409  1.55710344]. \t  -655.530863928991 \t -0.011474348161723396\n",
            "100    \t [-0.20069779  1.79982045]. \t  -311.04007298950785 \t -0.011474348161723396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHz_Jg2_uv7E",
        "outputId": "cd2a0d89-af27-457b-aa60-00b1f47096a7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_winner_20 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_20 = dGPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.2888388  0.26430978]. \t  -3.777577453542735 \t -3.777577453542735\n",
            "init   \t [-0.04734225 -0.66978712]. \t  -46.25914429040042 \t -3.777577453542735\n",
            "init   \t [-0.50844394  0.13121892]. \t  -3.895838569726608 \t -3.777577453542735\n",
            "init   \t [-1.76903664  0.34623103]. \t  -782.3209705533411 \t -3.777577453542735\n",
            "init   \t [-1.07357076 -1.38954104]. \t  -650.524506936468 \t -3.777577453542735\n",
            "1      \t [-0.12445779  0.09875914]. \t  \u001b[92m-1.9577846182863219\u001b[0m \t -1.9577846182863219\n",
            "2      \t [1.64016217 0.54577709]. \t  -460.235573921195 \t -1.9577846182863219\n",
            "3      \t [-0.2256773   1.36134238]. \t  -173.2202825201901 \t -1.9577846182863219\n",
            "4      \t [ 0.85350846 -1.17114372]. \t  -360.87723335375085 \t -1.9577846182863219\n",
            "5      \t [0.48513709 0.68749849]. \t  -20.708186393176724 \t -1.9577846182863219\n",
            "6      \t [0.50916609 0.28765757]. \t  \u001b[92m-0.32161633235493436\u001b[0m \t -0.32161633235493436\n",
            "7      \t [-0.5148578  -0.04700268]. \t  -12.03426373361602 \t -0.32161633235493436\n",
            "8      \t [0.47970759 0.29361569]. \t  -0.6738825244659234 \t -0.32161633235493436\n",
            "9      \t [-0.41842456  0.20426728]. \t  -2.0971231404831188 \t -0.32161633235493436\n",
            "10     \t [-0.40086277  0.13765651]. \t  -2.015475088038526 \t -0.32161633235493436\n",
            "11     \t [ 0.27116177 -0.17454124]. \t  -6.6850747636352335 \t -0.32161633235493436\n",
            "12     \t [-0.05490036 -0.11316394]. \t  -2.4625473552704724 \t -0.32161633235493436\n",
            "13     \t [1.03401895 1.94113572]. \t  -76.0291844325951 \t -0.32161633235493436\n",
            "14     \t [-1.71345588  1.82107432]. \t  -131.65339821463445 \t -0.32161633235493436\n",
            "15     \t [-0.3983729  -1.64194288]. \t  -326.1872740695626 \t -0.32161633235493436\n",
            "16     \t [ 0.44378849 -1.53929824]. \t  -301.76455004759686 \t -0.32161633235493436\n",
            "17     \t [-0.39119219  0.81005883]. \t  -45.10392970029373 \t -0.32161633235493436\n",
            "18     \t [ 0.25799003 -0.24867382]. \t  -10.4877429368878 \t -0.32161633235493436\n",
            "19     \t [ 0.20646555 -1.89909653]. \t  -377.6591207061545 \t -0.32161633235493436\n",
            "20     \t [-0.82503668 -1.59352367]. \t  -520.5335091103622 \t -0.32161633235493436\n",
            "21     \t [-0.48316149  0.04533448]. \t  -5.738325805710129 \t -0.32161633235493436\n",
            "22     \t [-0.19580929  0.08150855]. \t  -1.616301184436927 \t -0.32161633235493436\n",
            "23     \t [ 1.66764852 -1.71888722]. \t  -2025.3906879916842 \t -0.32161633235493436\n",
            "24     \t [-1.69537024 -1.39278119]. \t  -1828.0463414122016 \t -0.32161633235493436\n",
            "25     \t [-1.07632681  1.22996275]. \t  -4.822119808755361 \t -0.32161633235493436\n",
            "26     \t [-0.98123729  1.05877519]. \t  -4.8459141026627925 \t -0.32161633235493436\n",
            "27     \t [-1.08205075  1.68504945]. \t  -30.77670508733414 \t -0.32161633235493436\n",
            "28     \t [1.80318636 1.94411307]. \t  -171.56621188478246 \t -0.32161633235493436\n",
            "29     \t [-0.94976169  1.19013442]. \t  -12.100991321786697 \t -0.32161633235493436\n",
            "30     \t [-1.1342216  -0.37006358]. \t  -278.9614916305373 \t -0.32161633235493436\n",
            "31     \t [0.43348374 0.24789285]. \t  -0.6807571136465769 \t -0.32161633235493436\n",
            "32     \t [1.20459609 1.79563618]. \t  -11.915702706226021 \t -0.32161633235493436\n",
            "33     \t [1.0205953  1.22335086]. \t  -3.3032251248818136 \t -0.32161633235493436\n",
            "34     \t [-1.20550988 -0.05889056]. \t  -233.52241158111184 \t -0.32161633235493436\n",
            "35     \t [-0.52674853  0.54617333]. \t  -9.55143065929903 \t -0.32161633235493436\n",
            "36     \t [1.12745442 1.36324373]. \t  -0.8643062097502626 \t -0.32161633235493436\n",
            "37     \t [1.32115648 1.18152797]. \t  -31.904447673878682 \t -0.32161633235493436\n",
            "38     \t [-0.3040362   0.76544083]. \t  -46.9937903703847 \t -0.32161633235493436\n",
            "39     \t [0.85087995 0.77008763]. \t  \u001b[92m-0.23467415583651066\u001b[0m \t -0.23467415583651066\n",
            "40     \t [1.1673726  1.30861635]. \t  -0.32115404922858665 \t -0.23467415583651066\n",
            "41     \t [ 0.53483739 -1.11396349]. \t  -196.22044177322152 \t -0.23467415583651066\n",
            "42     \t [-0.4068628  -1.28073464]. \t  -211.14952512344544 \t -0.23467415583651066\n",
            "43     \t [0.17529204 1.15916279]. \t  -128.0168083834562 \t -0.23467415583651066\n",
            "44     \t [0.60187974 0.34939177]. \t  \u001b[92m-0.1750568811306341\u001b[0m \t -0.1750568811306341\n",
            "45     \t [0.47964708 1.26569985]. \t  -107.52548316444184 \t -0.1750568811306341\n",
            "46     \t [-1.50941603 -1.10529415]. \t  -1151.1929674840562 \t -0.1750568811306341\n",
            "47     \t [ 1.27091858 -1.70898462]. \t  -1105.116363693183 \t -0.1750568811306341\n",
            "48     \t [1.51949793 0.87348176]. \t  -206.30495661161856 \t -0.1750568811306341\n",
            "49     \t [0.65152894 0.43355151]. \t  \u001b[92m-0.12964324598611057\u001b[0m \t -0.12964324598611057\n",
            "50     \t [0.67134476 0.46935239]. \t  -0.14279133921161213 \t -0.12964324598611057\n",
            "51     \t [1.19417097 1.38528337]. \t  -0.20384763050883503 \t -0.12964324598611057\n",
            "52     \t [1.95650081 1.38439042]. \t  -597.9865725043503 \t -0.12964324598611057\n",
            "53     \t [ 1.71219045 -1.9877464 ]. \t  -2420.500308270193 \t -0.12964324598611057\n",
            "54     \t [0.46888773 1.61398374]. \t  -194.64137800239578 \t -0.12964324598611057\n",
            "55     \t [ 1.15212581 -1.08914448]. \t  -583.9889106648615 \t -0.12964324598611057\n",
            "56     \t [0.87455881 0.77278239]. \t  \u001b[92m-0.02202283006239304\u001b[0m \t -0.02202283006239304\n",
            "57     \t [0.97825923 0.93755616]. \t  -0.03824441295259288 \t -0.02202283006239304\n",
            "58     \t [-1.66676799  0.66994422]. \t  -451.5502786295779 \t -0.02202283006239304\n",
            "59     \t [-0.99523387  0.24252604]. \t  -59.926034914967154 \t -0.02202283006239304\n",
            "60     \t [1.4387704  1.99626182]. \t  -0.7371404119503033 \t -0.02202283006239304\n",
            "61     \t [ 1.9065061  -0.75279122]. \t  -1925.8871723897323 \t -0.02202283006239304\n",
            "62     \t [-0.94001591  2.04528928]. \t  -138.70891112332072 \t -0.02202283006239304\n",
            "63     \t [ 0.57187407 -1.66012158]. \t  -395.0643876039098 \t -0.02202283006239304\n",
            "64     \t [0.44374421 1.12386373]. \t  -86.23394152062177 \t -0.02202283006239304\n",
            "65     \t [1.18674958 1.09267993]. \t  -10.00118618641607 \t -0.02202283006239304\n",
            "66     \t [-0.91082315 -1.26232069]. \t  -441.26396375882774 \t -0.02202283006239304\n",
            "67     \t [0.0821836  0.58251462]. \t  -33.99239946621883 \t -0.02202283006239304\n",
            "68     \t [1.15112364 0.41629861]. \t  -82.61222519369333 \t -0.02202283006239304\n",
            "69     \t [0.9116035  1.07213758]. \t  -5.821536800474615 \t -0.02202283006239304\n",
            "70     \t [0.00420307 0.2388898 ]. \t  -6.697601156350904 \t -0.02202283006239304\n",
            "71     \t [1.11036593 1.21933424]. \t  -0.03061754712837163 \t -0.02202283006239304\n",
            "72     \t [2.02236401 0.80552465]. \t  -1079.7942702821092 \t -0.02202283006239304\n",
            "73     \t [-0.62886151 -1.17512401]. \t  -249.32873851716164 \t -0.02202283006239304\n",
            "74     \t [1.09253888 1.20403353]. \t  \u001b[92m-0.019363492099804456\u001b[0m \t -0.019363492099804456\n",
            "75     \t [0.38581887 1.73789702]. \t  -252.88229059851358 \t -0.019363492099804456\n",
            "76     \t [-1.96944417 -0.5409523 ]. \t  -1962.1593894476853 \t -0.019363492099804456\n",
            "77     \t [0.99800646 0.94708991]. \t  -0.2393889376866279 \t -0.019363492099804456\n",
            "78     \t [-0.51051861  1.68444812]. \t  -205.0076833315466 \t -0.019363492099804456\n",
            "79     \t [-1.06095289 -0.39755859]. \t  -236.25514667039812 \t -0.019363492099804456\n",
            "80     \t [ 0.36031694 -0.00897675]. \t  -2.335878478975027 \t -0.019363492099804456\n",
            "81     \t [0.94159171 0.87789667]. \t  \u001b[92m-0.010977516465638146\u001b[0m \t -0.010977516465638146\n",
            "82     \t [-0.82949027  1.47402342]. \t  -65.12181013587157 \t -0.010977516465638146\n",
            "83     \t [ 0.10348957 -1.23911708]. \t  -157.01052777221707 \t -0.010977516465638146\n",
            "84     \t [1.78402586 0.47746595]. \t  -732.4699348470266 \t -0.010977516465638146\n",
            "85     \t [0.97203986 0.93988444]. \t  \u001b[92m-0.0032588749019199564\u001b[0m \t -0.0032588749019199564\n",
            "86     \t [0.99871062 0.96539182]. \t  -0.10260069559168525 \t -0.0032588749019199564\n",
            "87     \t [-1.23659192 -0.58198804]. \t  -450.69676966562554 \t -0.0032588749019199564\n",
            "88     \t [-1.42980548  1.22539506]. \t  -72.97164514750482 \t -0.0032588749019199564\n",
            "89     \t [-1.88312686  1.82949245]. \t  -303.00949927111895 \t -0.0032588749019199564\n",
            "90     \t [-1.4660533   1.66046065]. \t  -29.979010274014193 \t -0.0032588749019199564\n",
            "91     \t [-0.0655655   0.64578957]. \t  -42.28646587945757 \t -0.0032588749019199564\n",
            "92     \t [-1.39975555  2.0100341 ]. \t  -6.016063268242253 \t -0.0032588749019199564\n",
            "93     \t [-0.18625368  0.67477663]. \t  -42.37823194204345 \t -0.0032588749019199564\n",
            "94     \t [1.19295637 0.88925704]. \t  -28.540856130884436 \t -0.0032588749019199564\n",
            "95     \t [ 1.25493264 -0.61556837]. \t  -479.8608501814786 \t -0.0032588749019199564\n",
            "96     \t [1.5768618  1.87960322]. \t  -37.1643059079143 \t -0.0032588749019199564\n",
            "97     \t [0.6027097  1.82522772]. \t  -213.89309756821692 \t -0.0032588749019199564\n",
            "98     \t [-1.99286282 -1.79067893]. \t  -3329.230399486352 \t -0.0032588749019199564\n",
            "99     \t [ 1.5240691  -0.14424668]. \t  -608.8999803784616 \t -0.0032588749019199564\n",
            "100    \t [-0.71917525  1.86097463]. \t  -183.5250858961645 \t -0.0032588749019199564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUnhsKpCuv9o",
        "outputId": "26a3a589-2ae0-4776-c408-fda2b320f40d"
      },
      "source": [
        "end_win = time.time()\r\n",
        "end_win\r\n",
        "\r\n",
        "time_win = end_win - start_win\r\n",
        "time_win"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1555.3340611457825"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJG0SLpwuwAL",
        "outputId": "4e86dde5-f29b-4961-86f9-c92cf38fc487"
      },
      "source": [
        "### Training regret minimization: run number = 1\r\n",
        "\r\n",
        "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_1 = np.log(y_global_orig - loser_output_1)\r\n",
        "regret_winner_1 = np.log(y_global_orig - winner_output_1)\r\n",
        "\r\n",
        "train_regret_loser_1 = min_max_array(regret_loser_1)\r\n",
        "train_regret_winner_1 = min_max_array(regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1 = min(train_regret_loser_1)\r\n",
        "min_train_regret_winner_1 = min(train_regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1, min_train_regret_winner_1"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.73488794011762, -4.752999537761259)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lA9eZf0uwCx",
        "outputId": "8875971b-8285-4920-faf1-c828aed8031a"
      },
      "source": [
        "### Training regret minimization: run number = 2\r\n",
        "\r\n",
        "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_2 = np.log(y_global_orig - loser_output_2)\r\n",
        "regret_winner_2 = np.log(y_global_orig - winner_output_2)\r\n",
        "\r\n",
        "train_regret_loser_2 = min_max_array(regret_loser_2)\r\n",
        "train_regret_winner_2 = min_max_array(regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2 = min(train_regret_loser_2)\r\n",
        "min_train_regret_winner_2 = min(train_regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2, min_train_regret_winner_2"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.578961856157758, -5.578961856157758)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glrTGcpAuwFa",
        "outputId": "b45de528-0f1f-449a-b2da-4d0694ea3505"
      },
      "source": [
        "### Training regret minimization: run number = 3\r\n",
        "\r\n",
        "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_3 = np.log(y_global_orig - loser_output_3)\r\n",
        "regret_winner_3 = np.log(y_global_orig - winner_output_3)\r\n",
        "\r\n",
        "train_regret_loser_3 = min_max_array(regret_loser_3)\r\n",
        "train_regret_winner_3 = min_max_array(regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3 = min(train_regret_loser_3)\r\n",
        "min_train_regret_winner_3 = min(train_regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3, min_train_regret_winner_3"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.095268459336573, -4.607714187290274)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaRwfbxeuwIR",
        "outputId": "80a1b889-8f07-4086-e92a-d590f9923a43"
      },
      "source": [
        "### Training regret minimization: run number = 4\r\n",
        "\r\n",
        "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_4 = np.log(y_global_orig - loser_output_4)\r\n",
        "regret_winner_4 = np.log(y_global_orig - winner_output_4)\r\n",
        "\r\n",
        "train_regret_loser_4 = min_max_array(regret_loser_4)\r\n",
        "train_regret_winner_4 = min_max_array(regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4 = min(train_regret_loser_4)\r\n",
        "min_train_regret_winner_4 = min(train_regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4, min_train_regret_winner_4"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.3242565690018555, -5.3242565690018555)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Nb5NkfyuwKp",
        "outputId": "ec65fd18-4bdc-46b6-f2d3-29de5349fc87"
      },
      "source": [
        "### Training regret minimization: run number = 5\r\n",
        "\r\n",
        "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_5 = np.log(y_global_orig - loser_output_5)\r\n",
        "regret_winner_5 = np.log(y_global_orig - winner_output_5)\r\n",
        "\r\n",
        "train_regret_loser_5 = min_max_array(regret_loser_5)\r\n",
        "train_regret_winner_5 = min_max_array(regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5 = min(train_regret_loser_5)\r\n",
        "min_train_regret_winner_5 = min(train_regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5, min_train_regret_winner_5"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.094264245172128, -3.0942642450802875)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0Q-WfXbuwNg",
        "outputId": "2e6f2d47-8310-409d-e8a7-faf5b4c496a1"
      },
      "source": [
        "### Training regret minimization: run number = 6\r\n",
        "\r\n",
        "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_6 = np.log(y_global_orig - loser_output_6)\r\n",
        "regret_winner_6 = np.log(y_global_orig - winner_output_6)\r\n",
        "\r\n",
        "train_regret_loser_6 = min_max_array(regret_loser_6)\r\n",
        "train_regret_winner_6 = min_max_array(regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6 = min(train_regret_loser_6)\r\n",
        "min_train_regret_winner_6 = min(train_regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6, min_train_regret_winner_6"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-2.4661365365879844, -6.48362698855227)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqqS7VLcuwPy",
        "outputId": "6c29e60b-aa1b-4c2e-b436-fa2c36d91c2e"
      },
      "source": [
        "### Training regret minimization: run number = 7\r\n",
        "\r\n",
        "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_7 = np.log(y_global_orig - loser_output_7)\r\n",
        "regret_winner_7 = np.log(y_global_orig - winner_output_7)\r\n",
        "\r\n",
        "train_regret_loser_7 = min_max_array(regret_loser_7)\r\n",
        "train_regret_winner_7 = min_max_array(regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7 = min(train_regret_loser_7)\r\n",
        "min_train_regret_winner_7 = min(train_regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7, min_train_regret_winner_7"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-7.1406456930929565, -7.1406456930929565)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOi5iX8guwSS",
        "outputId": "0fc487c8-e28c-4928-a21b-896de196f396"
      },
      "source": [
        "### Training regret minimization: run number = 8\r\n",
        "\r\n",
        "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_8 = np.log(y_global_orig - loser_output_8)\r\n",
        "regret_winner_8 = np.log(y_global_orig - winner_output_8)\r\n",
        "\r\n",
        "train_regret_loser_8 = min_max_array(regret_loser_8)\r\n",
        "train_regret_winner_8 = min_max_array(regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8 = min(train_regret_loser_8)\r\n",
        "min_train_regret_winner_8 = min(train_regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8, min_train_regret_winner_8"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-6.171869298629812, -3.223370584788274)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFVApeazuwU5",
        "outputId": "74c460f5-5c3d-4227-86be-79029b220442"
      },
      "source": [
        "### Training regret minimization: run number = 9\r\n",
        "\r\n",
        "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_9 = np.log(y_global_orig - loser_output_9)\r\n",
        "regret_winner_9 = np.log(y_global_orig - winner_output_9)\r\n",
        "\r\n",
        "train_regret_loser_9 = min_max_array(regret_loser_9)\r\n",
        "train_regret_winner_9 = min_max_array(regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9 = min(train_regret_loser_9)\r\n",
        "min_train_regret_winner_9 = min(train_regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9, min_train_regret_winner_9"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.147662257650419, -4.309773544381388)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g92jk9WJuwXb",
        "outputId": "98ee1f4f-78f7-4407-9456-aa112e9fb451"
      },
      "source": [
        "### Training regret minimization: run number = 10\r\n",
        "\r\n",
        "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_10 = np.log(y_global_orig - loser_output_10)\r\n",
        "regret_winner_10 = np.log(y_global_orig - winner_output_10)\r\n",
        "\r\n",
        "train_regret_loser_10 = min_max_array(regret_loser_10)\r\n",
        "train_regret_winner_10 = min_max_array(regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10 = min(train_regret_loser_10)\r\n",
        "min_train_regret_winner_10 = min(train_regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10, min_train_regret_winner_10"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.6040707371356917, -7.7634817434197885)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmcF1x-NuwZz",
        "outputId": "475b303c-36c8-4453-d54b-e9219ff20e58"
      },
      "source": [
        "### Training regret minimization: run number = 11\r\n",
        "\r\n",
        "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_11 = np.log(y_global_orig - loser_output_11)\r\n",
        "regret_winner_11 = np.log(y_global_orig - winner_output_11)\r\n",
        "\r\n",
        "train_regret_loser_11 = min_max_array(regret_loser_11)\r\n",
        "train_regret_winner_11 = min_max_array(regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11 = min(train_regret_loser_11)\r\n",
        "min_train_regret_winner_11 = min(train_regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11, min_train_regret_winner_11"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.634126667427902, -3.2581777689438174)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8axVhb6uwcc",
        "outputId": "3c64284e-0e07-4ca3-998f-66826fe7c405"
      },
      "source": [
        "### Training regret minimization: run number = 12\r\n",
        "\r\n",
        "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_12 = np.log(y_global_orig - loser_output_12)\r\n",
        "regret_winner_12 = np.log(y_global_orig - winner_output_12)\r\n",
        "\r\n",
        "train_regret_loser_12 = min_max_array(regret_loser_12)\r\n",
        "train_regret_winner_12 = min_max_array(regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12 = min(train_regret_loser_12)\r\n",
        "min_train_regret_winner_12 = min(train_regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12, min_train_regret_winner_12"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-8.371105951845212, -7.430256969105658)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzlrL8XFuwfB",
        "outputId": "1d15d804-7605-4a00-ce87-1828057cb59f"
      },
      "source": [
        "### Training regret minimization: run number = 13\r\n",
        "\r\n",
        "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_13 = np.log(y_global_orig - loser_output_13)\r\n",
        "regret_winner_13 = np.log(y_global_orig - winner_output_13)\r\n",
        "\r\n",
        "train_regret_loser_13 = min_max_array(regret_loser_13)\r\n",
        "train_regret_winner_13 = min_max_array(regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13 = min(train_regret_loser_13)\r\n",
        "min_train_regret_winner_13 = min(train_regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13, min_train_regret_winner_13"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-6.3857521505705215, -6.3857521505705215)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uZlLJ1quwh6",
        "outputId": "02d72ad1-991d-419d-9eb6-b1d737a7e9e1"
      },
      "source": [
        "### Training regret minimization: run number = 14\r\n",
        "\r\n",
        "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_14 = np.log(y_global_orig - loser_output_14)\r\n",
        "regret_winner_14 = np.log(y_global_orig - winner_output_14)\r\n",
        "\r\n",
        "train_regret_loser_14 = min_max_array(regret_loser_14)\r\n",
        "train_regret_winner_14 = min_max_array(regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14 = min(train_regret_loser_14)\r\n",
        "min_train_regret_winner_14 = min(train_regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14, min_train_regret_winner_14"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-6.606540174788199, -5.118283958431365)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVBcHRiMuwjx",
        "outputId": "7e0494ba-29a5-4720-beb4-ca9cddad14c1"
      },
      "source": [
        "### Training regret minimization: run number = 15\r\n",
        "\r\n",
        "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_15 = np.log(y_global_orig - loser_output_15)\r\n",
        "regret_winner_15 = np.log(y_global_orig - winner_output_15)\r\n",
        "\r\n",
        "train_regret_loser_15 = min_max_array(regret_loser_15)\r\n",
        "train_regret_winner_15 = min_max_array(regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15 = min(train_regret_loser_15)\r\n",
        "min_train_regret_winner_15 = min(train_regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15, min_train_regret_winner_15"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-6.136412798517651, -3.7551477515076397)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8nZ_DrKuwnA",
        "outputId": "39699b59-d9b9-4dfc-b6c0-d87117cbf7c4"
      },
      "source": [
        "### Training regret minimization: run number = 16\r\n",
        "\r\n",
        "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_16 = np.log(y_global_orig - loser_output_16)\r\n",
        "regret_winner_16 = np.log(y_global_orig - winner_output_16)\r\n",
        "\r\n",
        "train_regret_loser_16 = min_max_array(regret_loser_16)\r\n",
        "train_regret_winner_16 = min_max_array(regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16 = min(train_regret_loser_16)\r\n",
        "min_train_regret_winner_16 = min(train_regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16, min_train_regret_winner_16"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.506042141579035, -4.582991693064525)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg6qzzQFuwpU",
        "outputId": "d9712a09-999c-49d9-a756-d31b0a657573"
      },
      "source": [
        "### Training regret minimization: run number = 17\r\n",
        "\r\n",
        "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_17 = np.log(y_global_orig - loser_output_17)\r\n",
        "regret_winner_17 = np.log(y_global_orig - winner_output_17)\r\n",
        "\r\n",
        "train_regret_loser_17 = min_max_array(regret_loser_17)\r\n",
        "train_regret_winner_17 = min_max_array(regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17 = min(train_regret_loser_17)\r\n",
        "min_train_regret_winner_17 = min(train_regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17, min_train_regret_winner_17"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.7511879729952318, -3.6979437558226844)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrRtDLMFuwsB",
        "outputId": "84a3104b-1f48-4629-83b1-6feca79735ee"
      },
      "source": [
        "### Training regret minimization: run number = 18\r\n",
        "\r\n",
        "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_18 = np.log(y_global_orig - loser_output_18)\r\n",
        "regret_winner_18 = np.log(y_global_orig - winner_output_18)\r\n",
        "\r\n",
        "train_regret_loser_18 = min_max_array(regret_loser_18)\r\n",
        "train_regret_winner_18 = min_max_array(regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18 = min(train_regret_loser_18)\r\n",
        "min_train_regret_winner_18 = min(train_regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18, min_train_regret_winner_18"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.989993066641818, -4.973314633699159)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkmSu4CUuwuh",
        "outputId": "851e7762-00fb-4abc-8df4-e222a3b0fd31"
      },
      "source": [
        "### Training regret minimization: run number = 19\r\n",
        "\r\n",
        "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_19 = np.log(y_global_orig - loser_output_19)\r\n",
        "regret_winner_19 = np.log(y_global_orig - winner_output_19)\r\n",
        "\r\n",
        "train_regret_loser_19 = min_max_array(regret_loser_19)\r\n",
        "train_regret_winner_19 = min_max_array(regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19 = min(train_regret_loser_19)\r\n",
        "min_train_regret_winner_19 = min(train_regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19, min_train_regret_winner_19"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.495659948224313, -4.467641329727569)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eymecjwkuwxb",
        "outputId": "3f05c388-5911-4a85-da5b-e3a5299e6d1d"
      },
      "source": [
        "### Training regret minimization: run number = 20\r\n",
        "\r\n",
        "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_20 = np.log(y_global_orig - loser_output_20)\r\n",
        "regret_winner_20 = np.log(y_global_orig - winner_output_20)\r\n",
        "\r\n",
        "train_regret_loser_20 = min_max_array(regret_loser_20)\r\n",
        "train_regret_winner_20 = min_max_array(regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20 = min(train_regret_loser_20)\r\n",
        "min_train_regret_winner_20 = min(train_regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20, min_train_regret_winner_20"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.558711297082938, -5.726373265282305)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5P6kq2Kuw0d"
      },
      "source": [
        "# Iteration1 :\r\n",
        "\r\n",
        "slice1 = 0\r\n",
        "\r\n",
        "loser1 = [train_regret_loser_1[slice1],\r\n",
        "       train_regret_loser_2[slice1],\r\n",
        "       train_regret_loser_3[slice1],\r\n",
        "       train_regret_loser_4[slice1],\r\n",
        "       train_regret_loser_5[slice1],\r\n",
        "       train_regret_loser_6[slice1],\r\n",
        "       train_regret_loser_7[slice1],\r\n",
        "       train_regret_loser_8[slice1],\r\n",
        "       train_regret_loser_9[slice1],\r\n",
        "       train_regret_loser_10[slice1],\r\n",
        "       train_regret_loser_11[slice1],\r\n",
        "       train_regret_loser_12[slice1],\r\n",
        "       train_regret_loser_13[slice1],\r\n",
        "       train_regret_loser_14[slice1],\r\n",
        "       train_regret_loser_15[slice1],\r\n",
        "       train_regret_loser_16[slice1],\r\n",
        "       train_regret_loser_17[slice1],\r\n",
        "       train_regret_loser_18[slice1],\r\n",
        "       train_regret_loser_19[slice1],\r\n",
        "       train_regret_loser_20[slice1]]\r\n",
        "\r\n",
        "winner1 = [train_regret_winner_1[slice1],\r\n",
        "       train_regret_winner_2[slice1],\r\n",
        "       train_regret_winner_3[slice1],\r\n",
        "       train_regret_winner_4[slice1],\r\n",
        "       train_regret_winner_5[slice1],\r\n",
        "       train_regret_winner_6[slice1],\r\n",
        "       train_regret_winner_7[slice1],\r\n",
        "       train_regret_winner_8[slice1],\r\n",
        "       train_regret_winner_9[slice1],\r\n",
        "       train_regret_winner_10[slice1],\r\n",
        "       train_regret_winner_11[slice1],\r\n",
        "       train_regret_winner_12[slice1],\r\n",
        "       train_regret_winner_13[slice1],\r\n",
        "       train_regret_winner_14[slice1],\r\n",
        "       train_regret_winner_15[slice1],\r\n",
        "       train_regret_winner_16[slice1],\r\n",
        "       train_regret_winner_17[slice1],\r\n",
        "       train_regret_winner_18[slice1],\r\n",
        "       train_regret_winner_19[slice1],\r\n",
        "       train_regret_winner_20[slice1]]\r\n",
        "\r\n",
        "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\r\n",
        "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\r\n",
        "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\r\n",
        "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\r\n",
        "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\r\n",
        "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXcnj2pNuw3X"
      },
      "source": [
        "# Iteration11 :\r\n",
        "\r\n",
        "slice11 = 10\r\n",
        "\r\n",
        "loser11 = [train_regret_loser_1[slice11],\r\n",
        "       train_regret_loser_2[slice11],\r\n",
        "       train_regret_loser_3[slice11],\r\n",
        "       train_regret_loser_4[slice11],\r\n",
        "       train_regret_loser_5[slice11],\r\n",
        "       train_regret_loser_6[slice11],\r\n",
        "       train_regret_loser_7[slice11],\r\n",
        "       train_regret_loser_8[slice11],\r\n",
        "       train_regret_loser_9[slice11],\r\n",
        "       train_regret_loser_10[slice11],\r\n",
        "       train_regret_loser_11[slice11],\r\n",
        "       train_regret_loser_12[slice11],\r\n",
        "       train_regret_loser_13[slice11],\r\n",
        "       train_regret_loser_14[slice11],\r\n",
        "       train_regret_loser_15[slice11],\r\n",
        "       train_regret_loser_16[slice11],\r\n",
        "       train_regret_loser_17[slice11],\r\n",
        "       train_regret_loser_18[slice11],\r\n",
        "       train_regret_loser_19[slice11],\r\n",
        "       train_regret_loser_20[slice11]]\r\n",
        "\r\n",
        "winner11 = [train_regret_winner_1[slice11],\r\n",
        "       train_regret_winner_2[slice11],\r\n",
        "       train_regret_winner_3[slice11],\r\n",
        "       train_regret_winner_4[slice11],\r\n",
        "       train_regret_winner_5[slice11],\r\n",
        "       train_regret_winner_6[slice11],\r\n",
        "       train_regret_winner_7[slice11],\r\n",
        "       train_regret_winner_8[slice11],\r\n",
        "       train_regret_winner_9[slice11],\r\n",
        "       train_regret_winner_10[slice11],\r\n",
        "       train_regret_winner_11[slice11],\r\n",
        "       train_regret_winner_12[slice11],\r\n",
        "       train_regret_winner_13[slice11],\r\n",
        "       train_regret_winner_14[slice11],\r\n",
        "       train_regret_winner_15[slice11],\r\n",
        "       train_regret_winner_16[slice11],\r\n",
        "       train_regret_winner_17[slice11],\r\n",
        "       train_regret_winner_18[slice11],\r\n",
        "       train_regret_winner_19[slice11],\r\n",
        "       train_regret_winner_20[slice11]]\r\n",
        "\r\n",
        "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\r\n",
        "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\r\n",
        "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\r\n",
        "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\r\n",
        "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\r\n",
        "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxQa6BAVuw6L"
      },
      "source": [
        "# Iteration21 :\r\n",
        "\r\n",
        "slice21 = 20\r\n",
        "\r\n",
        "loser21 = [train_regret_loser_1[slice21],\r\n",
        "       train_regret_loser_2[slice21],\r\n",
        "       train_regret_loser_3[slice21],\r\n",
        "       train_regret_loser_4[slice21],\r\n",
        "       train_regret_loser_5[slice21],\r\n",
        "       train_regret_loser_6[slice21],\r\n",
        "       train_regret_loser_7[slice21],\r\n",
        "       train_regret_loser_8[slice21],\r\n",
        "       train_regret_loser_9[slice21],\r\n",
        "       train_regret_loser_10[slice21],\r\n",
        "       train_regret_loser_11[slice21],\r\n",
        "       train_regret_loser_12[slice21],\r\n",
        "       train_regret_loser_13[slice21],\r\n",
        "       train_regret_loser_14[slice21],\r\n",
        "       train_regret_loser_15[slice21],\r\n",
        "       train_regret_loser_16[slice21],\r\n",
        "       train_regret_loser_17[slice21],\r\n",
        "       train_regret_loser_18[slice21],\r\n",
        "       train_regret_loser_19[slice21],\r\n",
        "       train_regret_loser_20[slice21]]\r\n",
        "\r\n",
        "winner21 = [train_regret_winner_1[slice21],\r\n",
        "       train_regret_winner_2[slice21],\r\n",
        "       train_regret_winner_3[slice21],\r\n",
        "       train_regret_winner_4[slice21],\r\n",
        "       train_regret_winner_5[slice21],\r\n",
        "       train_regret_winner_6[slice21],\r\n",
        "       train_regret_winner_7[slice21],\r\n",
        "       train_regret_winner_8[slice21],\r\n",
        "       train_regret_winner_9[slice21],\r\n",
        "       train_regret_winner_10[slice21],\r\n",
        "       train_regret_winner_11[slice21],\r\n",
        "       train_regret_winner_12[slice21],\r\n",
        "       train_regret_winner_13[slice21],\r\n",
        "       train_regret_winner_14[slice21],\r\n",
        "       train_regret_winner_15[slice21],\r\n",
        "       train_regret_winner_16[slice21],\r\n",
        "       train_regret_winner_17[slice21],\r\n",
        "       train_regret_winner_18[slice21],\r\n",
        "       train_regret_winner_19[slice21],\r\n",
        "       train_regret_winner_20[slice21]]\r\n",
        "\r\n",
        "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\r\n",
        "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\r\n",
        "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\r\n",
        "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\r\n",
        "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\r\n",
        "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wdJdIsWuw8r"
      },
      "source": [
        "# Iteration31 :\r\n",
        "\r\n",
        "slice31 = 30\r\n",
        "\r\n",
        "loser31 = [train_regret_loser_1[slice31],\r\n",
        "       train_regret_loser_2[slice31],\r\n",
        "       train_regret_loser_3[slice31],\r\n",
        "       train_regret_loser_4[slice31],\r\n",
        "       train_regret_loser_5[slice31],\r\n",
        "       train_regret_loser_6[slice31],\r\n",
        "       train_regret_loser_7[slice31],\r\n",
        "       train_regret_loser_8[slice31],\r\n",
        "       train_regret_loser_9[slice31],\r\n",
        "       train_regret_loser_10[slice31],\r\n",
        "       train_regret_loser_11[slice31],\r\n",
        "       train_regret_loser_12[slice31],\r\n",
        "       train_regret_loser_13[slice31],\r\n",
        "       train_regret_loser_14[slice31],\r\n",
        "       train_regret_loser_15[slice31],\r\n",
        "       train_regret_loser_16[slice31],\r\n",
        "       train_regret_loser_17[slice31],\r\n",
        "       train_regret_loser_18[slice31],\r\n",
        "       train_regret_loser_19[slice31],\r\n",
        "       train_regret_loser_20[slice31]]\r\n",
        "\r\n",
        "winner31 = [train_regret_winner_1[slice31],\r\n",
        "       train_regret_winner_2[slice31],\r\n",
        "       train_regret_winner_3[slice31],\r\n",
        "       train_regret_winner_4[slice31],\r\n",
        "       train_regret_winner_5[slice31],\r\n",
        "       train_regret_winner_6[slice31],\r\n",
        "       train_regret_winner_7[slice31],\r\n",
        "       train_regret_winner_8[slice31],\r\n",
        "       train_regret_winner_9[slice31],\r\n",
        "       train_regret_winner_10[slice31],\r\n",
        "       train_regret_winner_11[slice31],\r\n",
        "       train_regret_winner_12[slice31],\r\n",
        "       train_regret_winner_13[slice31],\r\n",
        "       train_regret_winner_14[slice31],\r\n",
        "       train_regret_winner_15[slice31],\r\n",
        "       train_regret_winner_16[slice31],\r\n",
        "       train_regret_winner_17[slice31],\r\n",
        "       train_regret_winner_18[slice31],\r\n",
        "       train_regret_winner_19[slice31],\r\n",
        "       train_regret_winner_20[slice31]]\r\n",
        "\r\n",
        "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\r\n",
        "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\r\n",
        "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\r\n",
        "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\r\n",
        "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\r\n",
        "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWZBDufuw_Z"
      },
      "source": [
        "# Iteration41 :\r\n",
        "\r\n",
        "slice41 = 40\r\n",
        "\r\n",
        "loser41 = [train_regret_loser_1[slice41],\r\n",
        "       train_regret_loser_2[slice41],\r\n",
        "       train_regret_loser_3[slice41],\r\n",
        "       train_regret_loser_4[slice41],\r\n",
        "       train_regret_loser_5[slice41],\r\n",
        "       train_regret_loser_6[slice41],\r\n",
        "       train_regret_loser_7[slice41],\r\n",
        "       train_regret_loser_8[slice41],\r\n",
        "       train_regret_loser_9[slice41],\r\n",
        "       train_regret_loser_10[slice41],\r\n",
        "       train_regret_loser_11[slice41],\r\n",
        "       train_regret_loser_12[slice41],\r\n",
        "       train_regret_loser_13[slice41],\r\n",
        "       train_regret_loser_14[slice41],\r\n",
        "       train_regret_loser_15[slice41],\r\n",
        "       train_regret_loser_16[slice41],\r\n",
        "       train_regret_loser_17[slice41],\r\n",
        "       train_regret_loser_18[slice41],\r\n",
        "       train_regret_loser_19[slice41],\r\n",
        "       train_regret_loser_20[slice41]]\r\n",
        "\r\n",
        "winner41 = [train_regret_winner_1[slice41],\r\n",
        "       train_regret_winner_2[slice41],\r\n",
        "       train_regret_winner_3[slice41],\r\n",
        "       train_regret_winner_4[slice41],\r\n",
        "       train_regret_winner_5[slice41],\r\n",
        "       train_regret_winner_6[slice41],\r\n",
        "       train_regret_winner_7[slice41],\r\n",
        "       train_regret_winner_8[slice41],\r\n",
        "       train_regret_winner_9[slice41],\r\n",
        "       train_regret_winner_10[slice41],\r\n",
        "       train_regret_winner_11[slice41],\r\n",
        "       train_regret_winner_12[slice41],\r\n",
        "       train_regret_winner_13[slice41],\r\n",
        "       train_regret_winner_14[slice41],\r\n",
        "       train_regret_winner_15[slice41],\r\n",
        "       train_regret_winner_16[slice41],\r\n",
        "       train_regret_winner_17[slice41],\r\n",
        "       train_regret_winner_18[slice41],\r\n",
        "       train_regret_winner_19[slice41],\r\n",
        "       train_regret_winner_20[slice41]]\r\n",
        "\r\n",
        "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\r\n",
        "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\r\n",
        "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\r\n",
        "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\r\n",
        "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\r\n",
        "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwi_GzjiuxB6"
      },
      "source": [
        "# Iteration51 :\r\n",
        "\r\n",
        "slice51 = 50\r\n",
        "\r\n",
        "loser51 = [train_regret_loser_1[slice51],\r\n",
        "       train_regret_loser_2[slice51],\r\n",
        "       train_regret_loser_3[slice51],\r\n",
        "       train_regret_loser_4[slice51],\r\n",
        "       train_regret_loser_5[slice51],\r\n",
        "       train_regret_loser_6[slice51],\r\n",
        "       train_regret_loser_7[slice51],\r\n",
        "       train_regret_loser_8[slice51],\r\n",
        "       train_regret_loser_9[slice51],\r\n",
        "       train_regret_loser_10[slice51],\r\n",
        "       train_regret_loser_11[slice51],\r\n",
        "       train_regret_loser_12[slice51],\r\n",
        "       train_regret_loser_13[slice51],\r\n",
        "       train_regret_loser_14[slice51],\r\n",
        "       train_regret_loser_15[slice51],\r\n",
        "       train_regret_loser_16[slice51],\r\n",
        "       train_regret_loser_17[slice51],\r\n",
        "       train_regret_loser_18[slice51],\r\n",
        "       train_regret_loser_19[slice51],\r\n",
        "       train_regret_loser_20[slice51]]\r\n",
        "\r\n",
        "winner51 = [train_regret_winner_1[slice51],\r\n",
        "       train_regret_winner_2[slice51],\r\n",
        "       train_regret_winner_3[slice51],\r\n",
        "       train_regret_winner_4[slice51],\r\n",
        "       train_regret_winner_5[slice51],\r\n",
        "       train_regret_winner_6[slice51],\r\n",
        "       train_regret_winner_7[slice51],\r\n",
        "       train_regret_winner_8[slice51],\r\n",
        "       train_regret_winner_9[slice51],\r\n",
        "       train_regret_winner_10[slice51],\r\n",
        "       train_regret_winner_11[slice51],\r\n",
        "       train_regret_winner_12[slice51],\r\n",
        "       train_regret_winner_13[slice51],\r\n",
        "       train_regret_winner_14[slice51],\r\n",
        "       train_regret_winner_15[slice51],\r\n",
        "       train_regret_winner_16[slice51],\r\n",
        "       train_regret_winner_17[slice51],\r\n",
        "       train_regret_winner_18[slice51],\r\n",
        "       train_regret_winner_19[slice51],\r\n",
        "       train_regret_winner_20[slice51]]\r\n",
        "\r\n",
        "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\r\n",
        "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\r\n",
        "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\r\n",
        "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\r\n",
        "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\r\n",
        "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk9nlS6puxFZ"
      },
      "source": [
        "# Iteration61 :\r\n",
        "\r\n",
        "slice61 = 60\r\n",
        "\r\n",
        "loser61 = [train_regret_loser_1[slice61],\r\n",
        "       train_regret_loser_2[slice61],\r\n",
        "       train_regret_loser_3[slice61],\r\n",
        "       train_regret_loser_4[slice61],\r\n",
        "       train_regret_loser_5[slice61],\r\n",
        "       train_regret_loser_6[slice61],\r\n",
        "       train_regret_loser_7[slice61],\r\n",
        "       train_regret_loser_8[slice61],\r\n",
        "       train_regret_loser_9[slice61],\r\n",
        "       train_regret_loser_10[slice61],\r\n",
        "       train_regret_loser_11[slice61],\r\n",
        "       train_regret_loser_12[slice61],\r\n",
        "       train_regret_loser_13[slice61],\r\n",
        "       train_regret_loser_14[slice61],\r\n",
        "       train_regret_loser_15[slice61],\r\n",
        "       train_regret_loser_16[slice61],\r\n",
        "       train_regret_loser_17[slice61],\r\n",
        "       train_regret_loser_18[slice61],\r\n",
        "       train_regret_loser_19[slice61],\r\n",
        "       train_regret_loser_20[slice61]]\r\n",
        "\r\n",
        "winner61 = [train_regret_winner_1[slice61],\r\n",
        "       train_regret_winner_2[slice61],\r\n",
        "       train_regret_winner_3[slice61],\r\n",
        "       train_regret_winner_4[slice61],\r\n",
        "       train_regret_winner_5[slice61],\r\n",
        "       train_regret_winner_6[slice61],\r\n",
        "       train_regret_winner_7[slice61],\r\n",
        "       train_regret_winner_8[slice61],\r\n",
        "       train_regret_winner_9[slice61],\r\n",
        "       train_regret_winner_10[slice61],\r\n",
        "       train_regret_winner_11[slice61],\r\n",
        "       train_regret_winner_12[slice61],\r\n",
        "       train_regret_winner_13[slice61],\r\n",
        "       train_regret_winner_14[slice61],\r\n",
        "       train_regret_winner_15[slice61],\r\n",
        "       train_regret_winner_16[slice61],\r\n",
        "       train_regret_winner_17[slice61],\r\n",
        "       train_regret_winner_18[slice61],\r\n",
        "       train_regret_winner_19[slice61],\r\n",
        "       train_regret_winner_20[slice61]]\r\n",
        "\r\n",
        "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\r\n",
        "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\r\n",
        "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\r\n",
        "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\r\n",
        "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\r\n",
        "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMus9vDpuxHN"
      },
      "source": [
        "# Iteration71 :\r\n",
        "\r\n",
        "slice71 = 70\r\n",
        "\r\n",
        "loser71 = [train_regret_loser_1[slice71],\r\n",
        "       train_regret_loser_2[slice71],\r\n",
        "       train_regret_loser_3[slice71],\r\n",
        "       train_regret_loser_4[slice71],\r\n",
        "       train_regret_loser_5[slice71],\r\n",
        "       train_regret_loser_6[slice71],\r\n",
        "       train_regret_loser_7[slice71],\r\n",
        "       train_regret_loser_8[slice71],\r\n",
        "       train_regret_loser_9[slice71],\r\n",
        "       train_regret_loser_10[slice71],\r\n",
        "       train_regret_loser_11[slice71],\r\n",
        "       train_regret_loser_12[slice71],\r\n",
        "       train_regret_loser_13[slice71],\r\n",
        "       train_regret_loser_14[slice71],\r\n",
        "       train_regret_loser_15[slice71],\r\n",
        "       train_regret_loser_16[slice71],\r\n",
        "       train_regret_loser_17[slice71],\r\n",
        "       train_regret_loser_18[slice71],\r\n",
        "       train_regret_loser_19[slice71],\r\n",
        "       train_regret_loser_20[slice71]]\r\n",
        "\r\n",
        "winner71 = [train_regret_winner_1[slice71],\r\n",
        "       train_regret_winner_2[slice71],\r\n",
        "       train_regret_winner_3[slice71],\r\n",
        "       train_regret_winner_4[slice71],\r\n",
        "       train_regret_winner_5[slice71],\r\n",
        "       train_regret_winner_6[slice71],\r\n",
        "       train_regret_winner_7[slice71],\r\n",
        "       train_regret_winner_8[slice71],\r\n",
        "       train_regret_winner_9[slice71],\r\n",
        "       train_regret_winner_10[slice71],\r\n",
        "       train_regret_winner_11[slice71],\r\n",
        "       train_regret_winner_12[slice71],\r\n",
        "       train_regret_winner_13[slice71],\r\n",
        "       train_regret_winner_14[slice71],\r\n",
        "       train_regret_winner_15[slice71],\r\n",
        "       train_regret_winner_16[slice71],\r\n",
        "       train_regret_winner_17[slice71],\r\n",
        "       train_regret_winner_18[slice71],\r\n",
        "       train_regret_winner_19[slice71],\r\n",
        "       train_regret_winner_20[slice71]]\r\n",
        "\r\n",
        "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\r\n",
        "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\r\n",
        "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\r\n",
        "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\r\n",
        "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\r\n",
        "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Mg0x9vuxL4"
      },
      "source": [
        "# Iteration81 :\r\n",
        "\r\n",
        "slice81 = 80\r\n",
        "\r\n",
        "loser81 = [train_regret_loser_1[slice81],\r\n",
        "       train_regret_loser_2[slice81],\r\n",
        "       train_regret_loser_3[slice81],\r\n",
        "       train_regret_loser_4[slice81],\r\n",
        "       train_regret_loser_5[slice81],\r\n",
        "       train_regret_loser_6[slice81],\r\n",
        "       train_regret_loser_7[slice81],\r\n",
        "       train_regret_loser_8[slice81],\r\n",
        "       train_regret_loser_9[slice81],\r\n",
        "       train_regret_loser_10[slice81],\r\n",
        "       train_regret_loser_11[slice81],\r\n",
        "       train_regret_loser_12[slice81],\r\n",
        "       train_regret_loser_13[slice81],\r\n",
        "       train_regret_loser_14[slice81],\r\n",
        "       train_regret_loser_15[slice81],\r\n",
        "       train_regret_loser_16[slice81],\r\n",
        "       train_regret_loser_17[slice81],\r\n",
        "       train_regret_loser_18[slice81],\r\n",
        "       train_regret_loser_19[slice81],\r\n",
        "       train_regret_loser_20[slice81]]\r\n",
        "\r\n",
        "winner81 = [train_regret_winner_1[slice81],\r\n",
        "       train_regret_winner_2[slice81],\r\n",
        "       train_regret_winner_3[slice81],\r\n",
        "       train_regret_winner_4[slice81],\r\n",
        "       train_regret_winner_5[slice81],\r\n",
        "       train_regret_winner_6[slice81],\r\n",
        "       train_regret_winner_7[slice81],\r\n",
        "       train_regret_winner_8[slice81],\r\n",
        "       train_regret_winner_9[slice81],\r\n",
        "       train_regret_winner_10[slice81],\r\n",
        "       train_regret_winner_11[slice81],\r\n",
        "       train_regret_winner_12[slice81],\r\n",
        "       train_regret_winner_13[slice81],\r\n",
        "       train_regret_winner_14[slice81],\r\n",
        "       train_regret_winner_15[slice81],\r\n",
        "       train_regret_winner_16[slice81],\r\n",
        "       train_regret_winner_17[slice81],\r\n",
        "       train_regret_winner_18[slice81],\r\n",
        "       train_regret_winner_19[slice81],\r\n",
        "       train_regret_winner_20[slice81]]\r\n",
        "\r\n",
        "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\r\n",
        "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\r\n",
        "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\r\n",
        "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\r\n",
        "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\r\n",
        "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gu9K8XlteV4"
      },
      "source": [
        "# Iteration91 :\r\n",
        "\r\n",
        "slice91 = 90\r\n",
        "\r\n",
        "loser91 = [train_regret_loser_1[slice91],\r\n",
        "       train_regret_loser_2[slice91],\r\n",
        "       train_regret_loser_3[slice91],\r\n",
        "       train_regret_loser_4[slice91],\r\n",
        "       train_regret_loser_5[slice91],\r\n",
        "       train_regret_loser_6[slice91],\r\n",
        "       train_regret_loser_7[slice91],\r\n",
        "       train_regret_loser_8[slice91],\r\n",
        "       train_regret_loser_9[slice91],\r\n",
        "       train_regret_loser_10[slice91],\r\n",
        "       train_regret_loser_11[slice91],\r\n",
        "       train_regret_loser_12[slice91],\r\n",
        "       train_regret_loser_13[slice91],\r\n",
        "       train_regret_loser_14[slice91],\r\n",
        "       train_regret_loser_15[slice91],\r\n",
        "       train_regret_loser_16[slice91],\r\n",
        "       train_regret_loser_17[slice91],\r\n",
        "       train_regret_loser_18[slice91],\r\n",
        "       train_regret_loser_19[slice91],\r\n",
        "       train_regret_loser_20[slice91]]\r\n",
        "\r\n",
        "winner91 = [train_regret_winner_1[slice91],\r\n",
        "       train_regret_winner_2[slice91],\r\n",
        "       train_regret_winner_3[slice91],\r\n",
        "       train_regret_winner_4[slice91],\r\n",
        "       train_regret_winner_5[slice91],\r\n",
        "       train_regret_winner_6[slice91],\r\n",
        "       train_regret_winner_7[slice91],\r\n",
        "       train_regret_winner_8[slice91],\r\n",
        "       train_regret_winner_9[slice91],\r\n",
        "       train_regret_winner_10[slice91],\r\n",
        "       train_regret_winner_11[slice91],\r\n",
        "       train_regret_winner_12[slice91],\r\n",
        "       train_regret_winner_13[slice91],\r\n",
        "       train_regret_winner_14[slice91],\r\n",
        "       train_regret_winner_15[slice91],\r\n",
        "       train_regret_winner_16[slice91],\r\n",
        "       train_regret_winner_17[slice91],\r\n",
        "       train_regret_winner_18[slice91],\r\n",
        "       train_regret_winner_19[slice91],\r\n",
        "       train_regret_winner_20[slice91]]\r\n",
        "\r\n",
        "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\r\n",
        "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\r\n",
        "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\r\n",
        "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\r\n",
        "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\r\n",
        "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVVeft6steXv"
      },
      "source": [
        "# Iteration101 :\r\n",
        "\r\n",
        "slice101 = 100\r\n",
        "\r\n",
        "loser101 = [train_regret_loser_1[slice101],\r\n",
        "       train_regret_loser_2[slice101],\r\n",
        "       train_regret_loser_3[slice101],\r\n",
        "       train_regret_loser_4[slice101],\r\n",
        "       train_regret_loser_5[slice101],\r\n",
        "       train_regret_loser_6[slice101],\r\n",
        "       train_regret_loser_7[slice101],\r\n",
        "       train_regret_loser_8[slice101],\r\n",
        "       train_regret_loser_9[slice101],\r\n",
        "       train_regret_loser_10[slice101],\r\n",
        "       train_regret_loser_11[slice101],\r\n",
        "       train_regret_loser_12[slice101],\r\n",
        "       train_regret_loser_13[slice101],\r\n",
        "       train_regret_loser_14[slice101],\r\n",
        "       train_regret_loser_15[slice101],\r\n",
        "       train_regret_loser_16[slice101],\r\n",
        "       train_regret_loser_17[slice101],\r\n",
        "       train_regret_loser_18[slice101],\r\n",
        "       train_regret_loser_19[slice101],\r\n",
        "       train_regret_loser_20[slice101]]\r\n",
        "\r\n",
        "winner101 = [train_regret_winner_1[slice101],\r\n",
        "       train_regret_winner_2[slice101],\r\n",
        "       train_regret_winner_3[slice101],\r\n",
        "       train_regret_winner_4[slice101],\r\n",
        "       train_regret_winner_5[slice101],\r\n",
        "       train_regret_winner_6[slice101],\r\n",
        "       train_regret_winner_7[slice101],\r\n",
        "       train_regret_winner_8[slice101],\r\n",
        "       train_regret_winner_9[slice101],\r\n",
        "       train_regret_winner_10[slice101],\r\n",
        "       train_regret_winner_11[slice101],\r\n",
        "       train_regret_winner_12[slice101],\r\n",
        "       train_regret_winner_13[slice101],\r\n",
        "       train_regret_winner_14[slice101],\r\n",
        "       train_regret_winner_15[slice101],\r\n",
        "       train_regret_winner_16[slice101],\r\n",
        "       train_regret_winner_17[slice101],\r\n",
        "       train_regret_winner_18[slice101],\r\n",
        "       train_regret_winner_19[slice101],\r\n",
        "       train_regret_winner_20[slice101]]\r\n",
        "\r\n",
        "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\r\n",
        "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\r\n",
        "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\r\n",
        "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\r\n",
        "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\r\n",
        "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maqbtTXXtebT"
      },
      "source": [
        "# Iteration2 :\r\n",
        "\r\n",
        "slice2 = 1\r\n",
        "\r\n",
        "loser2 = [train_regret_loser_1[slice2],\r\n",
        "       train_regret_loser_2[slice2],\r\n",
        "       train_regret_loser_3[slice2],\r\n",
        "       train_regret_loser_4[slice2],\r\n",
        "       train_regret_loser_5[slice2],\r\n",
        "       train_regret_loser_6[slice2],\r\n",
        "       train_regret_loser_7[slice2],\r\n",
        "       train_regret_loser_8[slice2],\r\n",
        "       train_regret_loser_9[slice2],\r\n",
        "       train_regret_loser_10[slice2],\r\n",
        "       train_regret_loser_11[slice2],\r\n",
        "       train_regret_loser_12[slice2],\r\n",
        "       train_regret_loser_13[slice2],\r\n",
        "       train_regret_loser_14[slice2],\r\n",
        "       train_regret_loser_15[slice2],\r\n",
        "       train_regret_loser_16[slice2],\r\n",
        "       train_regret_loser_17[slice2],\r\n",
        "       train_regret_loser_18[slice2],\r\n",
        "       train_regret_loser_19[slice2],\r\n",
        "       train_regret_loser_20[slice2]]\r\n",
        "\r\n",
        "winner2 = [train_regret_winner_1[slice2],\r\n",
        "       train_regret_winner_2[slice2],\r\n",
        "       train_regret_winner_3[slice2],\r\n",
        "       train_regret_winner_4[slice2],\r\n",
        "       train_regret_winner_5[slice2],\r\n",
        "       train_regret_winner_6[slice2],\r\n",
        "       train_regret_winner_7[slice2],\r\n",
        "       train_regret_winner_8[slice2],\r\n",
        "       train_regret_winner_9[slice2],\r\n",
        "       train_regret_winner_10[slice2],\r\n",
        "       train_regret_winner_11[slice2],\r\n",
        "       train_regret_winner_12[slice2],\r\n",
        "       train_regret_winner_13[slice2],\r\n",
        "       train_regret_winner_14[slice2],\r\n",
        "       train_regret_winner_15[slice2],\r\n",
        "       train_regret_winner_16[slice2],\r\n",
        "       train_regret_winner_17[slice2],\r\n",
        "       train_regret_winner_18[slice2],\r\n",
        "       train_regret_winner_19[slice2],\r\n",
        "       train_regret_winner_20[slice2]]\r\n",
        "\r\n",
        "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\r\n",
        "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\r\n",
        "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\r\n",
        "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\r\n",
        "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\r\n",
        "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIBh0dQw-ZM"
      },
      "source": [
        "# Iteration12 :\r\n",
        "\r\n",
        "slice12 = 11\r\n",
        "\r\n",
        "loser12 = [train_regret_loser_1[slice12],\r\n",
        "       train_regret_loser_2[slice12],\r\n",
        "       train_regret_loser_3[slice12],\r\n",
        "       train_regret_loser_4[slice12],\r\n",
        "       train_regret_loser_5[slice12],\r\n",
        "       train_regret_loser_6[slice12],\r\n",
        "       train_regret_loser_7[slice12],\r\n",
        "       train_regret_loser_8[slice12],\r\n",
        "       train_regret_loser_9[slice12],\r\n",
        "       train_regret_loser_10[slice12],\r\n",
        "       train_regret_loser_11[slice12],\r\n",
        "       train_regret_loser_12[slice12],\r\n",
        "       train_regret_loser_13[slice12],\r\n",
        "       train_regret_loser_14[slice12],\r\n",
        "       train_regret_loser_15[slice12],\r\n",
        "       train_regret_loser_16[slice12],\r\n",
        "       train_regret_loser_17[slice12],\r\n",
        "       train_regret_loser_18[slice12],\r\n",
        "       train_regret_loser_19[slice12],\r\n",
        "       train_regret_loser_20[slice12]]\r\n",
        "\r\n",
        "winner12 = [train_regret_winner_1[slice12],\r\n",
        "       train_regret_winner_2[slice12],\r\n",
        "       train_regret_winner_3[slice12],\r\n",
        "       train_regret_winner_4[slice12],\r\n",
        "       train_regret_winner_5[slice12],\r\n",
        "       train_regret_winner_6[slice12],\r\n",
        "       train_regret_winner_7[slice12],\r\n",
        "       train_regret_winner_8[slice12],\r\n",
        "       train_regret_winner_9[slice12],\r\n",
        "       train_regret_winner_10[slice12],\r\n",
        "       train_regret_winner_11[slice12],\r\n",
        "       train_regret_winner_12[slice12],\r\n",
        "       train_regret_winner_13[slice12],\r\n",
        "       train_regret_winner_14[slice12],\r\n",
        "       train_regret_winner_15[slice12],\r\n",
        "       train_regret_winner_16[slice12],\r\n",
        "       train_regret_winner_17[slice12],\r\n",
        "       train_regret_winner_18[slice12],\r\n",
        "       train_regret_winner_19[slice12],\r\n",
        "       train_regret_winner_20[slice12]]\r\n",
        "\r\n",
        "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\r\n",
        "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\r\n",
        "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\r\n",
        "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\r\n",
        "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjIhsei3w-cV"
      },
      "source": [
        "# Iteration22 :\r\n",
        "\r\n",
        "slice22 = 21\r\n",
        "\r\n",
        "loser22 = [train_regret_loser_1[slice22],\r\n",
        "       train_regret_loser_2[slice22],\r\n",
        "       train_regret_loser_3[slice22],\r\n",
        "       train_regret_loser_4[slice22],\r\n",
        "       train_regret_loser_5[slice22],\r\n",
        "       train_regret_loser_6[slice22],\r\n",
        "       train_regret_loser_7[slice22],\r\n",
        "       train_regret_loser_8[slice22],\r\n",
        "       train_regret_loser_9[slice22],\r\n",
        "       train_regret_loser_10[slice22],\r\n",
        "       train_regret_loser_11[slice22],\r\n",
        "       train_regret_loser_12[slice22],\r\n",
        "       train_regret_loser_13[slice22],\r\n",
        "       train_regret_loser_14[slice22],\r\n",
        "       train_regret_loser_15[slice22],\r\n",
        "       train_regret_loser_16[slice22],\r\n",
        "       train_regret_loser_17[slice22],\r\n",
        "       train_regret_loser_18[slice22],\r\n",
        "       train_regret_loser_19[slice22],\r\n",
        "       train_regret_loser_20[slice22]]\r\n",
        "\r\n",
        "winner22 = [train_regret_winner_1[slice22],\r\n",
        "       train_regret_winner_2[slice22],\r\n",
        "       train_regret_winner_3[slice22],\r\n",
        "       train_regret_winner_4[slice22],\r\n",
        "       train_regret_winner_5[slice22],\r\n",
        "       train_regret_winner_6[slice22],\r\n",
        "       train_regret_winner_7[slice22],\r\n",
        "       train_regret_winner_8[slice22],\r\n",
        "       train_regret_winner_9[slice22],\r\n",
        "       train_regret_winner_10[slice22],\r\n",
        "       train_regret_winner_11[slice22],\r\n",
        "       train_regret_winner_12[slice22],\r\n",
        "       train_regret_winner_13[slice22],\r\n",
        "       train_regret_winner_14[slice22],\r\n",
        "       train_regret_winner_15[slice22],\r\n",
        "       train_regret_winner_16[slice22],\r\n",
        "       train_regret_winner_17[slice22],\r\n",
        "       train_regret_winner_18[slice22],\r\n",
        "       train_regret_winner_19[slice22],\r\n",
        "       train_regret_winner_20[slice22]]\r\n",
        "\r\n",
        "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\r\n",
        "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\r\n",
        "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\r\n",
        "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\r\n",
        "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\r\n",
        "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur9p9GkHw-fq"
      },
      "source": [
        "# Iteration32 :\r\n",
        "\r\n",
        "slice32 = 31\r\n",
        "\r\n",
        "loser32 = [train_regret_loser_1[slice32],\r\n",
        "       train_regret_loser_2[slice32],\r\n",
        "       train_regret_loser_3[slice32],\r\n",
        "       train_regret_loser_4[slice32],\r\n",
        "       train_regret_loser_5[slice32],\r\n",
        "       train_regret_loser_6[slice32],\r\n",
        "       train_regret_loser_7[slice32],\r\n",
        "       train_regret_loser_8[slice32],\r\n",
        "       train_regret_loser_9[slice32],\r\n",
        "       train_regret_loser_10[slice32],\r\n",
        "       train_regret_loser_11[slice32],\r\n",
        "       train_regret_loser_12[slice32],\r\n",
        "       train_regret_loser_13[slice32],\r\n",
        "       train_regret_loser_14[slice32],\r\n",
        "       train_regret_loser_15[slice32],\r\n",
        "       train_regret_loser_16[slice32],\r\n",
        "       train_regret_loser_17[slice32],\r\n",
        "       train_regret_loser_18[slice32],\r\n",
        "       train_regret_loser_19[slice32],\r\n",
        "       train_regret_loser_20[slice32]]\r\n",
        "\r\n",
        "winner32 = [train_regret_winner_1[slice32],\r\n",
        "       train_regret_winner_2[slice32],\r\n",
        "       train_regret_winner_3[slice32],\r\n",
        "       train_regret_winner_4[slice32],\r\n",
        "       train_regret_winner_5[slice32],\r\n",
        "       train_regret_winner_6[slice32],\r\n",
        "       train_regret_winner_7[slice32],\r\n",
        "       train_regret_winner_8[slice32],\r\n",
        "       train_regret_winner_9[slice32],\r\n",
        "       train_regret_winner_10[slice32],\r\n",
        "       train_regret_winner_11[slice32],\r\n",
        "       train_regret_winner_12[slice32],\r\n",
        "       train_regret_winner_13[slice32],\r\n",
        "       train_regret_winner_14[slice32],\r\n",
        "       train_regret_winner_15[slice32],\r\n",
        "       train_regret_winner_16[slice32],\r\n",
        "       train_regret_winner_17[slice32],\r\n",
        "       train_regret_winner_18[slice32],\r\n",
        "       train_regret_winner_19[slice32],\r\n",
        "       train_regret_winner_20[slice32]]\r\n",
        "\r\n",
        "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\r\n",
        "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\r\n",
        "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\r\n",
        "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\r\n",
        "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\r\n",
        "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_rzHB8w-hy"
      },
      "source": [
        "# Iteration42 :\r\n",
        "\r\n",
        "slice42 = 41\r\n",
        "\r\n",
        "loser42 = [train_regret_loser_1[slice42],\r\n",
        "       train_regret_loser_2[slice42],\r\n",
        "       train_regret_loser_3[slice42],\r\n",
        "       train_regret_loser_4[slice42],\r\n",
        "       train_regret_loser_5[slice42],\r\n",
        "       train_regret_loser_6[slice42],\r\n",
        "       train_regret_loser_7[slice42],\r\n",
        "       train_regret_loser_8[slice42],\r\n",
        "       train_regret_loser_9[slice42],\r\n",
        "       train_regret_loser_10[slice42],\r\n",
        "       train_regret_loser_11[slice42],\r\n",
        "       train_regret_loser_12[slice42],\r\n",
        "       train_regret_loser_13[slice42],\r\n",
        "       train_regret_loser_14[slice42],\r\n",
        "       train_regret_loser_15[slice42],\r\n",
        "       train_regret_loser_16[slice42],\r\n",
        "       train_regret_loser_17[slice42],\r\n",
        "       train_regret_loser_18[slice42],\r\n",
        "       train_regret_loser_19[slice42],\r\n",
        "       train_regret_loser_20[slice42]]\r\n",
        "\r\n",
        "winner42 = [train_regret_winner_1[slice42],\r\n",
        "       train_regret_winner_2[slice42],\r\n",
        "       train_regret_winner_3[slice42],\r\n",
        "       train_regret_winner_4[slice42],\r\n",
        "       train_regret_winner_5[slice42],\r\n",
        "       train_regret_winner_6[slice42],\r\n",
        "       train_regret_winner_7[slice42],\r\n",
        "       train_regret_winner_8[slice42],\r\n",
        "       train_regret_winner_9[slice42],\r\n",
        "       train_regret_winner_10[slice42],\r\n",
        "       train_regret_winner_11[slice42],\r\n",
        "       train_regret_winner_12[slice42],\r\n",
        "       train_regret_winner_13[slice42],\r\n",
        "       train_regret_winner_14[slice42],\r\n",
        "       train_regret_winner_15[slice42],\r\n",
        "       train_regret_winner_16[slice42],\r\n",
        "       train_regret_winner_17[slice42],\r\n",
        "       train_regret_winner_18[slice42],\r\n",
        "       train_regret_winner_19[slice42],\r\n",
        "       train_regret_winner_20[slice42]]\r\n",
        "\r\n",
        "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\r\n",
        "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\r\n",
        "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\r\n",
        "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\r\n",
        "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\r\n",
        "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKUUG6vAw-kz"
      },
      "source": [
        "# Iteration52 :\r\n",
        "\r\n",
        "slice52 = 51\r\n",
        "\r\n",
        "loser52 = [train_regret_loser_1[slice52],\r\n",
        "       train_regret_loser_2[slice52],\r\n",
        "       train_regret_loser_3[slice52],\r\n",
        "       train_regret_loser_4[slice52],\r\n",
        "       train_regret_loser_5[slice52],\r\n",
        "       train_regret_loser_6[slice52],\r\n",
        "       train_regret_loser_7[slice52],\r\n",
        "       train_regret_loser_8[slice52],\r\n",
        "       train_regret_loser_9[slice52],\r\n",
        "       train_regret_loser_10[slice52],\r\n",
        "       train_regret_loser_11[slice52],\r\n",
        "       train_regret_loser_12[slice52],\r\n",
        "       train_regret_loser_13[slice52],\r\n",
        "       train_regret_loser_14[slice52],\r\n",
        "       train_regret_loser_15[slice52],\r\n",
        "       train_regret_loser_16[slice52],\r\n",
        "       train_regret_loser_17[slice52],\r\n",
        "       train_regret_loser_18[slice52],\r\n",
        "       train_regret_loser_19[slice52],\r\n",
        "       train_regret_loser_20[slice52]]\r\n",
        "\r\n",
        "winner52 = [train_regret_winner_1[slice52],\r\n",
        "       train_regret_winner_2[slice52],\r\n",
        "       train_regret_winner_3[slice52],\r\n",
        "       train_regret_winner_4[slice52],\r\n",
        "       train_regret_winner_5[slice52],\r\n",
        "       train_regret_winner_6[slice52],\r\n",
        "       train_regret_winner_7[slice52],\r\n",
        "       train_regret_winner_8[slice52],\r\n",
        "       train_regret_winner_9[slice52],\r\n",
        "       train_regret_winner_10[slice52],\r\n",
        "       train_regret_winner_11[slice52],\r\n",
        "       train_regret_winner_12[slice52],\r\n",
        "       train_regret_winner_13[slice52],\r\n",
        "       train_regret_winner_14[slice52],\r\n",
        "       train_regret_winner_15[slice52],\r\n",
        "       train_regret_winner_16[slice52],\r\n",
        "       train_regret_winner_17[slice52],\r\n",
        "       train_regret_winner_18[slice52],\r\n",
        "       train_regret_winner_19[slice52],\r\n",
        "       train_regret_winner_20[slice52]]\r\n",
        "\r\n",
        "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\r\n",
        "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\r\n",
        "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\r\n",
        "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\r\n",
        "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\r\n",
        "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9cJNkew-nY"
      },
      "source": [
        "# Iteration62 :\r\n",
        "\r\n",
        "slice62 = 61\r\n",
        "\r\n",
        "loser62 = [train_regret_loser_1[slice62],\r\n",
        "       train_regret_loser_2[slice62],\r\n",
        "       train_regret_loser_3[slice62],\r\n",
        "       train_regret_loser_4[slice62],\r\n",
        "       train_regret_loser_5[slice62],\r\n",
        "       train_regret_loser_6[slice62],\r\n",
        "       train_regret_loser_7[slice62],\r\n",
        "       train_regret_loser_8[slice62],\r\n",
        "       train_regret_loser_9[slice62],\r\n",
        "       train_regret_loser_10[slice62],\r\n",
        "       train_regret_loser_11[slice62],\r\n",
        "       train_regret_loser_12[slice62],\r\n",
        "       train_regret_loser_13[slice62],\r\n",
        "       train_regret_loser_14[slice62],\r\n",
        "       train_regret_loser_15[slice62],\r\n",
        "       train_regret_loser_16[slice62],\r\n",
        "       train_regret_loser_17[slice62],\r\n",
        "       train_regret_loser_18[slice62],\r\n",
        "       train_regret_loser_19[slice62],\r\n",
        "       train_regret_loser_20[slice62]]\r\n",
        "\r\n",
        "winner62 = [train_regret_winner_1[slice62],\r\n",
        "       train_regret_winner_2[slice62],\r\n",
        "       train_regret_winner_3[slice62],\r\n",
        "       train_regret_winner_4[slice62],\r\n",
        "       train_regret_winner_5[slice62],\r\n",
        "       train_regret_winner_6[slice62],\r\n",
        "       train_regret_winner_7[slice62],\r\n",
        "       train_regret_winner_8[slice62],\r\n",
        "       train_regret_winner_9[slice62],\r\n",
        "       train_regret_winner_10[slice62],\r\n",
        "       train_regret_winner_11[slice62],\r\n",
        "       train_regret_winner_12[slice62],\r\n",
        "       train_regret_winner_13[slice62],\r\n",
        "       train_regret_winner_14[slice62],\r\n",
        "       train_regret_winner_15[slice62],\r\n",
        "       train_regret_winner_16[slice62],\r\n",
        "       train_regret_winner_17[slice62],\r\n",
        "       train_regret_winner_18[slice62],\r\n",
        "       train_regret_winner_19[slice62],\r\n",
        "       train_regret_winner_20[slice62]]\r\n",
        "\r\n",
        "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\r\n",
        "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\r\n",
        "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\r\n",
        "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\r\n",
        "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\r\n",
        "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DZ8kHHmw-qK"
      },
      "source": [
        "# Iteration72 :\r\n",
        "\r\n",
        "slice72 = 71\r\n",
        "\r\n",
        "loser72 = [train_regret_loser_1[slice72],\r\n",
        "       train_regret_loser_2[slice72],\r\n",
        "       train_regret_loser_3[slice72],\r\n",
        "       train_regret_loser_4[slice72],\r\n",
        "       train_regret_loser_5[slice72],\r\n",
        "       train_regret_loser_6[slice72],\r\n",
        "       train_regret_loser_7[slice72],\r\n",
        "       train_regret_loser_8[slice72],\r\n",
        "       train_regret_loser_9[slice72],\r\n",
        "       train_regret_loser_10[slice72],\r\n",
        "       train_regret_loser_11[slice72],\r\n",
        "       train_regret_loser_12[slice72],\r\n",
        "       train_regret_loser_13[slice72],\r\n",
        "       train_regret_loser_14[slice72],\r\n",
        "       train_regret_loser_15[slice72],\r\n",
        "       train_regret_loser_16[slice72],\r\n",
        "       train_regret_loser_17[slice72],\r\n",
        "       train_regret_loser_18[slice72],\r\n",
        "       train_regret_loser_19[slice72],\r\n",
        "       train_regret_loser_20[slice72]]\r\n",
        "\r\n",
        "winner72 = [train_regret_winner_1[slice72],\r\n",
        "       train_regret_winner_2[slice72],\r\n",
        "       train_regret_winner_3[slice72],\r\n",
        "       train_regret_winner_4[slice72],\r\n",
        "       train_regret_winner_5[slice72],\r\n",
        "       train_regret_winner_6[slice72],\r\n",
        "       train_regret_winner_7[slice72],\r\n",
        "       train_regret_winner_8[slice72],\r\n",
        "       train_regret_winner_9[slice72],\r\n",
        "       train_regret_winner_10[slice72],\r\n",
        "       train_regret_winner_11[slice72],\r\n",
        "       train_regret_winner_12[slice72],\r\n",
        "       train_regret_winner_13[slice72],\r\n",
        "       train_regret_winner_14[slice72],\r\n",
        "       train_regret_winner_15[slice72],\r\n",
        "       train_regret_winner_16[slice72],\r\n",
        "       train_regret_winner_17[slice72],\r\n",
        "       train_regret_winner_18[slice72],\r\n",
        "       train_regret_winner_19[slice72],\r\n",
        "       train_regret_winner_20[slice72]]\r\n",
        "\r\n",
        "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\r\n",
        "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\r\n",
        "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\r\n",
        "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\r\n",
        "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\r\n",
        "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBzBvnVyw-sz"
      },
      "source": [
        "# Iteration82 :\r\n",
        "\r\n",
        "slice82 = 81\r\n",
        "\r\n",
        "loser82 = [train_regret_loser_1[slice82],\r\n",
        "       train_regret_loser_2[slice82],\r\n",
        "       train_regret_loser_3[slice82],\r\n",
        "       train_regret_loser_4[slice82],\r\n",
        "       train_regret_loser_5[slice82],\r\n",
        "       train_regret_loser_6[slice82],\r\n",
        "       train_regret_loser_7[slice82],\r\n",
        "       train_regret_loser_8[slice82],\r\n",
        "       train_regret_loser_9[slice82],\r\n",
        "       train_regret_loser_10[slice82],\r\n",
        "       train_regret_loser_11[slice82],\r\n",
        "       train_regret_loser_12[slice82],\r\n",
        "       train_regret_loser_13[slice82],\r\n",
        "       train_regret_loser_14[slice82],\r\n",
        "       train_regret_loser_15[slice82],\r\n",
        "       train_regret_loser_16[slice82],\r\n",
        "       train_regret_loser_17[slice82],\r\n",
        "       train_regret_loser_18[slice82],\r\n",
        "       train_regret_loser_19[slice82],\r\n",
        "       train_regret_loser_20[slice82]]\r\n",
        "\r\n",
        "winner82 = [train_regret_winner_1[slice82],\r\n",
        "       train_regret_winner_2[slice82],\r\n",
        "       train_regret_winner_3[slice82],\r\n",
        "       train_regret_winner_4[slice82],\r\n",
        "       train_regret_winner_5[slice82],\r\n",
        "       train_regret_winner_6[slice82],\r\n",
        "       train_regret_winner_7[slice82],\r\n",
        "       train_regret_winner_8[slice82],\r\n",
        "       train_regret_winner_9[slice82],\r\n",
        "       train_regret_winner_10[slice82],\r\n",
        "       train_regret_winner_11[slice82],\r\n",
        "       train_regret_winner_12[slice82],\r\n",
        "       train_regret_winner_13[slice82],\r\n",
        "       train_regret_winner_14[slice82],\r\n",
        "       train_regret_winner_15[slice82],\r\n",
        "       train_regret_winner_16[slice82],\r\n",
        "       train_regret_winner_17[slice82],\r\n",
        "       train_regret_winner_18[slice82],\r\n",
        "       train_regret_winner_19[slice82],\r\n",
        "       train_regret_winner_20[slice82]]\r\n",
        "\r\n",
        "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\r\n",
        "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\r\n",
        "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\r\n",
        "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\r\n",
        "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\r\n",
        "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qj--n6Bw-y2"
      },
      "source": [
        "# Iteration92 :\r\n",
        "\r\n",
        "slice92 = 91\r\n",
        "\r\n",
        "loser92 = [train_regret_loser_1[slice92],\r\n",
        "       train_regret_loser_2[slice92],\r\n",
        "       train_regret_loser_3[slice92],\r\n",
        "       train_regret_loser_4[slice92],\r\n",
        "       train_regret_loser_5[slice92],\r\n",
        "       train_regret_loser_6[slice92],\r\n",
        "       train_regret_loser_7[slice92],\r\n",
        "       train_regret_loser_8[slice92],\r\n",
        "       train_regret_loser_9[slice92],\r\n",
        "       train_regret_loser_10[slice92],\r\n",
        "       train_regret_loser_11[slice92],\r\n",
        "       train_regret_loser_12[slice92],\r\n",
        "       train_regret_loser_13[slice92],\r\n",
        "       train_regret_loser_14[slice92],\r\n",
        "       train_regret_loser_15[slice92],\r\n",
        "       train_regret_loser_16[slice92],\r\n",
        "       train_regret_loser_17[slice92],\r\n",
        "       train_regret_loser_18[slice92],\r\n",
        "       train_regret_loser_19[slice92],\r\n",
        "       train_regret_loser_20[slice92]]\r\n",
        "\r\n",
        "winner92 = [train_regret_winner_1[slice92],\r\n",
        "       train_regret_winner_2[slice92],\r\n",
        "       train_regret_winner_3[slice92],\r\n",
        "       train_regret_winner_4[slice92],\r\n",
        "       train_regret_winner_5[slice92],\r\n",
        "       train_regret_winner_6[slice92],\r\n",
        "       train_regret_winner_7[slice92],\r\n",
        "       train_regret_winner_8[slice92],\r\n",
        "       train_regret_winner_9[slice92],\r\n",
        "       train_regret_winner_10[slice92],\r\n",
        "       train_regret_winner_11[slice92],\r\n",
        "       train_regret_winner_12[slice92],\r\n",
        "       train_regret_winner_13[slice92],\r\n",
        "       train_regret_winner_14[slice92],\r\n",
        "       train_regret_winner_15[slice92],\r\n",
        "       train_regret_winner_16[slice92],\r\n",
        "       train_regret_winner_17[slice92],\r\n",
        "       train_regret_winner_18[slice92],\r\n",
        "       train_regret_winner_19[slice92],\r\n",
        "       train_regret_winner_20[slice92]]\r\n",
        "\r\n",
        "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\r\n",
        "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\r\n",
        "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\r\n",
        "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\r\n",
        "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\r\n",
        "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJ5KqLdw-0L"
      },
      "source": [
        "# Iteration3 :\r\n",
        "\r\n",
        "slice3 = 2\r\n",
        "\r\n",
        "loser3 = [train_regret_loser_1[slice3],\r\n",
        "       train_regret_loser_2[slice3],\r\n",
        "       train_regret_loser_3[slice3],\r\n",
        "       train_regret_loser_4[slice3],\r\n",
        "       train_regret_loser_5[slice3],\r\n",
        "       train_regret_loser_6[slice3],\r\n",
        "       train_regret_loser_7[slice3],\r\n",
        "       train_regret_loser_8[slice3],\r\n",
        "       train_regret_loser_9[slice3],\r\n",
        "       train_regret_loser_10[slice3],\r\n",
        "       train_regret_loser_11[slice3],\r\n",
        "       train_regret_loser_12[slice3],\r\n",
        "       train_regret_loser_13[slice3],\r\n",
        "       train_regret_loser_14[slice3],\r\n",
        "       train_regret_loser_15[slice3],\r\n",
        "       train_regret_loser_16[slice3],\r\n",
        "       train_regret_loser_17[slice3],\r\n",
        "       train_regret_loser_18[slice3],\r\n",
        "       train_regret_loser_19[slice3],\r\n",
        "       train_regret_loser_20[slice3]]\r\n",
        "\r\n",
        "winner3 = [train_regret_winner_1[slice3],\r\n",
        "       train_regret_winner_2[slice3],\r\n",
        "       train_regret_winner_3[slice3],\r\n",
        "       train_regret_winner_4[slice3],\r\n",
        "       train_regret_winner_5[slice3],\r\n",
        "       train_regret_winner_6[slice3],\r\n",
        "       train_regret_winner_7[slice3],\r\n",
        "       train_regret_winner_8[slice3],\r\n",
        "       train_regret_winner_9[slice3],\r\n",
        "       train_regret_winner_10[slice3],\r\n",
        "       train_regret_winner_11[slice3],\r\n",
        "       train_regret_winner_12[slice3],\r\n",
        "       train_regret_winner_13[slice3],\r\n",
        "       train_regret_winner_14[slice3],\r\n",
        "       train_regret_winner_15[slice3],\r\n",
        "       train_regret_winner_16[slice3],\r\n",
        "       train_regret_winner_17[slice3],\r\n",
        "       train_regret_winner_18[slice3],\r\n",
        "       train_regret_winner_19[slice3],\r\n",
        "       train_regret_winner_20[slice3]]\r\n",
        "\r\n",
        "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\r\n",
        "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\r\n",
        "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\r\n",
        "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\r\n",
        "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\r\n",
        "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkaoT1dyw-16"
      },
      "source": [
        "# Iteration13 :\r\n",
        "\r\n",
        "slice13 = 12\r\n",
        "\r\n",
        "loser13 = [train_regret_loser_1[slice13],\r\n",
        "       train_regret_loser_2[slice13],\r\n",
        "       train_regret_loser_3[slice13],\r\n",
        "       train_regret_loser_4[slice13],\r\n",
        "       train_regret_loser_5[slice13],\r\n",
        "       train_regret_loser_6[slice13],\r\n",
        "       train_regret_loser_7[slice13],\r\n",
        "       train_regret_loser_8[slice13],\r\n",
        "       train_regret_loser_9[slice13],\r\n",
        "       train_regret_loser_10[slice13],\r\n",
        "       train_regret_loser_11[slice13],\r\n",
        "       train_regret_loser_12[slice13],\r\n",
        "       train_regret_loser_13[slice13],\r\n",
        "       train_regret_loser_14[slice13],\r\n",
        "       train_regret_loser_15[slice13],\r\n",
        "       train_regret_loser_16[slice13],\r\n",
        "       train_regret_loser_17[slice13],\r\n",
        "       train_regret_loser_18[slice13],\r\n",
        "       train_regret_loser_19[slice13],\r\n",
        "       train_regret_loser_20[slice13]]\r\n",
        "\r\n",
        "winner13 = [train_regret_winner_1[slice13],\r\n",
        "       train_regret_winner_2[slice13],\r\n",
        "       train_regret_winner_3[slice13],\r\n",
        "       train_regret_winner_4[slice13],\r\n",
        "       train_regret_winner_5[slice13],\r\n",
        "       train_regret_winner_6[slice13],\r\n",
        "       train_regret_winner_7[slice13],\r\n",
        "       train_regret_winner_8[slice13],\r\n",
        "       train_regret_winner_9[slice13],\r\n",
        "       train_regret_winner_10[slice13],\r\n",
        "       train_regret_winner_11[slice13],\r\n",
        "       train_regret_winner_12[slice13],\r\n",
        "       train_regret_winner_13[slice13],\r\n",
        "       train_regret_winner_14[slice13],\r\n",
        "       train_regret_winner_15[slice13],\r\n",
        "       train_regret_winner_16[slice13],\r\n",
        "       train_regret_winner_17[slice13],\r\n",
        "       train_regret_winner_18[slice13],\r\n",
        "       train_regret_winner_19[slice13],\r\n",
        "       train_regret_winner_20[slice13]]\r\n",
        "\r\n",
        "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\r\n",
        "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\r\n",
        "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\r\n",
        "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\r\n",
        "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZbVMrBaw-4B"
      },
      "source": [
        "# Iteration23 :\r\n",
        "\r\n",
        "slice23 = 22\r\n",
        "\r\n",
        "loser23 = [train_regret_loser_1[slice23],\r\n",
        "       train_regret_loser_2[slice23],\r\n",
        "       train_regret_loser_3[slice23],\r\n",
        "       train_regret_loser_4[slice23],\r\n",
        "       train_regret_loser_5[slice23],\r\n",
        "       train_regret_loser_6[slice23],\r\n",
        "       train_regret_loser_7[slice23],\r\n",
        "       train_regret_loser_8[slice23],\r\n",
        "       train_regret_loser_9[slice23],\r\n",
        "       train_regret_loser_10[slice23],\r\n",
        "       train_regret_loser_11[slice23],\r\n",
        "       train_regret_loser_12[slice23],\r\n",
        "       train_regret_loser_13[slice23],\r\n",
        "       train_regret_loser_14[slice23],\r\n",
        "       train_regret_loser_15[slice23],\r\n",
        "       train_regret_loser_16[slice23],\r\n",
        "       train_regret_loser_17[slice23],\r\n",
        "       train_regret_loser_18[slice23],\r\n",
        "       train_regret_loser_19[slice23],\r\n",
        "       train_regret_loser_20[slice23]]\r\n",
        "\r\n",
        "winner23 = [train_regret_winner_1[slice23],\r\n",
        "       train_regret_winner_2[slice23],\r\n",
        "       train_regret_winner_3[slice23],\r\n",
        "       train_regret_winner_4[slice23],\r\n",
        "       train_regret_winner_5[slice23],\r\n",
        "       train_regret_winner_6[slice23],\r\n",
        "       train_regret_winner_7[slice23],\r\n",
        "       train_regret_winner_8[slice23],\r\n",
        "       train_regret_winner_9[slice23],\r\n",
        "       train_regret_winner_10[slice23],\r\n",
        "       train_regret_winner_11[slice23],\r\n",
        "       train_regret_winner_12[slice23],\r\n",
        "       train_regret_winner_13[slice23],\r\n",
        "       train_regret_winner_14[slice23],\r\n",
        "       train_regret_winner_15[slice23],\r\n",
        "       train_regret_winner_16[slice23],\r\n",
        "       train_regret_winner_17[slice23],\r\n",
        "       train_regret_winner_18[slice23],\r\n",
        "       train_regret_winner_19[slice23],\r\n",
        "       train_regret_winner_20[slice23]]\r\n",
        "\r\n",
        "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\r\n",
        "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\r\n",
        "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\r\n",
        "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\r\n",
        "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\r\n",
        "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUa6HRCUw-7G"
      },
      "source": [
        "# Iteration33 :\r\n",
        "\r\n",
        "slice33 = 32\r\n",
        "\r\n",
        "loser33 = [train_regret_loser_1[slice33],\r\n",
        "       train_regret_loser_2[slice33],\r\n",
        "       train_regret_loser_3[slice33],\r\n",
        "       train_regret_loser_4[slice33],\r\n",
        "       train_regret_loser_5[slice33],\r\n",
        "       train_regret_loser_6[slice33],\r\n",
        "       train_regret_loser_7[slice33],\r\n",
        "       train_regret_loser_8[slice33],\r\n",
        "       train_regret_loser_9[slice33],\r\n",
        "       train_regret_loser_10[slice33],\r\n",
        "       train_regret_loser_11[slice33],\r\n",
        "       train_regret_loser_12[slice33],\r\n",
        "       train_regret_loser_13[slice33],\r\n",
        "       train_regret_loser_14[slice33],\r\n",
        "       train_regret_loser_15[slice33],\r\n",
        "       train_regret_loser_16[slice33],\r\n",
        "       train_regret_loser_17[slice33],\r\n",
        "       train_regret_loser_18[slice33],\r\n",
        "       train_regret_loser_19[slice33],\r\n",
        "       train_regret_loser_20[slice33]]\r\n",
        "\r\n",
        "winner33 = [train_regret_winner_1[slice33],\r\n",
        "       train_regret_winner_2[slice33],\r\n",
        "       train_regret_winner_3[slice33],\r\n",
        "       train_regret_winner_4[slice33],\r\n",
        "       train_regret_winner_5[slice33],\r\n",
        "       train_regret_winner_6[slice33],\r\n",
        "       train_regret_winner_7[slice33],\r\n",
        "       train_regret_winner_8[slice33],\r\n",
        "       train_regret_winner_9[slice33],\r\n",
        "       train_regret_winner_10[slice33],\r\n",
        "       train_regret_winner_11[slice33],\r\n",
        "       train_regret_winner_12[slice33],\r\n",
        "       train_regret_winner_13[slice33],\r\n",
        "       train_regret_winner_14[slice33],\r\n",
        "       train_regret_winner_15[slice33],\r\n",
        "       train_regret_winner_16[slice33],\r\n",
        "       train_regret_winner_17[slice33],\r\n",
        "       train_regret_winner_18[slice33],\r\n",
        "       train_regret_winner_19[slice33],\r\n",
        "       train_regret_winner_20[slice33]]\r\n",
        "\r\n",
        "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\r\n",
        "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\r\n",
        "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\r\n",
        "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\r\n",
        "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\r\n",
        "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EVxQwsxw-9n"
      },
      "source": [
        "# Iteration43 :\r\n",
        "\r\n",
        "slice43 = 42\r\n",
        "\r\n",
        "loser43 = [train_regret_loser_1[slice43],\r\n",
        "       train_regret_loser_2[slice43],\r\n",
        "       train_regret_loser_3[slice43],\r\n",
        "       train_regret_loser_4[slice43],\r\n",
        "       train_regret_loser_5[slice43],\r\n",
        "       train_regret_loser_6[slice43],\r\n",
        "       train_regret_loser_7[slice43],\r\n",
        "       train_regret_loser_8[slice43],\r\n",
        "       train_regret_loser_9[slice43],\r\n",
        "       train_regret_loser_10[slice43],\r\n",
        "       train_regret_loser_11[slice43],\r\n",
        "       train_regret_loser_12[slice43],\r\n",
        "       train_regret_loser_13[slice43],\r\n",
        "       train_regret_loser_14[slice43],\r\n",
        "       train_regret_loser_15[slice43],\r\n",
        "       train_regret_loser_16[slice43],\r\n",
        "       train_regret_loser_17[slice43],\r\n",
        "       train_regret_loser_18[slice43],\r\n",
        "       train_regret_loser_19[slice43],\r\n",
        "       train_regret_loser_20[slice43]]\r\n",
        "\r\n",
        "winner43 = [train_regret_winner_1[slice43],\r\n",
        "       train_regret_winner_2[slice43],\r\n",
        "       train_regret_winner_3[slice43],\r\n",
        "       train_regret_winner_4[slice43],\r\n",
        "       train_regret_winner_5[slice43],\r\n",
        "       train_regret_winner_6[slice43],\r\n",
        "       train_regret_winner_7[slice43],\r\n",
        "       train_regret_winner_8[slice43],\r\n",
        "       train_regret_winner_9[slice43],\r\n",
        "       train_regret_winner_10[slice43],\r\n",
        "       train_regret_winner_11[slice43],\r\n",
        "       train_regret_winner_12[slice43],\r\n",
        "       train_regret_winner_13[slice43],\r\n",
        "       train_regret_winner_14[slice43],\r\n",
        "       train_regret_winner_15[slice43],\r\n",
        "       train_regret_winner_16[slice43],\r\n",
        "       train_regret_winner_17[slice43],\r\n",
        "       train_regret_winner_18[slice43],\r\n",
        "       train_regret_winner_19[slice43],\r\n",
        "       train_regret_winner_20[slice43]]\r\n",
        "\r\n",
        "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\r\n",
        "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\r\n",
        "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\r\n",
        "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\r\n",
        "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\r\n",
        "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQtRyMv-w_AZ"
      },
      "source": [
        "# Iteration53 :\r\n",
        "\r\n",
        "slice53 = 52\r\n",
        "\r\n",
        "loser53 = [train_regret_loser_1[slice53],\r\n",
        "       train_regret_loser_2[slice53],\r\n",
        "       train_regret_loser_3[slice53],\r\n",
        "       train_regret_loser_4[slice53],\r\n",
        "       train_regret_loser_5[slice53],\r\n",
        "       train_regret_loser_6[slice53],\r\n",
        "       train_regret_loser_7[slice53],\r\n",
        "       train_regret_loser_8[slice53],\r\n",
        "       train_regret_loser_9[slice53],\r\n",
        "       train_regret_loser_10[slice53],\r\n",
        "       train_regret_loser_11[slice53],\r\n",
        "       train_regret_loser_12[slice53],\r\n",
        "       train_regret_loser_13[slice53],\r\n",
        "       train_regret_loser_14[slice53],\r\n",
        "       train_regret_loser_15[slice53],\r\n",
        "       train_regret_loser_16[slice53],\r\n",
        "       train_regret_loser_17[slice53],\r\n",
        "       train_regret_loser_18[slice53],\r\n",
        "       train_regret_loser_19[slice53],\r\n",
        "       train_regret_loser_20[slice53]]\r\n",
        "\r\n",
        "winner53 = [train_regret_winner_1[slice53],\r\n",
        "       train_regret_winner_2[slice53],\r\n",
        "       train_regret_winner_3[slice53],\r\n",
        "       train_regret_winner_4[slice53],\r\n",
        "       train_regret_winner_5[slice53],\r\n",
        "       train_regret_winner_6[slice53],\r\n",
        "       train_regret_winner_7[slice53],\r\n",
        "       train_regret_winner_8[slice53],\r\n",
        "       train_regret_winner_9[slice53],\r\n",
        "       train_regret_winner_10[slice53],\r\n",
        "       train_regret_winner_11[slice53],\r\n",
        "       train_regret_winner_12[slice53],\r\n",
        "       train_regret_winner_13[slice53],\r\n",
        "       train_regret_winner_14[slice53],\r\n",
        "       train_regret_winner_15[slice53],\r\n",
        "       train_regret_winner_16[slice53],\r\n",
        "       train_regret_winner_17[slice53],\r\n",
        "       train_regret_winner_18[slice53],\r\n",
        "       train_regret_winner_19[slice53],\r\n",
        "       train_regret_winner_20[slice53]]\r\n",
        "\r\n",
        "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\r\n",
        "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\r\n",
        "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\r\n",
        "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\r\n",
        "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\r\n",
        "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CIMEXXYw_DR"
      },
      "source": [
        "# Iteration63 :\r\n",
        "\r\n",
        "slice63 = 62\r\n",
        "\r\n",
        "loser63 = [train_regret_loser_1[slice63],\r\n",
        "       train_regret_loser_2[slice63],\r\n",
        "       train_regret_loser_3[slice63],\r\n",
        "       train_regret_loser_4[slice63],\r\n",
        "       train_regret_loser_5[slice63],\r\n",
        "       train_regret_loser_6[slice63],\r\n",
        "       train_regret_loser_7[slice63],\r\n",
        "       train_regret_loser_8[slice63],\r\n",
        "       train_regret_loser_9[slice63],\r\n",
        "       train_regret_loser_10[slice63],\r\n",
        "       train_regret_loser_11[slice63],\r\n",
        "       train_regret_loser_12[slice63],\r\n",
        "       train_regret_loser_13[slice63],\r\n",
        "       train_regret_loser_14[slice63],\r\n",
        "       train_regret_loser_15[slice63],\r\n",
        "       train_regret_loser_16[slice63],\r\n",
        "       train_regret_loser_17[slice63],\r\n",
        "       train_regret_loser_18[slice63],\r\n",
        "       train_regret_loser_19[slice63],\r\n",
        "       train_regret_loser_20[slice63]]\r\n",
        "\r\n",
        "winner63 = [train_regret_winner_1[slice63],\r\n",
        "       train_regret_winner_2[slice63],\r\n",
        "       train_regret_winner_3[slice63],\r\n",
        "       train_regret_winner_4[slice63],\r\n",
        "       train_regret_winner_5[slice63],\r\n",
        "       train_regret_winner_6[slice63],\r\n",
        "       train_regret_winner_7[slice63],\r\n",
        "       train_regret_winner_8[slice63],\r\n",
        "       train_regret_winner_9[slice63],\r\n",
        "       train_regret_winner_10[slice63],\r\n",
        "       train_regret_winner_11[slice63],\r\n",
        "       train_regret_winner_12[slice63],\r\n",
        "       train_regret_winner_13[slice63],\r\n",
        "       train_regret_winner_14[slice63],\r\n",
        "       train_regret_winner_15[slice63],\r\n",
        "       train_regret_winner_16[slice63],\r\n",
        "       train_regret_winner_17[slice63],\r\n",
        "       train_regret_winner_18[slice63],\r\n",
        "       train_regret_winner_19[slice63],\r\n",
        "       train_regret_winner_20[slice63]]\r\n",
        "\r\n",
        "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\r\n",
        "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\r\n",
        "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\r\n",
        "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\r\n",
        "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\r\n",
        "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg4GfwU_w_GF"
      },
      "source": [
        "# Iteration73 :\r\n",
        "\r\n",
        "slice73 = 72\r\n",
        "\r\n",
        "loser73 = [train_regret_loser_1[slice73],\r\n",
        "       train_regret_loser_2[slice73],\r\n",
        "       train_regret_loser_3[slice73],\r\n",
        "       train_regret_loser_4[slice73],\r\n",
        "       train_regret_loser_5[slice73],\r\n",
        "       train_regret_loser_6[slice73],\r\n",
        "       train_regret_loser_7[slice73],\r\n",
        "       train_regret_loser_8[slice73],\r\n",
        "       train_regret_loser_9[slice73],\r\n",
        "       train_regret_loser_10[slice73],\r\n",
        "       train_regret_loser_11[slice73],\r\n",
        "       train_regret_loser_12[slice73],\r\n",
        "       train_regret_loser_13[slice73],\r\n",
        "       train_regret_loser_14[slice73],\r\n",
        "       train_regret_loser_15[slice73],\r\n",
        "       train_regret_loser_16[slice73],\r\n",
        "       train_regret_loser_17[slice73],\r\n",
        "       train_regret_loser_18[slice73],\r\n",
        "       train_regret_loser_19[slice73],\r\n",
        "       train_regret_loser_20[slice73]]\r\n",
        "\r\n",
        "winner73 = [train_regret_winner_1[slice73],\r\n",
        "       train_regret_winner_2[slice73],\r\n",
        "       train_regret_winner_3[slice73],\r\n",
        "       train_regret_winner_4[slice73],\r\n",
        "       train_regret_winner_5[slice73],\r\n",
        "       train_regret_winner_6[slice73],\r\n",
        "       train_regret_winner_7[slice73],\r\n",
        "       train_regret_winner_8[slice73],\r\n",
        "       train_regret_winner_9[slice73],\r\n",
        "       train_regret_winner_10[slice73],\r\n",
        "       train_regret_winner_11[slice73],\r\n",
        "       train_regret_winner_12[slice73],\r\n",
        "       train_regret_winner_13[slice73],\r\n",
        "       train_regret_winner_14[slice73],\r\n",
        "       train_regret_winner_15[slice73],\r\n",
        "       train_regret_winner_16[slice73],\r\n",
        "       train_regret_winner_17[slice73],\r\n",
        "       train_regret_winner_18[slice73],\r\n",
        "       train_regret_winner_19[slice73],\r\n",
        "       train_regret_winner_20[slice73]]\r\n",
        "\r\n",
        "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\r\n",
        "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\r\n",
        "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\r\n",
        "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\r\n",
        "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\r\n",
        "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WmpcEV4w_I9"
      },
      "source": [
        "# Iteration83 :\r\n",
        "\r\n",
        "slice83 = 82\r\n",
        "\r\n",
        "loser83 = [train_regret_loser_1[slice83],\r\n",
        "       train_regret_loser_2[slice83],\r\n",
        "       train_regret_loser_3[slice83],\r\n",
        "       train_regret_loser_4[slice83],\r\n",
        "       train_regret_loser_5[slice83],\r\n",
        "       train_regret_loser_6[slice83],\r\n",
        "       train_regret_loser_7[slice83],\r\n",
        "       train_regret_loser_8[slice83],\r\n",
        "       train_regret_loser_9[slice83],\r\n",
        "       train_regret_loser_10[slice83],\r\n",
        "       train_regret_loser_11[slice83],\r\n",
        "       train_regret_loser_12[slice83],\r\n",
        "       train_regret_loser_13[slice83],\r\n",
        "       train_regret_loser_14[slice83],\r\n",
        "       train_regret_loser_15[slice83],\r\n",
        "       train_regret_loser_16[slice83],\r\n",
        "       train_regret_loser_17[slice83],\r\n",
        "       train_regret_loser_18[slice83],\r\n",
        "       train_regret_loser_19[slice83],\r\n",
        "       train_regret_loser_20[slice83]]\r\n",
        "\r\n",
        "winner83 = [train_regret_winner_1[slice83],\r\n",
        "       train_regret_winner_2[slice83],\r\n",
        "       train_regret_winner_3[slice83],\r\n",
        "       train_regret_winner_4[slice83],\r\n",
        "       train_regret_winner_5[slice83],\r\n",
        "       train_regret_winner_6[slice83],\r\n",
        "       train_regret_winner_7[slice83],\r\n",
        "       train_regret_winner_8[slice83],\r\n",
        "       train_regret_winner_9[slice83],\r\n",
        "       train_regret_winner_10[slice83],\r\n",
        "       train_regret_winner_11[slice83],\r\n",
        "       train_regret_winner_12[slice83],\r\n",
        "       train_regret_winner_13[slice83],\r\n",
        "       train_regret_winner_14[slice83],\r\n",
        "       train_regret_winner_15[slice83],\r\n",
        "       train_regret_winner_16[slice83],\r\n",
        "       train_regret_winner_17[slice83],\r\n",
        "       train_regret_winner_18[slice83],\r\n",
        "       train_regret_winner_19[slice83],\r\n",
        "       train_regret_winner_20[slice83]]\r\n",
        "\r\n",
        "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\r\n",
        "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\r\n",
        "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\r\n",
        "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\r\n",
        "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\r\n",
        "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGPwoBZyw_MF"
      },
      "source": [
        "# Iteration93 :\r\n",
        "\r\n",
        "slice93 = 92\r\n",
        "\r\n",
        "loser93 = [train_regret_loser_1[slice93],\r\n",
        "       train_regret_loser_2[slice93],\r\n",
        "       train_regret_loser_3[slice93],\r\n",
        "       train_regret_loser_4[slice93],\r\n",
        "       train_regret_loser_5[slice93],\r\n",
        "       train_regret_loser_6[slice93],\r\n",
        "       train_regret_loser_7[slice93],\r\n",
        "       train_regret_loser_8[slice93],\r\n",
        "       train_regret_loser_9[slice93],\r\n",
        "       train_regret_loser_10[slice93],\r\n",
        "       train_regret_loser_11[slice93],\r\n",
        "       train_regret_loser_12[slice93],\r\n",
        "       train_regret_loser_13[slice93],\r\n",
        "       train_regret_loser_14[slice93],\r\n",
        "       train_regret_loser_15[slice93],\r\n",
        "       train_regret_loser_16[slice93],\r\n",
        "       train_regret_loser_17[slice93],\r\n",
        "       train_regret_loser_18[slice93],\r\n",
        "       train_regret_loser_19[slice93],\r\n",
        "       train_regret_loser_20[slice93]]\r\n",
        "\r\n",
        "winner93 = [train_regret_winner_1[slice93],\r\n",
        "       train_regret_winner_2[slice93],\r\n",
        "       train_regret_winner_3[slice93],\r\n",
        "       train_regret_winner_4[slice93],\r\n",
        "       train_regret_winner_5[slice93],\r\n",
        "       train_regret_winner_6[slice93],\r\n",
        "       train_regret_winner_7[slice93],\r\n",
        "       train_regret_winner_8[slice93],\r\n",
        "       train_regret_winner_9[slice93],\r\n",
        "       train_regret_winner_10[slice93],\r\n",
        "       train_regret_winner_11[slice93],\r\n",
        "       train_regret_winner_12[slice93],\r\n",
        "       train_regret_winner_13[slice93],\r\n",
        "       train_regret_winner_14[slice93],\r\n",
        "       train_regret_winner_15[slice93],\r\n",
        "       train_regret_winner_16[slice93],\r\n",
        "       train_regret_winner_17[slice93],\r\n",
        "       train_regret_winner_18[slice93],\r\n",
        "       train_regret_winner_19[slice93],\r\n",
        "       train_regret_winner_20[slice93]]\r\n",
        "\r\n",
        "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\r\n",
        "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\r\n",
        "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\r\n",
        "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\r\n",
        "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\r\n",
        "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ghdGzPw_Oq"
      },
      "source": [
        "# Iteration4 :\r\n",
        "\r\n",
        "slice4 = 3\r\n",
        "\r\n",
        "loser4 = [train_regret_loser_1[slice4],\r\n",
        "       train_regret_loser_2[slice4],\r\n",
        "       train_regret_loser_3[slice4],\r\n",
        "       train_regret_loser_4[slice4],\r\n",
        "       train_regret_loser_5[slice4],\r\n",
        "       train_regret_loser_6[slice4],\r\n",
        "       train_regret_loser_7[slice4],\r\n",
        "       train_regret_loser_8[slice4],\r\n",
        "       train_regret_loser_9[slice4],\r\n",
        "       train_regret_loser_10[slice4],\r\n",
        "       train_regret_loser_11[slice4],\r\n",
        "       train_regret_loser_12[slice4],\r\n",
        "       train_regret_loser_13[slice4],\r\n",
        "       train_regret_loser_14[slice4],\r\n",
        "       train_regret_loser_15[slice4],\r\n",
        "       train_regret_loser_16[slice4],\r\n",
        "       train_regret_loser_17[slice4],\r\n",
        "       train_regret_loser_18[slice4],\r\n",
        "       train_regret_loser_19[slice4],\r\n",
        "       train_regret_loser_20[slice4]]\r\n",
        "\r\n",
        "winner4 = [train_regret_winner_1[slice4],\r\n",
        "       train_regret_winner_2[slice4],\r\n",
        "       train_regret_winner_3[slice4],\r\n",
        "       train_regret_winner_4[slice4],\r\n",
        "       train_regret_winner_5[slice4],\r\n",
        "       train_regret_winner_6[slice4],\r\n",
        "       train_regret_winner_7[slice4],\r\n",
        "       train_regret_winner_8[slice4],\r\n",
        "       train_regret_winner_9[slice4],\r\n",
        "       train_regret_winner_10[slice4],\r\n",
        "       train_regret_winner_11[slice4],\r\n",
        "       train_regret_winner_12[slice4],\r\n",
        "       train_regret_winner_13[slice4],\r\n",
        "       train_regret_winner_14[slice4],\r\n",
        "       train_regret_winner_15[slice4],\r\n",
        "       train_regret_winner_16[slice4],\r\n",
        "       train_regret_winner_17[slice4],\r\n",
        "       train_regret_winner_18[slice4],\r\n",
        "       train_regret_winner_19[slice4],\r\n",
        "       train_regret_winner_20[slice4]]\r\n",
        "\r\n",
        "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\r\n",
        "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\r\n",
        "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\r\n",
        "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\r\n",
        "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\r\n",
        "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNvDt4QMw_Rz"
      },
      "source": [
        "# Iteration14 :\r\n",
        "\r\n",
        "slice14 = 13\r\n",
        "\r\n",
        "loser14 = [train_regret_loser_1[slice14],\r\n",
        "       train_regret_loser_2[slice14],\r\n",
        "       train_regret_loser_3[slice14],\r\n",
        "       train_regret_loser_4[slice14],\r\n",
        "       train_regret_loser_5[slice14],\r\n",
        "       train_regret_loser_6[slice14],\r\n",
        "       train_regret_loser_7[slice14],\r\n",
        "       train_regret_loser_8[slice14],\r\n",
        "       train_regret_loser_9[slice14],\r\n",
        "       train_regret_loser_10[slice14],\r\n",
        "       train_regret_loser_11[slice14],\r\n",
        "       train_regret_loser_12[slice14],\r\n",
        "       train_regret_loser_13[slice14],\r\n",
        "       train_regret_loser_14[slice14],\r\n",
        "       train_regret_loser_15[slice14],\r\n",
        "       train_regret_loser_16[slice14],\r\n",
        "       train_regret_loser_17[slice14],\r\n",
        "       train_regret_loser_18[slice14],\r\n",
        "       train_regret_loser_19[slice14],\r\n",
        "       train_regret_loser_20[slice14]]\r\n",
        "\r\n",
        "winner14 = [train_regret_winner_1[slice14],\r\n",
        "       train_regret_winner_2[slice14],\r\n",
        "       train_regret_winner_3[slice14],\r\n",
        "       train_regret_winner_4[slice14],\r\n",
        "       train_regret_winner_5[slice14],\r\n",
        "       train_regret_winner_6[slice14],\r\n",
        "       train_regret_winner_7[slice14],\r\n",
        "       train_regret_winner_8[slice14],\r\n",
        "       train_regret_winner_9[slice14],\r\n",
        "       train_regret_winner_10[slice14],\r\n",
        "       train_regret_winner_11[slice14],\r\n",
        "       train_regret_winner_12[slice14],\r\n",
        "       train_regret_winner_13[slice14],\r\n",
        "       train_regret_winner_14[slice14],\r\n",
        "       train_regret_winner_15[slice14],\r\n",
        "       train_regret_winner_16[slice14],\r\n",
        "       train_regret_winner_17[slice14],\r\n",
        "       train_regret_winner_18[slice14],\r\n",
        "       train_regret_winner_19[slice14],\r\n",
        "       train_regret_winner_20[slice14]]\r\n",
        "\r\n",
        "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\r\n",
        "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\r\n",
        "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\r\n",
        "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\r\n",
        "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\r\n",
        "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3kMQV0w_Ua"
      },
      "source": [
        "# Iteration24 :\r\n",
        "\r\n",
        "slice24 = 23\r\n",
        "\r\n",
        "loser24 = [train_regret_loser_1[slice24],\r\n",
        "       train_regret_loser_2[slice24],\r\n",
        "       train_regret_loser_3[slice24],\r\n",
        "       train_regret_loser_4[slice24],\r\n",
        "       train_regret_loser_5[slice24],\r\n",
        "       train_regret_loser_6[slice24],\r\n",
        "       train_regret_loser_7[slice24],\r\n",
        "       train_regret_loser_8[slice24],\r\n",
        "       train_regret_loser_9[slice24],\r\n",
        "       train_regret_loser_10[slice24],\r\n",
        "       train_regret_loser_11[slice24],\r\n",
        "       train_regret_loser_12[slice24],\r\n",
        "       train_regret_loser_13[slice24],\r\n",
        "       train_regret_loser_14[slice24],\r\n",
        "       train_regret_loser_15[slice24],\r\n",
        "       train_regret_loser_16[slice24],\r\n",
        "       train_regret_loser_17[slice24],\r\n",
        "       train_regret_loser_18[slice24],\r\n",
        "       train_regret_loser_19[slice24],\r\n",
        "       train_regret_loser_20[slice24]]\r\n",
        "\r\n",
        "winner24 = [train_regret_winner_1[slice24],\r\n",
        "       train_regret_winner_2[slice24],\r\n",
        "       train_regret_winner_3[slice24],\r\n",
        "       train_regret_winner_4[slice24],\r\n",
        "       train_regret_winner_5[slice24],\r\n",
        "       train_regret_winner_6[slice24],\r\n",
        "       train_regret_winner_7[slice24],\r\n",
        "       train_regret_winner_8[slice24],\r\n",
        "       train_regret_winner_9[slice24],\r\n",
        "       train_regret_winner_10[slice24],\r\n",
        "       train_regret_winner_11[slice24],\r\n",
        "       train_regret_winner_12[slice24],\r\n",
        "       train_regret_winner_13[slice24],\r\n",
        "       train_regret_winner_14[slice24],\r\n",
        "       train_regret_winner_15[slice24],\r\n",
        "       train_regret_winner_16[slice24],\r\n",
        "       train_regret_winner_17[slice24],\r\n",
        "       train_regret_winner_18[slice24],\r\n",
        "       train_regret_winner_19[slice24],\r\n",
        "       train_regret_winner_20[slice24]]\r\n",
        "\r\n",
        "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\r\n",
        "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\r\n",
        "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\r\n",
        "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\r\n",
        "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\r\n",
        "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ljkyMfw_Xj"
      },
      "source": [
        "# Iteration34 :\r\n",
        "\r\n",
        "slice34 = 33\r\n",
        "\r\n",
        "loser34 = [train_regret_loser_1[slice34],\r\n",
        "       train_regret_loser_2[slice34],\r\n",
        "       train_regret_loser_3[slice34],\r\n",
        "       train_regret_loser_4[slice34],\r\n",
        "       train_regret_loser_5[slice34],\r\n",
        "       train_regret_loser_6[slice34],\r\n",
        "       train_regret_loser_7[slice34],\r\n",
        "       train_regret_loser_8[slice34],\r\n",
        "       train_regret_loser_9[slice34],\r\n",
        "       train_regret_loser_10[slice34],\r\n",
        "       train_regret_loser_11[slice34],\r\n",
        "       train_regret_loser_12[slice34],\r\n",
        "       train_regret_loser_13[slice34],\r\n",
        "       train_regret_loser_14[slice34],\r\n",
        "       train_regret_loser_15[slice34],\r\n",
        "       train_regret_loser_16[slice34],\r\n",
        "       train_regret_loser_17[slice34],\r\n",
        "       train_regret_loser_18[slice34],\r\n",
        "       train_regret_loser_19[slice34],\r\n",
        "       train_regret_loser_20[slice34]]\r\n",
        "\r\n",
        "winner34 = [train_regret_winner_1[slice34],\r\n",
        "       train_regret_winner_2[slice34],\r\n",
        "       train_regret_winner_3[slice34],\r\n",
        "       train_regret_winner_4[slice34],\r\n",
        "       train_regret_winner_5[slice34],\r\n",
        "       train_regret_winner_6[slice34],\r\n",
        "       train_regret_winner_7[slice34],\r\n",
        "       train_regret_winner_8[slice34],\r\n",
        "       train_regret_winner_9[slice34],\r\n",
        "       train_regret_winner_10[slice34],\r\n",
        "       train_regret_winner_11[slice34],\r\n",
        "       train_regret_winner_12[slice34],\r\n",
        "       train_regret_winner_13[slice34],\r\n",
        "       train_regret_winner_14[slice34],\r\n",
        "       train_regret_winner_15[slice34],\r\n",
        "       train_regret_winner_16[slice34],\r\n",
        "       train_regret_winner_17[slice34],\r\n",
        "       train_regret_winner_18[slice34],\r\n",
        "       train_regret_winner_19[slice34],\r\n",
        "       train_regret_winner_20[slice34]]\r\n",
        "\r\n",
        "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\r\n",
        "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\r\n",
        "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\r\n",
        "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\r\n",
        "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\r\n",
        "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGKj-NR8w_ae"
      },
      "source": [
        "# Iteration44 :\r\n",
        "\r\n",
        "slice44 = 43\r\n",
        "\r\n",
        "loser44 = [train_regret_loser_1[slice44],\r\n",
        "       train_regret_loser_2[slice44],\r\n",
        "       train_regret_loser_3[slice44],\r\n",
        "       train_regret_loser_4[slice44],\r\n",
        "       train_regret_loser_5[slice44],\r\n",
        "       train_regret_loser_6[slice44],\r\n",
        "       train_regret_loser_7[slice44],\r\n",
        "       train_regret_loser_8[slice44],\r\n",
        "       train_regret_loser_9[slice44],\r\n",
        "       train_regret_loser_10[slice44],\r\n",
        "       train_regret_loser_11[slice44],\r\n",
        "       train_regret_loser_12[slice44],\r\n",
        "       train_regret_loser_13[slice44],\r\n",
        "       train_regret_loser_14[slice44],\r\n",
        "       train_regret_loser_15[slice44],\r\n",
        "       train_regret_loser_16[slice44],\r\n",
        "       train_regret_loser_17[slice44],\r\n",
        "       train_regret_loser_18[slice44],\r\n",
        "       train_regret_loser_19[slice44],\r\n",
        "       train_regret_loser_20[slice44]]\r\n",
        "\r\n",
        "winner44 = [train_regret_winner_1[slice44],\r\n",
        "       train_regret_winner_2[slice44],\r\n",
        "       train_regret_winner_3[slice44],\r\n",
        "       train_regret_winner_4[slice44],\r\n",
        "       train_regret_winner_5[slice44],\r\n",
        "       train_regret_winner_6[slice44],\r\n",
        "       train_regret_winner_7[slice44],\r\n",
        "       train_regret_winner_8[slice44],\r\n",
        "       train_regret_winner_9[slice44],\r\n",
        "       train_regret_winner_10[slice44],\r\n",
        "       train_regret_winner_11[slice44],\r\n",
        "       train_regret_winner_12[slice44],\r\n",
        "       train_regret_winner_13[slice44],\r\n",
        "       train_regret_winner_14[slice44],\r\n",
        "       train_regret_winner_15[slice44],\r\n",
        "       train_regret_winner_16[slice44],\r\n",
        "       train_regret_winner_17[slice44],\r\n",
        "       train_regret_winner_18[slice44],\r\n",
        "       train_regret_winner_19[slice44],\r\n",
        "       train_regret_winner_20[slice44]]\r\n",
        "\r\n",
        "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\r\n",
        "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\r\n",
        "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\r\n",
        "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\r\n",
        "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\r\n",
        "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJzx094Fw_c_"
      },
      "source": [
        "# Iteration54 :\r\n",
        "\r\n",
        "slice54 = 53\r\n",
        "\r\n",
        "loser54 = [train_regret_loser_1[slice54],\r\n",
        "       train_regret_loser_2[slice54],\r\n",
        "       train_regret_loser_3[slice54],\r\n",
        "       train_regret_loser_4[slice54],\r\n",
        "       train_regret_loser_5[slice54],\r\n",
        "       train_regret_loser_6[slice54],\r\n",
        "       train_regret_loser_7[slice54],\r\n",
        "       train_regret_loser_8[slice54],\r\n",
        "       train_regret_loser_9[slice54],\r\n",
        "       train_regret_loser_10[slice54],\r\n",
        "       train_regret_loser_11[slice54],\r\n",
        "       train_regret_loser_12[slice54],\r\n",
        "       train_regret_loser_13[slice54],\r\n",
        "       train_regret_loser_14[slice54],\r\n",
        "       train_regret_loser_15[slice54],\r\n",
        "       train_regret_loser_16[slice54],\r\n",
        "       train_regret_loser_17[slice54],\r\n",
        "       train_regret_loser_18[slice54],\r\n",
        "       train_regret_loser_19[slice54],\r\n",
        "       train_regret_loser_20[slice54]]\r\n",
        "\r\n",
        "winner54 = [train_regret_winner_1[slice54],\r\n",
        "       train_regret_winner_2[slice54],\r\n",
        "       train_regret_winner_3[slice54],\r\n",
        "       train_regret_winner_4[slice54],\r\n",
        "       train_regret_winner_5[slice54],\r\n",
        "       train_regret_winner_6[slice54],\r\n",
        "       train_regret_winner_7[slice54],\r\n",
        "       train_regret_winner_8[slice54],\r\n",
        "       train_regret_winner_9[slice54],\r\n",
        "       train_regret_winner_10[slice54],\r\n",
        "       train_regret_winner_11[slice54],\r\n",
        "       train_regret_winner_12[slice54],\r\n",
        "       train_regret_winner_13[slice54],\r\n",
        "       train_regret_winner_14[slice54],\r\n",
        "       train_regret_winner_15[slice54],\r\n",
        "       train_regret_winner_16[slice54],\r\n",
        "       train_regret_winner_17[slice54],\r\n",
        "       train_regret_winner_18[slice54],\r\n",
        "       train_regret_winner_19[slice54],\r\n",
        "       train_regret_winner_20[slice54]]\r\n",
        "\r\n",
        "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\r\n",
        "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\r\n",
        "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\r\n",
        "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\r\n",
        "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\r\n",
        "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEVYMqj_w_fr"
      },
      "source": [
        "# Iteration64 :\r\n",
        "\r\n",
        "slice64 = 63\r\n",
        "\r\n",
        "loser64 = [train_regret_loser_1[slice64],\r\n",
        "       train_regret_loser_2[slice64],\r\n",
        "       train_regret_loser_3[slice64],\r\n",
        "       train_regret_loser_4[slice64],\r\n",
        "       train_regret_loser_5[slice64],\r\n",
        "       train_regret_loser_6[slice64],\r\n",
        "       train_regret_loser_7[slice64],\r\n",
        "       train_regret_loser_8[slice64],\r\n",
        "       train_regret_loser_9[slice64],\r\n",
        "       train_regret_loser_10[slice64],\r\n",
        "       train_regret_loser_11[slice64],\r\n",
        "       train_regret_loser_12[slice64],\r\n",
        "       train_regret_loser_13[slice64],\r\n",
        "       train_regret_loser_14[slice64],\r\n",
        "       train_regret_loser_15[slice64],\r\n",
        "       train_regret_loser_16[slice64],\r\n",
        "       train_regret_loser_17[slice64],\r\n",
        "       train_regret_loser_18[slice64],\r\n",
        "       train_regret_loser_19[slice64],\r\n",
        "       train_regret_loser_20[slice64]]\r\n",
        "\r\n",
        "winner64 = [train_regret_winner_1[slice64],\r\n",
        "       train_regret_winner_2[slice64],\r\n",
        "       train_regret_winner_3[slice64],\r\n",
        "       train_regret_winner_4[slice64],\r\n",
        "       train_regret_winner_5[slice64],\r\n",
        "       train_regret_winner_6[slice64],\r\n",
        "       train_regret_winner_7[slice64],\r\n",
        "       train_regret_winner_8[slice64],\r\n",
        "       train_regret_winner_9[slice64],\r\n",
        "       train_regret_winner_10[slice64],\r\n",
        "       train_regret_winner_11[slice64],\r\n",
        "       train_regret_winner_12[slice64],\r\n",
        "       train_regret_winner_13[slice64],\r\n",
        "       train_regret_winner_14[slice64],\r\n",
        "       train_regret_winner_15[slice64],\r\n",
        "       train_regret_winner_16[slice64],\r\n",
        "       train_regret_winner_17[slice64],\r\n",
        "       train_regret_winner_18[slice64],\r\n",
        "       train_regret_winner_19[slice64],\r\n",
        "       train_regret_winner_20[slice64]]\r\n",
        "\r\n",
        "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\r\n",
        "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\r\n",
        "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\r\n",
        "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\r\n",
        "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\r\n",
        "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvr3S_p0w_ii"
      },
      "source": [
        "# Iteration74 :\r\n",
        "\r\n",
        "slice74 = 73\r\n",
        "\r\n",
        "loser74 = [train_regret_loser_1[slice74],\r\n",
        "       train_regret_loser_2[slice74],\r\n",
        "       train_regret_loser_3[slice74],\r\n",
        "       train_regret_loser_4[slice74],\r\n",
        "       train_regret_loser_5[slice74],\r\n",
        "       train_regret_loser_6[slice74],\r\n",
        "       train_regret_loser_7[slice74],\r\n",
        "       train_regret_loser_8[slice74],\r\n",
        "       train_regret_loser_9[slice74],\r\n",
        "       train_regret_loser_10[slice74],\r\n",
        "       train_regret_loser_11[slice74],\r\n",
        "       train_regret_loser_12[slice74],\r\n",
        "       train_regret_loser_13[slice74],\r\n",
        "       train_regret_loser_14[slice74],\r\n",
        "       train_regret_loser_15[slice74],\r\n",
        "       train_regret_loser_16[slice74],\r\n",
        "       train_regret_loser_17[slice74],\r\n",
        "       train_regret_loser_18[slice74],\r\n",
        "       train_regret_loser_19[slice74],\r\n",
        "       train_regret_loser_20[slice74]]\r\n",
        "\r\n",
        "winner74 = [train_regret_winner_1[slice74],\r\n",
        "       train_regret_winner_2[slice74],\r\n",
        "       train_regret_winner_3[slice74],\r\n",
        "       train_regret_winner_4[slice74],\r\n",
        "       train_regret_winner_5[slice74],\r\n",
        "       train_regret_winner_6[slice74],\r\n",
        "       train_regret_winner_7[slice74],\r\n",
        "       train_regret_winner_8[slice74],\r\n",
        "       train_regret_winner_9[slice74],\r\n",
        "       train_regret_winner_10[slice74],\r\n",
        "       train_regret_winner_11[slice74],\r\n",
        "       train_regret_winner_12[slice74],\r\n",
        "       train_regret_winner_13[slice74],\r\n",
        "       train_regret_winner_14[slice74],\r\n",
        "       train_regret_winner_15[slice74],\r\n",
        "       train_regret_winner_16[slice74],\r\n",
        "       train_regret_winner_17[slice74],\r\n",
        "       train_regret_winner_18[slice74],\r\n",
        "       train_regret_winner_19[slice74],\r\n",
        "       train_regret_winner_20[slice74]]\r\n",
        "\r\n",
        "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\r\n",
        "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\r\n",
        "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\r\n",
        "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\r\n",
        "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\r\n",
        "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEAvUVK7w_lO"
      },
      "source": [
        "# Iteration84 :\r\n",
        "\r\n",
        "slice84 = 83\r\n",
        "\r\n",
        "loser84 = [train_regret_loser_1[slice84],\r\n",
        "       train_regret_loser_2[slice84],\r\n",
        "       train_regret_loser_3[slice84],\r\n",
        "       train_regret_loser_4[slice84],\r\n",
        "       train_regret_loser_5[slice84],\r\n",
        "       train_regret_loser_6[slice84],\r\n",
        "       train_regret_loser_7[slice84],\r\n",
        "       train_regret_loser_8[slice84],\r\n",
        "       train_regret_loser_9[slice84],\r\n",
        "       train_regret_loser_10[slice84],\r\n",
        "       train_regret_loser_11[slice84],\r\n",
        "       train_regret_loser_12[slice84],\r\n",
        "       train_regret_loser_13[slice84],\r\n",
        "       train_regret_loser_14[slice84],\r\n",
        "       train_regret_loser_15[slice84],\r\n",
        "       train_regret_loser_16[slice84],\r\n",
        "       train_regret_loser_17[slice84],\r\n",
        "       train_regret_loser_18[slice84],\r\n",
        "       train_regret_loser_19[slice84],\r\n",
        "       train_regret_loser_20[slice84]]\r\n",
        "\r\n",
        "winner84 = [train_regret_winner_1[slice84],\r\n",
        "       train_regret_winner_2[slice84],\r\n",
        "       train_regret_winner_3[slice84],\r\n",
        "       train_regret_winner_4[slice84],\r\n",
        "       train_regret_winner_5[slice84],\r\n",
        "       train_regret_winner_6[slice84],\r\n",
        "       train_regret_winner_7[slice84],\r\n",
        "       train_regret_winner_8[slice84],\r\n",
        "       train_regret_winner_9[slice84],\r\n",
        "       train_regret_winner_10[slice84],\r\n",
        "       train_regret_winner_11[slice84],\r\n",
        "       train_regret_winner_12[slice84],\r\n",
        "       train_regret_winner_13[slice84],\r\n",
        "       train_regret_winner_14[slice84],\r\n",
        "       train_regret_winner_15[slice84],\r\n",
        "       train_regret_winner_16[slice84],\r\n",
        "       train_regret_winner_17[slice84],\r\n",
        "       train_regret_winner_18[slice84],\r\n",
        "       train_regret_winner_19[slice84],\r\n",
        "       train_regret_winner_20[slice84]]\r\n",
        "\r\n",
        "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\r\n",
        "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\r\n",
        "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\r\n",
        "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\r\n",
        "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\r\n",
        "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMZzhdGw_oC"
      },
      "source": [
        "# Iteration94 :\r\n",
        "\r\n",
        "slice94 = 93\r\n",
        "\r\n",
        "loser94 = [train_regret_loser_1[slice94],\r\n",
        "       train_regret_loser_2[slice94],\r\n",
        "       train_regret_loser_3[slice94],\r\n",
        "       train_regret_loser_4[slice94],\r\n",
        "       train_regret_loser_5[slice94],\r\n",
        "       train_regret_loser_6[slice94],\r\n",
        "       train_regret_loser_7[slice94],\r\n",
        "       train_regret_loser_8[slice94],\r\n",
        "       train_regret_loser_9[slice94],\r\n",
        "       train_regret_loser_10[slice94],\r\n",
        "       train_regret_loser_11[slice94],\r\n",
        "       train_regret_loser_12[slice94],\r\n",
        "       train_regret_loser_13[slice94],\r\n",
        "       train_regret_loser_14[slice94],\r\n",
        "       train_regret_loser_15[slice94],\r\n",
        "       train_regret_loser_16[slice94],\r\n",
        "       train_regret_loser_17[slice94],\r\n",
        "       train_regret_loser_18[slice94],\r\n",
        "       train_regret_loser_19[slice94],\r\n",
        "       train_regret_loser_20[slice94]]\r\n",
        "\r\n",
        "winner94 = [train_regret_winner_1[slice94],\r\n",
        "       train_regret_winner_2[slice94],\r\n",
        "       train_regret_winner_3[slice94],\r\n",
        "       train_regret_winner_4[slice94],\r\n",
        "       train_regret_winner_5[slice94],\r\n",
        "       train_regret_winner_6[slice94],\r\n",
        "       train_regret_winner_7[slice94],\r\n",
        "       train_regret_winner_8[slice94],\r\n",
        "       train_regret_winner_9[slice94],\r\n",
        "       train_regret_winner_10[slice94],\r\n",
        "       train_regret_winner_11[slice94],\r\n",
        "       train_regret_winner_12[slice94],\r\n",
        "       train_regret_winner_13[slice94],\r\n",
        "       train_regret_winner_14[slice94],\r\n",
        "       train_regret_winner_15[slice94],\r\n",
        "       train_regret_winner_16[slice94],\r\n",
        "       train_regret_winner_17[slice94],\r\n",
        "       train_regret_winner_18[slice94],\r\n",
        "       train_regret_winner_19[slice94],\r\n",
        "       train_regret_winner_20[slice94]]\r\n",
        "\r\n",
        "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\r\n",
        "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\r\n",
        "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\r\n",
        "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\r\n",
        "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\r\n",
        "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMxHQOgaw_qu"
      },
      "source": [
        "# Iteration5 :\r\n",
        "\r\n",
        "slice5 = 4\r\n",
        "\r\n",
        "loser5 = [train_regret_loser_1[slice5],\r\n",
        "       train_regret_loser_2[slice5],\r\n",
        "       train_regret_loser_3[slice5],\r\n",
        "       train_regret_loser_4[slice5],\r\n",
        "       train_regret_loser_5[slice5],\r\n",
        "       train_regret_loser_6[slice5],\r\n",
        "       train_regret_loser_7[slice5],\r\n",
        "       train_regret_loser_8[slice5],\r\n",
        "       train_regret_loser_9[slice5],\r\n",
        "       train_regret_loser_10[slice5],\r\n",
        "       train_regret_loser_11[slice5],\r\n",
        "       train_regret_loser_12[slice5],\r\n",
        "       train_regret_loser_13[slice5],\r\n",
        "       train_regret_loser_14[slice5],\r\n",
        "       train_regret_loser_15[slice5],\r\n",
        "       train_regret_loser_16[slice5],\r\n",
        "       train_regret_loser_17[slice5],\r\n",
        "       train_regret_loser_18[slice5],\r\n",
        "       train_regret_loser_19[slice5],\r\n",
        "       train_regret_loser_20[slice5]]\r\n",
        "\r\n",
        "winner5 = [train_regret_winner_1[slice5],\r\n",
        "       train_regret_winner_2[slice5],\r\n",
        "       train_regret_winner_3[slice5],\r\n",
        "       train_regret_winner_4[slice5],\r\n",
        "       train_regret_winner_5[slice5],\r\n",
        "       train_regret_winner_6[slice5],\r\n",
        "       train_regret_winner_7[slice5],\r\n",
        "       train_regret_winner_8[slice5],\r\n",
        "       train_regret_winner_9[slice5],\r\n",
        "       train_regret_winner_10[slice5],\r\n",
        "       train_regret_winner_11[slice5],\r\n",
        "       train_regret_winner_12[slice5],\r\n",
        "       train_regret_winner_13[slice5],\r\n",
        "       train_regret_winner_14[slice5],\r\n",
        "       train_regret_winner_15[slice5],\r\n",
        "       train_regret_winner_16[slice5],\r\n",
        "       train_regret_winner_17[slice5],\r\n",
        "       train_regret_winner_18[slice5],\r\n",
        "       train_regret_winner_19[slice5],\r\n",
        "       train_regret_winner_20[slice5]]\r\n",
        "\r\n",
        "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\r\n",
        "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\r\n",
        "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\r\n",
        "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\r\n",
        "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\r\n",
        "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPVFQTfNw_wF"
      },
      "source": [
        "# Iteration15 :\r\n",
        "\r\n",
        "slice15 = 14\r\n",
        "\r\n",
        "loser15 = [train_regret_loser_1[slice15],\r\n",
        "       train_regret_loser_2[slice15],\r\n",
        "       train_regret_loser_3[slice15],\r\n",
        "       train_regret_loser_4[slice15],\r\n",
        "       train_regret_loser_5[slice15],\r\n",
        "       train_regret_loser_6[slice15],\r\n",
        "       train_regret_loser_7[slice15],\r\n",
        "       train_regret_loser_8[slice15],\r\n",
        "       train_regret_loser_9[slice15],\r\n",
        "       train_regret_loser_10[slice15],\r\n",
        "       train_regret_loser_11[slice15],\r\n",
        "       train_regret_loser_12[slice15],\r\n",
        "       train_regret_loser_13[slice15],\r\n",
        "       train_regret_loser_14[slice15],\r\n",
        "       train_regret_loser_15[slice15],\r\n",
        "       train_regret_loser_16[slice15],\r\n",
        "       train_regret_loser_17[slice15],\r\n",
        "       train_regret_loser_18[slice15],\r\n",
        "       train_regret_loser_19[slice15],\r\n",
        "       train_regret_loser_20[slice15]]\r\n",
        "\r\n",
        "winner15 = [train_regret_winner_1[slice15],\r\n",
        "       train_regret_winner_2[slice15],\r\n",
        "       train_regret_winner_3[slice15],\r\n",
        "       train_regret_winner_4[slice15],\r\n",
        "       train_regret_winner_5[slice15],\r\n",
        "       train_regret_winner_6[slice15],\r\n",
        "       train_regret_winner_7[slice15],\r\n",
        "       train_regret_winner_8[slice15],\r\n",
        "       train_regret_winner_9[slice15],\r\n",
        "       train_regret_winner_10[slice15],\r\n",
        "       train_regret_winner_11[slice15],\r\n",
        "       train_regret_winner_12[slice15],\r\n",
        "       train_regret_winner_13[slice15],\r\n",
        "       train_regret_winner_14[slice15],\r\n",
        "       train_regret_winner_15[slice15],\r\n",
        "       train_regret_winner_16[slice15],\r\n",
        "       train_regret_winner_17[slice15],\r\n",
        "       train_regret_winner_18[slice15],\r\n",
        "       train_regret_winner_19[slice15],\r\n",
        "       train_regret_winner_20[slice15]]\r\n",
        "\r\n",
        "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\r\n",
        "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\r\n",
        "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\r\n",
        "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\r\n",
        "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\r\n",
        "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADBmBA3ew_zK"
      },
      "source": [
        "# Iteration25 :\r\n",
        "\r\n",
        "slice25 = 24\r\n",
        "\r\n",
        "loser25 = [train_regret_loser_1[slice25],\r\n",
        "       train_regret_loser_2[slice25],\r\n",
        "       train_regret_loser_3[slice25],\r\n",
        "       train_regret_loser_4[slice25],\r\n",
        "       train_regret_loser_5[slice25],\r\n",
        "       train_regret_loser_6[slice25],\r\n",
        "       train_regret_loser_7[slice25],\r\n",
        "       train_regret_loser_8[slice25],\r\n",
        "       train_regret_loser_9[slice25],\r\n",
        "       train_regret_loser_10[slice25],\r\n",
        "       train_regret_loser_11[slice25],\r\n",
        "       train_regret_loser_12[slice25],\r\n",
        "       train_regret_loser_13[slice25],\r\n",
        "       train_regret_loser_14[slice25],\r\n",
        "       train_regret_loser_15[slice25],\r\n",
        "       train_regret_loser_16[slice25],\r\n",
        "       train_regret_loser_17[slice25],\r\n",
        "       train_regret_loser_18[slice25],\r\n",
        "       train_regret_loser_19[slice25],\r\n",
        "       train_regret_loser_20[slice25]]\r\n",
        "\r\n",
        "winner25 = [train_regret_winner_1[slice25],\r\n",
        "       train_regret_winner_2[slice25],\r\n",
        "       train_regret_winner_3[slice25],\r\n",
        "       train_regret_winner_4[slice25],\r\n",
        "       train_regret_winner_5[slice25],\r\n",
        "       train_regret_winner_6[slice25],\r\n",
        "       train_regret_winner_7[slice25],\r\n",
        "       train_regret_winner_8[slice25],\r\n",
        "       train_regret_winner_9[slice25],\r\n",
        "       train_regret_winner_10[slice25],\r\n",
        "       train_regret_winner_11[slice25],\r\n",
        "       train_regret_winner_12[slice25],\r\n",
        "       train_regret_winner_13[slice25],\r\n",
        "       train_regret_winner_14[slice25],\r\n",
        "       train_regret_winner_15[slice25],\r\n",
        "       train_regret_winner_16[slice25],\r\n",
        "       train_regret_winner_17[slice25],\r\n",
        "       train_regret_winner_18[slice25],\r\n",
        "       train_regret_winner_19[slice25],\r\n",
        "       train_regret_winner_20[slice25]]\r\n",
        "\r\n",
        "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\r\n",
        "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\r\n",
        "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\r\n",
        "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\r\n",
        "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\r\n",
        "upper_winner25 = np.asarray(winner25_results[14:15][0])[0]"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUk9UtSTw_2N"
      },
      "source": [
        "# Iteration35 :\r\n",
        "\r\n",
        "slice35 = 34\r\n",
        "\r\n",
        "loser35 = [train_regret_loser_1[slice35],\r\n",
        "       train_regret_loser_2[slice35],\r\n",
        "       train_regret_loser_3[slice35],\r\n",
        "       train_regret_loser_4[slice35],\r\n",
        "       train_regret_loser_5[slice35],\r\n",
        "       train_regret_loser_6[slice35],\r\n",
        "       train_regret_loser_7[slice35],\r\n",
        "       train_regret_loser_8[slice35],\r\n",
        "       train_regret_loser_9[slice35],\r\n",
        "       train_regret_loser_10[slice35],\r\n",
        "       train_regret_loser_11[slice35],\r\n",
        "       train_regret_loser_12[slice35],\r\n",
        "       train_regret_loser_13[slice35],\r\n",
        "       train_regret_loser_14[slice35],\r\n",
        "       train_regret_loser_15[slice35],\r\n",
        "       train_regret_loser_16[slice35],\r\n",
        "       train_regret_loser_17[slice35],\r\n",
        "       train_regret_loser_18[slice35],\r\n",
        "       train_regret_loser_19[slice35],\r\n",
        "       train_regret_loser_20[slice35]]\r\n",
        "\r\n",
        "winner35 = [train_regret_winner_1[slice35],\r\n",
        "       train_regret_winner_2[slice35],\r\n",
        "       train_regret_winner_3[slice35],\r\n",
        "       train_regret_winner_4[slice35],\r\n",
        "       train_regret_winner_5[slice35],\r\n",
        "       train_regret_winner_6[slice35],\r\n",
        "       train_regret_winner_7[slice35],\r\n",
        "       train_regret_winner_8[slice35],\r\n",
        "       train_regret_winner_9[slice35],\r\n",
        "       train_regret_winner_10[slice35],\r\n",
        "       train_regret_winner_11[slice35],\r\n",
        "       train_regret_winner_12[slice35],\r\n",
        "       train_regret_winner_13[slice35],\r\n",
        "       train_regret_winner_14[slice35],\r\n",
        "       train_regret_winner_15[slice35],\r\n",
        "       train_regret_winner_16[slice35],\r\n",
        "       train_regret_winner_17[slice35],\r\n",
        "       train_regret_winner_18[slice35],\r\n",
        "       train_regret_winner_19[slice35],\r\n",
        "       train_regret_winner_20[slice35]]\r\n",
        "\r\n",
        "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\r\n",
        "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\r\n",
        "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\r\n",
        "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\r\n",
        "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\r\n",
        "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX3VgqD_w_4z"
      },
      "source": [
        "# Iteration45 :\r\n",
        "\r\n",
        "slice45 = 44\r\n",
        "\r\n",
        "loser45 = [train_regret_loser_1[slice45],\r\n",
        "       train_regret_loser_2[slice45],\r\n",
        "       train_regret_loser_3[slice45],\r\n",
        "       train_regret_loser_4[slice45],\r\n",
        "       train_regret_loser_5[slice45],\r\n",
        "       train_regret_loser_6[slice45],\r\n",
        "       train_regret_loser_7[slice45],\r\n",
        "       train_regret_loser_8[slice45],\r\n",
        "       train_regret_loser_9[slice45],\r\n",
        "       train_regret_loser_10[slice45],\r\n",
        "       train_regret_loser_11[slice45],\r\n",
        "       train_regret_loser_12[slice45],\r\n",
        "       train_regret_loser_13[slice45],\r\n",
        "       train_regret_loser_14[slice45],\r\n",
        "       train_regret_loser_15[slice45],\r\n",
        "       train_regret_loser_16[slice45],\r\n",
        "       train_regret_loser_17[slice45],\r\n",
        "       train_regret_loser_18[slice45],\r\n",
        "       train_regret_loser_19[slice45],\r\n",
        "       train_regret_loser_20[slice45]]\r\n",
        "\r\n",
        "winner45 = [train_regret_winner_1[slice45],\r\n",
        "       train_regret_winner_2[slice45],\r\n",
        "       train_regret_winner_3[slice45],\r\n",
        "       train_regret_winner_4[slice45],\r\n",
        "       train_regret_winner_5[slice45],\r\n",
        "       train_regret_winner_6[slice45],\r\n",
        "       train_regret_winner_7[slice45],\r\n",
        "       train_regret_winner_8[slice45],\r\n",
        "       train_regret_winner_9[slice45],\r\n",
        "       train_regret_winner_10[slice45],\r\n",
        "       train_regret_winner_11[slice45],\r\n",
        "       train_regret_winner_12[slice45],\r\n",
        "       train_regret_winner_13[slice45],\r\n",
        "       train_regret_winner_14[slice45],\r\n",
        "       train_regret_winner_15[slice45],\r\n",
        "       train_regret_winner_16[slice45],\r\n",
        "       train_regret_winner_17[slice45],\r\n",
        "       train_regret_winner_18[slice45],\r\n",
        "       train_regret_winner_19[slice45],\r\n",
        "       train_regret_winner_20[slice45]]\r\n",
        "\r\n",
        "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\r\n",
        "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\r\n",
        "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\r\n",
        "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\r\n",
        "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\r\n",
        "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o35W-AOzw_8F"
      },
      "source": [
        "# Iteration55 :\r\n",
        "\r\n",
        "slice55 = 54\r\n",
        "\r\n",
        "loser55 = [train_regret_loser_1[slice55],\r\n",
        "       train_regret_loser_2[slice55],\r\n",
        "       train_regret_loser_3[slice55],\r\n",
        "       train_regret_loser_4[slice55],\r\n",
        "       train_regret_loser_5[slice55],\r\n",
        "       train_regret_loser_6[slice55],\r\n",
        "       train_regret_loser_7[slice55],\r\n",
        "       train_regret_loser_8[slice55],\r\n",
        "       train_regret_loser_9[slice55],\r\n",
        "       train_regret_loser_10[slice55],\r\n",
        "       train_regret_loser_11[slice55],\r\n",
        "       train_regret_loser_12[slice55],\r\n",
        "       train_regret_loser_13[slice55],\r\n",
        "       train_regret_loser_14[slice55],\r\n",
        "       train_regret_loser_15[slice55],\r\n",
        "       train_regret_loser_16[slice55],\r\n",
        "       train_regret_loser_17[slice55],\r\n",
        "       train_regret_loser_18[slice55],\r\n",
        "       train_regret_loser_19[slice55],\r\n",
        "       train_regret_loser_20[slice55]]\r\n",
        "\r\n",
        "winner55 = [train_regret_winner_1[slice55],\r\n",
        "       train_regret_winner_2[slice55],\r\n",
        "       train_regret_winner_3[slice55],\r\n",
        "       train_regret_winner_4[slice55],\r\n",
        "       train_regret_winner_5[slice55],\r\n",
        "       train_regret_winner_6[slice55],\r\n",
        "       train_regret_winner_7[slice55],\r\n",
        "       train_regret_winner_8[slice55],\r\n",
        "       train_regret_winner_9[slice55],\r\n",
        "       train_regret_winner_10[slice55],\r\n",
        "       train_regret_winner_11[slice55],\r\n",
        "       train_regret_winner_12[slice55],\r\n",
        "       train_regret_winner_13[slice55],\r\n",
        "       train_regret_winner_14[slice55],\r\n",
        "       train_regret_winner_15[slice55],\r\n",
        "       train_regret_winner_16[slice55],\r\n",
        "       train_regret_winner_17[slice55],\r\n",
        "       train_regret_winner_18[slice55],\r\n",
        "       train_regret_winner_19[slice55],\r\n",
        "       train_regret_winner_20[slice55]]\r\n",
        "\r\n",
        "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\r\n",
        "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\r\n",
        "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\r\n",
        "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\r\n",
        "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\r\n",
        "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUAjKXWdw__J"
      },
      "source": [
        "# Iteration65 :\r\n",
        "\r\n",
        "slice65 = 64\r\n",
        "\r\n",
        "loser65 = [train_regret_loser_1[slice65],\r\n",
        "       train_regret_loser_2[slice65],\r\n",
        "       train_regret_loser_3[slice65],\r\n",
        "       train_regret_loser_4[slice65],\r\n",
        "       train_regret_loser_5[slice65],\r\n",
        "       train_regret_loser_6[slice65],\r\n",
        "       train_regret_loser_7[slice65],\r\n",
        "       train_regret_loser_8[slice65],\r\n",
        "       train_regret_loser_9[slice65],\r\n",
        "       train_regret_loser_10[slice65],\r\n",
        "       train_regret_loser_11[slice65],\r\n",
        "       train_regret_loser_12[slice65],\r\n",
        "       train_regret_loser_13[slice65],\r\n",
        "       train_regret_loser_14[slice65],\r\n",
        "       train_regret_loser_15[slice65],\r\n",
        "       train_regret_loser_16[slice65],\r\n",
        "       train_regret_loser_17[slice65],\r\n",
        "       train_regret_loser_18[slice65],\r\n",
        "       train_regret_loser_19[slice65],\r\n",
        "       train_regret_loser_20[slice65]]\r\n",
        "\r\n",
        "winner65 = [train_regret_winner_1[slice65],\r\n",
        "       train_regret_winner_2[slice65],\r\n",
        "       train_regret_winner_3[slice65],\r\n",
        "       train_regret_winner_4[slice65],\r\n",
        "       train_regret_winner_5[slice65],\r\n",
        "       train_regret_winner_6[slice65],\r\n",
        "       train_regret_winner_7[slice65],\r\n",
        "       train_regret_winner_8[slice65],\r\n",
        "       train_regret_winner_9[slice65],\r\n",
        "       train_regret_winner_10[slice65],\r\n",
        "       train_regret_winner_11[slice65],\r\n",
        "       train_regret_winner_12[slice65],\r\n",
        "       train_regret_winner_13[slice65],\r\n",
        "       train_regret_winner_14[slice65],\r\n",
        "       train_regret_winner_15[slice65],\r\n",
        "       train_regret_winner_16[slice65],\r\n",
        "       train_regret_winner_17[slice65],\r\n",
        "       train_regret_winner_18[slice65],\r\n",
        "       train_regret_winner_19[slice65],\r\n",
        "       train_regret_winner_20[slice65]]\r\n",
        "\r\n",
        "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\r\n",
        "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\r\n",
        "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\r\n",
        "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\r\n",
        "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\r\n",
        "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D00FcmZwxAB9"
      },
      "source": [
        "# Iteration75 :\r\n",
        "\r\n",
        "slice75 = 74\r\n",
        "\r\n",
        "loser75 = [train_regret_loser_1[slice75],\r\n",
        "       train_regret_loser_2[slice75],\r\n",
        "       train_regret_loser_3[slice75],\r\n",
        "       train_regret_loser_4[slice75],\r\n",
        "       train_regret_loser_5[slice75],\r\n",
        "       train_regret_loser_6[slice75],\r\n",
        "       train_regret_loser_7[slice75],\r\n",
        "       train_regret_loser_8[slice75],\r\n",
        "       train_regret_loser_9[slice75],\r\n",
        "       train_regret_loser_10[slice75],\r\n",
        "       train_regret_loser_11[slice75],\r\n",
        "       train_regret_loser_12[slice75],\r\n",
        "       train_regret_loser_13[slice75],\r\n",
        "       train_regret_loser_14[slice75],\r\n",
        "       train_regret_loser_15[slice75],\r\n",
        "       train_regret_loser_16[slice75],\r\n",
        "       train_regret_loser_17[slice75],\r\n",
        "       train_regret_loser_18[slice75],\r\n",
        "       train_regret_loser_19[slice75],\r\n",
        "       train_regret_loser_20[slice75]]\r\n",
        "\r\n",
        "winner75 = [train_regret_winner_1[slice75],\r\n",
        "       train_regret_winner_2[slice75],\r\n",
        "       train_regret_winner_3[slice75],\r\n",
        "       train_regret_winner_4[slice75],\r\n",
        "       train_regret_winner_5[slice75],\r\n",
        "       train_regret_winner_6[slice75],\r\n",
        "       train_regret_winner_7[slice75],\r\n",
        "       train_regret_winner_8[slice75],\r\n",
        "       train_regret_winner_9[slice75],\r\n",
        "       train_regret_winner_10[slice75],\r\n",
        "       train_regret_winner_11[slice75],\r\n",
        "       train_regret_winner_12[slice75],\r\n",
        "       train_regret_winner_13[slice75],\r\n",
        "       train_regret_winner_14[slice75],\r\n",
        "       train_regret_winner_15[slice75],\r\n",
        "       train_regret_winner_16[slice75],\r\n",
        "       train_regret_winner_17[slice75],\r\n",
        "       train_regret_winner_18[slice75],\r\n",
        "       train_regret_winner_19[slice75],\r\n",
        "       train_regret_winner_20[slice75]]\r\n",
        "\r\n",
        "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\r\n",
        "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\r\n",
        "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\r\n",
        "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\r\n",
        "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\r\n",
        "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnzmprxxAFB"
      },
      "source": [
        "# Iteration85 :\r\n",
        "\r\n",
        "slice85 = 84\r\n",
        "\r\n",
        "loser85 = [train_regret_loser_1[slice85],\r\n",
        "       train_regret_loser_2[slice85],\r\n",
        "       train_regret_loser_3[slice85],\r\n",
        "       train_regret_loser_4[slice85],\r\n",
        "       train_regret_loser_5[slice85],\r\n",
        "       train_regret_loser_6[slice85],\r\n",
        "       train_regret_loser_7[slice85],\r\n",
        "       train_regret_loser_8[slice85],\r\n",
        "       train_regret_loser_9[slice85],\r\n",
        "       train_regret_loser_10[slice85],\r\n",
        "       train_regret_loser_11[slice85],\r\n",
        "       train_regret_loser_12[slice85],\r\n",
        "       train_regret_loser_13[slice85],\r\n",
        "       train_regret_loser_14[slice85],\r\n",
        "       train_regret_loser_15[slice85],\r\n",
        "       train_regret_loser_16[slice85],\r\n",
        "       train_regret_loser_17[slice85],\r\n",
        "       train_regret_loser_18[slice85],\r\n",
        "       train_regret_loser_19[slice85],\r\n",
        "       train_regret_loser_20[slice85]]\r\n",
        "\r\n",
        "winner85 = [train_regret_winner_1[slice85],\r\n",
        "       train_regret_winner_2[slice85],\r\n",
        "       train_regret_winner_3[slice85],\r\n",
        "       train_regret_winner_4[slice85],\r\n",
        "       train_regret_winner_5[slice85],\r\n",
        "       train_regret_winner_6[slice85],\r\n",
        "       train_regret_winner_7[slice85],\r\n",
        "       train_regret_winner_8[slice85],\r\n",
        "       train_regret_winner_9[slice85],\r\n",
        "       train_regret_winner_10[slice85],\r\n",
        "       train_regret_winner_11[slice85],\r\n",
        "       train_regret_winner_12[slice85],\r\n",
        "       train_regret_winner_13[slice85],\r\n",
        "       train_regret_winner_14[slice85],\r\n",
        "       train_regret_winner_15[slice85],\r\n",
        "       train_regret_winner_16[slice85],\r\n",
        "       train_regret_winner_17[slice85],\r\n",
        "       train_regret_winner_18[slice85],\r\n",
        "       train_regret_winner_19[slice85],\r\n",
        "       train_regret_winner_20[slice85]]\r\n",
        "\r\n",
        "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\r\n",
        "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\r\n",
        "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\r\n",
        "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\r\n",
        "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\r\n",
        "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztvJhrN6xAH1"
      },
      "source": [
        "# Iteration95 :\r\n",
        "\r\n",
        "slice95 = 94\r\n",
        "\r\n",
        "loser95 = [train_regret_loser_1[slice95],\r\n",
        "       train_regret_loser_2[slice95],\r\n",
        "       train_regret_loser_3[slice95],\r\n",
        "       train_regret_loser_4[slice95],\r\n",
        "       train_regret_loser_5[slice95],\r\n",
        "       train_regret_loser_6[slice95],\r\n",
        "       train_regret_loser_7[slice95],\r\n",
        "       train_regret_loser_8[slice95],\r\n",
        "       train_regret_loser_9[slice95],\r\n",
        "       train_regret_loser_10[slice95],\r\n",
        "       train_regret_loser_11[slice95],\r\n",
        "       train_regret_loser_12[slice95],\r\n",
        "       train_regret_loser_13[slice95],\r\n",
        "       train_regret_loser_14[slice95],\r\n",
        "       train_regret_loser_15[slice95],\r\n",
        "       train_regret_loser_16[slice95],\r\n",
        "       train_regret_loser_17[slice95],\r\n",
        "       train_regret_loser_18[slice95],\r\n",
        "       train_regret_loser_19[slice95],\r\n",
        "       train_regret_loser_20[slice95]]\r\n",
        "\r\n",
        "winner95 = [train_regret_winner_1[slice95],\r\n",
        "       train_regret_winner_2[slice95],\r\n",
        "       train_regret_winner_3[slice95],\r\n",
        "       train_regret_winner_4[slice95],\r\n",
        "       train_regret_winner_5[slice95],\r\n",
        "       train_regret_winner_6[slice95],\r\n",
        "       train_regret_winner_7[slice95],\r\n",
        "       train_regret_winner_8[slice95],\r\n",
        "       train_regret_winner_9[slice95],\r\n",
        "       train_regret_winner_10[slice95],\r\n",
        "       train_regret_winner_11[slice95],\r\n",
        "       train_regret_winner_12[slice95],\r\n",
        "       train_regret_winner_13[slice95],\r\n",
        "       train_regret_winner_14[slice95],\r\n",
        "       train_regret_winner_15[slice95],\r\n",
        "       train_regret_winner_16[slice95],\r\n",
        "       train_regret_winner_17[slice95],\r\n",
        "       train_regret_winner_18[slice95],\r\n",
        "       train_regret_winner_19[slice95],\r\n",
        "       train_regret_winner_20[slice95]]\r\n",
        "\r\n",
        "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\r\n",
        "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\r\n",
        "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\r\n",
        "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\r\n",
        "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\r\n",
        "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK-4nr-OxALA"
      },
      "source": [
        "# Iteration6 :\r\n",
        "\r\n",
        "slice6 = 5\r\n",
        "\r\n",
        "loser6 = [train_regret_loser_1[slice6],\r\n",
        "       train_regret_loser_2[slice6],\r\n",
        "       train_regret_loser_3[slice6],\r\n",
        "       train_regret_loser_4[slice6],\r\n",
        "       train_regret_loser_5[slice6],\r\n",
        "       train_regret_loser_6[slice6],\r\n",
        "       train_regret_loser_7[slice6],\r\n",
        "       train_regret_loser_8[slice6],\r\n",
        "       train_regret_loser_9[slice6],\r\n",
        "       train_regret_loser_10[slice6],\r\n",
        "       train_regret_loser_11[slice6],\r\n",
        "       train_regret_loser_12[slice6],\r\n",
        "       train_regret_loser_13[slice6],\r\n",
        "       train_regret_loser_14[slice6],\r\n",
        "       train_regret_loser_15[slice6],\r\n",
        "       train_regret_loser_16[slice6],\r\n",
        "       train_regret_loser_17[slice6],\r\n",
        "       train_regret_loser_18[slice6],\r\n",
        "       train_regret_loser_19[slice6],\r\n",
        "       train_regret_loser_20[slice6]]\r\n",
        "\r\n",
        "winner6 = [train_regret_winner_1[slice6],\r\n",
        "       train_regret_winner_2[slice6],\r\n",
        "       train_regret_winner_3[slice6],\r\n",
        "       train_regret_winner_4[slice6],\r\n",
        "       train_regret_winner_5[slice6],\r\n",
        "       train_regret_winner_6[slice6],\r\n",
        "       train_regret_winner_7[slice6],\r\n",
        "       train_regret_winner_8[slice6],\r\n",
        "       train_regret_winner_9[slice6],\r\n",
        "       train_regret_winner_10[slice6],\r\n",
        "       train_regret_winner_11[slice6],\r\n",
        "       train_regret_winner_12[slice6],\r\n",
        "       train_regret_winner_13[slice6],\r\n",
        "       train_regret_winner_14[slice6],\r\n",
        "       train_regret_winner_15[slice6],\r\n",
        "       train_regret_winner_16[slice6],\r\n",
        "       train_regret_winner_17[slice6],\r\n",
        "       train_regret_winner_18[slice6],\r\n",
        "       train_regret_winner_19[slice6],\r\n",
        "       train_regret_winner_20[slice6]]\r\n",
        "\r\n",
        "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\r\n",
        "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\r\n",
        "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\r\n",
        "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\r\n",
        "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\r\n",
        "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSwfNX5ExANt"
      },
      "source": [
        "# Iteration16 :\r\n",
        "\r\n",
        "slice16 = 15\r\n",
        "\r\n",
        "loser16 = [train_regret_loser_1[slice16],\r\n",
        "       train_regret_loser_2[slice16],\r\n",
        "       train_regret_loser_3[slice16],\r\n",
        "       train_regret_loser_4[slice16],\r\n",
        "       train_regret_loser_5[slice16],\r\n",
        "       train_regret_loser_6[slice16],\r\n",
        "       train_regret_loser_7[slice16],\r\n",
        "       train_regret_loser_8[slice16],\r\n",
        "       train_regret_loser_9[slice16],\r\n",
        "       train_regret_loser_10[slice16],\r\n",
        "       train_regret_loser_11[slice16],\r\n",
        "       train_regret_loser_12[slice16],\r\n",
        "       train_regret_loser_13[slice16],\r\n",
        "       train_regret_loser_14[slice16],\r\n",
        "       train_regret_loser_15[slice16],\r\n",
        "       train_regret_loser_16[slice16],\r\n",
        "       train_regret_loser_17[slice16],\r\n",
        "       train_regret_loser_18[slice16],\r\n",
        "       train_regret_loser_19[slice16],\r\n",
        "       train_regret_loser_20[slice16]]\r\n",
        "\r\n",
        "winner16 = [train_regret_winner_1[slice16],\r\n",
        "       train_regret_winner_2[slice16],\r\n",
        "       train_regret_winner_3[slice16],\r\n",
        "       train_regret_winner_4[slice16],\r\n",
        "       train_regret_winner_5[slice16],\r\n",
        "       train_regret_winner_6[slice16],\r\n",
        "       train_regret_winner_7[slice16],\r\n",
        "       train_regret_winner_8[slice16],\r\n",
        "       train_regret_winner_9[slice16],\r\n",
        "       train_regret_winner_10[slice16],\r\n",
        "       train_regret_winner_11[slice16],\r\n",
        "       train_regret_winner_12[slice16],\r\n",
        "       train_regret_winner_13[slice16],\r\n",
        "       train_regret_winner_14[slice16],\r\n",
        "       train_regret_winner_15[slice16],\r\n",
        "       train_regret_winner_16[slice16],\r\n",
        "       train_regret_winner_17[slice16],\r\n",
        "       train_regret_winner_18[slice16],\r\n",
        "       train_regret_winner_19[slice16],\r\n",
        "       train_regret_winner_20[slice16]]\r\n",
        "\r\n",
        "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\r\n",
        "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\r\n",
        "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\r\n",
        "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\r\n",
        "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\r\n",
        "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOB3hOLIxAQy"
      },
      "source": [
        "# Iteration26 :\r\n",
        "\r\n",
        "slice26 = 25\r\n",
        "\r\n",
        "loser26 = [train_regret_loser_1[slice26],\r\n",
        "       train_regret_loser_2[slice26],\r\n",
        "       train_regret_loser_3[slice26],\r\n",
        "       train_regret_loser_4[slice26],\r\n",
        "       train_regret_loser_5[slice26],\r\n",
        "       train_regret_loser_6[slice26],\r\n",
        "       train_regret_loser_7[slice26],\r\n",
        "       train_regret_loser_8[slice26],\r\n",
        "       train_regret_loser_9[slice26],\r\n",
        "       train_regret_loser_10[slice26],\r\n",
        "       train_regret_loser_11[slice26],\r\n",
        "       train_regret_loser_12[slice26],\r\n",
        "       train_regret_loser_13[slice26],\r\n",
        "       train_regret_loser_14[slice26],\r\n",
        "       train_regret_loser_15[slice26],\r\n",
        "       train_regret_loser_16[slice26],\r\n",
        "       train_regret_loser_17[slice26],\r\n",
        "       train_regret_loser_18[slice26],\r\n",
        "       train_regret_loser_19[slice26],\r\n",
        "       train_regret_loser_20[slice26]]\r\n",
        "\r\n",
        "winner26 = [train_regret_winner_1[slice26],\r\n",
        "       train_regret_winner_2[slice26],\r\n",
        "       train_regret_winner_3[slice26],\r\n",
        "       train_regret_winner_4[slice26],\r\n",
        "       train_regret_winner_5[slice26],\r\n",
        "       train_regret_winner_6[slice26],\r\n",
        "       train_regret_winner_7[slice26],\r\n",
        "       train_regret_winner_8[slice26],\r\n",
        "       train_regret_winner_9[slice26],\r\n",
        "       train_regret_winner_10[slice26],\r\n",
        "       train_regret_winner_11[slice26],\r\n",
        "       train_regret_winner_12[slice26],\r\n",
        "       train_regret_winner_13[slice26],\r\n",
        "       train_regret_winner_14[slice26],\r\n",
        "       train_regret_winner_15[slice26],\r\n",
        "       train_regret_winner_16[slice26],\r\n",
        "       train_regret_winner_17[slice26],\r\n",
        "       train_regret_winner_18[slice26],\r\n",
        "       train_regret_winner_19[slice26],\r\n",
        "       train_regret_winner_20[slice26]]\r\n",
        "\r\n",
        "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\r\n",
        "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\r\n",
        "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\r\n",
        "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\r\n",
        "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\r\n",
        "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-0hQMtOxATh"
      },
      "source": [
        "# Iteration36 :\r\n",
        "\r\n",
        "slice36 = 35\r\n",
        "\r\n",
        "loser36 = [train_regret_loser_1[slice36],\r\n",
        "       train_regret_loser_2[slice36],\r\n",
        "       train_regret_loser_3[slice36],\r\n",
        "       train_regret_loser_4[slice36],\r\n",
        "       train_regret_loser_5[slice36],\r\n",
        "       train_regret_loser_6[slice36],\r\n",
        "       train_regret_loser_7[slice36],\r\n",
        "       train_regret_loser_8[slice36],\r\n",
        "       train_regret_loser_9[slice36],\r\n",
        "       train_regret_loser_10[slice36],\r\n",
        "       train_regret_loser_11[slice36],\r\n",
        "       train_regret_loser_12[slice36],\r\n",
        "       train_regret_loser_13[slice36],\r\n",
        "       train_regret_loser_14[slice36],\r\n",
        "       train_regret_loser_15[slice36],\r\n",
        "       train_regret_loser_16[slice36],\r\n",
        "       train_regret_loser_17[slice36],\r\n",
        "       train_regret_loser_18[slice36],\r\n",
        "       train_regret_loser_19[slice36],\r\n",
        "       train_regret_loser_20[slice36]]\r\n",
        "\r\n",
        "winner36 = [train_regret_winner_1[slice36],\r\n",
        "       train_regret_winner_2[slice36],\r\n",
        "       train_regret_winner_3[slice36],\r\n",
        "       train_regret_winner_4[slice36],\r\n",
        "       train_regret_winner_5[slice36],\r\n",
        "       train_regret_winner_6[slice36],\r\n",
        "       train_regret_winner_7[slice36],\r\n",
        "       train_regret_winner_8[slice36],\r\n",
        "       train_regret_winner_9[slice36],\r\n",
        "       train_regret_winner_10[slice36],\r\n",
        "       train_regret_winner_11[slice36],\r\n",
        "       train_regret_winner_12[slice36],\r\n",
        "       train_regret_winner_13[slice36],\r\n",
        "       train_regret_winner_14[slice36],\r\n",
        "       train_regret_winner_15[slice36],\r\n",
        "       train_regret_winner_16[slice36],\r\n",
        "       train_regret_winner_17[slice36],\r\n",
        "       train_regret_winner_18[slice36],\r\n",
        "       train_regret_winner_19[slice36],\r\n",
        "       train_regret_winner_20[slice36]]\r\n",
        "\r\n",
        "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\r\n",
        "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\r\n",
        "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\r\n",
        "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\r\n",
        "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\r\n",
        "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSqBLqTgxAWs"
      },
      "source": [
        "# Iteration46 :\r\n",
        "\r\n",
        "slice46 = 45\r\n",
        "\r\n",
        "loser46 = [train_regret_loser_1[slice46],\r\n",
        "       train_regret_loser_2[slice46],\r\n",
        "       train_regret_loser_3[slice46],\r\n",
        "       train_regret_loser_4[slice46],\r\n",
        "       train_regret_loser_5[slice46],\r\n",
        "       train_regret_loser_6[slice46],\r\n",
        "       train_regret_loser_7[slice46],\r\n",
        "       train_regret_loser_8[slice46],\r\n",
        "       train_regret_loser_9[slice46],\r\n",
        "       train_regret_loser_10[slice46],\r\n",
        "       train_regret_loser_11[slice46],\r\n",
        "       train_regret_loser_12[slice46],\r\n",
        "       train_regret_loser_13[slice46],\r\n",
        "       train_regret_loser_14[slice46],\r\n",
        "       train_regret_loser_15[slice46],\r\n",
        "       train_regret_loser_16[slice46],\r\n",
        "       train_regret_loser_17[slice46],\r\n",
        "       train_regret_loser_18[slice46],\r\n",
        "       train_regret_loser_19[slice46],\r\n",
        "       train_regret_loser_20[slice46]]\r\n",
        "\r\n",
        "winner46 = [train_regret_winner_1[slice46],\r\n",
        "       train_regret_winner_2[slice46],\r\n",
        "       train_regret_winner_3[slice46],\r\n",
        "       train_regret_winner_4[slice46],\r\n",
        "       train_regret_winner_5[slice46],\r\n",
        "       train_regret_winner_6[slice46],\r\n",
        "       train_regret_winner_7[slice46],\r\n",
        "       train_regret_winner_8[slice46],\r\n",
        "       train_regret_winner_9[slice46],\r\n",
        "       train_regret_winner_10[slice46],\r\n",
        "       train_regret_winner_11[slice46],\r\n",
        "       train_regret_winner_12[slice46],\r\n",
        "       train_regret_winner_13[slice46],\r\n",
        "       train_regret_winner_14[slice46],\r\n",
        "       train_regret_winner_15[slice46],\r\n",
        "       train_regret_winner_16[slice46],\r\n",
        "       train_regret_winner_17[slice46],\r\n",
        "       train_regret_winner_18[slice46],\r\n",
        "       train_regret_winner_19[slice46],\r\n",
        "       train_regret_winner_20[slice46]]\r\n",
        "\r\n",
        "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\r\n",
        "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\r\n",
        "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\r\n",
        "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\r\n",
        "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\r\n",
        "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFwO80w6xAZZ"
      },
      "source": [
        "# Iteration56 :\r\n",
        "\r\n",
        "slice56 = 55\r\n",
        "\r\n",
        "loser56 = [train_regret_loser_1[slice56],\r\n",
        "       train_regret_loser_2[slice56],\r\n",
        "       train_regret_loser_3[slice56],\r\n",
        "       train_regret_loser_4[slice56],\r\n",
        "       train_regret_loser_5[slice56],\r\n",
        "       train_regret_loser_6[slice56],\r\n",
        "       train_regret_loser_7[slice56],\r\n",
        "       train_regret_loser_8[slice56],\r\n",
        "       train_regret_loser_9[slice56],\r\n",
        "       train_regret_loser_10[slice56],\r\n",
        "       train_regret_loser_11[slice56],\r\n",
        "       train_regret_loser_12[slice56],\r\n",
        "       train_regret_loser_13[slice56],\r\n",
        "       train_regret_loser_14[slice56],\r\n",
        "       train_regret_loser_15[slice56],\r\n",
        "       train_regret_loser_16[slice56],\r\n",
        "       train_regret_loser_17[slice56],\r\n",
        "       train_regret_loser_18[slice56],\r\n",
        "       train_regret_loser_19[slice56],\r\n",
        "       train_regret_loser_20[slice56]]\r\n",
        "\r\n",
        "winner56 = [train_regret_winner_1[slice56],\r\n",
        "       train_regret_winner_2[slice56],\r\n",
        "       train_regret_winner_3[slice56],\r\n",
        "       train_regret_winner_4[slice56],\r\n",
        "       train_regret_winner_5[slice56],\r\n",
        "       train_regret_winner_6[slice56],\r\n",
        "       train_regret_winner_7[slice56],\r\n",
        "       train_regret_winner_8[slice56],\r\n",
        "       train_regret_winner_9[slice56],\r\n",
        "       train_regret_winner_10[slice56],\r\n",
        "       train_regret_winner_11[slice56],\r\n",
        "       train_regret_winner_12[slice56],\r\n",
        "       train_regret_winner_13[slice56],\r\n",
        "       train_regret_winner_14[slice56],\r\n",
        "       train_regret_winner_15[slice56],\r\n",
        "       train_regret_winner_16[slice56],\r\n",
        "       train_regret_winner_17[slice56],\r\n",
        "       train_regret_winner_18[slice56],\r\n",
        "       train_regret_winner_19[slice56],\r\n",
        "       train_regret_winner_20[slice56]]\r\n",
        "\r\n",
        "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\r\n",
        "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\r\n",
        "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\r\n",
        "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\r\n",
        "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\r\n",
        "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfVZBDP8xAcC"
      },
      "source": [
        "# Iteration66 :\r\n",
        "\r\n",
        "slice66 = 65\r\n",
        "\r\n",
        "loser66 = [train_regret_loser_1[slice66],\r\n",
        "       train_regret_loser_2[slice66],\r\n",
        "       train_regret_loser_3[slice66],\r\n",
        "       train_regret_loser_4[slice66],\r\n",
        "       train_regret_loser_5[slice66],\r\n",
        "       train_regret_loser_6[slice66],\r\n",
        "       train_regret_loser_7[slice66],\r\n",
        "       train_regret_loser_8[slice66],\r\n",
        "       train_regret_loser_9[slice66],\r\n",
        "       train_regret_loser_10[slice66],\r\n",
        "       train_regret_loser_11[slice66],\r\n",
        "       train_regret_loser_12[slice66],\r\n",
        "       train_regret_loser_13[slice66],\r\n",
        "       train_regret_loser_14[slice66],\r\n",
        "       train_regret_loser_15[slice66],\r\n",
        "       train_regret_loser_16[slice66],\r\n",
        "       train_regret_loser_17[slice66],\r\n",
        "       train_regret_loser_18[slice66],\r\n",
        "       train_regret_loser_19[slice66],\r\n",
        "       train_regret_loser_20[slice66]]\r\n",
        "\r\n",
        "winner66 = [train_regret_winner_1[slice66],\r\n",
        "       train_regret_winner_2[slice66],\r\n",
        "       train_regret_winner_3[slice66],\r\n",
        "       train_regret_winner_4[slice66],\r\n",
        "       train_regret_winner_5[slice66],\r\n",
        "       train_regret_winner_6[slice66],\r\n",
        "       train_regret_winner_7[slice66],\r\n",
        "       train_regret_winner_8[slice66],\r\n",
        "       train_regret_winner_9[slice66],\r\n",
        "       train_regret_winner_10[slice66],\r\n",
        "       train_regret_winner_11[slice66],\r\n",
        "       train_regret_winner_12[slice66],\r\n",
        "       train_regret_winner_13[slice66],\r\n",
        "       train_regret_winner_14[slice66],\r\n",
        "       train_regret_winner_15[slice66],\r\n",
        "       train_regret_winner_16[slice66],\r\n",
        "       train_regret_winner_17[slice66],\r\n",
        "       train_regret_winner_18[slice66],\r\n",
        "       train_regret_winner_19[slice66],\r\n",
        "       train_regret_winner_20[slice66]]\r\n",
        "\r\n",
        "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\r\n",
        "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\r\n",
        "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\r\n",
        "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\r\n",
        "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\r\n",
        "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnSYoG7bxAfL"
      },
      "source": [
        "# Iteration76 :\r\n",
        "\r\n",
        "slice76 = 75\r\n",
        "\r\n",
        "loser76 = [train_regret_loser_1[slice76],\r\n",
        "       train_regret_loser_2[slice76],\r\n",
        "       train_regret_loser_3[slice76],\r\n",
        "       train_regret_loser_4[slice76],\r\n",
        "       train_regret_loser_5[slice76],\r\n",
        "       train_regret_loser_6[slice76],\r\n",
        "       train_regret_loser_7[slice76],\r\n",
        "       train_regret_loser_8[slice76],\r\n",
        "       train_regret_loser_9[slice76],\r\n",
        "       train_regret_loser_10[slice76],\r\n",
        "       train_regret_loser_11[slice76],\r\n",
        "       train_regret_loser_12[slice76],\r\n",
        "       train_regret_loser_13[slice76],\r\n",
        "       train_regret_loser_14[slice76],\r\n",
        "       train_regret_loser_15[slice76],\r\n",
        "       train_regret_loser_16[slice76],\r\n",
        "       train_regret_loser_17[slice76],\r\n",
        "       train_regret_loser_18[slice76],\r\n",
        "       train_regret_loser_19[slice76],\r\n",
        "       train_regret_loser_20[slice76]]\r\n",
        "\r\n",
        "winner76 = [train_regret_winner_1[slice76],\r\n",
        "       train_regret_winner_2[slice76],\r\n",
        "       train_regret_winner_3[slice76],\r\n",
        "       train_regret_winner_4[slice76],\r\n",
        "       train_regret_winner_5[slice76],\r\n",
        "       train_regret_winner_6[slice76],\r\n",
        "       train_regret_winner_7[slice76],\r\n",
        "       train_regret_winner_8[slice76],\r\n",
        "       train_regret_winner_9[slice76],\r\n",
        "       train_regret_winner_10[slice76],\r\n",
        "       train_regret_winner_11[slice76],\r\n",
        "       train_regret_winner_12[slice76],\r\n",
        "       train_regret_winner_13[slice76],\r\n",
        "       train_regret_winner_14[slice76],\r\n",
        "       train_regret_winner_15[slice76],\r\n",
        "       train_regret_winner_16[slice76],\r\n",
        "       train_regret_winner_17[slice76],\r\n",
        "       train_regret_winner_18[slice76],\r\n",
        "       train_regret_winner_19[slice76],\r\n",
        "       train_regret_winner_20[slice76]]\r\n",
        "\r\n",
        "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\r\n",
        "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\r\n",
        "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\r\n",
        "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\r\n",
        "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\r\n",
        "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ze_Oy9mxAiO"
      },
      "source": [
        "# Iteration86 :\r\n",
        "\r\n",
        "slice86 = 85\r\n",
        "\r\n",
        "loser86 = [train_regret_loser_1[slice86],\r\n",
        "       train_regret_loser_2[slice86],\r\n",
        "       train_regret_loser_3[slice86],\r\n",
        "       train_regret_loser_4[slice86],\r\n",
        "       train_regret_loser_5[slice86],\r\n",
        "       train_regret_loser_6[slice86],\r\n",
        "       train_regret_loser_7[slice86],\r\n",
        "       train_regret_loser_8[slice86],\r\n",
        "       train_regret_loser_9[slice86],\r\n",
        "       train_regret_loser_10[slice86],\r\n",
        "       train_regret_loser_11[slice86],\r\n",
        "       train_regret_loser_12[slice86],\r\n",
        "       train_regret_loser_13[slice86],\r\n",
        "       train_regret_loser_14[slice86],\r\n",
        "       train_regret_loser_15[slice86],\r\n",
        "       train_regret_loser_16[slice86],\r\n",
        "       train_regret_loser_17[slice86],\r\n",
        "       train_regret_loser_18[slice86],\r\n",
        "       train_regret_loser_19[slice86],\r\n",
        "       train_regret_loser_20[slice86]]\r\n",
        "\r\n",
        "winner86 = [train_regret_winner_1[slice86],\r\n",
        "       train_regret_winner_2[slice86],\r\n",
        "       train_regret_winner_3[slice86],\r\n",
        "       train_regret_winner_4[slice86],\r\n",
        "       train_regret_winner_5[slice86],\r\n",
        "       train_regret_winner_6[slice86],\r\n",
        "       train_regret_winner_7[slice86],\r\n",
        "       train_regret_winner_8[slice86],\r\n",
        "       train_regret_winner_9[slice86],\r\n",
        "       train_regret_winner_10[slice86],\r\n",
        "       train_regret_winner_11[slice86],\r\n",
        "       train_regret_winner_12[slice86],\r\n",
        "       train_regret_winner_13[slice86],\r\n",
        "       train_regret_winner_14[slice86],\r\n",
        "       train_regret_winner_15[slice86],\r\n",
        "       train_regret_winner_16[slice86],\r\n",
        "       train_regret_winner_17[slice86],\r\n",
        "       train_regret_winner_18[slice86],\r\n",
        "       train_regret_winner_19[slice86],\r\n",
        "       train_regret_winner_20[slice86]]\r\n",
        "\r\n",
        "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\r\n",
        "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\r\n",
        "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\r\n",
        "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\r\n",
        "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\r\n",
        "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glsE9_RPxAlS"
      },
      "source": [
        "# Iteration96 :\r\n",
        "\r\n",
        "slice96 = 95\r\n",
        "\r\n",
        "loser96 = [train_regret_loser_1[slice96],\r\n",
        "       train_regret_loser_2[slice96],\r\n",
        "       train_regret_loser_3[slice96],\r\n",
        "       train_regret_loser_4[slice96],\r\n",
        "       train_regret_loser_5[slice96],\r\n",
        "       train_regret_loser_6[slice96],\r\n",
        "       train_regret_loser_7[slice96],\r\n",
        "       train_regret_loser_8[slice96],\r\n",
        "       train_regret_loser_9[slice96],\r\n",
        "       train_regret_loser_10[slice96],\r\n",
        "       train_regret_loser_11[slice96],\r\n",
        "       train_regret_loser_12[slice96],\r\n",
        "       train_regret_loser_13[slice96],\r\n",
        "       train_regret_loser_14[slice96],\r\n",
        "       train_regret_loser_15[slice96],\r\n",
        "       train_regret_loser_16[slice96],\r\n",
        "       train_regret_loser_17[slice96],\r\n",
        "       train_regret_loser_18[slice96],\r\n",
        "       train_regret_loser_19[slice96],\r\n",
        "       train_regret_loser_20[slice96]]\r\n",
        "\r\n",
        "winner96 = [train_regret_winner_1[slice96],\r\n",
        "       train_regret_winner_2[slice96],\r\n",
        "       train_regret_winner_3[slice96],\r\n",
        "       train_regret_winner_4[slice96],\r\n",
        "       train_regret_winner_5[slice96],\r\n",
        "       train_regret_winner_6[slice96],\r\n",
        "       train_regret_winner_7[slice96],\r\n",
        "       train_regret_winner_8[slice96],\r\n",
        "       train_regret_winner_9[slice96],\r\n",
        "       train_regret_winner_10[slice96],\r\n",
        "       train_regret_winner_11[slice96],\r\n",
        "       train_regret_winner_12[slice96],\r\n",
        "       train_regret_winner_13[slice96],\r\n",
        "       train_regret_winner_14[slice96],\r\n",
        "       train_regret_winner_15[slice96],\r\n",
        "       train_regret_winner_16[slice96],\r\n",
        "       train_regret_winner_17[slice96],\r\n",
        "       train_regret_winner_18[slice96],\r\n",
        "       train_regret_winner_19[slice96],\r\n",
        "       train_regret_winner_20[slice96]]\r\n",
        "\r\n",
        "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\r\n",
        "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\r\n",
        "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\r\n",
        "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\r\n",
        "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\r\n",
        "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhsEfl1RxAoA"
      },
      "source": [
        "# Iteration7 :\r\n",
        "\r\n",
        "slice7 = 6\r\n",
        "\r\n",
        "loser7 = [train_regret_loser_1[slice7],\r\n",
        "       train_regret_loser_2[slice7],\r\n",
        "       train_regret_loser_3[slice7],\r\n",
        "       train_regret_loser_4[slice7],\r\n",
        "       train_regret_loser_5[slice7],\r\n",
        "       train_regret_loser_6[slice7],\r\n",
        "       train_regret_loser_7[slice7],\r\n",
        "       train_regret_loser_8[slice7],\r\n",
        "       train_regret_loser_9[slice7],\r\n",
        "       train_regret_loser_10[slice7],\r\n",
        "       train_regret_loser_11[slice7],\r\n",
        "       train_regret_loser_12[slice7],\r\n",
        "       train_regret_loser_13[slice7],\r\n",
        "       train_regret_loser_14[slice7],\r\n",
        "       train_regret_loser_15[slice7],\r\n",
        "       train_regret_loser_16[slice7],\r\n",
        "       train_regret_loser_17[slice7],\r\n",
        "       train_regret_loser_18[slice7],\r\n",
        "       train_regret_loser_19[slice7],\r\n",
        "       train_regret_loser_20[slice7]]\r\n",
        "\r\n",
        "winner7 = [train_regret_winner_1[slice7],\r\n",
        "       train_regret_winner_2[slice7],\r\n",
        "       train_regret_winner_3[slice7],\r\n",
        "       train_regret_winner_4[slice7],\r\n",
        "       train_regret_winner_5[slice7],\r\n",
        "       train_regret_winner_6[slice7],\r\n",
        "       train_regret_winner_7[slice7],\r\n",
        "       train_regret_winner_8[slice7],\r\n",
        "       train_regret_winner_9[slice7],\r\n",
        "       train_regret_winner_10[slice7],\r\n",
        "       train_regret_winner_11[slice7],\r\n",
        "       train_regret_winner_12[slice7],\r\n",
        "       train_regret_winner_13[slice7],\r\n",
        "       train_regret_winner_14[slice7],\r\n",
        "       train_regret_winner_15[slice7],\r\n",
        "       train_regret_winner_16[slice7],\r\n",
        "       train_regret_winner_17[slice7],\r\n",
        "       train_regret_winner_18[slice7],\r\n",
        "       train_regret_winner_19[slice7],\r\n",
        "       train_regret_winner_20[slice7]]\r\n",
        "\r\n",
        "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\r\n",
        "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\r\n",
        "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\r\n",
        "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\r\n",
        "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\r\n",
        "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUCMheLKxArL"
      },
      "source": [
        "# Iteration17 :\r\n",
        "\r\n",
        "slice17 = 16\r\n",
        "\r\n",
        "loser17 = [train_regret_loser_1[slice17],\r\n",
        "       train_regret_loser_2[slice17],\r\n",
        "       train_regret_loser_3[slice17],\r\n",
        "       train_regret_loser_4[slice17],\r\n",
        "       train_regret_loser_5[slice17],\r\n",
        "       train_regret_loser_6[slice17],\r\n",
        "       train_regret_loser_7[slice17],\r\n",
        "       train_regret_loser_8[slice17],\r\n",
        "       train_regret_loser_9[slice17],\r\n",
        "       train_regret_loser_10[slice17],\r\n",
        "       train_regret_loser_11[slice17],\r\n",
        "       train_regret_loser_12[slice17],\r\n",
        "       train_regret_loser_13[slice17],\r\n",
        "       train_regret_loser_14[slice17],\r\n",
        "       train_regret_loser_15[slice17],\r\n",
        "       train_regret_loser_16[slice17],\r\n",
        "       train_regret_loser_17[slice17],\r\n",
        "       train_regret_loser_18[slice17],\r\n",
        "       train_regret_loser_19[slice17],\r\n",
        "       train_regret_loser_20[slice17]]\r\n",
        "\r\n",
        "winner17 = [train_regret_winner_1[slice17],\r\n",
        "       train_regret_winner_2[slice17],\r\n",
        "       train_regret_winner_3[slice17],\r\n",
        "       train_regret_winner_4[slice17],\r\n",
        "       train_regret_winner_5[slice17],\r\n",
        "       train_regret_winner_6[slice17],\r\n",
        "       train_regret_winner_7[slice17],\r\n",
        "       train_regret_winner_8[slice17],\r\n",
        "       train_regret_winner_9[slice17],\r\n",
        "       train_regret_winner_10[slice17],\r\n",
        "       train_regret_winner_11[slice17],\r\n",
        "       train_regret_winner_12[slice17],\r\n",
        "       train_regret_winner_13[slice17],\r\n",
        "       train_regret_winner_14[slice17],\r\n",
        "       train_regret_winner_15[slice17],\r\n",
        "       train_regret_winner_16[slice17],\r\n",
        "       train_regret_winner_17[slice17],\r\n",
        "       train_regret_winner_18[slice17],\r\n",
        "       train_regret_winner_19[slice17],\r\n",
        "       train_regret_winner_20[slice17]]\r\n",
        "\r\n",
        "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\r\n",
        "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\r\n",
        "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\r\n",
        "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\r\n",
        "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\r\n",
        "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYnBmq7_xAuB"
      },
      "source": [
        "# Iteration27 :\r\n",
        "\r\n",
        "slice27 = 26\r\n",
        "\r\n",
        "loser27 = [train_regret_loser_1[slice27],\r\n",
        "       train_regret_loser_2[slice27],\r\n",
        "       train_regret_loser_3[slice27],\r\n",
        "       train_regret_loser_4[slice27],\r\n",
        "       train_regret_loser_5[slice27],\r\n",
        "       train_regret_loser_6[slice27],\r\n",
        "       train_regret_loser_7[slice27],\r\n",
        "       train_regret_loser_8[slice27],\r\n",
        "       train_regret_loser_9[slice27],\r\n",
        "       train_regret_loser_10[slice27],\r\n",
        "       train_regret_loser_11[slice27],\r\n",
        "       train_regret_loser_12[slice27],\r\n",
        "       train_regret_loser_13[slice27],\r\n",
        "       train_regret_loser_14[slice27],\r\n",
        "       train_regret_loser_15[slice27],\r\n",
        "       train_regret_loser_16[slice27],\r\n",
        "       train_regret_loser_17[slice27],\r\n",
        "       train_regret_loser_18[slice27],\r\n",
        "       train_regret_loser_19[slice27],\r\n",
        "       train_regret_loser_20[slice27]]\r\n",
        "\r\n",
        "winner27 = [train_regret_winner_1[slice27],\r\n",
        "       train_regret_winner_2[slice27],\r\n",
        "       train_regret_winner_3[slice27],\r\n",
        "       train_regret_winner_4[slice27],\r\n",
        "       train_regret_winner_5[slice27],\r\n",
        "       train_regret_winner_6[slice27],\r\n",
        "       train_regret_winner_7[slice27],\r\n",
        "       train_regret_winner_8[slice27],\r\n",
        "       train_regret_winner_9[slice27],\r\n",
        "       train_regret_winner_10[slice27],\r\n",
        "       train_regret_winner_11[slice27],\r\n",
        "       train_regret_winner_12[slice27],\r\n",
        "       train_regret_winner_13[slice27],\r\n",
        "       train_regret_winner_14[slice27],\r\n",
        "       train_regret_winner_15[slice27],\r\n",
        "       train_regret_winner_16[slice27],\r\n",
        "       train_regret_winner_17[slice27],\r\n",
        "       train_regret_winner_18[slice27],\r\n",
        "       train_regret_winner_19[slice27],\r\n",
        "       train_regret_winner_20[slice27]]\r\n",
        "\r\n",
        "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\r\n",
        "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\r\n",
        "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\r\n",
        "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\r\n",
        "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\r\n",
        "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ChA8KOUxAw6"
      },
      "source": [
        "# Iteration37 :\r\n",
        "\r\n",
        "slice37 = 36\r\n",
        "\r\n",
        "loser37 = [train_regret_loser_1[slice37],\r\n",
        "       train_regret_loser_2[slice37],\r\n",
        "       train_regret_loser_3[slice37],\r\n",
        "       train_regret_loser_4[slice37],\r\n",
        "       train_regret_loser_5[slice37],\r\n",
        "       train_regret_loser_6[slice37],\r\n",
        "       train_regret_loser_7[slice37],\r\n",
        "       train_regret_loser_8[slice37],\r\n",
        "       train_regret_loser_9[slice37],\r\n",
        "       train_regret_loser_10[slice37],\r\n",
        "       train_regret_loser_11[slice37],\r\n",
        "       train_regret_loser_12[slice37],\r\n",
        "       train_regret_loser_13[slice37],\r\n",
        "       train_regret_loser_14[slice37],\r\n",
        "       train_regret_loser_15[slice37],\r\n",
        "       train_regret_loser_16[slice37],\r\n",
        "       train_regret_loser_17[slice37],\r\n",
        "       train_regret_loser_18[slice37],\r\n",
        "       train_regret_loser_19[slice37],\r\n",
        "       train_regret_loser_20[slice37]]\r\n",
        "\r\n",
        "winner37 = [train_regret_winner_1[slice37],\r\n",
        "       train_regret_winner_2[slice37],\r\n",
        "       train_regret_winner_3[slice37],\r\n",
        "       train_regret_winner_4[slice37],\r\n",
        "       train_regret_winner_5[slice37],\r\n",
        "       train_regret_winner_6[slice37],\r\n",
        "       train_regret_winner_7[slice37],\r\n",
        "       train_regret_winner_8[slice37],\r\n",
        "       train_regret_winner_9[slice37],\r\n",
        "       train_regret_winner_10[slice37],\r\n",
        "       train_regret_winner_11[slice37],\r\n",
        "       train_regret_winner_12[slice37],\r\n",
        "       train_regret_winner_13[slice37],\r\n",
        "       train_regret_winner_14[slice37],\r\n",
        "       train_regret_winner_15[slice37],\r\n",
        "       train_regret_winner_16[slice37],\r\n",
        "       train_regret_winner_17[slice37],\r\n",
        "       train_regret_winner_18[slice37],\r\n",
        "       train_regret_winner_19[slice37],\r\n",
        "       train_regret_winner_20[slice37]]\r\n",
        "\r\n",
        "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\r\n",
        "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\r\n",
        "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\r\n",
        "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\r\n",
        "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\r\n",
        "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVsY1buDxAzx"
      },
      "source": [
        "# Iteration47 :\r\n",
        "\r\n",
        "slice47 = 46\r\n",
        "\r\n",
        "loser47 = [train_regret_loser_1[slice47],\r\n",
        "       train_regret_loser_2[slice47],\r\n",
        "       train_regret_loser_3[slice47],\r\n",
        "       train_regret_loser_4[slice47],\r\n",
        "       train_regret_loser_5[slice47],\r\n",
        "       train_regret_loser_6[slice47],\r\n",
        "       train_regret_loser_7[slice47],\r\n",
        "       train_regret_loser_8[slice47],\r\n",
        "       train_regret_loser_9[slice47],\r\n",
        "       train_regret_loser_10[slice47],\r\n",
        "       train_regret_loser_11[slice47],\r\n",
        "       train_regret_loser_12[slice47],\r\n",
        "       train_regret_loser_13[slice47],\r\n",
        "       train_regret_loser_14[slice47],\r\n",
        "       train_regret_loser_15[slice47],\r\n",
        "       train_regret_loser_16[slice47],\r\n",
        "       train_regret_loser_17[slice47],\r\n",
        "       train_regret_loser_18[slice47],\r\n",
        "       train_regret_loser_19[slice47],\r\n",
        "       train_regret_loser_20[slice47]]\r\n",
        "\r\n",
        "winner47 = [train_regret_winner_1[slice47],\r\n",
        "       train_regret_winner_2[slice47],\r\n",
        "       train_regret_winner_3[slice47],\r\n",
        "       train_regret_winner_4[slice47],\r\n",
        "       train_regret_winner_5[slice47],\r\n",
        "       train_regret_winner_6[slice47],\r\n",
        "       train_regret_winner_7[slice47],\r\n",
        "       train_regret_winner_8[slice47],\r\n",
        "       train_regret_winner_9[slice47],\r\n",
        "       train_regret_winner_10[slice47],\r\n",
        "       train_regret_winner_11[slice47],\r\n",
        "       train_regret_winner_12[slice47],\r\n",
        "       train_regret_winner_13[slice47],\r\n",
        "       train_regret_winner_14[slice47],\r\n",
        "       train_regret_winner_15[slice47],\r\n",
        "       train_regret_winner_16[slice47],\r\n",
        "       train_regret_winner_17[slice47],\r\n",
        "       train_regret_winner_18[slice47],\r\n",
        "       train_regret_winner_19[slice47],\r\n",
        "       train_regret_winner_20[slice47]]\r\n",
        "\r\n",
        "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\r\n",
        "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\r\n",
        "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\r\n",
        "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\r\n",
        "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\r\n",
        "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fIy_EyTxA20"
      },
      "source": [
        "# Iteration57 :\r\n",
        "\r\n",
        "slice57 = 56\r\n",
        "\r\n",
        "loser57 = [train_regret_loser_1[slice57],\r\n",
        "       train_regret_loser_2[slice57],\r\n",
        "       train_regret_loser_3[slice57],\r\n",
        "       train_regret_loser_4[slice57],\r\n",
        "       train_regret_loser_5[slice57],\r\n",
        "       train_regret_loser_6[slice57],\r\n",
        "       train_regret_loser_7[slice57],\r\n",
        "       train_regret_loser_8[slice57],\r\n",
        "       train_regret_loser_9[slice57],\r\n",
        "       train_regret_loser_10[slice57],\r\n",
        "       train_regret_loser_11[slice57],\r\n",
        "       train_regret_loser_12[slice57],\r\n",
        "       train_regret_loser_13[slice57],\r\n",
        "       train_regret_loser_14[slice57],\r\n",
        "       train_regret_loser_15[slice57],\r\n",
        "       train_regret_loser_16[slice57],\r\n",
        "       train_regret_loser_17[slice57],\r\n",
        "       train_regret_loser_18[slice57],\r\n",
        "       train_regret_loser_19[slice57],\r\n",
        "       train_regret_loser_20[slice57]]\r\n",
        "\r\n",
        "winner57 = [train_regret_winner_1[slice57],\r\n",
        "       train_regret_winner_2[slice57],\r\n",
        "       train_regret_winner_3[slice57],\r\n",
        "       train_regret_winner_4[slice57],\r\n",
        "       train_regret_winner_5[slice57],\r\n",
        "       train_regret_winner_6[slice57],\r\n",
        "       train_regret_winner_7[slice57],\r\n",
        "       train_regret_winner_8[slice57],\r\n",
        "       train_regret_winner_9[slice57],\r\n",
        "       train_regret_winner_10[slice57],\r\n",
        "       train_regret_winner_11[slice57],\r\n",
        "       train_regret_winner_12[slice57],\r\n",
        "       train_regret_winner_13[slice57],\r\n",
        "       train_regret_winner_14[slice57],\r\n",
        "       train_regret_winner_15[slice57],\r\n",
        "       train_regret_winner_16[slice57],\r\n",
        "       train_regret_winner_17[slice57],\r\n",
        "       train_regret_winner_18[slice57],\r\n",
        "       train_regret_winner_19[slice57],\r\n",
        "       train_regret_winner_20[slice57]]\r\n",
        "\r\n",
        "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\r\n",
        "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\r\n",
        "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\r\n",
        "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\r\n",
        "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\r\n",
        "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umwdBl8XxA5r"
      },
      "source": [
        "# Iteration67 :\r\n",
        "\r\n",
        "slice67 = 66\r\n",
        "\r\n",
        "loser67 = [train_regret_loser_1[slice67],\r\n",
        "       train_regret_loser_2[slice67],\r\n",
        "       train_regret_loser_3[slice67],\r\n",
        "       train_regret_loser_4[slice67],\r\n",
        "       train_regret_loser_5[slice67],\r\n",
        "       train_regret_loser_6[slice67],\r\n",
        "       train_regret_loser_7[slice67],\r\n",
        "       train_regret_loser_8[slice67],\r\n",
        "       train_regret_loser_9[slice67],\r\n",
        "       train_regret_loser_10[slice67],\r\n",
        "       train_regret_loser_11[slice67],\r\n",
        "       train_regret_loser_12[slice67],\r\n",
        "       train_regret_loser_13[slice67],\r\n",
        "       train_regret_loser_14[slice67],\r\n",
        "       train_regret_loser_15[slice67],\r\n",
        "       train_regret_loser_16[slice67],\r\n",
        "       train_regret_loser_17[slice67],\r\n",
        "       train_regret_loser_18[slice67],\r\n",
        "       train_regret_loser_19[slice67],\r\n",
        "       train_regret_loser_20[slice67]]\r\n",
        "\r\n",
        "winner67 = [train_regret_winner_1[slice67],\r\n",
        "       train_regret_winner_2[slice67],\r\n",
        "       train_regret_winner_3[slice67],\r\n",
        "       train_regret_winner_4[slice67],\r\n",
        "       train_regret_winner_5[slice67],\r\n",
        "       train_regret_winner_6[slice67],\r\n",
        "       train_regret_winner_7[slice67],\r\n",
        "       train_regret_winner_8[slice67],\r\n",
        "       train_regret_winner_9[slice67],\r\n",
        "       train_regret_winner_10[slice67],\r\n",
        "       train_regret_winner_11[slice67],\r\n",
        "       train_regret_winner_12[slice67],\r\n",
        "       train_regret_winner_13[slice67],\r\n",
        "       train_regret_winner_14[slice67],\r\n",
        "       train_regret_winner_15[slice67],\r\n",
        "       train_regret_winner_16[slice67],\r\n",
        "       train_regret_winner_17[slice67],\r\n",
        "       train_regret_winner_18[slice67],\r\n",
        "       train_regret_winner_19[slice67],\r\n",
        "       train_regret_winner_20[slice67]]\r\n",
        "\r\n",
        "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\r\n",
        "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\r\n",
        "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\r\n",
        "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\r\n",
        "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\r\n",
        "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF3KFX-ZxA8q"
      },
      "source": [
        "# Iteration77 :\r\n",
        "\r\n",
        "slice77 = 76\r\n",
        "\r\n",
        "loser77 = [train_regret_loser_1[slice77],\r\n",
        "       train_regret_loser_2[slice77],\r\n",
        "       train_regret_loser_3[slice77],\r\n",
        "       train_regret_loser_4[slice77],\r\n",
        "       train_regret_loser_5[slice77],\r\n",
        "       train_regret_loser_6[slice77],\r\n",
        "       train_regret_loser_7[slice77],\r\n",
        "       train_regret_loser_8[slice77],\r\n",
        "       train_regret_loser_9[slice77],\r\n",
        "       train_regret_loser_10[slice77],\r\n",
        "       train_regret_loser_11[slice77],\r\n",
        "       train_regret_loser_12[slice77],\r\n",
        "       train_regret_loser_13[slice77],\r\n",
        "       train_regret_loser_14[slice77],\r\n",
        "       train_regret_loser_15[slice77],\r\n",
        "       train_regret_loser_16[slice77],\r\n",
        "       train_regret_loser_17[slice77],\r\n",
        "       train_regret_loser_18[slice77],\r\n",
        "       train_regret_loser_19[slice77],\r\n",
        "       train_regret_loser_20[slice77]]\r\n",
        "\r\n",
        "winner77 = [train_regret_winner_1[slice77],\r\n",
        "       train_regret_winner_2[slice77],\r\n",
        "       train_regret_winner_3[slice77],\r\n",
        "       train_regret_winner_4[slice77],\r\n",
        "       train_regret_winner_5[slice77],\r\n",
        "       train_regret_winner_6[slice77],\r\n",
        "       train_regret_winner_7[slice77],\r\n",
        "       train_regret_winner_8[slice77],\r\n",
        "       train_regret_winner_9[slice77],\r\n",
        "       train_regret_winner_10[slice77],\r\n",
        "       train_regret_winner_11[slice77],\r\n",
        "       train_regret_winner_12[slice77],\r\n",
        "       train_regret_winner_13[slice77],\r\n",
        "       train_regret_winner_14[slice77],\r\n",
        "       train_regret_winner_15[slice77],\r\n",
        "       train_regret_winner_16[slice77],\r\n",
        "       train_regret_winner_17[slice77],\r\n",
        "       train_regret_winner_18[slice77],\r\n",
        "       train_regret_winner_19[slice77],\r\n",
        "       train_regret_winner_20[slice77]]\r\n",
        "\r\n",
        "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\r\n",
        "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\r\n",
        "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\r\n",
        "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\r\n",
        "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\r\n",
        "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVYft39xA_t"
      },
      "source": [
        "# Iteration87 :\r\n",
        "\r\n",
        "slice87 = 86\r\n",
        "\r\n",
        "loser87 = [train_regret_loser_1[slice87],\r\n",
        "       train_regret_loser_2[slice87],\r\n",
        "       train_regret_loser_3[slice87],\r\n",
        "       train_regret_loser_4[slice87],\r\n",
        "       train_regret_loser_5[slice87],\r\n",
        "       train_regret_loser_6[slice87],\r\n",
        "       train_regret_loser_7[slice87],\r\n",
        "       train_regret_loser_8[slice87],\r\n",
        "       train_regret_loser_9[slice87],\r\n",
        "       train_regret_loser_10[slice87],\r\n",
        "       train_regret_loser_11[slice87],\r\n",
        "       train_regret_loser_12[slice87],\r\n",
        "       train_regret_loser_13[slice87],\r\n",
        "       train_regret_loser_14[slice87],\r\n",
        "       train_regret_loser_15[slice87],\r\n",
        "       train_regret_loser_16[slice87],\r\n",
        "       train_regret_loser_17[slice87],\r\n",
        "       train_regret_loser_18[slice87],\r\n",
        "       train_regret_loser_19[slice87],\r\n",
        "       train_regret_loser_20[slice87]]\r\n",
        "\r\n",
        "winner87 = [train_regret_winner_1[slice87],\r\n",
        "       train_regret_winner_2[slice87],\r\n",
        "       train_regret_winner_3[slice87],\r\n",
        "       train_regret_winner_4[slice87],\r\n",
        "       train_regret_winner_5[slice87],\r\n",
        "       train_regret_winner_6[slice87],\r\n",
        "       train_regret_winner_7[slice87],\r\n",
        "       train_regret_winner_8[slice87],\r\n",
        "       train_regret_winner_9[slice87],\r\n",
        "       train_regret_winner_10[slice87],\r\n",
        "       train_regret_winner_11[slice87],\r\n",
        "       train_regret_winner_12[slice87],\r\n",
        "       train_regret_winner_13[slice87],\r\n",
        "       train_regret_winner_14[slice87],\r\n",
        "       train_regret_winner_15[slice87],\r\n",
        "       train_regret_winner_16[slice87],\r\n",
        "       train_regret_winner_17[slice87],\r\n",
        "       train_regret_winner_18[slice87],\r\n",
        "       train_regret_winner_19[slice87],\r\n",
        "       train_regret_winner_20[slice87]]\r\n",
        "\r\n",
        "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\r\n",
        "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\r\n",
        "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\r\n",
        "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\r\n",
        "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\r\n",
        "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gPkLC0yxBCs"
      },
      "source": [
        "# Iteration97 :\r\n",
        "\r\n",
        "slice97 = 96\r\n",
        "\r\n",
        "loser97 = [train_regret_loser_1[slice97],\r\n",
        "       train_regret_loser_2[slice97],\r\n",
        "       train_regret_loser_3[slice97],\r\n",
        "       train_regret_loser_4[slice97],\r\n",
        "       train_regret_loser_5[slice97],\r\n",
        "       train_regret_loser_6[slice97],\r\n",
        "       train_regret_loser_7[slice97],\r\n",
        "       train_regret_loser_8[slice97],\r\n",
        "       train_regret_loser_9[slice97],\r\n",
        "       train_regret_loser_10[slice97],\r\n",
        "       train_regret_loser_11[slice97],\r\n",
        "       train_regret_loser_12[slice97],\r\n",
        "       train_regret_loser_13[slice97],\r\n",
        "       train_regret_loser_14[slice97],\r\n",
        "       train_regret_loser_15[slice97],\r\n",
        "       train_regret_loser_16[slice97],\r\n",
        "       train_regret_loser_17[slice97],\r\n",
        "       train_regret_loser_18[slice97],\r\n",
        "       train_regret_loser_19[slice97],\r\n",
        "       train_regret_loser_20[slice97]]\r\n",
        "\r\n",
        "winner97 = [train_regret_winner_1[slice97],\r\n",
        "       train_regret_winner_2[slice97],\r\n",
        "       train_regret_winner_3[slice97],\r\n",
        "       train_regret_winner_4[slice97],\r\n",
        "       train_regret_winner_5[slice97],\r\n",
        "       train_regret_winner_6[slice97],\r\n",
        "       train_regret_winner_7[slice97],\r\n",
        "       train_regret_winner_8[slice97],\r\n",
        "       train_regret_winner_9[slice97],\r\n",
        "       train_regret_winner_10[slice97],\r\n",
        "       train_regret_winner_11[slice97],\r\n",
        "       train_regret_winner_12[slice97],\r\n",
        "       train_regret_winner_13[slice97],\r\n",
        "       train_regret_winner_14[slice97],\r\n",
        "       train_regret_winner_15[slice97],\r\n",
        "       train_regret_winner_16[slice97],\r\n",
        "       train_regret_winner_17[slice97],\r\n",
        "       train_regret_winner_18[slice97],\r\n",
        "       train_regret_winner_19[slice97],\r\n",
        "       train_regret_winner_20[slice97]]\r\n",
        "\r\n",
        "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\r\n",
        "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\r\n",
        "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\r\n",
        "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\r\n",
        "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\r\n",
        "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbgYEpYUxBFr"
      },
      "source": [
        "# Iteration8 :\r\n",
        "\r\n",
        "slice8 = 7\r\n",
        "\r\n",
        "loser8 = [train_regret_loser_1[slice8],\r\n",
        "       train_regret_loser_2[slice8],\r\n",
        "       train_regret_loser_3[slice8],\r\n",
        "       train_regret_loser_4[slice8],\r\n",
        "       train_regret_loser_5[slice8],\r\n",
        "       train_regret_loser_6[slice8],\r\n",
        "       train_regret_loser_7[slice8],\r\n",
        "       train_regret_loser_8[slice8],\r\n",
        "       train_regret_loser_9[slice8],\r\n",
        "       train_regret_loser_10[slice8],\r\n",
        "       train_regret_loser_11[slice8],\r\n",
        "       train_regret_loser_12[slice8],\r\n",
        "       train_regret_loser_13[slice8],\r\n",
        "       train_regret_loser_14[slice8],\r\n",
        "       train_regret_loser_15[slice8],\r\n",
        "       train_regret_loser_16[slice8],\r\n",
        "       train_regret_loser_17[slice8],\r\n",
        "       train_regret_loser_18[slice8],\r\n",
        "       train_regret_loser_19[slice8],\r\n",
        "       train_regret_loser_20[slice8]]\r\n",
        "\r\n",
        "winner8 = [train_regret_winner_1[slice8],\r\n",
        "       train_regret_winner_2[slice8],\r\n",
        "       train_regret_winner_3[slice8],\r\n",
        "       train_regret_winner_4[slice8],\r\n",
        "       train_regret_winner_5[slice8],\r\n",
        "       train_regret_winner_6[slice8],\r\n",
        "       train_regret_winner_7[slice8],\r\n",
        "       train_regret_winner_8[slice8],\r\n",
        "       train_regret_winner_9[slice8],\r\n",
        "       train_regret_winner_10[slice8],\r\n",
        "       train_regret_winner_11[slice8],\r\n",
        "       train_regret_winner_12[slice8],\r\n",
        "       train_regret_winner_13[slice8],\r\n",
        "       train_regret_winner_14[slice8],\r\n",
        "       train_regret_winner_15[slice8],\r\n",
        "       train_regret_winner_16[slice8],\r\n",
        "       train_regret_winner_17[slice8],\r\n",
        "       train_regret_winner_18[slice8],\r\n",
        "       train_regret_winner_19[slice8],\r\n",
        "       train_regret_winner_20[slice8]]\r\n",
        "\r\n",
        "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\r\n",
        "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\r\n",
        "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\r\n",
        "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\r\n",
        "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\r\n",
        "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im9dHUqvxBH9"
      },
      "source": [
        "# Iteration18 :\r\n",
        "\r\n",
        "slice18 = 17\r\n",
        "\r\n",
        "loser18 = [train_regret_loser_1[slice18],\r\n",
        "       train_regret_loser_2[slice18],\r\n",
        "       train_regret_loser_3[slice18],\r\n",
        "       train_regret_loser_4[slice18],\r\n",
        "       train_regret_loser_5[slice18],\r\n",
        "       train_regret_loser_6[slice18],\r\n",
        "       train_regret_loser_7[slice18],\r\n",
        "       train_regret_loser_8[slice18],\r\n",
        "       train_regret_loser_9[slice18],\r\n",
        "       train_regret_loser_10[slice18],\r\n",
        "       train_regret_loser_11[slice18],\r\n",
        "       train_regret_loser_12[slice18],\r\n",
        "       train_regret_loser_13[slice18],\r\n",
        "       train_regret_loser_14[slice18],\r\n",
        "       train_regret_loser_15[slice18],\r\n",
        "       train_regret_loser_16[slice18],\r\n",
        "       train_regret_loser_17[slice18],\r\n",
        "       train_regret_loser_18[slice18],\r\n",
        "       train_regret_loser_19[slice18],\r\n",
        "       train_regret_loser_20[slice18]]\r\n",
        "\r\n",
        "winner18 = [train_regret_winner_1[slice18],\r\n",
        "       train_regret_winner_2[slice18],\r\n",
        "       train_regret_winner_3[slice18],\r\n",
        "       train_regret_winner_4[slice18],\r\n",
        "       train_regret_winner_5[slice18],\r\n",
        "       train_regret_winner_6[slice18],\r\n",
        "       train_regret_winner_7[slice18],\r\n",
        "       train_regret_winner_8[slice18],\r\n",
        "       train_regret_winner_9[slice18],\r\n",
        "       train_regret_winner_10[slice18],\r\n",
        "       train_regret_winner_11[slice18],\r\n",
        "       train_regret_winner_12[slice18],\r\n",
        "       train_regret_winner_13[slice18],\r\n",
        "       train_regret_winner_14[slice18],\r\n",
        "       train_regret_winner_15[slice18],\r\n",
        "       train_regret_winner_16[slice18],\r\n",
        "       train_regret_winner_17[slice18],\r\n",
        "       train_regret_winner_18[slice18],\r\n",
        "       train_regret_winner_19[slice18],\r\n",
        "       train_regret_winner_20[slice18]]\r\n",
        "\r\n",
        "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\r\n",
        "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\r\n",
        "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\r\n",
        "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\r\n",
        "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\r\n",
        "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvqFjlBDxBK4"
      },
      "source": [
        "# Iteration28 :\r\n",
        "\r\n",
        "slice28 = 27\r\n",
        "\r\n",
        "loser28 = [train_regret_loser_1[slice28],\r\n",
        "       train_regret_loser_2[slice28],\r\n",
        "       train_regret_loser_3[slice28],\r\n",
        "       train_regret_loser_4[slice28],\r\n",
        "       train_regret_loser_5[slice28],\r\n",
        "       train_regret_loser_6[slice28],\r\n",
        "       train_regret_loser_7[slice28],\r\n",
        "       train_regret_loser_8[slice28],\r\n",
        "       train_regret_loser_9[slice28],\r\n",
        "       train_regret_loser_10[slice28],\r\n",
        "       train_regret_loser_11[slice28],\r\n",
        "       train_regret_loser_12[slice28],\r\n",
        "       train_regret_loser_13[slice28],\r\n",
        "       train_regret_loser_14[slice28],\r\n",
        "       train_regret_loser_15[slice28],\r\n",
        "       train_regret_loser_16[slice28],\r\n",
        "       train_regret_loser_17[slice28],\r\n",
        "       train_regret_loser_18[slice28],\r\n",
        "       train_regret_loser_19[slice28],\r\n",
        "       train_regret_loser_20[slice28]]\r\n",
        "\r\n",
        "winner28 = [train_regret_winner_1[slice28],\r\n",
        "       train_regret_winner_2[slice28],\r\n",
        "       train_regret_winner_3[slice28],\r\n",
        "       train_regret_winner_4[slice28],\r\n",
        "       train_regret_winner_5[slice28],\r\n",
        "       train_regret_winner_6[slice28],\r\n",
        "       train_regret_winner_7[slice28],\r\n",
        "       train_regret_winner_8[slice28],\r\n",
        "       train_regret_winner_9[slice28],\r\n",
        "       train_regret_winner_10[slice28],\r\n",
        "       train_regret_winner_11[slice28],\r\n",
        "       train_regret_winner_12[slice28],\r\n",
        "       train_regret_winner_13[slice28],\r\n",
        "       train_regret_winner_14[slice28],\r\n",
        "       train_regret_winner_15[slice28],\r\n",
        "       train_regret_winner_16[slice28],\r\n",
        "       train_regret_winner_17[slice28],\r\n",
        "       train_regret_winner_18[slice28],\r\n",
        "       train_regret_winner_19[slice28],\r\n",
        "       train_regret_winner_20[slice28]]\r\n",
        "\r\n",
        "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\r\n",
        "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\r\n",
        "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\r\n",
        "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\r\n",
        "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\r\n",
        "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It5daJCEted3"
      },
      "source": [
        "# Iteration38 :\r\n",
        "\r\n",
        "slice38 = 37\r\n",
        "\r\n",
        "loser38 = [train_regret_loser_1[slice38],\r\n",
        "       train_regret_loser_2[slice38],\r\n",
        "       train_regret_loser_3[slice38],\r\n",
        "       train_regret_loser_4[slice38],\r\n",
        "       train_regret_loser_5[slice38],\r\n",
        "       train_regret_loser_6[slice38],\r\n",
        "       train_regret_loser_7[slice38],\r\n",
        "       train_regret_loser_8[slice38],\r\n",
        "       train_regret_loser_9[slice38],\r\n",
        "       train_regret_loser_10[slice38],\r\n",
        "       train_regret_loser_11[slice38],\r\n",
        "       train_regret_loser_12[slice38],\r\n",
        "       train_regret_loser_13[slice38],\r\n",
        "       train_regret_loser_14[slice38],\r\n",
        "       train_regret_loser_15[slice38],\r\n",
        "       train_regret_loser_16[slice38],\r\n",
        "       train_regret_loser_17[slice38],\r\n",
        "       train_regret_loser_18[slice38],\r\n",
        "       train_regret_loser_19[slice38],\r\n",
        "       train_regret_loser_20[slice38]]\r\n",
        "\r\n",
        "winner38 = [train_regret_winner_1[slice38],\r\n",
        "       train_regret_winner_2[slice38],\r\n",
        "       train_regret_winner_3[slice38],\r\n",
        "       train_regret_winner_4[slice38],\r\n",
        "       train_regret_winner_5[slice38],\r\n",
        "       train_regret_winner_6[slice38],\r\n",
        "       train_regret_winner_7[slice38],\r\n",
        "       train_regret_winner_8[slice38],\r\n",
        "       train_regret_winner_9[slice38],\r\n",
        "       train_regret_winner_10[slice38],\r\n",
        "       train_regret_winner_11[slice38],\r\n",
        "       train_regret_winner_12[slice38],\r\n",
        "       train_regret_winner_13[slice38],\r\n",
        "       train_regret_winner_14[slice38],\r\n",
        "       train_regret_winner_15[slice38],\r\n",
        "       train_regret_winner_16[slice38],\r\n",
        "       train_regret_winner_17[slice38],\r\n",
        "       train_regret_winner_18[slice38],\r\n",
        "       train_regret_winner_19[slice38],\r\n",
        "       train_regret_winner_20[slice38]]\r\n",
        "\r\n",
        "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\r\n",
        "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\r\n",
        "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\r\n",
        "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\r\n",
        "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\r\n",
        "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GekXOf_gtegw"
      },
      "source": [
        "# Iteration48 :\r\n",
        "\r\n",
        "slice48 = 47\r\n",
        "\r\n",
        "loser48 = [train_regret_loser_1[slice48],\r\n",
        "       train_regret_loser_2[slice48],\r\n",
        "       train_regret_loser_3[slice48],\r\n",
        "       train_regret_loser_4[slice48],\r\n",
        "       train_regret_loser_5[slice48],\r\n",
        "       train_regret_loser_6[slice48],\r\n",
        "       train_regret_loser_7[slice48],\r\n",
        "       train_regret_loser_8[slice48],\r\n",
        "       train_regret_loser_9[slice48],\r\n",
        "       train_regret_loser_10[slice48],\r\n",
        "       train_regret_loser_11[slice48],\r\n",
        "       train_regret_loser_12[slice48],\r\n",
        "       train_regret_loser_13[slice48],\r\n",
        "       train_regret_loser_14[slice48],\r\n",
        "       train_regret_loser_15[slice48],\r\n",
        "       train_regret_loser_16[slice48],\r\n",
        "       train_regret_loser_17[slice48],\r\n",
        "       train_regret_loser_18[slice48],\r\n",
        "       train_regret_loser_19[slice48],\r\n",
        "       train_regret_loser_20[slice48]]\r\n",
        "\r\n",
        "winner48 = [train_regret_winner_1[slice48],\r\n",
        "       train_regret_winner_2[slice48],\r\n",
        "       train_regret_winner_3[slice48],\r\n",
        "       train_regret_winner_4[slice48],\r\n",
        "       train_regret_winner_5[slice48],\r\n",
        "       train_regret_winner_6[slice48],\r\n",
        "       train_regret_winner_7[slice48],\r\n",
        "       train_regret_winner_8[slice48],\r\n",
        "       train_regret_winner_9[slice48],\r\n",
        "       train_regret_winner_10[slice48],\r\n",
        "       train_regret_winner_11[slice48],\r\n",
        "       train_regret_winner_12[slice48],\r\n",
        "       train_regret_winner_13[slice48],\r\n",
        "       train_regret_winner_14[slice48],\r\n",
        "       train_regret_winner_15[slice48],\r\n",
        "       train_regret_winner_16[slice48],\r\n",
        "       train_regret_winner_17[slice48],\r\n",
        "       train_regret_winner_18[slice48],\r\n",
        "       train_regret_winner_19[slice48],\r\n",
        "       train_regret_winner_20[slice48]]\r\n",
        "\r\n",
        "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\r\n",
        "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\r\n",
        "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\r\n",
        "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\r\n",
        "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\r\n",
        "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNpV6knOtejv"
      },
      "source": [
        "# Iteration58 :\r\n",
        "\r\n",
        "slice58 = 57\r\n",
        "\r\n",
        "loser58 = [train_regret_loser_1[slice58],\r\n",
        "       train_regret_loser_2[slice58],\r\n",
        "       train_regret_loser_3[slice58],\r\n",
        "       train_regret_loser_4[slice58],\r\n",
        "       train_regret_loser_5[slice58],\r\n",
        "       train_regret_loser_6[slice58],\r\n",
        "       train_regret_loser_7[slice58],\r\n",
        "       train_regret_loser_8[slice58],\r\n",
        "       train_regret_loser_9[slice58],\r\n",
        "       train_regret_loser_10[slice58],\r\n",
        "       train_regret_loser_11[slice58],\r\n",
        "       train_regret_loser_12[slice58],\r\n",
        "       train_regret_loser_13[slice58],\r\n",
        "       train_regret_loser_14[slice58],\r\n",
        "       train_regret_loser_15[slice58],\r\n",
        "       train_regret_loser_16[slice58],\r\n",
        "       train_regret_loser_17[slice58],\r\n",
        "       train_regret_loser_18[slice58],\r\n",
        "       train_regret_loser_19[slice58],\r\n",
        "       train_regret_loser_20[slice58]]\r\n",
        "\r\n",
        "winner58 = [train_regret_winner_1[slice58],\r\n",
        "       train_regret_winner_2[slice58],\r\n",
        "       train_regret_winner_3[slice58],\r\n",
        "       train_regret_winner_4[slice58],\r\n",
        "       train_regret_winner_5[slice58],\r\n",
        "       train_regret_winner_6[slice58],\r\n",
        "       train_regret_winner_7[slice58],\r\n",
        "       train_regret_winner_8[slice58],\r\n",
        "       train_regret_winner_9[slice58],\r\n",
        "       train_regret_winner_10[slice58],\r\n",
        "       train_regret_winner_11[slice58],\r\n",
        "       train_regret_winner_12[slice58],\r\n",
        "       train_regret_winner_13[slice58],\r\n",
        "       train_regret_winner_14[slice58],\r\n",
        "       train_regret_winner_15[slice58],\r\n",
        "       train_regret_winner_16[slice58],\r\n",
        "       train_regret_winner_17[slice58],\r\n",
        "       train_regret_winner_18[slice58],\r\n",
        "       train_regret_winner_19[slice58],\r\n",
        "       train_regret_winner_20[slice58]]\r\n",
        "\r\n",
        "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\r\n",
        "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\r\n",
        "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\r\n",
        "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\r\n",
        "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\r\n",
        "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VFsqaBqtemn"
      },
      "source": [
        "# Iteration68 :\r\n",
        "\r\n",
        "slice68 = 67\r\n",
        "\r\n",
        "loser68 = [train_regret_loser_1[slice68],\r\n",
        "       train_regret_loser_2[slice68],\r\n",
        "       train_regret_loser_3[slice68],\r\n",
        "       train_regret_loser_4[slice68],\r\n",
        "       train_regret_loser_5[slice68],\r\n",
        "       train_regret_loser_6[slice68],\r\n",
        "       train_regret_loser_7[slice68],\r\n",
        "       train_regret_loser_8[slice68],\r\n",
        "       train_regret_loser_9[slice68],\r\n",
        "       train_regret_loser_10[slice68],\r\n",
        "       train_regret_loser_11[slice68],\r\n",
        "       train_regret_loser_12[slice68],\r\n",
        "       train_regret_loser_13[slice68],\r\n",
        "       train_regret_loser_14[slice68],\r\n",
        "       train_regret_loser_15[slice68],\r\n",
        "       train_regret_loser_16[slice68],\r\n",
        "       train_regret_loser_17[slice68],\r\n",
        "       train_regret_loser_18[slice68],\r\n",
        "       train_regret_loser_19[slice68],\r\n",
        "       train_regret_loser_20[slice68]]\r\n",
        "\r\n",
        "winner68 = [train_regret_winner_1[slice68],\r\n",
        "       train_regret_winner_2[slice68],\r\n",
        "       train_regret_winner_3[slice68],\r\n",
        "       train_regret_winner_4[slice68],\r\n",
        "       train_regret_winner_5[slice68],\r\n",
        "       train_regret_winner_6[slice68],\r\n",
        "       train_regret_winner_7[slice68],\r\n",
        "       train_regret_winner_8[slice68],\r\n",
        "       train_regret_winner_9[slice68],\r\n",
        "       train_regret_winner_10[slice68],\r\n",
        "       train_regret_winner_11[slice68],\r\n",
        "       train_regret_winner_12[slice68],\r\n",
        "       train_regret_winner_13[slice68],\r\n",
        "       train_regret_winner_14[slice68],\r\n",
        "       train_regret_winner_15[slice68],\r\n",
        "       train_regret_winner_16[slice68],\r\n",
        "       train_regret_winner_17[slice68],\r\n",
        "       train_regret_winner_18[slice68],\r\n",
        "       train_regret_winner_19[slice68],\r\n",
        "       train_regret_winner_20[slice68]]\r\n",
        "\r\n",
        "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\r\n",
        "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\r\n",
        "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\r\n",
        "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\r\n",
        "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\r\n",
        "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XlFH0Wxtepo"
      },
      "source": [
        "# Iteration78 :\r\n",
        "\r\n",
        "slice78 = 77\r\n",
        "\r\n",
        "loser78 = [train_regret_loser_1[slice78],\r\n",
        "       train_regret_loser_2[slice78],\r\n",
        "       train_regret_loser_3[slice78],\r\n",
        "       train_regret_loser_4[slice78],\r\n",
        "       train_regret_loser_5[slice78],\r\n",
        "       train_regret_loser_6[slice78],\r\n",
        "       train_regret_loser_7[slice78],\r\n",
        "       train_regret_loser_8[slice78],\r\n",
        "       train_regret_loser_9[slice78],\r\n",
        "       train_regret_loser_10[slice78],\r\n",
        "       train_regret_loser_11[slice78],\r\n",
        "       train_regret_loser_12[slice78],\r\n",
        "       train_regret_loser_13[slice78],\r\n",
        "       train_regret_loser_14[slice78],\r\n",
        "       train_regret_loser_15[slice78],\r\n",
        "       train_regret_loser_16[slice78],\r\n",
        "       train_regret_loser_17[slice78],\r\n",
        "       train_regret_loser_18[slice78],\r\n",
        "       train_regret_loser_19[slice78],\r\n",
        "       train_regret_loser_20[slice78]]\r\n",
        "\r\n",
        "winner78 = [train_regret_winner_1[slice78],\r\n",
        "       train_regret_winner_2[slice78],\r\n",
        "       train_regret_winner_3[slice78],\r\n",
        "       train_regret_winner_4[slice78],\r\n",
        "       train_regret_winner_5[slice78],\r\n",
        "       train_regret_winner_6[slice78],\r\n",
        "       train_regret_winner_7[slice78],\r\n",
        "       train_regret_winner_8[slice78],\r\n",
        "       train_regret_winner_9[slice78],\r\n",
        "       train_regret_winner_10[slice78],\r\n",
        "       train_regret_winner_11[slice78],\r\n",
        "       train_regret_winner_12[slice78],\r\n",
        "       train_regret_winner_13[slice78],\r\n",
        "       train_regret_winner_14[slice78],\r\n",
        "       train_regret_winner_15[slice78],\r\n",
        "       train_regret_winner_16[slice78],\r\n",
        "       train_regret_winner_17[slice78],\r\n",
        "       train_regret_winner_18[slice78],\r\n",
        "       train_regret_winner_19[slice78],\r\n",
        "       train_regret_winner_20[slice78]]\r\n",
        "\r\n",
        "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\r\n",
        "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\r\n",
        "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\r\n",
        "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\r\n",
        "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\r\n",
        "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsL0PJVetesC"
      },
      "source": [
        "# Iteration88 :\r\n",
        "\r\n",
        "slice88 = 87\r\n",
        "\r\n",
        "loser88 = [train_regret_loser_1[slice88],\r\n",
        "       train_regret_loser_2[slice88],\r\n",
        "       train_regret_loser_3[slice88],\r\n",
        "       train_regret_loser_4[slice88],\r\n",
        "       train_regret_loser_5[slice88],\r\n",
        "       train_regret_loser_6[slice88],\r\n",
        "       train_regret_loser_7[slice88],\r\n",
        "       train_regret_loser_8[slice88],\r\n",
        "       train_regret_loser_9[slice88],\r\n",
        "       train_regret_loser_10[slice88],\r\n",
        "       train_regret_loser_11[slice88],\r\n",
        "       train_regret_loser_12[slice88],\r\n",
        "       train_regret_loser_13[slice88],\r\n",
        "       train_regret_loser_14[slice88],\r\n",
        "       train_regret_loser_15[slice88],\r\n",
        "       train_regret_loser_16[slice88],\r\n",
        "       train_regret_loser_17[slice88],\r\n",
        "       train_regret_loser_18[slice88],\r\n",
        "       train_regret_loser_19[slice88],\r\n",
        "       train_regret_loser_20[slice88]]\r\n",
        "\r\n",
        "winner88 = [train_regret_winner_1[slice88],\r\n",
        "       train_regret_winner_2[slice88],\r\n",
        "       train_regret_winner_3[slice88],\r\n",
        "       train_regret_winner_4[slice88],\r\n",
        "       train_regret_winner_5[slice88],\r\n",
        "       train_regret_winner_6[slice88],\r\n",
        "       train_regret_winner_7[slice88],\r\n",
        "       train_regret_winner_8[slice88],\r\n",
        "       train_regret_winner_9[slice88],\r\n",
        "       train_regret_winner_10[slice88],\r\n",
        "       train_regret_winner_11[slice88],\r\n",
        "       train_regret_winner_12[slice88],\r\n",
        "       train_regret_winner_13[slice88],\r\n",
        "       train_regret_winner_14[slice88],\r\n",
        "       train_regret_winner_15[slice88],\r\n",
        "       train_regret_winner_16[slice88],\r\n",
        "       train_regret_winner_17[slice88],\r\n",
        "       train_regret_winner_18[slice88],\r\n",
        "       train_regret_winner_19[slice88],\r\n",
        "       train_regret_winner_20[slice88]]\r\n",
        "\r\n",
        "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\r\n",
        "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\r\n",
        "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\r\n",
        "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\r\n",
        "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\r\n",
        "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYWEWGku1o_9"
      },
      "source": [
        "# Iteration98 :\r\n",
        "\r\n",
        "slice98 = 97\r\n",
        "\r\n",
        "loser98 = [train_regret_loser_1[slice98],\r\n",
        "       train_regret_loser_2[slice98],\r\n",
        "       train_regret_loser_3[slice98],\r\n",
        "       train_regret_loser_4[slice98],\r\n",
        "       train_regret_loser_5[slice98],\r\n",
        "       train_regret_loser_6[slice98],\r\n",
        "       train_regret_loser_7[slice98],\r\n",
        "       train_regret_loser_8[slice98],\r\n",
        "       train_regret_loser_9[slice98],\r\n",
        "       train_regret_loser_10[slice98],\r\n",
        "       train_regret_loser_11[slice98],\r\n",
        "       train_regret_loser_12[slice98],\r\n",
        "       train_regret_loser_13[slice98],\r\n",
        "       train_regret_loser_14[slice98],\r\n",
        "       train_regret_loser_15[slice98],\r\n",
        "       train_regret_loser_16[slice98],\r\n",
        "       train_regret_loser_17[slice98],\r\n",
        "       train_regret_loser_18[slice98],\r\n",
        "       train_regret_loser_19[slice98],\r\n",
        "       train_regret_loser_20[slice98]]\r\n",
        "\r\n",
        "winner98 = [train_regret_winner_1[slice98],\r\n",
        "       train_regret_winner_2[slice98],\r\n",
        "       train_regret_winner_3[slice98],\r\n",
        "       train_regret_winner_4[slice98],\r\n",
        "       train_regret_winner_5[slice98],\r\n",
        "       train_regret_winner_6[slice98],\r\n",
        "       train_regret_winner_7[slice98],\r\n",
        "       train_regret_winner_8[slice98],\r\n",
        "       train_regret_winner_9[slice98],\r\n",
        "       train_regret_winner_10[slice98],\r\n",
        "       train_regret_winner_11[slice98],\r\n",
        "       train_regret_winner_12[slice98],\r\n",
        "       train_regret_winner_13[slice98],\r\n",
        "       train_regret_winner_14[slice98],\r\n",
        "       train_regret_winner_15[slice98],\r\n",
        "       train_regret_winner_16[slice98],\r\n",
        "       train_regret_winner_17[slice98],\r\n",
        "       train_regret_winner_18[slice98],\r\n",
        "       train_regret_winner_19[slice98],\r\n",
        "       train_regret_winner_20[slice98]]\r\n",
        "\r\n",
        "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\r\n",
        "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\r\n",
        "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\r\n",
        "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\r\n",
        "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\r\n",
        "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guLGgcX91pDK"
      },
      "source": [
        "# Iteration9 :\r\n",
        "\r\n",
        "slice9 = 8\r\n",
        "\r\n",
        "loser9 = [train_regret_loser_1[slice9],\r\n",
        "       train_regret_loser_2[slice9],\r\n",
        "       train_regret_loser_3[slice9],\r\n",
        "       train_regret_loser_4[slice9],\r\n",
        "       train_regret_loser_5[slice9],\r\n",
        "       train_regret_loser_6[slice9],\r\n",
        "       train_regret_loser_7[slice9],\r\n",
        "       train_regret_loser_8[slice9],\r\n",
        "       train_regret_loser_9[slice9],\r\n",
        "       train_regret_loser_10[slice9],\r\n",
        "       train_regret_loser_11[slice9],\r\n",
        "       train_regret_loser_12[slice9],\r\n",
        "       train_regret_loser_13[slice9],\r\n",
        "       train_regret_loser_14[slice9],\r\n",
        "       train_regret_loser_15[slice9],\r\n",
        "       train_regret_loser_16[slice9],\r\n",
        "       train_regret_loser_17[slice9],\r\n",
        "       train_regret_loser_18[slice9],\r\n",
        "       train_regret_loser_19[slice9],\r\n",
        "       train_regret_loser_20[slice9]]\r\n",
        "\r\n",
        "winner9 = [train_regret_winner_1[slice9],\r\n",
        "       train_regret_winner_2[slice9],\r\n",
        "       train_regret_winner_3[slice9],\r\n",
        "       train_regret_winner_4[slice9],\r\n",
        "       train_regret_winner_5[slice9],\r\n",
        "       train_regret_winner_6[slice9],\r\n",
        "       train_regret_winner_7[slice9],\r\n",
        "       train_regret_winner_8[slice9],\r\n",
        "       train_regret_winner_9[slice9],\r\n",
        "       train_regret_winner_10[slice9],\r\n",
        "       train_regret_winner_11[slice9],\r\n",
        "       train_regret_winner_12[slice9],\r\n",
        "       train_regret_winner_13[slice9],\r\n",
        "       train_regret_winner_14[slice9],\r\n",
        "       train_regret_winner_15[slice9],\r\n",
        "       train_regret_winner_16[slice9],\r\n",
        "       train_regret_winner_17[slice9],\r\n",
        "       train_regret_winner_18[slice9],\r\n",
        "       train_regret_winner_19[slice9],\r\n",
        "       train_regret_winner_20[slice9]]\r\n",
        "\r\n",
        "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\r\n",
        "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\r\n",
        "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\r\n",
        "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\r\n",
        "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\r\n",
        "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3_9-eoT1pGd"
      },
      "source": [
        "# Iteration19 :\r\n",
        "\r\n",
        "slice19 = 18\r\n",
        "\r\n",
        "loser19 = [train_regret_loser_1[slice19],\r\n",
        "       train_regret_loser_2[slice19],\r\n",
        "       train_regret_loser_3[slice19],\r\n",
        "       train_regret_loser_4[slice19],\r\n",
        "       train_regret_loser_5[slice19],\r\n",
        "       train_regret_loser_6[slice19],\r\n",
        "       train_regret_loser_7[slice19],\r\n",
        "       train_regret_loser_8[slice19],\r\n",
        "       train_regret_loser_9[slice19],\r\n",
        "       train_regret_loser_10[slice19],\r\n",
        "       train_regret_loser_11[slice19],\r\n",
        "       train_regret_loser_12[slice19],\r\n",
        "       train_regret_loser_13[slice19],\r\n",
        "       train_regret_loser_14[slice19],\r\n",
        "       train_regret_loser_15[slice19],\r\n",
        "       train_regret_loser_16[slice19],\r\n",
        "       train_regret_loser_17[slice19],\r\n",
        "       train_regret_loser_18[slice19],\r\n",
        "       train_regret_loser_19[slice19],\r\n",
        "       train_regret_loser_20[slice19]]\r\n",
        "\r\n",
        "winner19 = [train_regret_winner_1[slice19],\r\n",
        "       train_regret_winner_2[slice19],\r\n",
        "       train_regret_winner_3[slice19],\r\n",
        "       train_regret_winner_4[slice19],\r\n",
        "       train_regret_winner_5[slice19],\r\n",
        "       train_regret_winner_6[slice19],\r\n",
        "       train_regret_winner_7[slice19],\r\n",
        "       train_regret_winner_8[slice19],\r\n",
        "       train_regret_winner_9[slice19],\r\n",
        "       train_regret_winner_10[slice19],\r\n",
        "       train_regret_winner_11[slice19],\r\n",
        "       train_regret_winner_12[slice19],\r\n",
        "       train_regret_winner_13[slice19],\r\n",
        "       train_regret_winner_14[slice19],\r\n",
        "       train_regret_winner_15[slice19],\r\n",
        "       train_regret_winner_16[slice19],\r\n",
        "       train_regret_winner_17[slice19],\r\n",
        "       train_regret_winner_18[slice19],\r\n",
        "       train_regret_winner_19[slice19],\r\n",
        "       train_regret_winner_20[slice19]]\r\n",
        "\r\n",
        "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\r\n",
        "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\r\n",
        "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\r\n",
        "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\r\n",
        "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\r\n",
        "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ae5x13M1pRp"
      },
      "source": [
        "# Iteration29 :\r\n",
        "\r\n",
        "slice29 = 28\r\n",
        "\r\n",
        "loser29 = [train_regret_loser_1[slice29],\r\n",
        "       train_regret_loser_2[slice29],\r\n",
        "       train_regret_loser_3[slice29],\r\n",
        "       train_regret_loser_4[slice29],\r\n",
        "       train_regret_loser_5[slice29],\r\n",
        "       train_regret_loser_6[slice29],\r\n",
        "       train_regret_loser_7[slice29],\r\n",
        "       train_regret_loser_8[slice29],\r\n",
        "       train_regret_loser_9[slice29],\r\n",
        "       train_regret_loser_10[slice29],\r\n",
        "       train_regret_loser_11[slice29],\r\n",
        "       train_regret_loser_12[slice29],\r\n",
        "       train_regret_loser_13[slice29],\r\n",
        "       train_regret_loser_14[slice29],\r\n",
        "       train_regret_loser_15[slice29],\r\n",
        "       train_regret_loser_16[slice29],\r\n",
        "       train_regret_loser_17[slice29],\r\n",
        "       train_regret_loser_18[slice29],\r\n",
        "       train_regret_loser_19[slice29],\r\n",
        "       train_regret_loser_20[slice29]]\r\n",
        "\r\n",
        "winner29 = [train_regret_winner_1[slice29],\r\n",
        "       train_regret_winner_2[slice29],\r\n",
        "       train_regret_winner_3[slice29],\r\n",
        "       train_regret_winner_4[slice29],\r\n",
        "       train_regret_winner_5[slice29],\r\n",
        "       train_regret_winner_6[slice29],\r\n",
        "       train_regret_winner_7[slice29],\r\n",
        "       train_regret_winner_8[slice29],\r\n",
        "       train_regret_winner_9[slice29],\r\n",
        "       train_regret_winner_10[slice29],\r\n",
        "       train_regret_winner_11[slice29],\r\n",
        "       train_regret_winner_12[slice29],\r\n",
        "       train_regret_winner_13[slice29],\r\n",
        "       train_regret_winner_14[slice29],\r\n",
        "       train_regret_winner_15[slice29],\r\n",
        "       train_regret_winner_16[slice29],\r\n",
        "       train_regret_winner_17[slice29],\r\n",
        "       train_regret_winner_18[slice29],\r\n",
        "       train_regret_winner_19[slice29],\r\n",
        "       train_regret_winner_20[slice29]]\r\n",
        "\r\n",
        "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\r\n",
        "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\r\n",
        "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\r\n",
        "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\r\n",
        "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\r\n",
        "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR9aPvBy1pUc"
      },
      "source": [
        "# Iteration39 :\r\n",
        "\r\n",
        "slice39 = 38\r\n",
        "\r\n",
        "loser39 = [train_regret_loser_1[slice39],\r\n",
        "       train_regret_loser_2[slice39],\r\n",
        "       train_regret_loser_3[slice39],\r\n",
        "       train_regret_loser_4[slice39],\r\n",
        "       train_regret_loser_5[slice39],\r\n",
        "       train_regret_loser_6[slice39],\r\n",
        "       train_regret_loser_7[slice39],\r\n",
        "       train_regret_loser_8[slice39],\r\n",
        "       train_regret_loser_9[slice39],\r\n",
        "       train_regret_loser_10[slice39],\r\n",
        "       train_regret_loser_11[slice39],\r\n",
        "       train_regret_loser_12[slice39],\r\n",
        "       train_regret_loser_13[slice39],\r\n",
        "       train_regret_loser_14[slice39],\r\n",
        "       train_regret_loser_15[slice39],\r\n",
        "       train_regret_loser_16[slice39],\r\n",
        "       train_regret_loser_17[slice39],\r\n",
        "       train_regret_loser_18[slice39],\r\n",
        "       train_regret_loser_19[slice39],\r\n",
        "       train_regret_loser_20[slice39]]\r\n",
        "\r\n",
        "winner39 = [train_regret_winner_1[slice39],\r\n",
        "       train_regret_winner_2[slice39],\r\n",
        "       train_regret_winner_3[slice39],\r\n",
        "       train_regret_winner_4[slice39],\r\n",
        "       train_regret_winner_5[slice39],\r\n",
        "       train_regret_winner_6[slice39],\r\n",
        "       train_regret_winner_7[slice39],\r\n",
        "       train_regret_winner_8[slice39],\r\n",
        "       train_regret_winner_9[slice39],\r\n",
        "       train_regret_winner_10[slice39],\r\n",
        "       train_regret_winner_11[slice39],\r\n",
        "       train_regret_winner_12[slice39],\r\n",
        "       train_regret_winner_13[slice39],\r\n",
        "       train_regret_winner_14[slice39],\r\n",
        "       train_regret_winner_15[slice39],\r\n",
        "       train_regret_winner_16[slice39],\r\n",
        "       train_regret_winner_17[slice39],\r\n",
        "       train_regret_winner_18[slice39],\r\n",
        "       train_regret_winner_19[slice39],\r\n",
        "       train_regret_winner_20[slice39]]\r\n",
        "\r\n",
        "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\r\n",
        "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\r\n",
        "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\r\n",
        "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\r\n",
        "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\r\n",
        "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vb7vh2K1pXp"
      },
      "source": [
        "# Iteration49 :\r\n",
        "\r\n",
        "slice49 = 48\r\n",
        "\r\n",
        "loser49 = [train_regret_loser_1[slice49],\r\n",
        "       train_regret_loser_2[slice49],\r\n",
        "       train_regret_loser_3[slice49],\r\n",
        "       train_regret_loser_4[slice49],\r\n",
        "       train_regret_loser_5[slice49],\r\n",
        "       train_regret_loser_6[slice49],\r\n",
        "       train_regret_loser_7[slice49],\r\n",
        "       train_regret_loser_8[slice49],\r\n",
        "       train_regret_loser_9[slice49],\r\n",
        "       train_regret_loser_10[slice49],\r\n",
        "       train_regret_loser_11[slice49],\r\n",
        "       train_regret_loser_12[slice49],\r\n",
        "       train_regret_loser_13[slice49],\r\n",
        "       train_regret_loser_14[slice49],\r\n",
        "       train_regret_loser_15[slice49],\r\n",
        "       train_regret_loser_16[slice49],\r\n",
        "       train_regret_loser_17[slice49],\r\n",
        "       train_regret_loser_18[slice49],\r\n",
        "       train_regret_loser_19[slice49],\r\n",
        "       train_regret_loser_20[slice49]]\r\n",
        "\r\n",
        "winner49 = [train_regret_winner_1[slice49],\r\n",
        "       train_regret_winner_2[slice49],\r\n",
        "       train_regret_winner_3[slice49],\r\n",
        "       train_regret_winner_4[slice49],\r\n",
        "       train_regret_winner_5[slice49],\r\n",
        "       train_regret_winner_6[slice49],\r\n",
        "       train_regret_winner_7[slice49],\r\n",
        "       train_regret_winner_8[slice49],\r\n",
        "       train_regret_winner_9[slice49],\r\n",
        "       train_regret_winner_10[slice49],\r\n",
        "       train_regret_winner_11[slice49],\r\n",
        "       train_regret_winner_12[slice49],\r\n",
        "       train_regret_winner_13[slice49],\r\n",
        "       train_regret_winner_14[slice49],\r\n",
        "       train_regret_winner_15[slice49],\r\n",
        "       train_regret_winner_16[slice49],\r\n",
        "       train_regret_winner_17[slice49],\r\n",
        "       train_regret_winner_18[slice49],\r\n",
        "       train_regret_winner_19[slice49],\r\n",
        "       train_regret_winner_20[slice49]]\r\n",
        "\r\n",
        "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\r\n",
        "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\r\n",
        "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\r\n",
        "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\r\n",
        "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\r\n",
        "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wN7DWwt1paM"
      },
      "source": [
        "# Iteration59 :\r\n",
        "\r\n",
        "slice59 = 58\r\n",
        "\r\n",
        "loser59 = [train_regret_loser_1[slice59],\r\n",
        "       train_regret_loser_2[slice59],\r\n",
        "       train_regret_loser_3[slice59],\r\n",
        "       train_regret_loser_4[slice59],\r\n",
        "       train_regret_loser_5[slice59],\r\n",
        "       train_regret_loser_6[slice59],\r\n",
        "       train_regret_loser_7[slice59],\r\n",
        "       train_regret_loser_8[slice59],\r\n",
        "       train_regret_loser_9[slice59],\r\n",
        "       train_regret_loser_10[slice59],\r\n",
        "       train_regret_loser_11[slice59],\r\n",
        "       train_regret_loser_12[slice59],\r\n",
        "       train_regret_loser_13[slice59],\r\n",
        "       train_regret_loser_14[slice59],\r\n",
        "       train_regret_loser_15[slice59],\r\n",
        "       train_regret_loser_16[slice59],\r\n",
        "       train_regret_loser_17[slice59],\r\n",
        "       train_regret_loser_18[slice59],\r\n",
        "       train_regret_loser_19[slice59],\r\n",
        "       train_regret_loser_20[slice59]]\r\n",
        "\r\n",
        "winner59 = [train_regret_winner_1[slice59],\r\n",
        "       train_regret_winner_2[slice59],\r\n",
        "       train_regret_winner_3[slice59],\r\n",
        "       train_regret_winner_4[slice59],\r\n",
        "       train_regret_winner_5[slice59],\r\n",
        "       train_regret_winner_6[slice59],\r\n",
        "       train_regret_winner_7[slice59],\r\n",
        "       train_regret_winner_8[slice59],\r\n",
        "       train_regret_winner_9[slice59],\r\n",
        "       train_regret_winner_10[slice59],\r\n",
        "       train_regret_winner_11[slice59],\r\n",
        "       train_regret_winner_12[slice59],\r\n",
        "       train_regret_winner_13[slice59],\r\n",
        "       train_regret_winner_14[slice59],\r\n",
        "       train_regret_winner_15[slice59],\r\n",
        "       train_regret_winner_16[slice59],\r\n",
        "       train_regret_winner_17[slice59],\r\n",
        "       train_regret_winner_18[slice59],\r\n",
        "       train_regret_winner_19[slice59],\r\n",
        "       train_regret_winner_20[slice59]]\r\n",
        "\r\n",
        "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\r\n",
        "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\r\n",
        "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\r\n",
        "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\r\n",
        "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\r\n",
        "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqtgekml1pde"
      },
      "source": [
        "# Iteration69 :\r\n",
        "\r\n",
        "slice69 = 68\r\n",
        "\r\n",
        "loser69 = [train_regret_loser_1[slice69],\r\n",
        "       train_regret_loser_2[slice69],\r\n",
        "       train_regret_loser_3[slice69],\r\n",
        "       train_regret_loser_4[slice69],\r\n",
        "       train_regret_loser_5[slice69],\r\n",
        "       train_regret_loser_6[slice69],\r\n",
        "       train_regret_loser_7[slice69],\r\n",
        "       train_regret_loser_8[slice69],\r\n",
        "       train_regret_loser_9[slice69],\r\n",
        "       train_regret_loser_10[slice69],\r\n",
        "       train_regret_loser_11[slice69],\r\n",
        "       train_regret_loser_12[slice69],\r\n",
        "       train_regret_loser_13[slice69],\r\n",
        "       train_regret_loser_14[slice69],\r\n",
        "       train_regret_loser_15[slice69],\r\n",
        "       train_regret_loser_16[slice69],\r\n",
        "       train_regret_loser_17[slice69],\r\n",
        "       train_regret_loser_18[slice69],\r\n",
        "       train_regret_loser_19[slice69],\r\n",
        "       train_regret_loser_20[slice69]]\r\n",
        "\r\n",
        "winner69 = [train_regret_winner_1[slice69],\r\n",
        "       train_regret_winner_2[slice69],\r\n",
        "       train_regret_winner_3[slice69],\r\n",
        "       train_regret_winner_4[slice69],\r\n",
        "       train_regret_winner_5[slice69],\r\n",
        "       train_regret_winner_6[slice69],\r\n",
        "       train_regret_winner_7[slice69],\r\n",
        "       train_regret_winner_8[slice69],\r\n",
        "       train_regret_winner_9[slice69],\r\n",
        "       train_regret_winner_10[slice69],\r\n",
        "       train_regret_winner_11[slice69],\r\n",
        "       train_regret_winner_12[slice69],\r\n",
        "       train_regret_winner_13[slice69],\r\n",
        "       train_regret_winner_14[slice69],\r\n",
        "       train_regret_winner_15[slice69],\r\n",
        "       train_regret_winner_16[slice69],\r\n",
        "       train_regret_winner_17[slice69],\r\n",
        "       train_regret_winner_18[slice69],\r\n",
        "       train_regret_winner_19[slice69],\r\n",
        "       train_regret_winner_20[slice69]]\r\n",
        "\r\n",
        "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\r\n",
        "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\r\n",
        "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\r\n",
        "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\r\n",
        "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\r\n",
        "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPFTa1171pgF"
      },
      "source": [
        "# Iteration79 :\r\n",
        "\r\n",
        "slice79 = 78\r\n",
        "\r\n",
        "loser79 = [train_regret_loser_1[slice79],\r\n",
        "       train_regret_loser_2[slice79],\r\n",
        "       train_regret_loser_3[slice79],\r\n",
        "       train_regret_loser_4[slice79],\r\n",
        "       train_regret_loser_5[slice79],\r\n",
        "       train_regret_loser_6[slice79],\r\n",
        "       train_regret_loser_7[slice79],\r\n",
        "       train_regret_loser_8[slice79],\r\n",
        "       train_regret_loser_9[slice79],\r\n",
        "       train_regret_loser_10[slice79],\r\n",
        "       train_regret_loser_11[slice79],\r\n",
        "       train_regret_loser_12[slice79],\r\n",
        "       train_regret_loser_13[slice79],\r\n",
        "       train_regret_loser_14[slice79],\r\n",
        "       train_regret_loser_15[slice79],\r\n",
        "       train_regret_loser_16[slice79],\r\n",
        "       train_regret_loser_17[slice79],\r\n",
        "       train_regret_loser_18[slice79],\r\n",
        "       train_regret_loser_19[slice79],\r\n",
        "       train_regret_loser_20[slice79]]\r\n",
        "\r\n",
        "winner79 = [train_regret_winner_1[slice79],\r\n",
        "       train_regret_winner_2[slice79],\r\n",
        "       train_regret_winner_3[slice79],\r\n",
        "       train_regret_winner_4[slice79],\r\n",
        "       train_regret_winner_5[slice79],\r\n",
        "       train_regret_winner_6[slice79],\r\n",
        "       train_regret_winner_7[slice79],\r\n",
        "       train_regret_winner_8[slice79],\r\n",
        "       train_regret_winner_9[slice79],\r\n",
        "       train_regret_winner_10[slice79],\r\n",
        "       train_regret_winner_11[slice79],\r\n",
        "       train_regret_winner_12[slice79],\r\n",
        "       train_regret_winner_13[slice79],\r\n",
        "       train_regret_winner_14[slice79],\r\n",
        "       train_regret_winner_15[slice79],\r\n",
        "       train_regret_winner_16[slice79],\r\n",
        "       train_regret_winner_17[slice79],\r\n",
        "       train_regret_winner_18[slice79],\r\n",
        "       train_regret_winner_19[slice79],\r\n",
        "       train_regret_winner_20[slice79]]\r\n",
        "\r\n",
        "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\r\n",
        "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\r\n",
        "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\r\n",
        "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\r\n",
        "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\r\n",
        "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTnRF4lx1pio"
      },
      "source": [
        "# Iteration89 :\r\n",
        "\r\n",
        "slice89 = 88\r\n",
        "\r\n",
        "loser89 = [train_regret_loser_1[slice89],\r\n",
        "       train_regret_loser_2[slice89],\r\n",
        "       train_regret_loser_3[slice89],\r\n",
        "       train_regret_loser_4[slice89],\r\n",
        "       train_regret_loser_5[slice89],\r\n",
        "       train_regret_loser_6[slice89],\r\n",
        "       train_regret_loser_7[slice89],\r\n",
        "       train_regret_loser_8[slice89],\r\n",
        "       train_regret_loser_9[slice89],\r\n",
        "       train_regret_loser_10[slice89],\r\n",
        "       train_regret_loser_11[slice89],\r\n",
        "       train_regret_loser_12[slice89],\r\n",
        "       train_regret_loser_13[slice89],\r\n",
        "       train_regret_loser_14[slice89],\r\n",
        "       train_regret_loser_15[slice89],\r\n",
        "       train_regret_loser_16[slice89],\r\n",
        "       train_regret_loser_17[slice89],\r\n",
        "       train_regret_loser_18[slice89],\r\n",
        "       train_regret_loser_19[slice89],\r\n",
        "       train_regret_loser_20[slice89]]\r\n",
        "\r\n",
        "winner89 = [train_regret_winner_1[slice89],\r\n",
        "       train_regret_winner_2[slice89],\r\n",
        "       train_regret_winner_3[slice89],\r\n",
        "       train_regret_winner_4[slice89],\r\n",
        "       train_regret_winner_5[slice89],\r\n",
        "       train_regret_winner_6[slice89],\r\n",
        "       train_regret_winner_7[slice89],\r\n",
        "       train_regret_winner_8[slice89],\r\n",
        "       train_regret_winner_9[slice89],\r\n",
        "       train_regret_winner_10[slice89],\r\n",
        "       train_regret_winner_11[slice89],\r\n",
        "       train_regret_winner_12[slice89],\r\n",
        "       train_regret_winner_13[slice89],\r\n",
        "       train_regret_winner_14[slice89],\r\n",
        "       train_regret_winner_15[slice89],\r\n",
        "       train_regret_winner_16[slice89],\r\n",
        "       train_regret_winner_17[slice89],\r\n",
        "       train_regret_winner_18[slice89],\r\n",
        "       train_regret_winner_19[slice89],\r\n",
        "       train_regret_winner_20[slice89]]\r\n",
        "\r\n",
        "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\r\n",
        "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\r\n",
        "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\r\n",
        "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\r\n",
        "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\r\n",
        "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiLAF38i1plO"
      },
      "source": [
        "# Iteration99 :\r\n",
        "\r\n",
        "slice99 = 98\r\n",
        "\r\n",
        "loser99 = [train_regret_loser_1[slice99],\r\n",
        "       train_regret_loser_2[slice99],\r\n",
        "       train_regret_loser_3[slice99],\r\n",
        "       train_regret_loser_4[slice99],\r\n",
        "       train_regret_loser_5[slice99],\r\n",
        "       train_regret_loser_6[slice99],\r\n",
        "       train_regret_loser_7[slice99],\r\n",
        "       train_regret_loser_8[slice99],\r\n",
        "       train_regret_loser_9[slice99],\r\n",
        "       train_regret_loser_10[slice99],\r\n",
        "       train_regret_loser_11[slice99],\r\n",
        "       train_regret_loser_12[slice99],\r\n",
        "       train_regret_loser_13[slice99],\r\n",
        "       train_regret_loser_14[slice99],\r\n",
        "       train_regret_loser_15[slice99],\r\n",
        "       train_regret_loser_16[slice99],\r\n",
        "       train_regret_loser_17[slice99],\r\n",
        "       train_regret_loser_18[slice99],\r\n",
        "       train_regret_loser_19[slice99],\r\n",
        "       train_regret_loser_20[slice99]]\r\n",
        "\r\n",
        "winner99 = [train_regret_winner_1[slice99],\r\n",
        "       train_regret_winner_2[slice99],\r\n",
        "       train_regret_winner_3[slice99],\r\n",
        "       train_regret_winner_4[slice99],\r\n",
        "       train_regret_winner_5[slice99],\r\n",
        "       train_regret_winner_6[slice99],\r\n",
        "       train_regret_winner_7[slice99],\r\n",
        "       train_regret_winner_8[slice99],\r\n",
        "       train_regret_winner_9[slice99],\r\n",
        "       train_regret_winner_10[slice99],\r\n",
        "       train_regret_winner_11[slice99],\r\n",
        "       train_regret_winner_12[slice99],\r\n",
        "       train_regret_winner_13[slice99],\r\n",
        "       train_regret_winner_14[slice99],\r\n",
        "       train_regret_winner_15[slice99],\r\n",
        "       train_regret_winner_16[slice99],\r\n",
        "       train_regret_winner_17[slice99],\r\n",
        "       train_regret_winner_18[slice99],\r\n",
        "       train_regret_winner_19[slice99],\r\n",
        "       train_regret_winner_20[slice99]]\r\n",
        "\r\n",
        "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\r\n",
        "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\r\n",
        "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\r\n",
        "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\r\n",
        "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\r\n",
        "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBd_N0F21pn0"
      },
      "source": [
        "# Iteration10 :\r\n",
        "\r\n",
        "slice10 = 9\r\n",
        "\r\n",
        "loser10 = [train_regret_loser_1[slice10],\r\n",
        "       train_regret_loser_2[slice10],\r\n",
        "       train_regret_loser_3[slice10],\r\n",
        "       train_regret_loser_4[slice10],\r\n",
        "       train_regret_loser_5[slice10],\r\n",
        "       train_regret_loser_6[slice10],\r\n",
        "       train_regret_loser_7[slice10],\r\n",
        "       train_regret_loser_8[slice10],\r\n",
        "       train_regret_loser_9[slice10],\r\n",
        "       train_regret_loser_10[slice10],\r\n",
        "       train_regret_loser_11[slice10],\r\n",
        "       train_regret_loser_12[slice10],\r\n",
        "       train_regret_loser_13[slice10],\r\n",
        "       train_regret_loser_14[slice10],\r\n",
        "       train_regret_loser_15[slice10],\r\n",
        "       train_regret_loser_16[slice10],\r\n",
        "       train_regret_loser_17[slice10],\r\n",
        "       train_regret_loser_18[slice10],\r\n",
        "       train_regret_loser_19[slice10],\r\n",
        "       train_regret_loser_20[slice10]]\r\n",
        "\r\n",
        "winner10 = [train_regret_winner_1[slice10],\r\n",
        "       train_regret_winner_2[slice10],\r\n",
        "       train_regret_winner_3[slice10],\r\n",
        "       train_regret_winner_4[slice10],\r\n",
        "       train_regret_winner_5[slice10],\r\n",
        "       train_regret_winner_6[slice10],\r\n",
        "       train_regret_winner_7[slice10],\r\n",
        "       train_regret_winner_8[slice10],\r\n",
        "       train_regret_winner_9[slice10],\r\n",
        "       train_regret_winner_10[slice10],\r\n",
        "       train_regret_winner_11[slice10],\r\n",
        "       train_regret_winner_12[slice10],\r\n",
        "       train_regret_winner_13[slice10],\r\n",
        "       train_regret_winner_14[slice10],\r\n",
        "       train_regret_winner_15[slice10],\r\n",
        "       train_regret_winner_16[slice10],\r\n",
        "       train_regret_winner_17[slice10],\r\n",
        "       train_regret_winner_18[slice10],\r\n",
        "       train_regret_winner_19[slice10],\r\n",
        "       train_regret_winner_20[slice10]]\r\n",
        "\r\n",
        "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\r\n",
        "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\r\n",
        "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\r\n",
        "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\r\n",
        "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\r\n",
        "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXjdqc4s1pqb"
      },
      "source": [
        "# Iteration20 :\r\n",
        "\r\n",
        "slice20 = 19\r\n",
        "\r\n",
        "loser20 = [train_regret_loser_1[slice20],\r\n",
        "       train_regret_loser_2[slice20],\r\n",
        "       train_regret_loser_3[slice20],\r\n",
        "       train_regret_loser_4[slice20],\r\n",
        "       train_regret_loser_5[slice20],\r\n",
        "       train_regret_loser_6[slice20],\r\n",
        "       train_regret_loser_7[slice20],\r\n",
        "       train_regret_loser_8[slice20],\r\n",
        "       train_regret_loser_9[slice20],\r\n",
        "       train_regret_loser_10[slice20],\r\n",
        "       train_regret_loser_11[slice20],\r\n",
        "       train_regret_loser_12[slice20],\r\n",
        "       train_regret_loser_13[slice20],\r\n",
        "       train_regret_loser_14[slice20],\r\n",
        "       train_regret_loser_15[slice20],\r\n",
        "       train_regret_loser_16[slice20],\r\n",
        "       train_regret_loser_17[slice20],\r\n",
        "       train_regret_loser_18[slice20],\r\n",
        "       train_regret_loser_19[slice20],\r\n",
        "       train_regret_loser_20[slice20]]\r\n",
        "\r\n",
        "winner20 = [train_regret_winner_1[slice20],\r\n",
        "       train_regret_winner_2[slice20],\r\n",
        "       train_regret_winner_3[slice20],\r\n",
        "       train_regret_winner_4[slice20],\r\n",
        "       train_regret_winner_5[slice20],\r\n",
        "       train_regret_winner_6[slice20],\r\n",
        "       train_regret_winner_7[slice20],\r\n",
        "       train_regret_winner_8[slice20],\r\n",
        "       train_regret_winner_9[slice20],\r\n",
        "       train_regret_winner_10[slice20],\r\n",
        "       train_regret_winner_11[slice20],\r\n",
        "       train_regret_winner_12[slice20],\r\n",
        "       train_regret_winner_13[slice20],\r\n",
        "       train_regret_winner_14[slice20],\r\n",
        "       train_regret_winner_15[slice20],\r\n",
        "       train_regret_winner_16[slice20],\r\n",
        "       train_regret_winner_17[slice20],\r\n",
        "       train_regret_winner_18[slice20],\r\n",
        "       train_regret_winner_19[slice20],\r\n",
        "       train_regret_winner_20[slice20]]\r\n",
        "\r\n",
        "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\r\n",
        "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\r\n",
        "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\r\n",
        "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\r\n",
        "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\r\n",
        "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQBVS6dx1ptB"
      },
      "source": [
        "# Iteration30 :\r\n",
        "\r\n",
        "slice30 = 29\r\n",
        "\r\n",
        "loser30 = [train_regret_loser_1[slice30],\r\n",
        "       train_regret_loser_2[slice30],\r\n",
        "       train_regret_loser_3[slice30],\r\n",
        "       train_regret_loser_4[slice30],\r\n",
        "       train_regret_loser_5[slice30],\r\n",
        "       train_regret_loser_6[slice30],\r\n",
        "       train_regret_loser_7[slice30],\r\n",
        "       train_regret_loser_8[slice30],\r\n",
        "       train_regret_loser_9[slice30],\r\n",
        "       train_regret_loser_10[slice30],\r\n",
        "       train_regret_loser_11[slice30],\r\n",
        "       train_regret_loser_12[slice30],\r\n",
        "       train_regret_loser_13[slice30],\r\n",
        "       train_regret_loser_14[slice30],\r\n",
        "       train_regret_loser_15[slice30],\r\n",
        "       train_regret_loser_16[slice30],\r\n",
        "       train_regret_loser_17[slice30],\r\n",
        "       train_regret_loser_18[slice30],\r\n",
        "       train_regret_loser_19[slice30],\r\n",
        "       train_regret_loser_20[slice30]]\r\n",
        "\r\n",
        "winner30 = [train_regret_winner_1[slice30],\r\n",
        "       train_regret_winner_2[slice30],\r\n",
        "       train_regret_winner_3[slice30],\r\n",
        "       train_regret_winner_4[slice30],\r\n",
        "       train_regret_winner_5[slice30],\r\n",
        "       train_regret_winner_6[slice30],\r\n",
        "       train_regret_winner_7[slice30],\r\n",
        "       train_regret_winner_8[slice30],\r\n",
        "       train_regret_winner_9[slice30],\r\n",
        "       train_regret_winner_10[slice30],\r\n",
        "       train_regret_winner_11[slice30],\r\n",
        "       train_regret_winner_12[slice30],\r\n",
        "       train_regret_winner_13[slice30],\r\n",
        "       train_regret_winner_14[slice30],\r\n",
        "       train_regret_winner_15[slice30],\r\n",
        "       train_regret_winner_16[slice30],\r\n",
        "       train_regret_winner_17[slice30],\r\n",
        "       train_regret_winner_18[slice30],\r\n",
        "       train_regret_winner_19[slice30],\r\n",
        "       train_regret_winner_20[slice30]]\r\n",
        "\r\n",
        "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\r\n",
        "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\r\n",
        "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\r\n",
        "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\r\n",
        "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\r\n",
        "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-iWfLkj1pvn"
      },
      "source": [
        "# Iteration40 :\r\n",
        "\r\n",
        "slice40 = 39\r\n",
        "\r\n",
        "loser40 = [train_regret_loser_1[slice40],\r\n",
        "       train_regret_loser_2[slice40],\r\n",
        "       train_regret_loser_3[slice40],\r\n",
        "       train_regret_loser_4[slice40],\r\n",
        "       train_regret_loser_5[slice40],\r\n",
        "       train_regret_loser_6[slice40],\r\n",
        "       train_regret_loser_7[slice40],\r\n",
        "       train_regret_loser_8[slice40],\r\n",
        "       train_regret_loser_9[slice40],\r\n",
        "       train_regret_loser_10[slice40],\r\n",
        "       train_regret_loser_11[slice40],\r\n",
        "       train_regret_loser_12[slice40],\r\n",
        "       train_regret_loser_13[slice40],\r\n",
        "       train_regret_loser_14[slice40],\r\n",
        "       train_regret_loser_15[slice40],\r\n",
        "       train_regret_loser_16[slice40],\r\n",
        "       train_regret_loser_17[slice40],\r\n",
        "       train_regret_loser_18[slice40],\r\n",
        "       train_regret_loser_19[slice40],\r\n",
        "       train_regret_loser_20[slice40]]\r\n",
        "\r\n",
        "winner40 = [train_regret_winner_1[slice40],\r\n",
        "       train_regret_winner_2[slice40],\r\n",
        "       train_regret_winner_3[slice40],\r\n",
        "       train_regret_winner_4[slice40],\r\n",
        "       train_regret_winner_5[slice40],\r\n",
        "       train_regret_winner_6[slice40],\r\n",
        "       train_regret_winner_7[slice40],\r\n",
        "       train_regret_winner_8[slice40],\r\n",
        "       train_regret_winner_9[slice40],\r\n",
        "       train_regret_winner_10[slice40],\r\n",
        "       train_regret_winner_11[slice40],\r\n",
        "       train_regret_winner_12[slice40],\r\n",
        "       train_regret_winner_13[slice40],\r\n",
        "       train_regret_winner_14[slice40],\r\n",
        "       train_regret_winner_15[slice40],\r\n",
        "       train_regret_winner_16[slice40],\r\n",
        "       train_regret_winner_17[slice40],\r\n",
        "       train_regret_winner_18[slice40],\r\n",
        "       train_regret_winner_19[slice40],\r\n",
        "       train_regret_winner_20[slice40]]\r\n",
        "\r\n",
        "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\r\n",
        "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\r\n",
        "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\r\n",
        "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\r\n",
        "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\r\n",
        "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G5i-Cjw2U7K"
      },
      "source": [
        "# Iteration50 :\r\n",
        "\r\n",
        "slice50 = 49\r\n",
        "\r\n",
        "loser50 = [train_regret_loser_1[slice50],\r\n",
        "       train_regret_loser_2[slice50],\r\n",
        "       train_regret_loser_3[slice50],\r\n",
        "       train_regret_loser_4[slice50],\r\n",
        "       train_regret_loser_5[slice50],\r\n",
        "       train_regret_loser_6[slice50],\r\n",
        "       train_regret_loser_7[slice50],\r\n",
        "       train_regret_loser_8[slice50],\r\n",
        "       train_regret_loser_9[slice50],\r\n",
        "       train_regret_loser_10[slice50],\r\n",
        "       train_regret_loser_11[slice50],\r\n",
        "       train_regret_loser_12[slice50],\r\n",
        "       train_regret_loser_13[slice50],\r\n",
        "       train_regret_loser_14[slice50],\r\n",
        "       train_regret_loser_15[slice50],\r\n",
        "       train_regret_loser_16[slice50],\r\n",
        "       train_regret_loser_17[slice50],\r\n",
        "       train_regret_loser_18[slice50],\r\n",
        "       train_regret_loser_19[slice50],\r\n",
        "       train_regret_loser_20[slice50]]\r\n",
        "\r\n",
        "winner50 = [train_regret_winner_1[slice50],\r\n",
        "       train_regret_winner_2[slice50],\r\n",
        "       train_regret_winner_3[slice50],\r\n",
        "       train_regret_winner_4[slice50],\r\n",
        "       train_regret_winner_5[slice50],\r\n",
        "       train_regret_winner_6[slice50],\r\n",
        "       train_regret_winner_7[slice50],\r\n",
        "       train_regret_winner_8[slice50],\r\n",
        "       train_regret_winner_9[slice50],\r\n",
        "       train_regret_winner_10[slice50],\r\n",
        "       train_regret_winner_11[slice50],\r\n",
        "       train_regret_winner_12[slice50],\r\n",
        "       train_regret_winner_13[slice50],\r\n",
        "       train_regret_winner_14[slice50],\r\n",
        "       train_regret_winner_15[slice50],\r\n",
        "       train_regret_winner_16[slice50],\r\n",
        "       train_regret_winner_17[slice50],\r\n",
        "       train_regret_winner_18[slice50],\r\n",
        "       train_regret_winner_19[slice50],\r\n",
        "       train_regret_winner_20[slice50]]\r\n",
        "\r\n",
        "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\r\n",
        "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\r\n",
        "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\r\n",
        "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\r\n",
        "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\r\n",
        "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY6qxZQA2U-r"
      },
      "source": [
        "# Iteration60 :\r\n",
        "\r\n",
        "slice60 = 59\r\n",
        "\r\n",
        "loser60 = [train_regret_loser_1[slice60],\r\n",
        "       train_regret_loser_2[slice60],\r\n",
        "       train_regret_loser_3[slice60],\r\n",
        "       train_regret_loser_4[slice60],\r\n",
        "       train_regret_loser_5[slice60],\r\n",
        "       train_regret_loser_6[slice60],\r\n",
        "       train_regret_loser_7[slice60],\r\n",
        "       train_regret_loser_8[slice60],\r\n",
        "       train_regret_loser_9[slice60],\r\n",
        "       train_regret_loser_10[slice60],\r\n",
        "       train_regret_loser_11[slice60],\r\n",
        "       train_regret_loser_12[slice60],\r\n",
        "       train_regret_loser_13[slice60],\r\n",
        "       train_regret_loser_14[slice60],\r\n",
        "       train_regret_loser_15[slice60],\r\n",
        "       train_regret_loser_16[slice60],\r\n",
        "       train_regret_loser_17[slice60],\r\n",
        "       train_regret_loser_18[slice60],\r\n",
        "       train_regret_loser_19[slice60],\r\n",
        "       train_regret_loser_20[slice60]]\r\n",
        "\r\n",
        "winner60 = [train_regret_winner_1[slice60],\r\n",
        "       train_regret_winner_2[slice60],\r\n",
        "       train_regret_winner_3[slice60],\r\n",
        "       train_regret_winner_4[slice60],\r\n",
        "       train_regret_winner_5[slice60],\r\n",
        "       train_regret_winner_6[slice60],\r\n",
        "       train_regret_winner_7[slice60],\r\n",
        "       train_regret_winner_8[slice60],\r\n",
        "       train_regret_winner_9[slice60],\r\n",
        "       train_regret_winner_10[slice60],\r\n",
        "       train_regret_winner_11[slice60],\r\n",
        "       train_regret_winner_12[slice60],\r\n",
        "       train_regret_winner_13[slice60],\r\n",
        "       train_regret_winner_14[slice60],\r\n",
        "       train_regret_winner_15[slice60],\r\n",
        "       train_regret_winner_16[slice60],\r\n",
        "       train_regret_winner_17[slice60],\r\n",
        "       train_regret_winner_18[slice60],\r\n",
        "       train_regret_winner_19[slice60],\r\n",
        "       train_regret_winner_20[slice60]]\r\n",
        "\r\n",
        "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\r\n",
        "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\r\n",
        "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\r\n",
        "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\r\n",
        "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\r\n",
        "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqgVxVH2VCU"
      },
      "source": [
        "# Iteration70 :\r\n",
        "\r\n",
        "slice70 = 69\r\n",
        "\r\n",
        "loser70 = [train_regret_loser_1[slice70],\r\n",
        "       train_regret_loser_2[slice70],\r\n",
        "       train_regret_loser_3[slice70],\r\n",
        "       train_regret_loser_4[slice70],\r\n",
        "       train_regret_loser_5[slice70],\r\n",
        "       train_regret_loser_6[slice70],\r\n",
        "       train_regret_loser_7[slice70],\r\n",
        "       train_regret_loser_8[slice70],\r\n",
        "       train_regret_loser_9[slice70],\r\n",
        "       train_regret_loser_10[slice70],\r\n",
        "       train_regret_loser_11[slice70],\r\n",
        "       train_regret_loser_12[slice70],\r\n",
        "       train_regret_loser_13[slice70],\r\n",
        "       train_regret_loser_14[slice70],\r\n",
        "       train_regret_loser_15[slice70],\r\n",
        "       train_regret_loser_16[slice70],\r\n",
        "       train_regret_loser_17[slice70],\r\n",
        "       train_regret_loser_18[slice70],\r\n",
        "       train_regret_loser_19[slice70],\r\n",
        "       train_regret_loser_20[slice70]]\r\n",
        "\r\n",
        "winner70 = [train_regret_winner_1[slice70],\r\n",
        "       train_regret_winner_2[slice70],\r\n",
        "       train_regret_winner_3[slice70],\r\n",
        "       train_regret_winner_4[slice70],\r\n",
        "       train_regret_winner_5[slice70],\r\n",
        "       train_regret_winner_6[slice70],\r\n",
        "       train_regret_winner_7[slice70],\r\n",
        "       train_regret_winner_8[slice70],\r\n",
        "       train_regret_winner_9[slice70],\r\n",
        "       train_regret_winner_10[slice70],\r\n",
        "       train_regret_winner_11[slice70],\r\n",
        "       train_regret_winner_12[slice70],\r\n",
        "       train_regret_winner_13[slice70],\r\n",
        "       train_regret_winner_14[slice70],\r\n",
        "       train_regret_winner_15[slice70],\r\n",
        "       train_regret_winner_16[slice70],\r\n",
        "       train_regret_winner_17[slice70],\r\n",
        "       train_regret_winner_18[slice70],\r\n",
        "       train_regret_winner_19[slice70],\r\n",
        "       train_regret_winner_20[slice70]]\r\n",
        "\r\n",
        "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\r\n",
        "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\r\n",
        "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\r\n",
        "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\r\n",
        "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\r\n",
        "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF7mlDZL1pyN"
      },
      "source": [
        "# Iteration80 :\r\n",
        "\r\n",
        "slice80 = 79\r\n",
        "\r\n",
        "loser80 = [train_regret_loser_1[slice80],\r\n",
        "       train_regret_loser_2[slice80],\r\n",
        "       train_regret_loser_3[slice80],\r\n",
        "       train_regret_loser_4[slice80],\r\n",
        "       train_regret_loser_5[slice80],\r\n",
        "       train_regret_loser_6[slice80],\r\n",
        "       train_regret_loser_7[slice80],\r\n",
        "       train_regret_loser_8[slice80],\r\n",
        "       train_regret_loser_9[slice80],\r\n",
        "       train_regret_loser_10[slice80],\r\n",
        "       train_regret_loser_11[slice80],\r\n",
        "       train_regret_loser_12[slice80],\r\n",
        "       train_regret_loser_13[slice80],\r\n",
        "       train_regret_loser_14[slice80],\r\n",
        "       train_regret_loser_15[slice80],\r\n",
        "       train_regret_loser_16[slice80],\r\n",
        "       train_regret_loser_17[slice80],\r\n",
        "       train_regret_loser_18[slice80],\r\n",
        "       train_regret_loser_19[slice80],\r\n",
        "       train_regret_loser_20[slice80]]\r\n",
        "\r\n",
        "winner80 = [train_regret_winner_1[slice80],\r\n",
        "       train_regret_winner_2[slice80],\r\n",
        "       train_regret_winner_3[slice80],\r\n",
        "       train_regret_winner_4[slice80],\r\n",
        "       train_regret_winner_5[slice80],\r\n",
        "       train_regret_winner_6[slice80],\r\n",
        "       train_regret_winner_7[slice80],\r\n",
        "       train_regret_winner_8[slice80],\r\n",
        "       train_regret_winner_9[slice80],\r\n",
        "       train_regret_winner_10[slice80],\r\n",
        "       train_regret_winner_11[slice80],\r\n",
        "       train_regret_winner_12[slice80],\r\n",
        "       train_regret_winner_13[slice80],\r\n",
        "       train_regret_winner_14[slice80],\r\n",
        "       train_regret_winner_15[slice80],\r\n",
        "       train_regret_winner_16[slice80],\r\n",
        "       train_regret_winner_17[slice80],\r\n",
        "       train_regret_winner_18[slice80],\r\n",
        "       train_regret_winner_19[slice80],\r\n",
        "       train_regret_winner_20[slice80]]\r\n",
        "\r\n",
        "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\r\n",
        "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\r\n",
        "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\r\n",
        "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\r\n",
        "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\r\n",
        "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Rw8IkD1p0z"
      },
      "source": [
        "# Iteration90 :\r\n",
        "\r\n",
        "slice90 = 89\r\n",
        "\r\n",
        "loser90 = [train_regret_loser_1[slice90],\r\n",
        "       train_regret_loser_2[slice90],\r\n",
        "       train_regret_loser_3[slice90],\r\n",
        "       train_regret_loser_4[slice90],\r\n",
        "       train_regret_loser_5[slice90],\r\n",
        "       train_regret_loser_6[slice90],\r\n",
        "       train_regret_loser_7[slice90],\r\n",
        "       train_regret_loser_8[slice90],\r\n",
        "       train_regret_loser_9[slice90],\r\n",
        "       train_regret_loser_10[slice90],\r\n",
        "       train_regret_loser_11[slice90],\r\n",
        "       train_regret_loser_12[slice90],\r\n",
        "       train_regret_loser_13[slice90],\r\n",
        "       train_regret_loser_14[slice90],\r\n",
        "       train_regret_loser_15[slice90],\r\n",
        "       train_regret_loser_16[slice90],\r\n",
        "       train_regret_loser_17[slice90],\r\n",
        "       train_regret_loser_18[slice90],\r\n",
        "       train_regret_loser_19[slice90],\r\n",
        "       train_regret_loser_20[slice90]]\r\n",
        "\r\n",
        "winner90 = [train_regret_winner_1[slice90],\r\n",
        "       train_regret_winner_2[slice90],\r\n",
        "       train_regret_winner_3[slice90],\r\n",
        "       train_regret_winner_4[slice90],\r\n",
        "       train_regret_winner_5[slice90],\r\n",
        "       train_regret_winner_6[slice90],\r\n",
        "       train_regret_winner_7[slice90],\r\n",
        "       train_regret_winner_8[slice90],\r\n",
        "       train_regret_winner_9[slice90],\r\n",
        "       train_regret_winner_10[slice90],\r\n",
        "       train_regret_winner_11[slice90],\r\n",
        "       train_regret_winner_12[slice90],\r\n",
        "       train_regret_winner_13[slice90],\r\n",
        "       train_regret_winner_14[slice90],\r\n",
        "       train_regret_winner_15[slice90],\r\n",
        "       train_regret_winner_16[slice90],\r\n",
        "       train_regret_winner_17[slice90],\r\n",
        "       train_regret_winner_18[slice90],\r\n",
        "       train_regret_winner_19[slice90],\r\n",
        "       train_regret_winner_20[slice90]]\r\n",
        "\r\n",
        "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\r\n",
        "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\r\n",
        "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\r\n",
        "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\r\n",
        "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\r\n",
        "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiljCrq21p3a"
      },
      "source": [
        "# Iteration100 :\r\n",
        "\r\n",
        "slice100 = 99\r\n",
        "\r\n",
        "loser100 = [train_regret_loser_1[slice100],\r\n",
        "       train_regret_loser_2[slice100],\r\n",
        "       train_regret_loser_3[slice100],\r\n",
        "       train_regret_loser_4[slice100],\r\n",
        "       train_regret_loser_5[slice100],\r\n",
        "       train_regret_loser_6[slice100],\r\n",
        "       train_regret_loser_7[slice100],\r\n",
        "       train_regret_loser_8[slice100],\r\n",
        "       train_regret_loser_9[slice100],\r\n",
        "       train_regret_loser_10[slice100],\r\n",
        "       train_regret_loser_11[slice100],\r\n",
        "       train_regret_loser_12[slice100],\r\n",
        "       train_regret_loser_13[slice100],\r\n",
        "       train_regret_loser_14[slice100],\r\n",
        "       train_regret_loser_15[slice100],\r\n",
        "       train_regret_loser_16[slice100],\r\n",
        "       train_regret_loser_17[slice100],\r\n",
        "       train_regret_loser_18[slice100],\r\n",
        "       train_regret_loser_19[slice100],\r\n",
        "       train_regret_loser_20[slice100]]\r\n",
        "\r\n",
        "winner100 = [train_regret_winner_1[slice100],\r\n",
        "       train_regret_winner_2[slice100],\r\n",
        "       train_regret_winner_3[slice100],\r\n",
        "       train_regret_winner_4[slice100],\r\n",
        "       train_regret_winner_5[slice100],\r\n",
        "       train_regret_winner_6[slice100],\r\n",
        "       train_regret_winner_7[slice100],\r\n",
        "       train_regret_winner_8[slice100],\r\n",
        "       train_regret_winner_9[slice100],\r\n",
        "       train_regret_winner_10[slice100],\r\n",
        "       train_regret_winner_11[slice100],\r\n",
        "       train_regret_winner_12[slice100],\r\n",
        "       train_regret_winner_13[slice100],\r\n",
        "       train_regret_winner_14[slice100],\r\n",
        "       train_regret_winner_15[slice100],\r\n",
        "       train_regret_winner_16[slice100],\r\n",
        "       train_regret_winner_17[slice100],\r\n",
        "       train_regret_winner_18[slice100],\r\n",
        "       train_regret_winner_19[slice100],\r\n",
        "       train_regret_winner_20[slice100]]\r\n",
        "\r\n",
        "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\r\n",
        "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\r\n",
        "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\r\n",
        "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\r\n",
        "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\r\n",
        "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OItJgqUT2oNb"
      },
      "source": [
        "### Summarize arrays: 'Loser'\r\n",
        "\r\n",
        "lower_loser = [lower_loser1,\r\n",
        "            lower_loser2,\r\n",
        "            lower_loser3,\r\n",
        "            lower_loser4,\r\n",
        "            lower_loser5,\r\n",
        "            lower_loser6,\r\n",
        "            lower_loser7,\r\n",
        "            lower_loser8,\r\n",
        "            lower_loser9,\r\n",
        "            lower_loser10,\r\n",
        "            lower_loser11,\r\n",
        "            lower_loser12,\r\n",
        "            lower_loser13,\r\n",
        "            lower_loser14,\r\n",
        "            lower_loser15,\r\n",
        "            lower_loser16,\r\n",
        "            lower_loser17,\r\n",
        "            lower_loser18,\r\n",
        "            lower_loser19,\r\n",
        "            lower_loser20,\r\n",
        "            lower_loser21,\r\n",
        "            lower_loser22,\r\n",
        "            lower_loser23,\r\n",
        "            lower_loser24,\r\n",
        "            lower_loser25,\r\n",
        "            lower_loser26,\r\n",
        "            lower_loser27,\r\n",
        "            lower_loser28,\r\n",
        "            lower_loser29,\r\n",
        "            lower_loser30,\r\n",
        "            lower_loser31,\r\n",
        "            lower_loser32,\r\n",
        "            lower_loser33,\r\n",
        "            lower_loser34,\r\n",
        "            lower_loser35,\r\n",
        "            lower_loser36,\r\n",
        "            lower_loser37,\r\n",
        "            lower_loser38,\r\n",
        "            lower_loser39,\r\n",
        "            lower_loser40,\r\n",
        "            lower_loser41,\r\n",
        "            lower_loser42,\r\n",
        "            lower_loser43,\r\n",
        "            lower_loser44,\r\n",
        "            lower_loser45,\r\n",
        "            lower_loser46,\r\n",
        "            lower_loser47,\r\n",
        "            lower_loser48,\r\n",
        "            lower_loser49,\r\n",
        "            lower_loser50,\r\n",
        "            lower_loser51,\r\n",
        "            lower_loser52,\r\n",
        "            lower_loser53,\r\n",
        "            lower_loser54,\r\n",
        "            lower_loser55,\r\n",
        "            lower_loser56,\r\n",
        "            lower_loser57,\r\n",
        "            lower_loser58,\r\n",
        "            lower_loser59,\r\n",
        "            lower_loser60,\r\n",
        "            lower_loser61,\r\n",
        "            lower_loser62,\r\n",
        "            lower_loser63,\r\n",
        "            lower_loser64,\r\n",
        "            lower_loser65,\r\n",
        "            lower_loser66,\r\n",
        "            lower_loser67,\r\n",
        "            lower_loser68,\r\n",
        "            lower_loser69,\r\n",
        "            lower_loser70,\r\n",
        "            lower_loser71,\r\n",
        "            lower_loser72,\r\n",
        "            lower_loser73,\r\n",
        "            lower_loser74,\r\n",
        "            lower_loser75,\r\n",
        "            lower_loser76,\r\n",
        "            lower_loser77,\r\n",
        "            lower_loser78,\r\n",
        "            lower_loser79,\r\n",
        "            lower_loser80,\r\n",
        "            lower_loser81,\r\n",
        "            lower_loser82,\r\n",
        "            lower_loser83,\r\n",
        "            lower_loser84,\r\n",
        "            lower_loser85,\r\n",
        "            lower_loser86,\r\n",
        "            lower_loser87,\r\n",
        "            lower_loser88,\r\n",
        "            lower_loser89,\r\n",
        "            lower_loser90,\r\n",
        "            lower_loser91,\r\n",
        "            lower_loser92,\r\n",
        "            lower_loser93,\r\n",
        "            lower_loser94,\r\n",
        "            lower_loser95,\r\n",
        "            lower_loser96,\r\n",
        "            lower_loser97,\r\n",
        "            lower_loser98,\r\n",
        "            lower_loser99,\r\n",
        "            lower_loser100,\r\n",
        "            lower_loser101]\r\n",
        "\r\n",
        "median_loser = [median_loser1,\r\n",
        "            median_loser2,\r\n",
        "            median_loser3,\r\n",
        "            median_loser4,\r\n",
        "            median_loser5,\r\n",
        "            median_loser6,\r\n",
        "            median_loser7,\r\n",
        "            median_loser8,\r\n",
        "            median_loser9,\r\n",
        "            median_loser10,\r\n",
        "            median_loser11,\r\n",
        "            median_loser12,\r\n",
        "            median_loser13,\r\n",
        "            median_loser14,\r\n",
        "            median_loser15,\r\n",
        "            median_loser16,\r\n",
        "            median_loser17,\r\n",
        "            median_loser18,\r\n",
        "            median_loser19,\r\n",
        "            median_loser20,\r\n",
        "            median_loser21,\r\n",
        "            median_loser22,\r\n",
        "            median_loser23,\r\n",
        "            median_loser24,\r\n",
        "            median_loser25,\r\n",
        "            median_loser26,\r\n",
        "            median_loser27,\r\n",
        "            median_loser28,\r\n",
        "            median_loser29,\r\n",
        "            median_loser30,\r\n",
        "            median_loser31,\r\n",
        "            median_loser32,\r\n",
        "            median_loser33,\r\n",
        "            median_loser34,\r\n",
        "            median_loser35,\r\n",
        "            median_loser36,\r\n",
        "            median_loser37,\r\n",
        "            median_loser38,\r\n",
        "            median_loser39,\r\n",
        "            median_loser40,\r\n",
        "            median_loser41,\r\n",
        "            median_loser42,\r\n",
        "            median_loser43,\r\n",
        "            median_loser44,\r\n",
        "            median_loser45,\r\n",
        "            median_loser46,\r\n",
        "            median_loser47,\r\n",
        "            median_loser48,\r\n",
        "            median_loser49,\r\n",
        "            median_loser50,\r\n",
        "            median_loser51,\r\n",
        "            median_loser52,\r\n",
        "            median_loser53,\r\n",
        "            median_loser54,\r\n",
        "            median_loser55,\r\n",
        "            median_loser56,\r\n",
        "            median_loser57,\r\n",
        "            median_loser58,\r\n",
        "            median_loser59,\r\n",
        "            median_loser60,\r\n",
        "            median_loser61,\r\n",
        "            median_loser62,\r\n",
        "            median_loser63,\r\n",
        "            median_loser64,\r\n",
        "            median_loser65,\r\n",
        "            median_loser66,\r\n",
        "            median_loser67,\r\n",
        "            median_loser68,\r\n",
        "            median_loser69,\r\n",
        "            median_loser70,\r\n",
        "            median_loser71,\r\n",
        "            median_loser72,\r\n",
        "            median_loser73,\r\n",
        "            median_loser74,\r\n",
        "            median_loser75,\r\n",
        "            median_loser76,\r\n",
        "            median_loser77,\r\n",
        "            median_loser78,\r\n",
        "            median_loser79,\r\n",
        "            median_loser80,\r\n",
        "            median_loser81,\r\n",
        "            median_loser82,\r\n",
        "            median_loser83,\r\n",
        "            median_loser84,\r\n",
        "            median_loser85,\r\n",
        "            median_loser86,\r\n",
        "            median_loser87,\r\n",
        "            median_loser88,\r\n",
        "            median_loser89,\r\n",
        "            median_loser90,\r\n",
        "            median_loser91,\r\n",
        "            median_loser92,\r\n",
        "            median_loser93,\r\n",
        "            median_loser94,\r\n",
        "            median_loser95,\r\n",
        "            median_loser96,\r\n",
        "            median_loser97,\r\n",
        "            median_loser98,\r\n",
        "            median_loser99,\r\n",
        "            median_loser100,\r\n",
        "            median_loser101]\r\n",
        "\r\n",
        "upper_loser = [upper_loser1,\r\n",
        "            upper_loser2,\r\n",
        "            upper_loser3,\r\n",
        "            upper_loser4,\r\n",
        "            upper_loser5,\r\n",
        "            upper_loser6,\r\n",
        "            upper_loser7,\r\n",
        "            upper_loser8,\r\n",
        "            upper_loser9,\r\n",
        "            upper_loser10,\r\n",
        "            upper_loser11,\r\n",
        "            upper_loser12,\r\n",
        "            upper_loser13,\r\n",
        "            upper_loser14,\r\n",
        "            upper_loser15,\r\n",
        "            upper_loser16,\r\n",
        "            upper_loser17,\r\n",
        "            upper_loser18,\r\n",
        "            upper_loser19,\r\n",
        "            upper_loser20,\r\n",
        "            upper_loser21,\r\n",
        "            upper_loser22,\r\n",
        "            upper_loser23,\r\n",
        "            upper_loser24,\r\n",
        "            upper_loser25,\r\n",
        "            upper_loser26,\r\n",
        "            upper_loser27,\r\n",
        "            upper_loser28,\r\n",
        "            upper_loser29,\r\n",
        "            upper_loser30,\r\n",
        "            upper_loser31,\r\n",
        "            upper_loser32,\r\n",
        "            upper_loser33,\r\n",
        "            upper_loser34,\r\n",
        "            upper_loser35,\r\n",
        "            upper_loser36,\r\n",
        "            upper_loser37,\r\n",
        "            upper_loser38,\r\n",
        "            upper_loser39,\r\n",
        "            upper_loser40,\r\n",
        "            upper_loser41,\r\n",
        "            upper_loser42,\r\n",
        "            upper_loser43,\r\n",
        "            upper_loser44,\r\n",
        "            upper_loser45,\r\n",
        "            upper_loser46,\r\n",
        "            upper_loser47,\r\n",
        "            upper_loser48,\r\n",
        "            upper_loser49,\r\n",
        "            upper_loser50,\r\n",
        "            upper_loser51,\r\n",
        "            upper_loser52,\r\n",
        "            upper_loser53,\r\n",
        "            upper_loser54,\r\n",
        "            upper_loser55,\r\n",
        "            upper_loser56,\r\n",
        "            upper_loser57,\r\n",
        "            upper_loser58,\r\n",
        "            upper_loser59,\r\n",
        "            upper_loser60,\r\n",
        "            upper_loser61,\r\n",
        "            upper_loser62,\r\n",
        "            upper_loser63,\r\n",
        "            upper_loser64,\r\n",
        "            upper_loser65,\r\n",
        "            upper_loser66,\r\n",
        "            upper_loser67,\r\n",
        "            upper_loser68,\r\n",
        "            upper_loser69,\r\n",
        "            upper_loser70,\r\n",
        "            upper_loser71,\r\n",
        "            upper_loser72,\r\n",
        "            upper_loser73,\r\n",
        "            upper_loser74,\r\n",
        "            upper_loser75,\r\n",
        "            upper_loser76,\r\n",
        "            upper_loser77,\r\n",
        "            upper_loser78,\r\n",
        "            upper_loser79,\r\n",
        "            upper_loser80,\r\n",
        "            upper_loser81,\r\n",
        "            upper_loser82,\r\n",
        "            upper_loser83,\r\n",
        "            upper_loser84,\r\n",
        "            upper_loser85,\r\n",
        "            upper_loser86,\r\n",
        "            upper_loser87,\r\n",
        "            upper_loser88,\r\n",
        "            upper_loser89,\r\n",
        "            upper_loser90,\r\n",
        "            upper_loser91,\r\n",
        "            upper_loser92,\r\n",
        "            upper_loser93,\r\n",
        "            upper_loser94,\r\n",
        "            upper_loser95,\r\n",
        "            upper_loser96,\r\n",
        "            upper_loser97,\r\n",
        "            upper_loser98,\r\n",
        "            upper_loser99,\r\n",
        "            upper_loser100,\r\n",
        "            upper_loser101]"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u19FndLT2oU3"
      },
      "source": [
        "### Summarize arrays: 'Winner'\r\n",
        "\r\n",
        "lower_winner = [lower_winner1,\r\n",
        "            lower_winner2,\r\n",
        "            lower_winner3,\r\n",
        "            lower_winner4,\r\n",
        "            lower_winner5,\r\n",
        "            lower_winner6,\r\n",
        "            lower_winner7,\r\n",
        "            lower_winner8,\r\n",
        "            lower_winner9,\r\n",
        "            lower_winner10,\r\n",
        "            lower_winner11,\r\n",
        "            lower_winner12,\r\n",
        "            lower_winner13,\r\n",
        "            lower_winner14,\r\n",
        "            lower_winner15,\r\n",
        "            lower_winner16,\r\n",
        "            lower_winner17,\r\n",
        "            lower_winner18,\r\n",
        "            lower_winner19,\r\n",
        "            lower_winner20,\r\n",
        "            lower_winner21,\r\n",
        "            lower_winner22,\r\n",
        "            lower_winner23,\r\n",
        "            lower_winner24,\r\n",
        "            lower_winner25,\r\n",
        "            lower_winner26,\r\n",
        "            lower_winner27,\r\n",
        "            lower_winner28,\r\n",
        "            lower_winner29,\r\n",
        "            lower_winner30,\r\n",
        "            lower_winner31,\r\n",
        "            lower_winner32,\r\n",
        "            lower_winner33,\r\n",
        "            lower_winner34,\r\n",
        "            lower_winner35,\r\n",
        "            lower_winner36,\r\n",
        "            lower_winner37,\r\n",
        "            lower_winner38,\r\n",
        "            lower_winner39,\r\n",
        "            lower_winner40,\r\n",
        "            lower_winner41,\r\n",
        "            lower_winner42,\r\n",
        "            lower_winner43,\r\n",
        "            lower_winner44,\r\n",
        "            lower_winner45,\r\n",
        "            lower_winner46,\r\n",
        "            lower_winner47,\r\n",
        "            lower_winner48,\r\n",
        "            lower_winner49,\r\n",
        "            lower_winner50,\r\n",
        "            lower_winner51,\r\n",
        "            lower_winner52,\r\n",
        "            lower_winner53,\r\n",
        "            lower_winner54,\r\n",
        "            lower_winner55,\r\n",
        "            lower_winner56,\r\n",
        "            lower_winner57,\r\n",
        "            lower_winner58,\r\n",
        "            lower_winner59,\r\n",
        "            lower_winner60,\r\n",
        "            lower_winner61,\r\n",
        "            lower_winner62,\r\n",
        "            lower_winner63,\r\n",
        "            lower_winner64,\r\n",
        "            lower_winner65,\r\n",
        "            lower_winner66,\r\n",
        "            lower_winner67,\r\n",
        "            lower_winner68,\r\n",
        "            lower_winner69,\r\n",
        "            lower_winner70,\r\n",
        "            lower_winner71,\r\n",
        "            lower_winner72,\r\n",
        "            lower_winner73,\r\n",
        "            lower_winner74,\r\n",
        "            lower_winner75,\r\n",
        "            lower_winner76,\r\n",
        "            lower_winner77,\r\n",
        "            lower_winner78,\r\n",
        "            lower_winner79,\r\n",
        "            lower_winner80,\r\n",
        "            lower_winner81,\r\n",
        "            lower_winner82,\r\n",
        "            lower_winner83,\r\n",
        "            lower_winner84,\r\n",
        "            lower_winner85,\r\n",
        "            lower_winner86,\r\n",
        "            lower_winner87,\r\n",
        "            lower_winner88,\r\n",
        "            lower_winner89,\r\n",
        "            lower_winner90,\r\n",
        "            lower_winner91,\r\n",
        "            lower_winner92,\r\n",
        "            lower_winner93,\r\n",
        "            lower_winner94,\r\n",
        "            lower_winner95,\r\n",
        "            lower_winner96,\r\n",
        "            lower_winner97,\r\n",
        "            lower_winner98,\r\n",
        "            lower_winner99,\r\n",
        "            lower_winner100,\r\n",
        "            lower_winner101]\r\n",
        "\r\n",
        "median_winner = [median_winner1,\r\n",
        "            median_winner2,\r\n",
        "            median_winner3,\r\n",
        "            median_winner4,\r\n",
        "            median_winner5,\r\n",
        "            median_winner6,\r\n",
        "            median_winner7,\r\n",
        "            median_winner8,\r\n",
        "            median_winner9,\r\n",
        "            median_winner10,\r\n",
        "            median_winner11,\r\n",
        "            median_winner12,\r\n",
        "            median_winner13,\r\n",
        "            median_winner14,\r\n",
        "            median_winner15,\r\n",
        "            median_winner16,\r\n",
        "            median_winner17,\r\n",
        "            median_winner18,\r\n",
        "            median_winner19,\r\n",
        "            median_winner20,\r\n",
        "            median_winner21,\r\n",
        "            median_winner22,\r\n",
        "            median_winner23,\r\n",
        "            median_winner24,\r\n",
        "            median_winner25,\r\n",
        "            median_winner26,\r\n",
        "            median_winner27,\r\n",
        "            median_winner28,\r\n",
        "            median_winner29,\r\n",
        "            median_winner30,\r\n",
        "            median_winner31,\r\n",
        "            median_winner32,\r\n",
        "            median_winner33,\r\n",
        "            median_winner34,\r\n",
        "            median_winner35,\r\n",
        "            median_winner36,\r\n",
        "            median_winner37,\r\n",
        "            median_winner38,\r\n",
        "            median_winner39,\r\n",
        "            median_winner40,\r\n",
        "            median_winner41,\r\n",
        "            median_winner42,\r\n",
        "            median_winner43,\r\n",
        "            median_winner44,\r\n",
        "            median_winner45,\r\n",
        "            median_winner46,\r\n",
        "            median_winner47,\r\n",
        "            median_winner48,\r\n",
        "            median_winner49,\r\n",
        "            median_winner50,\r\n",
        "            median_winner51,\r\n",
        "            median_winner52,\r\n",
        "            median_winner53,\r\n",
        "            median_winner54,\r\n",
        "            median_winner55,\r\n",
        "            median_winner56,\r\n",
        "            median_winner57,\r\n",
        "            median_winner58,\r\n",
        "            median_winner59,\r\n",
        "            median_winner60,\r\n",
        "            median_winner61,\r\n",
        "            median_winner62,\r\n",
        "            median_winner63,\r\n",
        "            median_winner64,\r\n",
        "            median_winner65,\r\n",
        "            median_winner66,\r\n",
        "            median_winner67,\r\n",
        "            median_winner68,\r\n",
        "            median_winner69,\r\n",
        "            median_winner70,\r\n",
        "            median_winner71,\r\n",
        "            median_winner72,\r\n",
        "            median_winner73,\r\n",
        "            median_winner74,\r\n",
        "            median_winner75,\r\n",
        "            median_winner76,\r\n",
        "            median_winner77,\r\n",
        "            median_winner78,\r\n",
        "            median_winner79,\r\n",
        "            median_winner80,\r\n",
        "            median_winner81,\r\n",
        "            median_winner82,\r\n",
        "            median_winner83,\r\n",
        "            median_winner84,\r\n",
        "            median_winner85,\r\n",
        "            median_winner86,\r\n",
        "            median_winner87,\r\n",
        "            median_winner88,\r\n",
        "            median_winner89,\r\n",
        "            median_winner90,\r\n",
        "            median_winner91,\r\n",
        "            median_winner92,\r\n",
        "            median_winner93,\r\n",
        "            median_winner94,\r\n",
        "            median_winner95,\r\n",
        "            median_winner96,\r\n",
        "            median_winner97,\r\n",
        "            median_winner98,\r\n",
        "            median_winner99,\r\n",
        "            median_winner100,\r\n",
        "            median_winner101]\r\n",
        "\r\n",
        "upper_winner = [upper_winner1,\r\n",
        "            upper_winner2,\r\n",
        "            upper_winner3,\r\n",
        "            upper_winner4,\r\n",
        "            upper_winner5,\r\n",
        "            upper_winner6,\r\n",
        "            upper_winner7,\r\n",
        "            upper_winner8,\r\n",
        "            upper_winner9,\r\n",
        "            upper_winner10,\r\n",
        "            upper_winner11,\r\n",
        "            upper_winner12,\r\n",
        "            upper_winner13,\r\n",
        "            upper_winner14,\r\n",
        "            upper_winner15,\r\n",
        "            upper_winner16,\r\n",
        "            upper_winner17,\r\n",
        "            upper_winner18,\r\n",
        "            upper_winner19,\r\n",
        "            upper_winner20,\r\n",
        "            upper_winner21,\r\n",
        "            upper_winner22,\r\n",
        "            upper_winner23,\r\n",
        "            upper_winner24,\r\n",
        "            upper_winner25,\r\n",
        "            upper_winner26,\r\n",
        "            upper_winner27,\r\n",
        "            upper_winner28,\r\n",
        "            upper_winner29,\r\n",
        "            upper_winner30,\r\n",
        "            upper_winner31,\r\n",
        "            upper_winner32,\r\n",
        "            upper_winner33,\r\n",
        "            upper_winner34,\r\n",
        "            upper_winner35,\r\n",
        "            upper_winner36,\r\n",
        "            upper_winner37,\r\n",
        "            upper_winner38,\r\n",
        "            upper_winner39,\r\n",
        "            upper_winner40,\r\n",
        "            upper_winner41,\r\n",
        "            upper_winner42,\r\n",
        "            upper_winner43,\r\n",
        "            upper_winner44,\r\n",
        "            upper_winner45,\r\n",
        "            upper_winner46,\r\n",
        "            upper_winner47,\r\n",
        "            upper_winner48,\r\n",
        "            upper_winner49,\r\n",
        "            upper_winner50,\r\n",
        "            upper_winner51,\r\n",
        "            upper_winner52,\r\n",
        "            upper_winner53,\r\n",
        "            upper_winner54,\r\n",
        "            upper_winner55,\r\n",
        "            upper_winner56,\r\n",
        "            upper_winner57,\r\n",
        "            upper_winner58,\r\n",
        "            upper_winner59,\r\n",
        "            upper_winner60,\r\n",
        "            upper_winner61,\r\n",
        "            upper_winner62,\r\n",
        "            upper_winner63,\r\n",
        "            upper_winner64,\r\n",
        "            upper_winner65,\r\n",
        "            upper_winner66,\r\n",
        "            upper_winner67,\r\n",
        "            upper_winner68,\r\n",
        "            upper_winner69,\r\n",
        "            upper_winner70,\r\n",
        "            upper_winner71,\r\n",
        "            upper_winner72,\r\n",
        "            upper_winner73,\r\n",
        "            upper_winner74,\r\n",
        "            upper_winner75,\r\n",
        "            upper_winner76,\r\n",
        "            upper_winner77,\r\n",
        "            upper_winner78,\r\n",
        "            upper_winner79,\r\n",
        "            upper_winner80,\r\n",
        "            upper_winner81,\r\n",
        "            upper_winner82,\r\n",
        "            upper_winner83,\r\n",
        "            upper_winner84,\r\n",
        "            upper_winner85,\r\n",
        "            upper_winner86,\r\n",
        "            upper_winner87,\r\n",
        "            upper_winner88,\r\n",
        "            upper_winner89,\r\n",
        "            upper_winner90,\r\n",
        "            upper_winner91,\r\n",
        "            upper_winner92,\r\n",
        "            upper_winner93,\r\n",
        "            upper_winner94,\r\n",
        "            upper_winner95,\r\n",
        "            upper_winner96,\r\n",
        "            upper_winner97,\r\n",
        "            upper_winner98,\r\n",
        "            upper_winner99,\r\n",
        "            upper_winner100,\r\n",
        "            upper_winner101]"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "84XIzmWD2oba",
        "outputId": "0487420b-04c9-4c1c-bc77-0a442a309faf"
      },
      "source": [
        "### Visualize!\r\n",
        "\r\n",
        "title = obj_func\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "\r\n",
        "plt.plot(median_loser, color = 'Red')\r\n",
        "plt.plot(median_winner, color = 'Yellow')\r\n",
        "\r\n",
        "xstar = np.arange(0, max_iter+1, step=1)\r\n",
        "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Yellow', alpha=0.4, label='GP EI Regret IQR: L-BFGS-B')\r\n",
        "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Red', alpha=0.4, label='GP EI Regret IQR: Newton-CG with GP d$^{2}$EI')\r\n",
        "\r\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\r\n",
        "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') \r\n",
        "plt.ylabel('log(Regret)', weight = 'bold', family = 'Arial') \r\n",
        "plt.legend(loc=1) # add plot legend\r\n",
        "\r\n",
        "plt.show() "
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5dkR2Id2oiu",
        "outputId": "d8d7f11e-5d0a-4b61-e881-0289999b24d3"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2269.4519028663635, 1555.3340611457825)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNLBF57p2opi"
      },
      "source": [
        ""
      ],
      "execution_count": 179,
      "outputs": []
    }
  ]
}