{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NYC__TaxiFare__XGBoost__GP__dERM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9wHutsqZUcn"
      },
      "source": [
        "XGBoost Regression - 'real-world' example: NYC Taxi-Fare Predictor\n",
        "\n",
        "GP dEI versus GP dERM (winner)\n",
        "\n",
        "https://www.kaggle.com/c/new-york-city-taxi-fare-prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7PwmXsgZO8D",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "101802c8-04a8-47f4-b8e9-617c26670430"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-bdfc2777-7a5f-43f9-b8f4-26fb5d31a736\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-bdfc2777-7a5f-43f9-b8f4-26fb5d31a736\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"conorc2006\",\"key\":\"c5c5a6382a7d50c022aab991694fc17f\"}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMwbJ6hjZltI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13f28abd-4c78-4ab7-c555-4d18d2f15baf"
      },
      "source": [
        "## Ensure the kaggle.json file is present:\n",
        "!ls -lha kaggle.json"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 66 Mar  1 10:22 kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Pu-UlWZovH"
      },
      "source": [
        "## Next, install the Kaggle API client:\n",
        "!pip install -q kaggle"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUOQ4SE7Zuj3"
      },
      "source": [
        "## The Kaggle API Client expects this file to be ~/.kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJcEztjCZxOn"
      },
      "source": [
        "## Permissions' change\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-u4Tmj7ZUD3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d96033c-2133-4692-e97e-cb081ba66d62"
      },
      "source": [
        "!kaggle competitions download -c new-york-city-taxi-fare-prediction"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.10 / client 1.5.4)\n",
            "Downloading sample_submission.csv to /content\n",
            "  0% 0.00/335k [00:00<?, ?B/s]\n",
            "100% 335k/335k [00:00<00:00, 49.8MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 99% 1.55G/1.56G [00:16<00:00, 109MB/s] \n",
            "100% 1.56G/1.56G [00:16<00:00, 98.6MB/s]\n",
            "Downloading GCP-Coupons-Instructions.rtf to /content\n",
            "  0% 0.00/486 [00:00<?, ?B/s]\n",
            "100% 486/486 [00:00<00:00, 435kB/s]\n",
            "Downloading test.csv to /content\n",
            "  0% 0.00/960k [00:00<?, ?B/s]\n",
            "100% 960k/960k [00:00<00:00, 130MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-0Pe1i4Z2R_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "285589f6-933b-4d6a-e96d-6ad18b98ffab"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyGPGO\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/87/a113c91ba014708114f7635d5c0f6a5e5c773480c5f0a537b257a02d180d/pyGPGO-0.4.0.dev1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n",
            "Building wheels for collected packages: pyGPGO\n",
            "  Building wheel for pyGPGO (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyGPGO: filename=pyGPGO-0.4.0.dev1-cp37-none-any.whl size=19866 sha256=2c4e26f80d4ab5a0f8373ed502a4b75622c9447eba155df65d190a2363c420f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/27/04/c4fa3bfe194d36e3cd51542132f43415a6813114a5e8301acb\n",
            "Successfully built pyGPGO\n",
            "Installing collected packages: pyGPGO\n",
            "Successfully installed pyGPGO-0.4.0.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7zDTf1naBsH"
      },
      "source": [
        "# Load some default Python modules:\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import pymc3 as pm\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "import time\n",
        "\n",
        "from matplotlib.pyplot import rc\n",
        "rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
        "rc('text', usetex=False)\n",
        "### % matplotlib inline\n",
        "import seaborn as sns\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "from collections import OrderedDict\n",
        "from joblib import Parallel, delayed\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\n",
        "from scipy.optimize import minimize\n",
        "from scipy.spatial.distance import cdist\n",
        "from scipy.special import gamma\n",
        "from scipy.stats import norm, t\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "from pyGPGO.logger import EventLogger\n",
        "from pyGPGO.GPGO import GPGO\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\n",
        "from pyGPGO.surrogates.tStudentProcess import logpdf\n",
        "from pyGPGO.acquisition import Acquisition\n",
        "from pyGPGO.covfunc import squaredExponential\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "from pandas_datareader import data\n",
        "\n",
        "import warnings\n",
        "import random\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXicekJhaE0P"
      },
      "source": [
        "# Read data in pandas dataframe:\n",
        "\n",
        "df_train =  pd.read_csv('/content/train.csv.zip', nrows = 1_000_000, parse_dates=[\"pickup_datetime\"])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ0mDzt_cBmw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f71c03e9-750d-4c11-c079-9ed5752df454"
      },
      "source": [
        "# List first rows:\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2009-06-15 17:26:21.0000001</td>\n",
              "      <td>4.5</td>\n",
              "      <td>2009-06-15 17:26:21+00:00</td>\n",
              "      <td>-73.844311</td>\n",
              "      <td>40.721319</td>\n",
              "      <td>-73.841610</td>\n",
              "      <td>40.712278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-01-05 16:52:16.0000002</td>\n",
              "      <td>16.9</td>\n",
              "      <td>2010-01-05 16:52:16+00:00</td>\n",
              "      <td>-74.016048</td>\n",
              "      <td>40.711303</td>\n",
              "      <td>-73.979268</td>\n",
              "      <td>40.782004</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2011-08-18 00:35:00.00000049</td>\n",
              "      <td>5.7</td>\n",
              "      <td>2011-08-18 00:35:00+00:00</td>\n",
              "      <td>-73.982738</td>\n",
              "      <td>40.761270</td>\n",
              "      <td>-73.991242</td>\n",
              "      <td>40.750562</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2012-04-21 04:30:42.0000001</td>\n",
              "      <td>7.7</td>\n",
              "      <td>2012-04-21 04:30:42+00:00</td>\n",
              "      <td>-73.987130</td>\n",
              "      <td>40.733143</td>\n",
              "      <td>-73.991567</td>\n",
              "      <td>40.758092</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-03-09 07:51:00.000000135</td>\n",
              "      <td>5.3</td>\n",
              "      <td>2010-03-09 07:51:00+00:00</td>\n",
              "      <td>-73.968095</td>\n",
              "      <td>40.768008</td>\n",
              "      <td>-73.956655</td>\n",
              "      <td>40.783762</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             key  ...  passenger_count\n",
              "0    2009-06-15 17:26:21.0000001  ...                1\n",
              "1    2010-01-05 16:52:16.0000002  ...                1\n",
              "2   2011-08-18 00:35:00.00000049  ...                2\n",
              "3    2012-04-21 04:30:42.0000001  ...                1\n",
              "4  2010-03-09 07:51:00.000000135  ...                1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9fZujMycFMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c6c0515-057f-4030-a954-1e7801b11a43"
      },
      "source": [
        "# Format 'pickup_datetime' variable:\n",
        "\n",
        "df_train['pickup_datetime'] =  pd.to_datetime(df_train['pickup_datetime'], utc=True, format='%Y-%m-%d %H:%M')\n",
        "df_train['pickup_datetime'].head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0   2009-06-15 17:26:21+00:00\n",
              "1   2010-01-05 16:52:16+00:00\n",
              "2   2011-08-18 00:35:00+00:00\n",
              "3   2012-04-21 04:30:42+00:00\n",
              "4   2010-03-09 07:51:00+00:00\n",
              "Name: pickup_datetime, dtype: datetime64[ns, UTC]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nReKu62HcVFI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "43ef094c-2dde-43de-c649-aa55f332630c"
      },
      "source": [
        "df_train.sort_values(by = 'pickup_datetime').tail() ### June 2015 the final month"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>key</th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_datetime</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>286276</th>\n",
              "      <td>2015-06-30 23:38:21.0000003</td>\n",
              "      <td>26.5</td>\n",
              "      <td>2015-06-30 23:38:21+00:00</td>\n",
              "      <td>-74.008385</td>\n",
              "      <td>40.711571</td>\n",
              "      <td>-73.884071</td>\n",
              "      <td>40.737385</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>955575</th>\n",
              "      <td>2015-06-30 23:45:57.0000003</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2015-06-30 23:45:57+00:00</td>\n",
              "      <td>-74.002342</td>\n",
              "      <td>40.739819</td>\n",
              "      <td>-74.005829</td>\n",
              "      <td>40.745239</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>915826</th>\n",
              "      <td>2015-06-30 23:48:35.0000005</td>\n",
              "      <td>30.5</td>\n",
              "      <td>2015-06-30 23:48:35+00:00</td>\n",
              "      <td>-73.983826</td>\n",
              "      <td>40.729546</td>\n",
              "      <td>-73.927917</td>\n",
              "      <td>40.661186</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751350</th>\n",
              "      <td>2015-06-30 23:53:23.0000002</td>\n",
              "      <td>3.5</td>\n",
              "      <td>2015-06-30 23:53:23+00:00</td>\n",
              "      <td>-73.978020</td>\n",
              "      <td>40.757439</td>\n",
              "      <td>-73.980705</td>\n",
              "      <td>40.753544</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>785182</th>\n",
              "      <td>2015-06-30 23:53:49.0000003</td>\n",
              "      <td>7.5</td>\n",
              "      <td>2015-06-30 23:53:49+00:00</td>\n",
              "      <td>-73.959969</td>\n",
              "      <td>40.762405</td>\n",
              "      <td>-73.953064</td>\n",
              "      <td>40.782688</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                key  ...  passenger_count\n",
              "286276  2015-06-30 23:38:21.0000003  ...                5\n",
              "955575  2015-06-30 23:45:57.0000003  ...                1\n",
              "915826  2015-06-30 23:48:35.0000005  ...                2\n",
              "751350  2015-06-30 23:53:23.0000002  ...                1\n",
              "785182  2015-06-30 23:53:49.0000003  ...                1\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9j9LnIfcXcX"
      },
      "source": [
        "# Add time variables:\n",
        "\n",
        "df_train['hour'] = df_train['pickup_datetime'].dt.hour\n",
        "df_train['weekday'] = df_train['pickup_datetime'].dt.weekday\n",
        "df_train['month'] = df_train['pickup_datetime'].dt.month\n",
        "df_train['year'] = df_train['pickup_datetime'].dt.year"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVyFZIVIcaj3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d3886e2e-2301-4c2f-e346-1a584315f167"
      },
      "source": [
        "df_train = df_train.drop(['pickup_datetime','key'], axis = 1)\n",
        "df_train.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.5</td>\n",
              "      <td>-73.844311</td>\n",
              "      <td>40.721319</td>\n",
              "      <td>-73.841610</td>\n",
              "      <td>40.712278</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16.9</td>\n",
              "      <td>-74.016048</td>\n",
              "      <td>40.711303</td>\n",
              "      <td>-73.979268</td>\n",
              "      <td>40.782004</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.7</td>\n",
              "      <td>-73.982738</td>\n",
              "      <td>40.761270</td>\n",
              "      <td>-73.991242</td>\n",
              "      <td>40.750562</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>8</td>\n",
              "      <td>2011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.7</td>\n",
              "      <td>-73.987130</td>\n",
              "      <td>40.733143</td>\n",
              "      <td>-73.991567</td>\n",
              "      <td>40.758092</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.3</td>\n",
              "      <td>-73.968095</td>\n",
              "      <td>40.768008</td>\n",
              "      <td>-73.956655</td>\n",
              "      <td>40.783762</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2010</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fare_amount  pickup_longitude  pickup_latitude  ...  weekday  month  year\n",
              "0          4.5        -73.844311        40.721319  ...        0      6  2009\n",
              "1         16.9        -74.016048        40.711303  ...        1      1  2010\n",
              "2          5.7        -73.982738        40.761270  ...        3      8  2011\n",
              "3          7.7        -73.987130        40.733143  ...        5      4  2012\n",
              "4          5.3        -73.968095        40.768008  ...        1      3  2010\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVfm-KSqcdVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2525aeb4-41a2-425e-a564-169afb4d9c9c"
      },
      "source": [
        "# Remove negative fares and postive outliers:\n",
        "\n",
        "df_train = df_train[df_train.fare_amount>=0]\n",
        "df_train = df_train[df_train.fare_amount<=60]\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 997297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTVDAD2KchTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e29d335-316a-402c-ed3b-aa1a377c7231"
      },
      "source": [
        "# Remove missing data:\n",
        "\n",
        "df_train = df_train.dropna(how = 'any', axis = 'rows')\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 997288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUYksJ2cclVQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e88cddb-8b5d-4581-90a6-fdc7bffa1b6e"
      },
      "source": [
        "# June 2015 NYC taxi data (Wu et al, 2017):\n",
        "\n",
        "df_train = df_train[df_train.month==6]\n",
        "df_train = df_train[df_train.year==2015]\n",
        "print('New size: %d' % len(df_train))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New size: 11269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXgSHPyYcnuv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "e7219c0c-049b-4408-8eca-3e0b96b477b7"
      },
      "source": [
        "# Histogram fare plot:\n",
        "\n",
        "df_train[df_train.fare_amount<60].fare_amount.hist(bins=100, figsize=(16,5), color = \"red\")\n",
        "plt.xlabel('$USD')\n",
        "plt.title('NYC Taxi Fares: Dataset');\n",
        "plt.grid(b=None)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMSdAAjcr4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63c893f0-7b99-4278-9925-084427aeb3bc"
      },
      "source": [
        "y = df_train.fare_amount.values + 1e-10\n",
        "y ### for supervised learning: output vector y"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22.54,  8.  , 34.  , ...,  4.5 ,  6.5 ,  7.  ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FOeHvi3cu1n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "adc5ddfc-5f7d-4f8b-9168-1ca8e2ca60cd"
      },
      "source": [
        "# List first rows (post-cleaning):\n",
        "\n",
        "df_train.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fare_amount</th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "      <th>month</th>\n",
              "      <th>year</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>22.54</td>\n",
              "      <td>-74.010483</td>\n",
              "      <td>40.717667</td>\n",
              "      <td>-73.985771</td>\n",
              "      <td>40.660366</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>8.00</td>\n",
              "      <td>-74.010727</td>\n",
              "      <td>40.710091</td>\n",
              "      <td>-73.998100</td>\n",
              "      <td>40.722900</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>34.00</td>\n",
              "      <td>-73.974899</td>\n",
              "      <td>40.751095</td>\n",
              "      <td>-73.908546</td>\n",
              "      <td>40.881878</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>8.00</td>\n",
              "      <td>-73.961784</td>\n",
              "      <td>40.759579</td>\n",
              "      <td>-73.978943</td>\n",
              "      <td>40.772606</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>11.50</td>\n",
              "      <td>-73.957443</td>\n",
              "      <td>40.761703</td>\n",
              "      <td>-73.973236</td>\n",
              "      <td>40.787079</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     fare_amount  pickup_longitude  pickup_latitude  ...  weekday  month  year\n",
              "31         22.54        -74.010483        40.717667  ...        6      6  2015\n",
              "310         8.00        -74.010727        40.710091  ...        5      6  2015\n",
              "314        34.00        -73.974899        40.751095  ...        1      6  2015\n",
              "321         8.00        -73.961784        40.759579  ...        0      6  2015\n",
              "486        11.50        -73.957443        40.761703  ...        0      6  2015\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-lT9BBicw4P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7e08bfb0-e829-4eb0-fe61-708f4be96f02"
      },
      "source": [
        "X = df_train.drop(['fare_amount', 'month', 'year'], axis = 1)\n",
        "X.head() ### for supervised learning: input matrix X"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pickup_longitude</th>\n",
              "      <th>pickup_latitude</th>\n",
              "      <th>dropoff_longitude</th>\n",
              "      <th>dropoff_latitude</th>\n",
              "      <th>passenger_count</th>\n",
              "      <th>hour</th>\n",
              "      <th>weekday</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>-74.010483</td>\n",
              "      <td>40.717667</td>\n",
              "      <td>-73.985771</td>\n",
              "      <td>40.660366</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>-74.010727</td>\n",
              "      <td>40.710091</td>\n",
              "      <td>-73.998100</td>\n",
              "      <td>40.722900</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>-73.974899</td>\n",
              "      <td>40.751095</td>\n",
              "      <td>-73.908546</td>\n",
              "      <td>40.881878</td>\n",
              "      <td>0</td>\n",
              "      <td>23</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>-73.961784</td>\n",
              "      <td>40.759579</td>\n",
              "      <td>-73.978943</td>\n",
              "      <td>40.772606</td>\n",
              "      <td>4</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>-73.957443</td>\n",
              "      <td>40.761703</td>\n",
              "      <td>-73.973236</td>\n",
              "      <td>40.787079</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     pickup_longitude  pickup_latitude  ...  hour  weekday\n",
              "31         -74.010483        40.717667  ...    21        6\n",
              "310        -74.010727        40.710091  ...     9        5\n",
              "314        -73.974899        40.751095  ...    23        1\n",
              "321        -73.961784        40.759579  ...    21        0\n",
              "486        -73.957443        40.761703  ...    19        0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eC8SDPzczNY"
      },
      "source": [
        "### Optimum rmse: regression model objective function is Root Mean Square Error (RMSE); \n",
        "### Should be minimized (as close to zero as possible):\n",
        "\n",
        "y_global_orig = 0"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoTmWEhSc1qQ"
      },
      "source": [
        "### Bayesian Optimization - inputs:\n",
        "\n",
        "obj_func = 'XGBoost'\n",
        "n_start_AcqFunc = 100\n",
        "n_test = n_start_AcqFunc # test points\n",
        "df = 3 # nu\n",
        "\n",
        "util_loser = 'dEI_GP'\n",
        "util_winner = 'dERM_GP'\n",
        "n_init = 5 # random initialisations\n",
        "\n",
        "test_perc = 0.15\n",
        "train_perc = 1 - test_perc\n",
        "\n",
        "n_test = int(len(df_train) * test_perc)\n",
        "n_train = int(len(df_train) - n_test)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2ngnRxbc7cg"
      },
      "source": [
        "### Objective function:\n",
        "\n",
        "if obj_func == 'XGBoost': # 6-D\n",
        "            \n",
        "    # Constraints:\n",
        "    param_lb_alpha = 0\n",
        "    param_ub_alpha = 10\n",
        "    \n",
        "    param_lb_gamma = 0\n",
        "    param_ub_gamma = 10\n",
        "    \n",
        "    param_lb_max_depth = 5\n",
        "    param_ub_max_depth = 15\n",
        "    \n",
        "    param_lb_min_child_weight = 1\n",
        "    param_ub_min_child_weight = 20\n",
        "    \n",
        "    param_lb_subsample = .5\n",
        "    param_ub_subsample = 1\n",
        "    \n",
        "    param_lb_colsample = .1\n",
        "    param_ub_colsample = 1\n",
        "    \n",
        "    # 6-D inputs' parameter bounds:\n",
        "    param = { 'alpha':  ('cont', (param_lb_alpha, param_ub_alpha)),\n",
        "         'gamma':  ('cont', (param_lb_gamma, param_ub_gamma)),     \n",
        "         'max_depth':  ('int', (param_lb_max_depth, param_ub_max_depth)),\n",
        "         'subsample':  ('cont', (param_lb_subsample, param_ub_subsample)),\n",
        "          'min_child_weight':  ('int', (param_lb_min_child_weight, param_ub_min_child_weight)),\n",
        "            'colsample': ('cont', (param_lb_colsample, param_ub_colsample))\n",
        "        }\n",
        "       \n",
        "    # True y bounds:\n",
        "    dim = 6\n",
        "    \n",
        "    max_iter = 20  # iterations of Bayesian optimization\n",
        "    \n",
        "    operator = 1 \n",
        "    \n",
        "    n_est = 3"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmJsNX29c_xA"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\n",
        "\n",
        "def l2norm_(X, Xstar):\n",
        "    \n",
        "    return cdist(X, Xstar)\n",
        "\n",
        "def kronDelta(X, Xstar):\n",
        "\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\n",
        "\n",
        "class squaredExponentialDeriv(squaredExponential):\n",
        "    \n",
        "    def K(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\n",
        "        return K\n",
        "    \n",
        "    def dK(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\n",
        "        return dK\n",
        "    \n",
        "        \n",
        "    def d2K(self, X, Xstar):\n",
        "        \n",
        "        r = (l2norm_(X, Xstar)/self.l)\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (1 - r **2)\n",
        "        return d2K\n",
        "    \n",
        "cov_func = squaredExponentialDeriv()\n",
        "d_cov_func = squaredExponentialDeriv()\n",
        "    "
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9ZuEB2VdE0W"
      },
      "source": [
        "### Set-seeds:\n",
        "\n",
        "run_num_1 = 111\n",
        "run_num_2 = 113\n",
        "run_num_3 = 3333\n",
        "run_num_4 = 4444\n",
        "run_num_5 = 5555\n",
        "run_num_6 = 6\n",
        "run_num_7 = 7777\n",
        "run_num_8 = 8878\n",
        "run_num_9 = 999\n",
        "run_num_10 = 1000\n",
        "run_num_11 = 1113\n",
        "run_num_12 = 1234\n",
        "run_num_13 = 234\n",
        "run_num_14 = 888\n",
        "run_num_15 = 1557\n",
        "run_num_16 = 1666\n",
        "run_num_17 = 71\n",
        "run_num_18 = 8\n",
        "run_num_19 = 1999\n",
        "run_num_20 = 2000"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgHMFEyPdCk4"
      },
      "source": [
        "### Cumulative Regret Calculator:\n",
        "\n",
        "def min_max_array(x):\n",
        "    new_list = []\n",
        "    for i, num in enumerate(x):\n",
        "            new_list.append(np.min(x[0:i+1]))\n",
        "    return new_list"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJMhL70fdHz_"
      },
      "source": [
        "class Acquisition_new(Acquisition):    \n",
        "    def __init__(self, mode, eps=1e-08, **params):\n",
        "        \n",
        "        self.params = params\n",
        "        self.eps = eps\n",
        "\n",
        "        mode_dict = {\n",
        "            'dEI_GP': self.dEI_GP,\n",
        "            'dERM_GP': self.dERM_GP\n",
        "        }\n",
        "\n",
        "        self.f = mode_dict[mode]\n",
        "    \n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\n",
        "        \n",
        "        dsdx = ds / 2 * (std + self.eps)\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\n",
        "        \n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\n",
        "            \n",
        "        return f, df, d2f\n",
        "\n",
        "    def dERM_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\n",
        "        z = -1 * (y_global_orig - mean - self.eps) / (std + self.eps)\n",
        "        \n",
        "        dsdx = ds / 2 * (std + self.eps)\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\n",
        "        \n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\n",
        "            \n",
        "        return f, df, d2f\n",
        "    \n",
        "    def _eval(self, tau, mean, std):\n",
        "    \n",
        "        return self.f(tau, mean, std, **self.params)\n",
        "    \n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\n",
        "    \n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, **self.params)\n",
        "\n",
        "    def d_eval_stp(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\n",
        "    \n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0, **self.params)\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comHkJv9dH_O"
      },
      "source": [
        "### Surrogate derivatives: \n",
        "\n",
        "from scipy.linalg import cholesky, solve\n",
        "\n",
        "class dGaussianProcess(GaussianProcess):\n",
        "    l = 1\n",
        "    sigmaf = 1\n",
        "    sigman = 1e-6\n",
        "\n",
        "    def AcqGrad(self, Xstar, return_std=False):\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\n",
        "        L = cholesky(K).T\n",
        "        alpha = solve(L.T, solve(L, self.y))\n",
        "        Xstar = np.atleast_2d(Xstar)\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\n",
        "        v = solve(self.L, Kstar.T)\n",
        "        dv = solve(self.L, dKstar.T)\n",
        "        d2v = solve(self.L, d2Kstar.T)\n",
        "        \n",
        "        ds = -2 * np.dot(dv.T, v)\n",
        "        dvdv = np.dot(dv.T, dv)\n",
        "        d2s = -2 * (dvdv + d2v)\n",
        "        \n",
        "        dm = np.dot(dKstar, alpha)\n",
        "        d2m = np.dot(d2Kstar, alpha)\n",
        "        return ds, dm, dvdv, d2v, d2m\n",
        "\n",
        "class dtStudentProcess(tStudentProcess):\n",
        "    l = 1\n",
        "    sigmaf = 1\n",
        "    sigman = 1e-6\n",
        "    \n",
        "    def AcqGrad(self, Xstar, return_std=False):\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\n",
        "        L = cholesky(self.K11).T\n",
        "        alpha = solve(L.T, solve(L, self.y))\n",
        "        Xstar = np.atleast_2d(Xstar)\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\n",
        "        v = solve(L, Kstar.T)\n",
        "        dv = solve(L, dKstar.T)\n",
        "        d2v = solve(L, d2Kstar.T)\n",
        "        \n",
        "        ds = -2 * np.dot(dv.T, v)\n",
        "        dvdv = np.dot(dv.T, dv)\n",
        "        d2s = -2 * (dvdv + d2v)\n",
        "        \n",
        "        dm = np.dot(dKstar, alpha)\n",
        "        d2m = np.dot(d2Kstar, alpha)\n",
        "        return ds, dm, dvdv, d2v, d2m"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S422jNLsdIMm"
      },
      "source": [
        "class dGPGO(GPGO):  \n",
        "    n_start = n_start_AcqFunc\n",
        "    eps = 1e-08\n",
        "        \n",
        "    def func(self, xnew):\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "        new_std = np.sqrt(new_var + 1e-6)\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\n",
        "        f  = np.empty((self.n_start,))\n",
        "        df = np.empty((self.n_start,))\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\n",
        "        df_array = np.full((len(xnew),),df)\n",
        "        return f, df_array\n",
        "        \n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\n",
        "        start_points_arr = np.array([list(s.values())\n",
        "                                     for s in start_points_dict])\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\n",
        "        f_best = np.empty((n_start,))\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\n",
        "                                                                 x0=start_point,\n",
        "                                                                 method=method,\n",
        "                                                                 jac = True,\n",
        "                                                                 bounds=self.parameter_range) for start_point in\n",
        "                                               start_points_arr)\n",
        "        x_best = np.array([res.x for res in opt])\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
        "\n",
        "        self.x_best = x_best\n",
        "        self.f_best = f_best\n",
        "        self.best = x_best[np.argmin(f_best)]\n",
        "    \n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\n",
        "        \n",
        "        if not resume:\n",
        "            self.init_evals = init_evals\n",
        "            self._firstRun(self.init_evals)\n",
        "            self.logger._printInit(self)\n",
        "        for iteration in range(max_iter):\n",
        "            self.d_optimizeAcq()\n",
        "            self.updateGP()\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwEwZD0qdIPO"
      },
      "source": [
        "## dGPGO_stp - BayesOpt derivatives' class: Student's-t\n",
        "\n",
        "class dGPGO_stp(GPGO):  \n",
        "    n_start = 100\n",
        "        \n",
        "    def func_stp(self, xnew):\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\n",
        "        new_std = np.sqrt(new_var + 1e-6)\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\n",
        "        f  = np.empty((self.n_start,))\n",
        "        df = np.empty((self.n_start,))\n",
        "        f  = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[0]\n",
        "        df = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[1]\n",
        "        df_array = np.full((len(xnew),),df)\n",
        "        return f, df_array\n",
        "        \n",
        "    def d_optimizeAcq_stp(self, method='L-BFGS-B', n_start=n_start):\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\n",
        "        start_points_arr = np.array([list(s.values())\n",
        "                                     for s in start_points_dict])\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\n",
        "        f_best = np.empty((n_start,))\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func_stp,\n",
        "                                                                 x0=start_point,\n",
        "                                                                 method=method,\n",
        "                                                                 jac = True,\n",
        "                                                                 bounds=self.parameter_range) for start_point in\n",
        "                                               start_points_arr)\n",
        "        x_best = np.array([res.x for res in opt])\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\n",
        "\n",
        "        self.x_best = x_best\n",
        "        self.f_best = f_best\n",
        "        self.best = x_best[np.argmin(f_best)]\n",
        "    \n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\n",
        "        \n",
        "        if not resume:\n",
        "            self.init_evals = init_evals\n",
        "            self._firstRun(self.init_evals)\n",
        "            self.logger._printInit(self)\n",
        "        for iteration in range(max_iter):\n",
        "            self.d_optimizeAcq_stp()\n",
        "            self.updateGP()\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlilveEgdIR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab3ea49-fa21-4cb4-8864-56605497e1b7"
      },
      "source": [
        "start_lose = time.time()\n",
        "start_lose"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1614594328.9489803"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wlzDSHbUG-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cb7ff3a-b68e-4833-8544-3d4578bae9a1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\n",
        "\n",
        "np.random.seed(run_num_1)\n",
        "surrogate_loser_1 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
        "\n",
        "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_1 = dGPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity1, param, n_jobs = -1) # define BayesOpt\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_1 = loser_1.getResult()[0]\n",
        "params_loser_1['max_depth'] = int(params_loser_1['max_depth'])\n",
        "params_loser_1['min_child_weight'] = int(params_loser_1['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train1 = xgb.DMatrix(X_train1, y_train1)\n",
        "dX_loser_test1 = xgb.DMatrix(X_test1, y_test1)\n",
        "model_loser_1 = xgb.train(params_loser_1, dX_loser_train1)\n",
        "pred_loser_1 = model_loser_1.predict(dX_loser_test1)\n",
        "\n",
        "rmse_loser_1 = np.sqrt(mean_squared_error(pred_loser_1, y_test1))\n",
        "rmse_loser_1"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  -0.5875926438135084 \t -0.45245711231879204\n",
            "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  -0.6811472770543097 \t -0.45245711231879204\n",
            "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  -0.46117277972616416 \t -0.45245711231879204\n",
            "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  -0.45245711231879204 \t -0.45245711231879204\n",
            "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  -0.5612893767574513 \t -0.45245711231879204\n",
            "1      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6826322236836013 \t -0.45245711231879204\n",
            "2      \t [ 9.60774341  8.94642175  5.          0.60295867 15.          0.76784922]. \t  -0.5001571385184761 \t -0.45245711231879204\n",
            "3      \t [ 0.4810761   9.31563532  8.          0.92845546 14.          0.70014982]. \t  -0.5056468669267833 \t -0.45245711231879204\n",
            "4      \t [ 9.85849625  0.83439907  7.          0.7705582  19.          0.15487995]. \t  -0.6815452883308839 \t -0.45245711231879204\n",
            "5      \t [ 9.1348132   9.54667443 13.          0.95557113 18.          0.11016711]. \t  -0.6791252728059957 \t -0.45245711231879204\n",
            "6      \t [ 9.97122318  8.75901621 13.          0.9904996   9.          0.60161775]. \t  -0.4895864123043516 \t -0.45245711231879204\n",
            "7      \t [7.19079936 0.87565629 5.         0.54470564 8.         0.4449021 ]. \t  -0.5736147518216788 \t -0.45245711231879204\n",
            "8      \t [ 5.24989998  8.90398408 13.          0.54165135  1.          0.74771929]. \t  -0.46141851807096074 \t -0.45245711231879204\n",
            "9      \t [ 1.7157918   5.89141512 14.          0.5296881  10.          0.1803042 ]. \t  -0.6825568503758982 \t -0.45245711231879204\n",
            "10     \t [ 1.33491429  4.9488224  14.          0.57825982 18.          0.42075517]. \t  -0.5921600742966667 \t -0.45245711231879204\n",
            "11     \t [8.99095022 4.91206514 6.         0.87212346 1.         0.53198502]. \t  -0.5638457176592562 \t -0.45245711231879204\n",
            "12     \t [0.51753753 1.68511981 9.         0.80360039 9.         0.45200858]. \t  -0.5426731188677099 \t -0.45245711231879204\n",
            "13     \t [ 0.47819106  0.69361363 14.          0.7271902   3.          0.48615024]. \t  -0.5388370338312534 \t -0.45245711231879204\n",
            "14     \t [ 3.39786838  7.15897283  5.          0.7138544  19.          0.22047835]. \t  -0.6821252760743777 \t -0.45245711231879204\n",
            "15     \t [0.4939583  9.37390762 8.         0.7641092  1.         0.4917045 ]. \t  -0.5481894484993145 \t -0.45245711231879204\n",
            "16     \t [ 8.70209222  0.21363429 14.          0.98391211 18.          0.50653374]. \t  -0.5341942124085062 \t -0.45245711231879204\n",
            "17     \t [ 5.30693639  9.58195731  9.          0.53796245 11.          0.46099251]. \t  -0.5549909911648074 \t -0.45245711231879204\n",
            "18     \t [ 3.83524595  9.61883757 12.          0.81445794 17.          0.17700713]. \t  -0.6797728963073484 \t -0.45245711231879204\n",
            "19     \t [ 0.25234021  8.87254669 14.          0.7101496   5.          0.12345742]. \t  -0.684133223230854 \t -0.45245711231879204\n",
            "20     \t [9.81917647 9.19103216 5.         0.82619792 9.         0.10869763]. \t  -0.6822789922076578 \t -0.45245711231879204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.87018129255363"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClJ9rN2KUJzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6e1df95-3307-419d-94d7-bacb2f3c45ea"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\n",
        "\n",
        "np.random.seed(run_num_2)\n",
        "surrogate_loser_2 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
        "\n",
        "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_2 = dGPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity2, param, n_jobs = -1) # define BayesOpt\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_2 = loser_2.getResult()[0]\n",
        "params_loser_2['max_depth'] = int(params_loser_2['max_depth'])\n",
        "params_loser_2['min_child_weight'] = int(params_loser_2['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train2 = xgb.DMatrix(X_train2, y_train2)\n",
        "dX_loser_test2 = xgb.DMatrix(X_test2, y_test2)\n",
        "model_loser_2 = xgb.train(params_loser_2, dX_loser_train2)\n",
        "pred_loser_2 = model_loser_2.predict(dX_loser_test2)\n",
        "\n",
        "rmse_loser_2 = np.sqrt(mean_squared_error(pred_loser_2, y_test2))\n",
        "rmse_loser_2"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.51985493  0.73903599  7.          0.71824677 17.          0.61827209]. \t  -0.5233872966482411 \t -0.42848969661986275\n",
            "init   \t [ 8.4047092   4.35120551 12.          0.84231905 16.          0.80172484]. \t  -0.42848969661986275 \t -0.42848969661986275\n",
            "init   \t [ 6.42749368  9.61026168 10.          0.73363448 15.          0.84932207]. \t  -0.43419399571353007 \t -0.42848969661986275\n",
            "init   \t [ 2.66008363  8.36685389  6.          0.92082163 12.          0.69999264]. \t  -0.5353675801921922 \t -0.42848969661986275\n",
            "init   \t [ 1.74779478  2.17691512 12.          0.71794562  2.          0.48990646]. \t  -0.5346620615324694 \t -0.42848969661986275\n",
            "1      \t [0.65899411 0.08911785 5.         0.57513718 8.         0.6649358 ]. \t  -0.5435492302872315 \t -0.42848969661986275\n",
            "2      \t [7.50154303 9.49874736 7.         0.83842364 4.         0.5714185 ]. \t  -0.5422231033003287 \t -0.42848969661986275\n",
            "3      \t [9.88263606 1.71181353 7.         0.97275193 1.         0.74825108]. \t  -0.441747506473179 \t -0.42848969661986275\n",
            "4      \t [ 0.50045611  0.9041258  14.          0.89701685 13.          0.41057236]. \t  -0.5651338432083423 \t -0.42848969661986275\n",
            "5      \t [ 8.92213252  4.56062357 13.          0.93869871  8.          0.53052713]. \t  -0.5289605254310144 \t -0.42848969661986275\n",
            "6      \t [ 3.60181415  8.33097203 14.          0.9882562   5.          0.89449689]. \t  \u001b[92m-0.4137972346428497\u001b[0m \t -0.4137972346428497\n",
            "7      \t [ 0.11546318  7.06183776 14.          0.68323086 19.          0.99255453]. \t  -0.42239160139562504 \t -0.4137972346428497\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6860882831213788 \t -0.4137972346428497\n",
            "9      \t [ 0.10082071  2.20765876  6.          0.79154439 17.          0.83321591]. \t  -0.45448270216379194 \t -0.4137972346428497\n",
            "10     \t [0.32359376 9.19341132 6.         0.74153554 1.         0.25113354]. \t  -0.6853813278526151 \t -0.4137972346428497\n",
            "11     \t [ 9.07851132  6.67468648 14.          0.65555017  1.          0.96610958]. \t  -0.424975755228691 \t -0.4137972346428497\n",
            "12     \t [ 9.80974078  5.67295504  6.          0.91127307 13.          0.44957804]. \t  -0.5591662642332185 \t -0.4137972346428497\n",
            "13     \t [ 2.77198081  0.67245678 14.          0.56883806 19.          0.9553332 ]. \t  -0.4333861030323646 \t -0.4137972346428497\n",
            "14     \t [6.86928577 3.72419603 7.         0.55484489 8.         0.11419186]. \t  -0.6867308079831548 \t -0.4137972346428497\n",
            "15     \t [ 0.83926825  9.31286624 13.          0.83595717 11.          0.92691071]. \t  -0.41668725948491847 \t -0.4137972346428497\n",
            "16     \t [ 2.2599948   8.5777314   7.          0.92101066 18.          0.81278145]. \t  -0.44458847008023056 \t -0.4137972346428497\n",
            "17     \t [ 7.48756016  0.13555104 14.          0.75269236 12.          0.41177876]. \t  -0.5678014017572689 \t -0.4137972346428497\n",
            "18     \t [2.94548912 4.25555055 5.         0.68007425 4.         0.7533029 ]. \t  -0.45906401928062657 \t -0.4137972346428497\n",
            "19     \t [ 8.06744935  9.40296538  6.          0.60147647 19.          0.17678037]. \t  -0.6864884735391246 \t -0.4137972346428497\n",
            "20     \t [ 3.87077387  0.04333662 11.          0.79311966  8.          0.7797114 ]. \t  -0.423652004810958 \t -0.4137972346428497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.343487868410211"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-45l3NU4UNiI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cef64b7-49eb-46c9-d65a-341332d5c058"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\n",
        "\n",
        "np.random.seed(run_num_3)\n",
        "surrogate_loser_3 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
        "\n",
        "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_3 = dGPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity3, param, n_jobs = -1) # define BayesOpt\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_3 = loser_3.getResult()[0]\n",
        "params_loser_3['max_depth'] = int(params_loser_3['max_depth'])\n",
        "params_loser_3['min_child_weight'] = int(params_loser_3['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train3 = xgb.DMatrix(X_train3, y_train3)\n",
        "dX_loser_test3 = xgb.DMatrix(X_test3, y_test3)\n",
        "model_loser_3 = xgb.train(params_loser_3, dX_loser_train3)\n",
        "pred_loser_3 = model_loser_3.predict(dX_loser_test3)\n",
        "\n",
        "rmse_loser_3 = np.sqrt(mean_squared_error(pred_loser_3, y_test3))\n",
        "rmse_loser_3"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.51575607  1.09251275 13.          0.80377147 16.          0.31229624]. \t  -0.6858957949546655 \t -0.5081673732303724\n",
            "init   \t [6.12794891 1.19652403 5.         0.74822174 8.         0.33695838]. \t  -0.6849895205987827 \t -0.5081673732303724\n",
            "init   \t [ 6.21631223  4.4909976  10.          0.66764529  9.          0.77207323]. \t  -0.5081673732303724 \t -0.5081673732303724\n",
            "init   \t [ 0.58127233  4.79918477 11.          0.64526552 15.          0.62431773]. \t  -0.5106501747238601 \t -0.5081673732303724\n",
            "init   \t [ 5.76228258  0.67219724  7.          0.88228691 13.          0.29867219]. \t  -0.6839256266327581 \t -0.5081673732303724\n",
            "1      \t [9.79994489 9.57810833 8.         0.93763368 1.         0.1772549 ]. \t  -0.7316297465421655 \t -0.5081673732303724\n",
            "2      \t [ 3.27075931  5.07207678 14.          0.99587284  2.          0.51564135]. \t  -0.6620507374994331 \t -0.5081673732303724\n",
            "3      \t [0.         0.         5.         0.5        1.19792586 0.1       ]. \t  -0.732320887566852 \t -0.5081673732303724\n",
            "4      \t [ 8.88346709  9.03427619  8.          0.525814   19.          0.22866902]. \t  -0.732406939137863 \t -0.5081673732303724\n",
            "5      \t [1.21637479 8.34226514 5.         0.57939054 5.         0.55479603]. \t  -0.6531491980452168 \t -0.5081673732303724\n",
            "6      \t [ 2.93278685  9.15897164 14.          0.67343746 10.          0.85376406]. \t  \u001b[92m-0.504811636744922\u001b[0m \t -0.504811636744922\n",
            "7      \t [ 1.1995358   9.87930749  5.          0.84448568 18.          0.26022822]. \t  -0.7297224033867351 \t -0.504811636744922\n",
            "8      \t [ 1.96525323  0.16048396 14.          0.65133015  9.          0.3003508 ]. \t  -0.6860931128073492 \t -0.504811636744922\n",
            "9      \t [6.19566593 0.54575662 9.         0.53247577 1.         0.58149275]. \t  -0.5171483404397559 \t -0.504811636744922\n",
            "10     \t [ 3.97093318  9.58939295  6.          0.5381492  12.          0.74760437]. \t  -0.5217629748454513 \t -0.504811636744922\n",
            "11     \t [ 9.94719165  9.26136843 13.          0.64663072 11.          0.47018827]. \t  -0.6546346492088932 \t -0.504811636744922\n",
            "12     \t [9.55919323 7.66342182 6.         0.63144136 7.         0.82664124]. \t  -0.5195220027397864 \t -0.504811636744922\n",
            "13     \t [ 2.80079796  0.61761702  5.          0.62606225 19.          0.71334647]. \t  -0.5274647167409906 \t -0.504811636744922\n",
            "14     \t [ 9.59599852  3.11576364  5.          0.59256945 17.          0.56621848]. \t  -0.6538676177721936 \t -0.504811636744922\n",
            "15     \t [ 8.91533927  0.16704192 13.          0.66647549  6.          0.25014   ]. \t  -0.7314177593008601 \t -0.504811636744922\n",
            "16     \t [ 4.33253274  9.14516752 14.          0.94398901 19.          0.74581708]. \t  \u001b[92m-0.5007721901039625\u001b[0m \t -0.5007721901039625\n",
            "17     \t [9.75857066 2.84550968 5.         0.7181157  3.         0.67923616]. \t  -0.5274197984747595 \t -0.5007721901039625\n",
            "18     \t [ 0.          0.          5.          0.5        10.44274008  0.1       ]. \t  -0.7323784626096901 \t -0.5007721901039625\n",
            "19     \t [ 9.94497871  9.34276152 14.          0.66308198  5.          0.66061477]. \t  -0.5102482930096841 \t -0.5007721901039625\n",
            "20     \t [ 0.45956381  8.89338741 11.          0.67604128  4.          0.20043998]. \t  -0.7307540586635348 \t -0.5007721901039625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.6249009343932785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voPfk1UDUQU0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f7b309-5a3e-4464-e8db-e4b0e0018457"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\n",
        "\n",
        "np.random.seed(run_num_4)\n",
        "surrogate_loser_4 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
        "\n",
        "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_4 = dGPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity4, param, n_jobs = -1) # define BayesOpt\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_4 = loser_4.getResult()[0]\n",
        "params_loser_4['max_depth'] = int(params_loser_4['max_depth'])\n",
        "params_loser_4['min_child_weight'] = int(params_loser_4['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train4 = xgb.DMatrix(X_train4, y_train4)\n",
        "dX_loser_test4 = xgb.DMatrix(X_test4, y_test4)\n",
        "model_loser_4 = xgb.train(params_loser_4, dX_loser_train4)\n",
        "pred_loser_4 = model_loser_4.predict(dX_loser_test4)\n",
        "\n",
        "rmse_loser_4 = np.sqrt(mean_squared_error(pred_loser_4, y_test4))\n",
        "rmse_loser_4"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.42798506  1.23319667 14.          0.81910755  7.          0.7021549 ]. \t  -0.5581090891563811 \t -0.5581090891563811\n",
            "init   \t [ 5.54158317  7.71233962 10.          0.59117644  6.          0.31476458]. \t  -0.6204181120564479 \t -0.5581090891563811\n",
            "init   \t [2.18282634 2.93794127 7.         0.84739327 5.         0.62716229]. \t  -0.5626656925564042 \t -0.5581090891563811\n",
            "init   \t [ 5.66159581  4.68678814 11.          0.54147808  9.          0.58609221]. \t  -0.5595885941852147 \t -0.5581090891563811\n",
            "init   \t [ 3.02160958  6.07432614  6.          0.92038559 14.          0.12778609]. \t  -0.6361772582153176 \t -0.5581090891563811\n",
            "1      \t [ 1.89460697  0.10324856 13.          0.70356349 16.          0.67776197]. \t  \u001b[92m-0.5568243993303088\u001b[0m \t -0.5568243993303088\n",
            "2      \t [ 9.69969     4.32607677 14.          0.87739158 16.          0.66014279]. \t  -0.5573098211239071 \t -0.5568243993303088\n",
            "3      \t [ 2.2210328   8.31054196 13.          0.70317243 19.          0.56953182]. \t  -0.573504425404327 \t -0.5568243993303088\n",
            "4      \t [ 0.14706294  0.75702217 14.          0.56309891  8.          0.29887971]. \t  -0.6254087880261816 \t -0.5568243993303088\n",
            "5      \t [ 6.94218243  0.32144048  6.          0.59286079 19.          0.70636825]. \t  -0.5706574487524134 \t -0.5568243993303088\n",
            "6      \t [ 9.24159749  9.43575685  7.          0.98380459 19.          0.34937346]. \t  -0.6187589419936593 \t -0.5568243993303088\n",
            "7      \t [7.57184449 0.0324327  9.         0.86318723 1.         0.6413689 ]. \t  -0.5611723012436254 \t -0.5568243993303088\n",
            "8      \t [ 1.62755426  6.7921099  14.          0.91903975  1.          0.61791343]. \t  -0.5680865264437001 \t -0.5568243993303088\n",
            "9      \t [ 9.43074486  0.98044049  5.          0.96290856 10.          0.27397556]. \t  -0.6390681992417737 \t -0.5568243993303088\n",
            "10     \t [ 8.73128599  9.22658128 14.          0.98344346  2.          0.13489277]. \t  -0.6368164613179 \t -0.5568243993303088\n",
            "11     \t [ 0.          0.          7.37879691  0.5        12.37879691  0.1       ]. \t  -0.6353530029272869 \t -0.5568243993303088\n",
            "12     \t [8.91398595 7.00476796 6.         0.72156795 1.         0.41673037]. \t  -0.6210807206819127 \t -0.5568243993303088\n",
            "13     \t [ 9.58851274  9.75423983 14.          0.60815214 12.          0.97572229]. \t  \u001b[92m-0.44045254560081915\u001b[0m \t -0.44045254560081915\n",
            "14     \t [ 9.87497198  6.68701692  6.          0.55980787 13.          0.32605223]. \t  -0.6200123103666009 \t -0.44045254560081915\n",
            "15     \t [ 2.16464782  9.35692886 11.          0.97431128 12.          0.43257419]. \t  -0.5747879420555002 \t -0.44045254560081915\n",
            "16     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6351353738423114 \t -0.44045254560081915\n",
            "17     \t [0.94508697 7.16234343 8.48038022 0.5        1.48038022 0.1       ]. \t  -0.6346230590373485 \t -0.44045254560081915\n",
            "18     \t [7.36355448 5.3077281  5.         0.5709887  7.         0.87569884]. \t  -0.467633753337906 \t -0.44045254560081915\n",
            "19     \t [ 7.93069008  0.11830338 11.          0.97110397 13.          0.22887984]. \t  -0.6356672520315936 \t -0.44045254560081915\n",
            "20     \t [ 6.39787933  3.84205635 14.          0.72880814  3.          0.21974221]. \t  -0.6364956155454153 \t -0.44045254560081915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.469425637772456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kEnTd7MUdlv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5cbd161-e811-443a-caaf-f7cb11f8cb48"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\n",
        "\n",
        "np.random.seed(run_num_5)\n",
        "surrogate_loser_5 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
        "\n",
        "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_5 = dGPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity5, param, n_jobs = -1) # define BayesOpt\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_5 = loser_5.getResult()[0]\n",
        "params_loser_5['max_depth'] = int(params_loser_5['max_depth'])\n",
        "params_loser_5['min_child_weight'] = int(params_loser_5['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train5 = xgb.DMatrix(X_train5, y_train5)\n",
        "dX_loser_test5 = xgb.DMatrix(X_test5, y_test5)\n",
        "model_loser_5 = xgb.train(params_loser_5, dX_loser_train5)\n",
        "pred_loser_5 = model_loser_5.predict(dX_loser_test5)\n",
        "\n",
        "rmse_loser_5 = np.sqrt(mean_squared_error(pred_loser_5, y_test5))\n",
        "rmse_loser_5"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.57353274  5.41760627  8.          0.53710825 19.          0.26034376]. \t  -0.6523797617249512 \t -0.466901759574539\n",
            "init   \t [5.17826437 9.78817462 5.         0.84048437 6.         0.79712615]. \t  -0.4886603987288415 \t -0.466901759574539\n",
            "init   \t [ 9.84476201  9.65484856  5.          0.59464714 19.          0.34528747]. \t  -0.564220001422124 \t -0.466901759574539\n",
            "init   \t [ 9.17476842  4.53262239 13.          0.64368565 10.          0.515308  ]. \t  -0.5049042831858346 \t -0.466901759574539\n",
            "init   \t [ 8.66719465  7.99685372 12.          0.79833571 17.          0.72443466]. \t  -0.466901759574539 \t -0.466901759574539\n",
            "1      \t [ 3.31480734  6.81602635 12.          0.95330947  1.          0.27334927]. \t  -0.6531167190113543 \t -0.466901759574539\n",
            "2      \t [2.53140694 0.29740057 9.         0.7505917  7.         0.82287404]. \t  -0.47475119282304395 \t -0.466901759574539\n",
            "3      \t [ 9.84728748  0.28222034 13.          0.93969018  3.          0.22339517]. \t  -0.6522247903981228 \t -0.466901759574539\n",
            "4      \t [ 0.68116737  3.94603776 14.          0.80380753 14.          0.77317996]. \t  \u001b[92m-0.4613270217842209\u001b[0m \t -0.4613270217842209\n",
            "5      \t [ 9.6928838   0.38736829  6.          0.56167539 11.          0.4775038 ]. \t  -0.5239028011959734 \t -0.4613270217842209\n",
            "6      \t [ 1.62699639  9.0444625   7.          0.60549869 13.          0.13190699]. \t  -0.6526723866694732 \t -0.4613270217842209\n",
            "7      \t [1.904453  0.        5.1118395 0.5       1.        0.1      ]. \t  -0.654517615956795 \t -0.4613270217842209\n",
            "8      \t [ 9.56204849  0.80573312 12.          0.60476955 16.          0.23612747]. \t  -0.6524022297735614 \t -0.4613270217842209\n",
            "9      \t [ 1.05461775  1.17661042  5.          0.5648338  17.          0.90649901]. \t  -0.4702337390488589 \t -0.4613270217842209\n",
            "10     \t [ 0.29281635  9.83144868 11.          0.57517626 19.          0.13846438]. \t  -0.6543975191191339 \t -0.4613270217842209\n",
            "11     \t [9.66604659 1.51730123 5.         0.59597224 4.         0.66643448]. \t  -0.5016316198301449 \t -0.4613270217842209\n",
            "12     \t [0.57140531 6.2823043  5.         0.70475062 2.         0.3328708 ]. \t  -0.5594485185267623 \t -0.4613270217842209\n",
            "13     \t [ 2.5240907   6.84169344 14.          0.61623097  8.          0.50182056]. \t  -0.5037582468108619 \t -0.4613270217842209\n",
            "14     \t [0.0858834  4.38043443 5.         0.57114514 9.         0.11102852]. \t  -0.6531285980012218 \t -0.4613270217842209\n",
            "15     \t [ 9.08851157  7.7443441   6.          0.74314397 12.          0.52508293]. \t  -0.5238407378055661 \t -0.4613270217842209\n",
            "16     \t [ 0.62266475  1.9018111  14.          0.63329124  3.          0.94054341]. \t  \u001b[92m-0.42529491633232686\u001b[0m \t -0.42529491633232686\n",
            "17     \t [ 9.23494676  7.58606552 12.          0.78487568  2.          0.96478853]. \t  -0.42939677763278594 \t -0.42529491633232686\n",
            "18     \t [ 5.11817103  4.38089559  9.          0.53776664 13.          0.7209118 ]. \t  -0.4792507830025482 \t -0.42529491633232686\n",
            "19     \t [8.44899175 7.77761284 6.         0.53217155 1.         0.55958377]. \t  -0.5238817589404079 \t -0.42529491633232686\n",
            "20     \t [ 3.67612744  5.42599858 14.          0.93140269 19.          0.80853077]. \t  -0.4626498801811607 \t -0.42529491633232686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.31722690080873"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjVSH6caUgyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42acb008-d170-4b55-f909-5f0ea3a88253"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\n",
        "\n",
        "np.random.seed(run_num_6)\n",
        "surrogate_loser_6 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
        "\n",
        "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_6 = dGPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity6, param, n_jobs = -1) # define BayesOpt\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_6 = loser_6.getResult()[0]\n",
        "params_loser_6['max_depth'] = int(params_loser_6['max_depth'])\n",
        "params_loser_6['min_child_weight'] = int(params_loser_6['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train6 = xgb.DMatrix(X_train6, y_train6)\n",
        "dX_loser_test6 = xgb.DMatrix(X_test6, y_test6)\n",
        "model_loser_6 = xgb.train(params_loser_6, dX_loser_train6)\n",
        "pred_loser_6 = model_loser_6.predict(dX_loser_test6)\n",
        "\n",
        "rmse_loser_6 = np.sqrt(mean_squared_error(pred_loser_6, y_test6))\n",
        "rmse_loser_6"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  -0.5471181110640053 \t -0.5219867521874206\n",
            "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  -0.629286701398377 \t -0.5219867521874206\n",
            "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  -0.5219867521874206 \t -0.5219867521874206\n",
            "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  -0.5476166257785141 \t -0.5219867521874206\n",
            "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  -0.6655867879161768 \t -0.5219867521874206\n",
            "1      \t [ 7.09628606  0.82458553  6.          0.52978945 17.          0.78827589]. \t  -0.5368551114917748 \t -0.5219867521874206\n",
            "2      \t [ 1.88885215  8.62524025  6.          0.90916563 14.          0.40379086]. \t  -0.6680072912825885 \t -0.5219867521874206\n",
            "3      \t [ 0.5654966   9.52584762 14.          0.82016688  4.          0.10717254]. \t  -0.7389555129670606 \t -0.5219867521874206\n",
            "4      \t [ 9.63033822  0.03793409 13.          0.68076302  7.          0.91391681]. \t  \u001b[92m-0.4301920669254681\u001b[0m \t -0.4301920669254681\n",
            "5      \t [ 9.97469028  9.15579642 12.          0.66745021 19.          0.28959158]. \t  -0.6668248751122167 \t -0.4301920669254681\n",
            "6      \t [ 9.85880187  8.48475629 13.          0.76006171  2.          0.58751173]. \t  -0.5403277210407594 \t -0.4301920669254681\n",
            "7      \t [ 9.51844     9.62786679  5.          0.56989798 15.          0.51068013]. \t  -0.6345288616058514 \t -0.4301920669254681\n",
            "8      \t [ 0.15151883  3.04875397  5.          0.61495161 19.          0.79420245]. \t  -0.5357132607573185 \t -0.4301920669254681\n",
            "9      \t [ 3.08077333  0.89569015  6.          0.84149914 11.          0.94992477]. \t  -0.4472711154368675 \t -0.4301920669254681\n",
            "10     \t [ 0.19816257  1.95434851 14.          0.59292238  7.          0.17971262]. \t  -0.7383261448869979 \t -0.4301920669254681\n",
            "11     \t [ 0.56219975  9.42928268 11.          0.51903033 19.          0.52214278]. \t  -0.6326946461691906 \t -0.4301920669254681\n",
            "12     \t [9.58176304 9.94093363 6.         0.96280567 8.         0.32855082]. \t  -0.6677341620025121 \t -0.4301920669254681\n",
            "13     \t [ 5.43877488  0.17504551 13.          0.58721411 19.          0.60115325]. \t  -0.5401916158723044 \t -0.4301920669254681\n",
            "14     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7382709735940614 \t -0.4301920669254681\n",
            "15     \t [9.18163606 3.89469538 8.         0.59772622 8.         0.18901946]. \t  -0.7380580836797348 \t -0.4301920669254681\n",
            "16     \t [0.52725495 9.82259272 6.         0.62080595 7.         0.68770749]. \t  -0.5497290569747493 \t -0.4301920669254681\n",
            "17     \t [ 6.71573436  6.18076517  8.          0.50736725 18.          0.99692636]. \t  -0.4548470613295324 \t -0.4301920669254681\n",
            "18     \t [ 7.23381367  0.21465732 13.          0.75484136 13.          0.44717401]. \t  -0.6306144837329356 \t -0.4301920669254681\n",
            "19     \t [ 0.17525712  4.19183624 13.          0.93093846  1.          0.77496282]. \t  -0.5254340278392874 \t -0.4301920669254681\n",
            "20     \t [ 0.22896089  2.69806987 14.          0.53297649 18.          0.65056758]. \t  -0.5430966302037052 \t -0.4301920669254681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.1451215046465775"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1WsphKSUj19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e7aa4d5-d405-4c82-9f91-770575ca9bab"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\n",
        "\n",
        "np.random.seed(run_num_7)\n",
        "surrogate_loser_7 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
        "\n",
        "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_7 = dGPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity7, param, n_jobs = -1) # define BayesOpt\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_7 = loser_7.getResult()[0]\n",
        "params_loser_7['max_depth'] = int(params_loser_7['max_depth'])\n",
        "params_loser_7['min_child_weight'] = int(params_loser_7['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train7 = xgb.DMatrix(X_train7, y_train7)\n",
        "dX_loser_test7 = xgb.DMatrix(X_test7, y_test7)\n",
        "model_loser_7 = xgb.train(params_loser_7, dX_loser_train7)\n",
        "pred_loser_7 = model_loser_7.predict(dX_loser_test7)\n",
        "\n",
        "rmse_loser_7 = np.sqrt(mean_squared_error(pred_loser_7, y_test7))\n",
        "rmse_loser_7"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.11505126  6.92324143 13.          0.94046175 12.          0.3650384 ]. \t  -0.612852517751432 \t -0.4284992540085738\n",
            "init   \t [ 5.15357029  9.0066636   6.          0.74104227 15.          0.8966337 ]. \t  -0.45500673865987984 \t -0.4284992540085738\n",
            "init   \t [ 2.63548913  9.6610934  14.          0.64712408  1.          0.80820948]. \t  -0.43730487111474525 \t -0.4284992540085738\n",
            "init   \t [ 1.40821426  1.72758589 12.          0.70495069  5.          0.86632715]. \t  -0.4284992540085738 \t -0.4284992540085738\n",
            "init   \t [2.85126987 2.73434054 7.         0.88394733 8.         0.65545939]. \t  -0.48253390492139586 \t -0.4284992540085738\n",
            "1      \t [1.03697999e-02 2.51695028e+00 1.40000000e+01 6.19497248e-01\n",
            " 1.90000000e+01 7.01827417e-01]. \t  -0.47758423778274467 \t -0.4284992540085738\n",
            "2      \t [9.9089961  7.50235155 7.         0.67925503 5.         0.2471192 ]. \t  -0.6736712023667494 \t -0.4284992540085738\n",
            "3      \t [ 7.21196079  0.11384945  8.          0.67755731 17.          0.74323626]. \t  -0.4647919397201027 \t -0.4284992540085738\n",
            "4      \t [0.35283087 9.2950614  5.         0.93054416 3.         0.27625531]. \t  -0.670508947897148 \t -0.4284992540085738\n",
            "5      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6745888440506937 \t -0.4284992540085738\n",
            "6      \t [9.76759527 0.48761621 7.         0.96362191 1.         0.137157  ]. \t  -0.6715547907624335 \t -0.4284992540085738\n",
            "7      \t [ 8.53387117  0.35946342 13.          0.65332712  6.          0.68302293]. \t  -0.4712802345283902 \t -0.4284992540085738\n",
            "8      \t [ 0.45155781  2.33209808  5.          0.73660734 17.          0.83890491]. \t  -0.4804814871680856 \t -0.4284992540085738\n",
            "9      \t [ 0.30506512  9.51761383 13.          0.5277377  16.          0.92234588]. \t  -0.4390633386106087 \t -0.4284992540085738\n",
            "10     \t [ 9.21287749  3.65759887  5.          0.85687073 12.          0.88459516]. \t  -0.45867733996321497 \t -0.4284992540085738\n",
            "11     \t [ 1.14642054  8.54710172 12.          0.83923566  8.          0.25317439]. \t  -0.6716592254658202 \t -0.4284992540085738\n",
            "12     \t [ 6.43879816  6.53341304 14.          0.70290258 19.          0.43578856]. \t  -0.5879212739277196 \t -0.4284992540085738\n",
            "13     \t [ 0.28968499  1.98768694 14.          0.97996073 12.          0.15615797]. \t  -0.6725851052423111 \t -0.4284992540085738\n",
            "14     \t [5.4721942  4.70341961 9.         0.675658   1.         0.91263074]. \t  -0.44024324260844283 \t -0.4284992540085738\n",
            "15     \t [ 7.56572451  6.67046024 14.          0.88731871  4.          0.29940932]. \t  -0.6117537685654454 \t -0.4284992540085738\n",
            "16     \t [4.54036829 9.23672573 7.         0.78071654 7.         0.925943  ]. \t  -0.44422074369565967 \t -0.4284992540085738\n",
            "17     \t [ 9.52566208  5.8884151   6.          0.78376025 19.          0.40148749]. \t  -0.6258103990077785 \t -0.4284992540085738\n",
            "18     \t [ 7.79919254  0.6285715  14.          0.87477175 15.          0.15632002]. \t  -0.6708453498973665 \t -0.4284992540085738\n",
            "19     \t [ 0.          0.          8.44191182  0.5        11.44191182  0.1       ]. \t  -0.6744471853974358 \t -0.4284992540085738\n",
            "20     \t [ 3.1237334   5.58211355  9.          0.8525378  19.          0.1292964 ]. \t  -0.6703702828267247 \t -0.4284992540085738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.492050628419268"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI8sFP4ZUmOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c8ec520-52c5-4c8d-d41c-5f589b0dfbbc"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\n",
        "\n",
        "np.random.seed(run_num_8)\n",
        "surrogate_loser_8 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
        "\n",
        "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_8 = dGPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity8, param, n_jobs = -1) # define BayesOpt\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_8 = loser_8.getResult()[0]\n",
        "params_loser_8['max_depth'] = int(params_loser_8['max_depth'])\n",
        "params_loser_8['min_child_weight'] = int(params_loser_8['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train8 = xgb.DMatrix(X_train8, y_train8)\n",
        "dX_loser_test8 = xgb.DMatrix(X_test8, y_test8)\n",
        "model_loser_8 = xgb.train(params_loser_8, dX_loser_train8)\n",
        "pred_loser_8 = model_loser_8.predict(dX_loser_test8)\n",
        "\n",
        "rmse_loser_8 = np.sqrt(mean_squared_error(pred_loser_8, y_test8))\n",
        "rmse_loser_8"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.29221489  0.77396898 11.          0.56062468  3.          0.25451879]. \t  -0.6231794271723858 \t -0.4674926150198253\n",
            "init   \t [ 0.10611429  1.37648061  8.          0.6050375  15.          0.790205  ]. \t  -0.4674926150198253 \t -0.4674926150198253\n",
            "init   \t [ 8.35239571  4.16908043  7.          0.67902746 14.          0.16683749]. \t  -0.6176439249709439 \t -0.4674926150198253\n",
            "init   \t [ 6.06301777  7.27127491 12.          0.87295531 11.          0.37694382]. \t  -0.5951520828762356 \t -0.4674926150198253\n",
            "init   \t [8.85209637 7.89269955 6.         0.72444854 3.         0.47766986]. \t  -0.5688519156580504 \t -0.4674926150198253\n",
            "1      \t [ 1.47825581  9.16010046 11.          0.96606546  1.          0.92358292]. \t  \u001b[92m-0.43518063190798095\u001b[0m \t -0.43518063190798095\n",
            "2      \t [ 7.52273592  2.30141271 14.          0.75505047 19.          0.77804046]. \t  -0.45158957529537797 \t -0.43518063190798095\n",
            "3      \t [1.64420908 9.47353882 5.         0.79993338 7.         0.15673599]. \t  -0.6175838614360663 \t -0.43518063190798095\n",
            "4      \t [ 1.02399475  9.65113789  9.          0.53947972 17.          0.8292164 ]. \t  -0.4719352875928771 \t -0.43518063190798095\n",
            "5      \t [8.14212583 0.61314553 6.         0.86488725 6.         0.28574491]. \t  -0.5954481959734638 \t -0.43518063190798095\n",
            "6      \t [ 8.76387285  7.83054608 13.          0.54159783  4.          0.24710422]. \t  -0.6207082177098945 \t -0.43518063190798095\n",
            "7      \t [ 9.0517509   9.45530828 11.          0.85305105 19.          0.79961763]. \t  -0.4589725747508552 \t -0.43518063190798095\n",
            "8      \t [ 7.77762087  0.97608919 14.          0.56979605  8.          0.73436998]. \t  -0.4483480686509709 \t -0.43518063190798095\n",
            "9      \t [2.67676373 3.95611485 5.         0.79739163 2.         0.89987835]. \t  -0.45674643004535687 \t -0.43518063190798095\n",
            "10     \t [ 0.62854908  4.77207773 14.          0.78182185 18.          0.73284936]. \t  -0.4461225555243119 \t -0.43518063190798095\n",
            "11     \t [3.06290827 1.66002331 6.         0.51411222 9.         0.98529356]. \t  -0.461628022517463 \t -0.43518063190798095\n",
            "12     \t [ 0.55545334  6.5757175   9.          0.88529579 12.          0.24203797]. \t  -0.6187020960077592 \t -0.43518063190798095\n",
            "13     \t [ 9.10053278  1.48753729 11.          0.9741735   1.          0.65977303]. \t  -0.5223704878111852 \t -0.43518063190798095\n",
            "14     \t [ 9.48329103  0.2076913   7.          0.76655383 19.          0.88164364]. \t  -0.4616094756962891 \t -0.43518063190798095\n",
            "15     \t [ 0.81390458  8.42248516 12.          0.78729971  7.          0.71328057]. \t  -0.5176932464043367 \t -0.43518063190798095\n",
            "16     \t [ 5.51535579  8.63063044  6.          0.64760098 18.          0.14185561]. \t  -0.6172074564902792 \t -0.43518063190798095\n",
            "17     \t [ 0.81210087  0.8533868  13.          0.66456412 12.          0.37769434]. \t  -0.5971747957799627 \t -0.43518063190798095\n",
            "18     \t [ 8.74769008  9.46041181  5.          0.76888057 12.          0.28611188]. \t  -0.5996998823178632 \t -0.43518063190798095\n",
            "19     \t [ 6.67918913  1.91467598 12.          0.83062216 13.          0.72788563]. \t  -0.45035357506572193 \t -0.43518063190798095\n",
            "20     \t [ 4.53564763  5.14157265 14.          0.52558575  1.          0.72718907]. \t  -0.45119962298136185 \t -0.43518063190798095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.711175290813375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw5IYus6UpAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56bfb9a-2bb2-4fee-cf9f-fa15d2185340"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\n",
        "\n",
        "np.random.seed(run_num_9)\n",
        "surrogate_loser_9 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
        "\n",
        "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_9 = dGPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity9, param, n_jobs = -1) # define BayesOpt\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_9 = loser_9.getResult()[0]\n",
        "params_loser_9['max_depth'] = int(params_loser_9['max_depth'])\n",
        "params_loser_9['min_child_weight'] = int(params_loser_9['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train9 = xgb.DMatrix(X_train9, y_train9)\n",
        "dX_loser_test9 = xgb.DMatrix(X_test9, y_test9)\n",
        "model_loser_9 = xgb.train(params_loser_9, dX_loser_train9)\n",
        "pred_loser_9 = model_loser_9.predict(dX_loser_test9)\n",
        "\n",
        "rmse_loser_9 = np.sqrt(mean_squared_error(pred_loser_9, y_test9))\n",
        "rmse_loser_9"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.0342804   5.27522296 13.          0.60741904 17.          0.15336222]. \t  -0.6890593508433319 \t -0.4886355541929722\n",
            "init   \t [7.56063644 1.79876369 7.         0.84869647 5.         0.55956942]. \t  -0.5532305414240943 \t -0.4886355541929722\n",
            "init   \t [2.43675539 0.20225128 6.         0.51669963 7.         0.4646446 ]. \t  -0.5609155518755553 \t -0.4886355541929722\n",
            "init   \t [ 7.6043547   4.73758376  9.          0.87564624 15.          0.65751286]. \t  -0.4886355541929722 \t -0.4886355541929722\n",
            "init   \t [ 4.70349314  6.71029586  6.          0.97680389 11.          0.41971652]. \t  -0.6548660487332898 \t -0.4886355541929722\n",
            "1      \t [ 0.20910387  7.25545521 14.          0.59407892  3.          0.95410586]. \t  \u001b[92m-0.4312356520914486\u001b[0m \t -0.4312356520914486\n",
            "2      \t [ 2.14892068  9.25577985 14.          0.84828472 11.          0.2793841 ]. \t  -0.6883959263608518 \t -0.4312356520914486\n",
            "3      \t [ 0.37305557  1.02756836 13.          0.57348052 13.          0.68410192]. \t  -0.4836872799099695 \t -0.4312356520914486\n",
            "4      \t [ 7.98252507  8.04957695 12.          0.6367811   3.          0.85775382]. \t  -0.4368210491589304 \t -0.4312356520914486\n",
            "5      \t [ 1.2588195   0.04438454  7.          0.70342864 18.          0.58554745]. \t  -0.4951281025154505 \t -0.4312356520914486\n",
            "6      \t [ 9.17710587  9.71573287  5.          0.9584376  19.          0.78072441]. \t  -0.4682275209845145 \t -0.4312356520914486\n",
            "7      \t [6.11416909 9.54959201 5.         0.91943284 1.         0.10968146]. \t  -0.6901756354808697 \t -0.4312356520914486\n",
            "8      \t [ 0.31270412  9.94818438  8.          0.84397658 19.          0.43974503]. \t  -0.5483333357303245 \t -0.4312356520914486\n",
            "9      \t [ 8.76504058  0.46155195 14.          0.61459938 10.          0.19095511]. \t  -0.6902913672662703 \t -0.4312356520914486\n",
            "10     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6893514282293631 \t -0.4312356520914486\n",
            "11     \t [ 0.76412088  5.02527282 14.          0.53794557 19.          0.6779987 ]. \t  -0.4862552398836468 \t -0.4312356520914486\n",
            "12     \t [ 8.7007577   8.84182108 11.          0.88074977 10.          0.89589347]. \t  -0.43700927635364534 \t -0.4312356520914486\n",
            "13     \t [1.26882135 8.44257007 8.         0.93932154 4.         0.79202967]. \t  -0.44680835713197 \t -0.4312356520914486\n",
            "14     \t [ 3.38351629  2.89927867 12.          0.76452212  7.          0.61222347]. \t  -0.4809111856529227 \t -0.4312356520914486\n",
            "15     \t [ 7.69267031  0.52709121  5.          0.97712342 11.          0.47111142]. \t  -0.5671609865381374 \t -0.4312356520914486\n",
            "16     \t [ 5.04465897  1.7242392  11.          0.82507363  1.          0.66070113]. \t  -0.48737194712175996 \t -0.4312356520914486\n",
            "17     \t [ 9.35257803  1.01913578 10.          0.61034129 19.          0.44798121]. \t  -0.5514224578726403 \t -0.4312356520914486\n",
            "18     \t [ 3.56621236  4.91536137  7.          0.71255145 19.          0.80604757]. \t  -0.4579607623923458 \t -0.4312356520914486\n",
            "19     \t [ 0.         0.         5.         0.5       12.6024416  0.1      ]. \t  -0.6889398896191611 \t -0.4312356520914486\n",
            "20     \t [9.96752263 8.45749074 7.         0.92612294 6.         0.22771617]. \t  -0.6878594070452866 \t -0.4312356520914486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.130636454660636"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD494io_Ur7V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa00388-2f73-4058-ce56-47c39be298a1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\n",
        "\n",
        "np.random.seed(run_num_10)\n",
        "surrogate_loser_10 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
        "\n",
        "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_10 = dGPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity10, param, n_jobs = -1) # define BayesOpt\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_10 = loser_10.getResult()[0]\n",
        "params_loser_10['max_depth'] = int(params_loser_10['max_depth'])\n",
        "params_loser_10['min_child_weight'] = int(params_loser_10['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train10 = xgb.DMatrix(X_train10, y_train10)\n",
        "dX_loser_test10 = xgb.DMatrix(X_test10, y_test10)\n",
        "model_loser_10 = xgb.train(params_loser_10, dX_loser_train10)\n",
        "pred_loser_10 = model_loser_10.predict(dX_loser_test10)\n",
        "\n",
        "rmse_loser_10 = np.sqrt(mean_squared_error(pred_loser_10, y_test10))\n",
        "rmse_loser_10"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  -0.6929852048256443 \t -0.4483221221388197\n",
            "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  -0.6932702114662359 \t -0.4483221221388197\n",
            "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  -0.6931242497450955 \t -0.4483221221388197\n",
            "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  -0.4483221221388197 \t -0.4483221221388197\n",
            "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  -0.6946096327074635 \t -0.4483221221388197\n",
            "1      \t [ 3.17878299  6.40666671 10.          0.71928645  2.          0.37920433]. \t  -0.6059745513602156 \t -0.4483221221388197\n",
            "2      \t [ 3.37448201  8.10780464 13.          0.50477079 10.          0.914384  ]. \t  \u001b[92m-0.43629727149692393\u001b[0m \t -0.43629727149692393\n",
            "3      \t [ 9.185601    0.61355962 14.          0.74945204  2.          0.47381106]. \t  -0.5791988425691649 \t -0.43629727149692393\n",
            "4      \t [8.35411693 8.91554079 5.         0.73130633 7.         0.67857606]. \t  -0.5771647264709212 \t -0.43629727149692393\n",
            "5      \t [ 6.07704325  9.68005151  5.          0.52932858 19.          0.88896368]. \t  -0.4736105624167554 \t -0.43629727149692393\n",
            "6      \t [ 8.63718494  8.55246455 14.          0.55824531 18.          0.4228152 ]. \t  -0.6085826477793935 \t -0.43629727149692393\n",
            "7      \t [9.97805381 2.33960922 6.         0.72769906 2.         0.16112568]. \t  -0.693543381593374 \t -0.43629727149692393\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6914754878687801 \t -0.43629727149692393\n",
            "9      \t [ 0.03680135  0.50694581 14.          0.91124751  8.          0.87026982]. \t  \u001b[92m-0.4215933675272626\u001b[0m \t -0.4215933675272626\n",
            "10     \t [ 1.86936633  8.33951478  6.          0.64608958 13.          0.87352832]. \t  -0.45750043698066534 \t -0.4215933675272626\n",
            "11     \t [ 9.77077394  8.65786606 12.          0.7706968   5.          0.42456779]. \t  -0.6030224397086726 \t -0.4215933675272626\n",
            "12     \t [ 0.1202295   0.91999335  5.          0.65757131 10.          0.24500243]. \t  -0.6930477249479814 \t -0.4215933675272626\n",
            "13     \t [0.50440288 9.75457432 5.         0.8137431  6.         0.15250773]. \t  -0.6941630128319229 \t -0.4215933675272626\n",
            "14     \t [ 9.91477928  9.35319949  6.          0.75202808 14.          0.13042843]. \t  -0.6936633884299808 \t -0.4215933675272626\n",
            "15     \t [2.78752086e+00 7.83961972e-03 1.10000000e+01 9.72670765e-01\n",
            " 2.00000000e+00 4.67980644e-01]. \t  -0.5809464009440812 \t -0.4215933675272626\n",
            "16     \t [ 0.60051063  0.93658443 10.          0.71787415 13.          0.3650692 ]. \t  -0.6037929489655063 \t -0.4215933675272626\n",
            "17     \t [7.64536232 8.37707893 5.         0.8688534  1.         0.76688319]. \t  -0.5294614889177678 \t -0.4215933675272626\n",
            "18     \t [ 8.58082048  3.39962732 14.          0.7151638   9.          0.76214186]. \t  -0.5049104315367501 \t -0.4215933675272626\n",
            "19     \t [ 8.52646717  0.12245704 14.          0.80155281 15.          0.7100538 ]. \t  -0.5709119331337676 \t -0.4215933675272626\n",
            "20     \t [5.51569281 2.48923068 5.         0.97468437 6.         0.88188042]. \t  -0.4524796762835521 \t -0.4215933675272626\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.436171669561738"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N03Sq0TvUuhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95deb168-1c81-4b45-b1f8-917480afd056"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\n",
        "\n",
        "np.random.seed(run_num_11)\n",
        "surrogate_loser_11 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
        "\n",
        "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_11 = dGPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity11, param, n_jobs = -1) # define BayesOpt\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_11 = loser_11.getResult()[0]\n",
        "params_loser_11['max_depth'] = int(params_loser_11['max_depth'])\n",
        "params_loser_11['min_child_weight'] = int(params_loser_11['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train11 = xgb.DMatrix(X_train11, y_train11)\n",
        "dX_loser_test11 = xgb.DMatrix(X_test11, y_test11)\n",
        "model_loser_11 = xgb.train(params_loser_11, dX_loser_train11)\n",
        "pred_loser_11 = model_loser_11.predict(dX_loser_test11)\n",
        "\n",
        "rmse_loser_11 = np.sqrt(mean_squared_error(pred_loser_11, y_test11))\n",
        "rmse_loser_11"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.81035663  8.2124785   9.          0.68487173 15.          0.28207988]. \t  -0.7133787013189323 \t -0.47317045732943896\n",
            "init   \t [ 4.91906771  1.38558332  6.          0.618878   19.          0.10263204]. \t  -0.7112023662943636 \t -0.47317045732943896\n",
            "init   \t [ 5.23426175  1.89971159 13.          0.76313779 17.          0.46597853]. \t  -0.47317045732943896 \t -0.47317045732943896\n",
            "init   \t [ 4.0204056   3.75167577  9.          0.86204818 17.          0.26560056]. \t  -0.7132057194891379 \t -0.47317045732943896\n",
            "init   \t [8.8704459  5.64917815 5.         0.51406171 1.         0.82826731]. \t  -0.48048685248123163 \t -0.47317045732943896\n",
            "1      \t [0.08349406 1.89250335 6.         0.66386636 1.         0.88641723]. \t  \u001b[92m-0.4484239383130805\u001b[0m \t -0.4484239383130805\n",
            "2      \t [ 5.35837408  0.31510164 14.          0.97519815  6.          0.3034681 ]. \t  -0.607905230502132 \t -0.4484239383130805\n",
            "3      \t [ 4.66315557  9.30076713 11.          0.83534598  8.          0.21303816]. \t  -0.7135282416096518 \t -0.4484239383130805\n",
            "4      \t [9.02046973 1.16865322 7.         0.6807515  8.         0.2962635 ]. \t  -0.6068266615066733 \t -0.4484239383130805\n",
            "5      \t [0.79988529 9.85424348 5.         0.5743044  3.         0.67901433]. \t  -0.48209943859810334 \t -0.4484239383130805\n",
            "6      \t [ 0.02057794  7.14089873 13.          0.90855163  1.          0.61823707]. \t  -0.45805892074172583 \t -0.4484239383130805\n",
            "7      \t [ 0.97603689  3.49871064  9.          0.91846837 10.          0.35158251]. \t  -0.6085109103402477 \t -0.4484239383130805\n",
            "8      \t [ 9.27240029  9.57413463 14.          0.58544186  3.          0.73779027]. \t  -0.45267877796490286 \t -0.4484239383130805\n",
            "9      \t [ 0.4027375   9.38447167 12.          0.94595612 15.          0.85310792]. \t  -0.451136347669006 \t -0.4484239383130805\n",
            "10     \t [ 1.3440527   8.97080429  5.          0.80277597 12.          0.63284111]. \t  -0.48349651102568225 \t -0.4484239383130805\n",
            "11     \t [ 7.94225687  3.68352192 13.          0.94902199 11.          0.95471054]. \t  \u001b[92m-0.422771965919715\u001b[0m \t -0.422771965919715\n",
            "12     \t [9.20332934 9.98167579 5.         0.57575366 6.         0.3717594 ]. \t  -0.6046860081027627 \t -0.422771965919715\n",
            "13     \t [5.68758712 2.62033511 9.         0.67614771 3.         0.67632008]. \t  -0.47052528509931824 \t -0.422771965919715\n",
            "14     \t [2.94143112e-03 7.03829535e-01 1.30000000e+01 5.98722168e-01\n",
            " 2.00000000e+00 1.01335355e-01]. \t  -0.7122743790062083 \t -0.422771965919715\n",
            "15     \t [ 0.81561387  9.31451412  5.          0.70594876 19.          0.98284163]. \t  -0.45988817082889427 \t -0.422771965919715\n",
            "16     \t [ 9.8968071   1.89203294 13.          0.7888373   2.          0.82539259]. \t  -0.45294282860990764 \t -0.422771965919715\n",
            "17     \t [ 8.37835592  2.52983026  5.          0.97806207 14.          0.6500905 ]. \t  -0.48098936825483635 \t -0.422771965919715\n",
            "18     \t [ 0.56769151  4.18340522 14.          0.91967306 13.          0.92813967]. \t  \u001b[92m-0.41912566738485735\u001b[0m \t -0.41912566738485735\n",
            "19     \t [ 4.05595265  0.          5.          0.5        10.09765104  0.1       ]. \t  -0.7116217008807669 \t -0.41912566738485735\n",
            "20     \t [ 8.98906297  7.01351387 13.          0.77197065 19.          0.47265223]. \t  -0.475345282554151 \t -0.41912566738485735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.3224670398414355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_nP9lQjUztV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37df2d90-4d81-4fa5-86b4-df9db003793b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\n",
        "\n",
        "np.random.seed(run_num_12)\n",
        "surrogate_loser_12 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
        "\n",
        "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_12 = dGPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity12, param, n_jobs = -1) # define BayesOpt\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_12 = loser_12.getResult()[0]\n",
        "params_loser_12['max_depth'] = int(params_loser_12['max_depth'])\n",
        "params_loser_12['min_child_weight'] = int(params_loser_12['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train12 = xgb.DMatrix(X_train12, y_train12)\n",
        "dX_loser_test12 = xgb.DMatrix(X_test12, y_test12)\n",
        "model_loser_12 = xgb.train(params_loser_12, dX_loser_train12)\n",
        "pred_loser_12 = model_loser_12.predict(dX_loser_test12)\n",
        "\n",
        "rmse_loser_12 = np.sqrt(mean_squared_error(pred_loser_12, y_test12))\n",
        "rmse_loser_12"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.9151945   6.22108771  9.          0.89267929 16.          0.87460279]. \t  -0.4420077321695894 \t -0.4420077321695894\n",
            "init   \t [ 1.50636966  1.98518759 11.          0.67890863 17.          0.5381501 ]. \t  -0.5998539491151165 \t -0.4420077321695894\n",
            "init   \t [ 3.31015428  8.02639569  7.          0.75154158 13.          0.11992952]. \t  -0.7324503600335561 \t -0.4420077321695894\n",
            "init   \t [ 2.90728553  2.46394443  5.          0.94461307 17.          0.43194161]. \t  -0.5995790338913064 \t -0.4420077321695894\n",
            "init   \t [ 9.33140102  6.51378143  8.          0.76907392 18.          0.38515251]. \t  -0.5982523495604211 \t -0.4420077321695894\n",
            "1      \t [5.68098653 8.6912739  6.         0.52027807 1.         0.51631894]. \t  -0.5999713561677879 \t -0.4420077321695894\n",
            "2      \t [ 4.61089026  7.78540446 14.          0.61560224  6.          0.60910898]. \t  -0.5310432089168232 \t -0.4420077321695894\n",
            "3      \t [ 7.71510638  1.71782806 11.          0.9748513   2.          0.33442931]. \t  -0.5954035506799399 \t -0.4420077321695894\n",
            "4      \t [ 0.42901773  1.02682351 10.          0.62582208  7.          0.40519792]. \t  -0.600033100066767 \t -0.4420077321695894\n",
            "5      \t [ 9.48743388  2.23858916 14.          0.84020064 13.          0.41361313]. \t  -0.5921503481987865 \t -0.4420077321695894\n",
            "6      \t [ 7.09099339  0.08964775  6.          0.68605854 11.          0.16899055]. \t  -0.7330665763811677 \t -0.4420077321695894\n",
            "7      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7334076855722197 \t -0.4420077321695894\n",
            "8      \t [ 4.8875203   9.38595347 14.          0.7902783  19.          0.67669595]. \t  -0.5289545232453541 \t -0.4420077321695894\n",
            "9      \t [ 0.50387213  8.28483675 14.          0.60639358 12.          0.19061796]. \t  -0.7343015225078143 \t -0.4420077321695894\n",
            "10     \t [ 1.81216609  5.49695067 10.52048254  0.5         1.          0.1       ]. \t  -0.7336073261479503 \t -0.4420077321695894\n",
            "11     \t [ 9.8847664   8.41680653 10.          0.63099084  9.          0.83145315]. \t  -0.5042925534600994 \t -0.4420077321695894\n",
            "12     \t [ 0.          0.          5.          0.5        11.52941427  0.1       ]. \t  -0.7334775686570922 \t -0.4420077321695894\n",
            "13     \t [9.43877299 3.79912488 5.         0.53841687 4.         0.20857531]. \t  -0.7351835256464294 \t -0.4420077321695894\n",
            "14     \t [0.41721862 8.87551388 5.         0.69088851 6.         0.86973069]. \t  -0.468169334397084 \t -0.4420077321695894\n",
            "15     \t [ 9.68675553  3.68641979 14.          0.63310045  7.          0.87631691]. \t  \u001b[92m-0.44169445577667543\u001b[0m \t -0.44169445577667543\n",
            "16     \t [ 9.64220337  0.08866879  7.          0.55877602 19.          0.80403324]. \t  -0.5167375732665379 \t -0.44169445577667543\n",
            "17     \t [ 8.95792842  8.74686238 12.          0.60340754  1.          0.64076061]. \t  -0.5341583767040248 \t -0.44169445577667543\n",
            "18     \t [ 3.80794615  1.17683    14.          0.6618018  11.          0.78665992]. \t  -0.497041017578099 \t -0.44169445577667543\n",
            "19     \t [ 0.60473619  9.7310361   5.          0.764657   19.          0.44899982]. \t  -0.602200593591361 \t -0.44169445577667543\n",
            "20     \t [ 7.25143535  1.86402411 14.          0.59310957 19.          0.53879904]. \t  -0.5987977809908415 \t -0.44169445577667543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.9962203731848227"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDI2Bi9vU05U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "759ef747-4288-4829-afb1-b35d1ed110bc"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\n",
        "\n",
        "np.random.seed(run_num_13)\n",
        "surrogate_loser_13 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
        "\n",
        "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_13 = dGPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity13, param, n_jobs = -1) # define BayesOpt\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_13 = loser_13.getResult()[0]\n",
        "params_loser_13['max_depth'] = int(params_loser_13['max_depth'])\n",
        "params_loser_13['min_child_weight'] = int(params_loser_13['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train13 = xgb.DMatrix(X_train13, y_train13)\n",
        "dX_loser_test13 = xgb.DMatrix(X_test13, y_test13)\n",
        "model_loser_13 = xgb.train(params_loser_13, dX_loser_train13)\n",
        "pred_loser_13 = model_loser_13.predict(dX_loser_test13)\n",
        "\n",
        "rmse_loser_13 = np.sqrt(mean_squared_error(pred_loser_13, y_test13))\n",
        "rmse_loser_13"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.30967343  2.54282073  6.          0.9850871  17.          0.50848087]. \t  -0.5693403578256062 \t -0.4459810811310791\n",
            "init   \t [ 6.95119021  3.25593423 14.          0.8602375   8.          0.82055713]. \t  -0.5112217679187058 \t -0.4459810811310791\n",
            "init   \t [ 5.09252675  8.19921021 10.          0.78792002 16.          0.382158  ]. \t  -0.581693558072188 \t -0.4459810811310791\n",
            "init   \t [ 5.4465191   3.19841462 14.          0.90654581  5.          0.10359415]. \t  -0.6818818523119694 \t -0.4459810811310791\n",
            "init   \t [ 9.8393864   9.89619708  9.          0.63387763 10.          0.88979174]. \t  -0.4459810811310791 \t -0.4459810811310791\n",
            "1      \t [ 9.27118046  9.4344906  10.          0.71578295  1.          0.5463837 ]. \t  -0.5604994598662556 \t -0.4459810811310791\n",
            "2      \t [0.21910824 0.53051294 5.         0.62754372 4.         0.55299122]. \t  -0.578556985871312 \t -0.4459810811310791\n",
            "3      \t [0.32904954 9.80442032 8.         0.7285653  2.         0.94646442]. \t  \u001b[92m-0.43885135786496293\u001b[0m \t -0.43885135786496293\n",
            "4      \t [ 8.54553338  0.49573541 12.          0.81406546 19.          0.20999807]. \t  -0.6795516887212301 \t -0.43885135786496293\n",
            "5      \t [9.40145549 1.43629395 5.         0.63655396 1.         0.63002732]. \t  -0.5662893508870628 \t -0.43885135786496293\n",
            "6      \t [ 8.7238127   0.85665784  6.          0.75815556 12.          0.87180469]. \t  -0.4644522650882914 \t -0.43885135786496293\n",
            "7      \t [ 0.11400844  9.5774521   6.          0.90472821 11.          0.18513309]. \t  -0.6793968950126906 \t -0.43885135786496293\n",
            "8      \t [ 1.95513735  0.12334702 13.          0.54467307 14.          0.86324917]. \t  \u001b[92m-0.43794830902637205\u001b[0m \t -0.43794830902637205\n",
            "9      \t [ 2.89959158  8.10209185 14.          0.9094105   1.          0.86003598]. \t  \u001b[92m-0.42582758559630707\u001b[0m \t -0.42582758559630707\n",
            "10     \t [ 0.63533588  7.88973169 13.          0.71777205  8.          0.22258282]. \t  -0.6847473450330067 \t -0.42582758559630707\n",
            "11     \t [4.59853484 7.19189873 5.         0.92777191 6.         0.5760847 ]. \t  -0.5604008171822471 \t -0.42582758559630707\n",
            "12     \t [ 9.78022669  6.34740254  5.          0.81941991 15.          0.17866883]. \t  -0.6810440437950237 \t -0.42582758559630707\n",
            "13     \t [ 0.44924058  6.01817024 13.          0.62113095 19.          0.22629481]. \t  -0.6838258323776948 \t -0.42582758559630707\n",
            "14     \t [2.08434216 0.20801851 5.85566131 0.5        9.85566131 0.1       ]. \t  -0.6829533902190683 \t -0.42582758559630707\n",
            "15     \t [ 8.93459678  5.75013258 14.          0.86506851 14.          0.11026757]. \t  -0.6798789215383618 \t -0.42582758559630707\n",
            "16     \t [ 1.2199304   9.32862427  5.          0.98694875 18.          0.69275841]. \t  -0.5593626212561581 \t -0.42582758559630707\n",
            "17     \t [7.30733696 1.06961871 9.         0.6583612  6.         0.41488291]. \t  -0.5857440675365031 \t -0.42582758559630707\n",
            "18     \t [ 0.25964514  2.37009305 11.          0.85577938  4.          0.42985865]. \t  -0.556481072440868 \t -0.42582758559630707\n",
            "19     \t [ 5.54919942  8.87232522  5.          0.84416975 12.          0.31830005]. \t  -0.5998464916577737 \t -0.42582758559630707\n",
            "20     \t [ 9.03439965  9.39442441 14.          0.98025678 19.          0.55921955]. \t  -0.550861248380592 \t -0.42582758559630707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.159065314760995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2F_Q194U3uu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "760ce57d-68cc-4923-b384-bd8b6e350c22"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\n",
        "\n",
        "np.random.seed(run_num_14)\n",
        "surrogate_loser_14 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
        "\n",
        "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_14 = dGPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity14, param, n_jobs = -1) # define BayesOpt\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_14 = loser_14.getResult()[0]\n",
        "params_loser_14['max_depth'] = int(params_loser_14['max_depth'])\n",
        "params_loser_14['min_child_weight'] = int(params_loser_14['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train14 = xgb.DMatrix(X_train14, y_train14)\n",
        "dX_loser_test14 = xgb.DMatrix(X_test14, y_test14)\n",
        "model_loser_14 = xgb.train(params_loser_14, dX_loser_train14)\n",
        "pred_loser_14 = model_loser_14.predict(dX_loser_test14)\n",
        "\n",
        "rmse_loser_14 = np.sqrt(mean_squared_error(pred_loser_14, y_test14))\n",
        "rmse_loser_14"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.59560606  1.64569495 11.          0.89021195 15.          0.4857008 ]. \t  -0.6102441482204597 \t -0.4517949441804544\n",
            "init   \t [ 0.57460093  9.25007434  5.          0.56647642 17.          0.7533211 ]. \t  -0.5614002279921607 \t -0.4517949441804544\n",
            "init   \t [1.90674396 4.32019547 9.         0.93708225 3.         0.91775905]. \t  -0.4517949441804544 \t -0.4517949441804544\n",
            "init   \t [ 0.36770475  8.43368681 14.          0.6826844  15.          0.6291966 ]. \t  -0.5407924410004543 \t -0.4517949441804544\n",
            "init   \t [ 5.33840324  0.449182   14.          0.52045439  2.          0.48604556]. \t  -0.6242722768101235 \t -0.4517949441804544\n",
            "1      \t [ 9.54492219  9.07631824  8.          0.54579372 12.          0.38295996]. \t  -0.6262431675183743 \t -0.4517949441804544\n",
            "2      \t [9.53270461 0.83608685 6.         0.99976104 3.         0.60747265]. \t  -0.5533922160484371 \t -0.4517949441804544\n",
            "3      \t [ 1.15250646  0.51569303  5.          0.69322415 17.          0.16291682]. \t  -0.6917729046487754 \t -0.4517949441804544\n",
            "4      \t [ 6.37447116  9.98862868 14.          0.72259558  4.          0.61899393]. \t  -0.5434760859463281 \t -0.4517949441804544\n",
            "5      \t [ 8.08070558  7.65123431 14.          0.59104644 19.          0.9302005 ]. \t  -0.45873776977159153 \t -0.4517949441804544\n",
            "6      \t [ 6.26901123  2.37239677  5.          0.95063922 11.          0.52876485]. \t  -0.6158458956422692 \t -0.4517949441804544\n",
            "7      \t [1.0907871  9.11150389 5.         0.58744161 9.         0.87439163]. \t  -0.4933476055445382 \t -0.4517949441804544\n",
            "8      \t [ 2.21919917  0.31865167 14.          0.54399684 14.          0.18056849]. \t  -0.6942842535038787 \t -0.4517949441804544\n",
            "9      \t [ 7.34435618  8.39108681  7.          0.81547994 19.          0.62767067]. \t  -0.546522525570801 \t -0.4517949441804544\n",
            "10     \t [ 6.57539862  4.15362663 14.          0.96053826  9.          0.2080482 ]. \t  -0.691194495047777 \t -0.4517949441804544\n",
            "11     \t [6.00529466 8.33461243 6.         0.97581801 4.         0.31582778]. \t  -0.6226835813168086 \t -0.4517949441804544\n",
            "12     \t [0.56405811 0.         5.         0.5        7.860223   0.1       ]. \t  -0.6942634907462908 \t -0.4517949441804544\n",
            "13     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6943016112587715 \t -0.4517949441804544\n",
            "14     \t [ 0.84209312  7.86749962 14.          0.8648448   1.          0.1570582 ]. \t  -0.6937301125121068 \t -0.4517949441804544\n",
            "15     \t [ 1.22436035  9.67689861 12.          0.7460835   9.          0.69284382]. \t  -0.5399272231860202 \t -0.4517949441804544\n",
            "16     \t [ 4.4386325   0.37972087 10.          0.61118217  8.          0.15198669]. \t  -0.6912722224579347 \t -0.4517949441804544\n",
            "17     \t [ 7.0496565   9.86456792 14.          0.62416475 11.          0.68843039]. \t  -0.5429606831910581 \t -0.4517949441804544\n",
            "18     \t [ 3.32112913  6.27078367  9.          0.98238085 14.          0.82374888]. \t  -0.5322541144293694 \t -0.4517949441804544\n",
            "19     \t [ 9.66203958  6.20819976 11.          0.71875858  5.          0.43480687]. \t  -0.6129974118264951 \t -0.4517949441804544\n",
            "20     \t [ 0.13684155  3.86104617 14.          0.82673561  9.          0.20283653]. \t  -0.6919355943160154 \t -0.4517949441804544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.334202643456426"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Po5wImJaU6VC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "734a6a03-1330-41fb-c1ac-cb487ce99832"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\n",
        "\n",
        "np.random.seed(run_num_15)\n",
        "surrogate_loser_15 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
        "\n",
        "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_15 = dGPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity15, param, n_jobs = -1) # define BayesOpt\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_15 = loser_15.getResult()[0]\n",
        "params_loser_15['max_depth'] = int(params_loser_15['max_depth'])\n",
        "params_loser_15['min_child_weight'] = int(params_loser_15['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train15 = xgb.DMatrix(X_train15, y_train15)\n",
        "dX_loser_test15 = xgb.DMatrix(X_test15, y_test15)\n",
        "model_loser_15 = xgb.train(params_loser_15, dX_loser_train15)\n",
        "pred_loser_15 = model_loser_15.predict(dX_loser_test15)\n",
        "\n",
        "rmse_loser_15 = np.sqrt(mean_squared_error(pred_loser_15, y_test15))\n",
        "rmse_loser_15"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.64315424  8.97297685  6.          0.81493635 11.          0.33008564]. \t  -0.5946480910132583 \t -0.4392277798535851\n",
            "init   \t [9.51455383 9.60803032 5.         0.68485728 1.         0.30145081]. \t  -0.5968379804084254 \t -0.4392277798535851\n",
            "init   \t [ 8.11809889  6.32508536  7.          0.61766329 19.          0.51383159]. \t  -0.5092318398204285 \t -0.4392277798535851\n",
            "init   \t [ 7.28571251  5.36071389 10.          0.76240297 12.          0.12035683]. \t  -0.6346120183758519 \t -0.4392277798535851\n",
            "init   \t [ 6.80847768  3.59767194 14.          0.51365585 16.          0.89730673]. \t  -0.4392277798535851 \t -0.4392277798535851\n",
            "1      \t [ 2.28792065  8.58421878 12.          0.67551126  1.          0.37971264]. \t  -0.596161946051267 \t -0.4392277798535851\n",
            "2      \t [ 9.35473751  1.48895841 12.          0.55362474  4.          0.11106606]. \t  -0.6327865627642436 \t -0.4392277798535851\n",
            "3      \t [3.91572323 2.23174129 6.         0.50937407 1.         0.51336364]. \t  -0.5160141940687206 \t -0.4392277798535851\n",
            "4      \t [ 0.74184801  3.17830864 14.          0.90502664  8.          0.31107615]. \t  -0.5905073342805591 \t -0.4392277798535851\n",
            "5      \t [ 1.33182259  0.8125478   9.          0.94558338 18.          0.71777254]. \t  -0.4467705016632985 \t -0.4392277798535851\n",
            "6      \t [ 0.48538268  1.39730148  6.          0.8238236  11.          0.45741795]. \t  -0.5144696373796631 \t -0.4392277798535851\n",
            "7      \t [ 0.57885492  9.35143843  6.          0.69337529 17.          0.15510814]. \t  -0.6331051904120661 \t -0.4392277798535851\n",
            "8      \t [0.24960251 9.45540541 5.         0.62890215 3.         0.73551484]. \t  -0.47327438015853385 \t -0.4392277798535851\n",
            "9      \t [ 8.23428085  9.77240559 13.          0.85977854  8.          0.93876208]. \t  \u001b[92m-0.4281095031928478\u001b[0m \t -0.4281095031928478\n",
            "10     \t [ 1.35750397  9.83931966 14.          0.92698791 15.          0.2272101 ]. \t  -0.6341880539033447 \t -0.4281095031928478\n",
            "11     \t [ 9.85965179  9.04589828 13.          0.95564786 19.          0.1426314 ]. \t  -0.632071954329094 \t -0.4281095031928478\n",
            "12     \t [9.06175259 4.11518452 5.         0.87621161 6.         0.52300377]. \t  -0.5192107686497183 \t -0.4281095031928478\n",
            "13     \t [ 0.15053668  0.43786928 13.          0.62231098  1.          0.88145445]. \t  -0.43523183945633176 \t -0.4281095031928478\n",
            "14     \t [ 7.18775342  0.2062489   5.          0.98239776 17.          0.28043211]. \t  -0.634545784625454 \t -0.4281095031928478\n",
            "15     \t [0.10178544 7.49949902 9.         0.60008085 8.         0.35520416]. \t  -0.5902884239300654 \t -0.4281095031928478\n",
            "16     \t [0.22091899 0.18516021 8.21358087 0.5        5.21358087 0.1       ]. \t  -0.6326798838280104 \t -0.4281095031928478\n",
            "17     \t [ 8.4324691   8.27251391 11.          0.71354715  2.          0.54685464]. \t  -0.498601956775111 \t -0.4281095031928478\n",
            "18     \t [ 2.12400437  9.06769045 14.          0.70585245  9.          0.90846764]. \t  -0.4286407197787245 \t -0.4281095031928478\n",
            "19     \t [9.58390631 0.99765082 6.         0.72021045 1.         0.39555686]. \t  -0.5960083204162412 \t -0.4281095031928478\n",
            "20     \t [ 9.96011127  1.55235537 14.          0.63004724 11.          0.14483121]. \t  -0.6331081546428745 \t -0.4281095031928478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.189953800077167"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9HrAQN-pU9Qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ad2eff-eb12-4e13-d58a-106622d5a017"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\n",
        "\n",
        "np.random.seed(run_num_16)\n",
        "surrogate_loser_16 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
        "\n",
        "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_16 = dGPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity16, param, n_jobs = -1) # define BayesOpt\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_16 = loser_16.getResult()[0]\n",
        "params_loser_16['max_depth'] = int(params_loser_16['max_depth'])\n",
        "params_loser_16['min_child_weight'] = int(params_loser_16['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train16 = xgb.DMatrix(X_train16, y_train16)\n",
        "dX_loser_test16 = xgb.DMatrix(X_test16, y_test16)\n",
        "model_loser_16 = xgb.train(params_loser_16, dX_loser_train16)\n",
        "pred_loser_16 = model_loser_16.predict(dX_loser_test16)\n",
        "\n",
        "rmse_loser_16 = np.sqrt(mean_squared_error(pred_loser_16, y_test16))\n",
        "rmse_loser_16"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  -0.5713220948397237 \t -0.4331621293825035\n",
            "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  -0.4331621293825035 \t -0.4331621293825035\n",
            "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  -0.5561611494046279 \t -0.4331621293825035\n",
            "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  -0.6835926126526815 \t -0.4331621293825035\n",
            "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  -0.7677243885128009 \t -0.4331621293825035\n",
            "1      \t [ 0.60074712  1.1402995   5.          0.75926551 10.          0.30612858]. \t  -0.6844266759595645 \t -0.4331621293825035\n",
            "2      \t [ 3.6191694   8.20490707  5.          0.81280746 16.          0.99306688]. \t  -0.4581574120389309 \t -0.4331621293825035\n",
            "3      \t [ 0.55111138  3.69634376 11.          0.53864409  3.          0.92267876]. \t  -0.43468988385343615 \t -0.4331621293825035\n",
            "4      \t [9.52987381 8.92087345 5.         0.95441282 7.         0.10412548]. \t  -0.7679863416880384 \t -0.4331621293825035\n",
            "5      \t [ 8.31345174  8.15216774 14.          0.81052242  4.          0.5053732 ]. \t  -0.5586554806770512 \t -0.4331621293825035\n",
            "6      \t [ 7.65476591  1.74203379 12.          0.78837413  7.          0.71460419]. \t  -0.4382687724745528 \t -0.4331621293825035\n",
            "7      \t [1.68021993 9.25252958 9.         0.78643474 9.         0.55639556]. \t  -0.5630452959468858 \t -0.4331621293825035\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7670392049994392 \t -0.4331621293825035\n",
            "9      \t [ 2.6903245   0.3560009   7.          0.98255651 19.          0.9851534 ]. \t  -0.4412750880856537 \t -0.4331621293825035\n",
            "10     \t [ 0.4751313   0.18137204 13.          0.70186285 10.          0.37625565]. \t  -0.68582313055112 \t -0.4331621293825035\n",
            "11     \t [ 8.54895835  0.69961229 14.          0.57654595 19.          0.20454627]. \t  -0.7677953049378372 \t -0.4331621293825035\n",
            "12     \t [ 8.76905299  8.15213676  8.          0.5845007  19.          0.27314045]. \t  -0.7678874315811665 \t -0.4331621293825035\n",
            "13     \t [ 1.48533436  9.57311484 14.          0.8193199   3.          0.99329178]. \t  \u001b[92m-0.4185372839160708\u001b[0m \t -0.4185372839160708\n",
            "14     \t [4.49490325 9.22009337 8.         0.71314799 3.         0.58255672]. \t  -0.48560746523878195 \t -0.4185372839160708\n",
            "15     \t [ 0.76287087  0.49621071 13.          0.81150873 18.          0.76762748]. \t  -0.4363142295126112 \t -0.4185372839160708\n",
            "16     \t [0.56061777 6.48476685 5.         0.81826035 5.         0.59167243]. \t  -0.5183862929687044 \t -0.4185372839160708\n",
            "17     \t [8.8152352  0.5898958  5.         0.831707   9.         0.92137997]. \t  -0.45519808186832184 \t -0.4185372839160708\n",
            "18     \t [ 8.76208148  1.79224635  5.          0.52306305 19.          0.78632749]. \t  -0.4871095639687443 \t -0.4185372839160708\n",
            "19     \t [ 9.63332031  8.534049   10.          0.55556169 10.          0.35894848]. \t  -0.6847217417593005 \t -0.4185372839160708\n",
            "20     \t [ 4.52692506  0.27496659 10.          0.6345842   1.          0.46474283]. \t  -0.5631095705887621 \t -0.4185372839160708\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.361483761377183"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXelbcAVVCqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5dd441f-bc7b-4467-e5d1-ca181da9dd9e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\n",
        "\n",
        "np.random.seed(run_num_17)\n",
        "surrogate_loser_17 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
        "\n",
        "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_17 = dGPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity17, param, n_jobs = -1) # define BayesOpt\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_17 = loser_17.getResult()[0]\n",
        "params_loser_17['max_depth'] = int(params_loser_17['max_depth'])\n",
        "params_loser_17['min_child_weight'] = int(params_loser_17['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train17 = xgb.DMatrix(X_train17, y_train17)\n",
        "dX_loser_test17 = xgb.DMatrix(X_test17, y_test17)\n",
        "model_loser_17 = xgb.train(params_loser_17, dX_loser_train17)\n",
        "pred_loser_17 = model_loser_17.predict(dX_loser_test17)\n",
        "\n",
        "rmse_loser_17 = np.sqrt(mean_squared_error(pred_loser_17, y_test17))\n",
        "rmse_loser_17"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.85575275  3.86599303 13.          0.85645882  2.          0.78699813]. \t  -0.4597056260580034 \t -0.4597056260580034\n",
            "init   \t [1.97780831 2.92527676 8.         0.92268607 9.         0.39017299]. \t  -0.5687207024308397 \t -0.4597056260580034\n",
            "init   \t [ 7.16656007  1.028324    5.          0.96975483 15.          0.90932849]. \t  -0.4688025715067707 \t -0.4597056260580034\n",
            "init   \t [ 2.46007304  9.53596657  7.          0.71704714 11.          0.17659118]. \t  -0.6983036523024195 \t -0.4597056260580034\n",
            "init   \t [3.44161135 9.92427612 8.         0.96331165 2.         0.29160186]. \t  -0.5675253504862205 \t -0.4597056260580034\n",
            "1      \t [ 9.98828606  3.49693094 13.          0.57156882  8.          0.14604355]. \t  -0.6992595730796743 \t -0.4597056260580034\n",
            "2      \t [ 6.95619362  6.95953721 12.          0.92317057 19.          0.34981799]. \t  -0.5628121282495753 \t -0.4597056260580034\n",
            "3      \t [8.58397417 0.63190934 7.         0.80049626 2.         0.98851802]. \t  \u001b[92m-0.453932525933606\u001b[0m \t -0.453932525933606\n",
            "4      \t [ 1.79054908  0.97767001 14.          0.72051649 17.          0.75055942]. \t  -0.4586950539387324 \t -0.453932525933606\n",
            "5      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6990710246949499 \t -0.453932525933606\n",
            "6      \t [ 0.19331023  6.0899655   6.          0.97237498 19.          0.59441475]. \t  -0.54171231691412 \t -0.453932525933606\n",
            "7      \t [ 8.95440724  5.31304705 13.          0.73043408  1.          0.56398539]. \t  -0.5373247826064305 \t -0.453932525933606\n",
            "8      \t [ 8.26790477  9.9524702   5.          0.59716297 16.          0.88254342]. \t  -0.47691281251538375 \t -0.453932525933606\n",
            "9      \t [ 0.90443942  7.22834774 14.          0.92580232 12.          0.13465218]. \t  -0.6965996897404226 \t -0.453932525933606\n",
            "10     \t [ 8.52285971  0.31262108 12.          0.8797653  16.          0.34954268]. \t  -0.5627660085466504 \t -0.453932525933606\n",
            "11     \t [ 7.87455923  9.60434668 13.          0.6097695  11.          0.18672661]. \t  -0.6988278637935389 \t -0.453932525933606\n",
            "12     \t [7.77469206 6.81026607 6.         0.84173013 6.         0.4321758 ]. \t  -0.5485988100854867 \t -0.453932525933606\n",
            "13     \t [ 1.78559475  9.94726594 14.          0.84124706  5.          0.41412983]. \t  -0.5643771257102934 \t -0.453932525933606\n",
            "14     \t [ 3.55475249  2.77437165 14.          0.64501595  8.          0.71111614]. \t  -0.5328675498676292 \t -0.453932525933606\n",
            "15     \t [9.7645138  6.96943344 7.         0.64264761 1.         0.5601612 ]. \t  -0.5443333775093777 \t -0.453932525933606\n",
            "16     \t [ 1.13635919  9.59512209 12.          0.5730343  19.          0.36892674]. \t  -0.5697966935770927 \t -0.453932525933606\n",
            "17     \t [6.7007636  0.         5.         0.5        9.02769561 0.1       ]. \t  -0.6997654771340948 \t -0.453932525933606\n",
            "18     \t [2.67758022 4.58792547 5.         0.80193375 3.         0.36129163]. \t  -0.5801383609237367 \t -0.453932525933606\n",
            "19     \t [ 0.          0.11898767  8.66620882  0.5        15.66620882  0.1       ]. \t  -0.6994511860723951 \t -0.453932525933606\n",
            "20     \t [ 4.7028715   4.84232296 11.          0.86477956 14.          0.62900428]. \t  -0.534274537118095 \t -0.453932525933606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.347883948412589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJG2fAtAVFDZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b91b9a-5f6e-4484-9cf0-5eade6f3242f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\n",
        "\n",
        "np.random.seed(run_num_18)\n",
        "surrogate_loser_18 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
        "\n",
        "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_18 = dGPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity18, param, n_jobs = -1) # define BayesOpt\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_18 = loser_18.getResult()[0]\n",
        "params_loser_18['max_depth'] = int(params_loser_18['max_depth'])\n",
        "params_loser_18['min_child_weight'] = int(params_loser_18['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train18 = xgb.DMatrix(X_train18, y_train18)\n",
        "dX_loser_test18 = xgb.DMatrix(X_test18, y_test18)\n",
        "model_loser_18 = xgb.train(params_loser_18, dX_loser_train18)\n",
        "pred_loser_18 = model_loser_18.predict(dX_loser_test18)\n",
        "\n",
        "rmse_loser_18 = np.sqrt(mean_squared_error(pred_loser_18, y_test18))\n",
        "rmse_loser_18"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  -0.48312028405718427 \t -0.4337207096533448\n",
            "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  -0.44701743563076113 \t -0.4337207096533448\n",
            "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  -0.6042122061654378 \t -0.4337207096533448\n",
            "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  -0.47205818581725156 \t -0.4337207096533448\n",
            "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  -0.4337207096533448 \t -0.4337207096533448\n",
            "1      \t [ 1.46586883  1.13810325 14.          0.90052794  5.          0.16182112]. \t  -0.7143993241707929 \t -0.4337207096533448\n",
            "2      \t [ 9.17850108  5.22481201 14.          0.60105853 18.          0.63442006]. \t  -0.47652759630987357 \t -0.4337207096533448\n",
            "3      \t [6.20612396 4.37552923 6.         0.71941082 3.         0.90943147]. \t  -0.4522652783180721 \t -0.4337207096533448\n",
            "4      \t [ 0.42107725  9.92938812 12.          0.92462675 18.          0.88352347]. \t  \u001b[92m-0.42696094495506925\u001b[0m \t -0.42696094495506925\n",
            "5      \t [ 1.45418057  8.89399    11.          0.93175096  1.          0.52391385]. \t  -0.4748883821619788 \t -0.42696094495506925\n",
            "6      \t [0.         0.         5.21827599 0.5        6.21827599 0.1       ]. \t  -0.7142978598833383 \t -0.42696094495506925\n",
            "7      \t [ 8.67626106  0.1397848   6.          0.55771681 18.          0.89120888]. \t  -0.4678231542313444 \t -0.42696094495506925\n",
            "8      \t [ 7.20807715  7.41236348  5.          0.52425285 19.          0.24651851]. \t  -0.7151531754828971 \t -0.42696094495506925\n",
            "9      \t [7.94412856 1.24068931 7.         0.96649935 9.         0.47928692]. \t  -0.48764145519377255 \t -0.42696094495506925\n",
            "10     \t [ 2.66410938  9.83743919 13.          0.58899688  8.          0.29675269]. \t  -0.6087724199154481 \t -0.42696094495506925\n",
            "11     \t [ 8.62753755  4.44899956 12.          0.98231134  1.          0.88389967]. \t  \u001b[92m-0.42538744453232347\u001b[0m \t -0.42538744453232347\n",
            "12     \t [ 1.2277143   1.05411485  5.          0.50198686 13.          0.89258454]. \t  -0.4690426518871574 \t -0.42538744453232347\n",
            "13     \t [ 3.86727486  0.04582369 10.          0.54629234  1.          0.59406244]. \t  -0.47507759871416455 \t -0.42538744453232347\n",
            "14     \t [9.89654007 7.91459554 5.         0.64703423 6.         0.95875063]. \t  -0.4610204259667869 \t -0.42538744453232347\n",
            "15     \t [ 9.49381141  1.31696272 11.          0.51283555 13.          0.25364349]. \t  -0.7155727045133423 \t -0.42538744453232347\n",
            "16     \t [0.         3.85327138 5.71339306 0.5        1.         0.1       ]. \t  -0.7130688739294866 \t -0.42538744453232347\n",
            "17     \t [ 1.04974938  7.3117691   5.          0.60120794 17.          0.83587336]. \t  -0.49012043280719225 \t -0.42538744453232347\n",
            "18     \t [5.83203328 9.4812958  5.         0.60479193 1.         0.24995758]. \t  -0.7139401129279975 \t -0.42538744453232347\n",
            "19     \t [ 3.86883296  7.31704712 14.          0.75047101 14.          0.80828494]. \t  -0.4565506921595134 \t -0.42538744453232347\n",
            "20     \t [ 4.0233419   3.91847735 10.          0.87935081  7.          0.57035954]. \t  -0.47552190244029113 \t -0.42538744453232347\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.937438334950126"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHidSEGcVHvG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f20492c-6db5-44f2-a32b-e4d22ae02e1a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\n",
        "\n",
        "np.random.seed(run_num_19)\n",
        "surrogate_loser_19 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
        "\n",
        "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_19 = dGPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity19, param, n_jobs = -1) # define BayesOpt\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_19 = loser_19.getResult()[0]\n",
        "params_loser_19['max_depth'] = int(params_loser_19['max_depth'])\n",
        "params_loser_19['min_child_weight'] = int(params_loser_19['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train19 = xgb.DMatrix(X_train19, y_train19)\n",
        "dX_loser_test19 = xgb.DMatrix(X_test19, y_test19)\n",
        "model_loser_19 = xgb.train(params_loser_19, dX_loser_train19)\n",
        "pred_loser_19 = model_loser_19.predict(dX_loser_test19)\n",
        "\n",
        "rmse_loser_19 = np.sqrt(mean_squared_error(pred_loser_19, y_test19))\n",
        "rmse_loser_19"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.24520174  9.84626759 12.          0.81846905 12.          0.27594309]. \t  -0.6624515538441494 \t -0.4321567975765851\n",
            "init   \t [ 8.6021676   8.64300514 14.          0.95067208 18.          0.13363564]. \t  -0.663790961968593 \t -0.4321567975765851\n",
            "init   \t [ 3.02486657  6.06229509 10.          0.50494482  9.          0.10218675]. \t  -0.668875700942164 \t -0.4321567975765851\n",
            "init   \t [ 6.57586195  8.01231848  7.          0.63945407 18.          0.49206406]. \t  -0.5640092362987262 \t -0.4321567975765851\n",
            "init   \t [ 6.05008856  8.15558548 14.          0.64718943  8.          0.90611106]. \t  -0.4321567975765851 \t -0.4321567975765851\n",
            "1      \t [ 0.85655458  0.69294498  9.          0.90992265 19.          0.25605959]. \t  -0.6632879756016564 \t -0.4321567975765851\n",
            "2      \t [ 1.16239725  9.89254371 11.          0.98469188  1.          0.20271996]. \t  -0.6659068700895453 \t -0.4321567975765851\n",
            "3      \t [9.26425961 9.717621   5.         0.8856331  5.         0.65589573]. \t  -0.5256636565447839 \t -0.4321567975765851\n",
            "4      \t [8.38158931 1.39953008 5.         0.6836482  9.         0.46934447]. \t  -0.5657479189361881 \t -0.4321567975765851\n",
            "5      \t [ 9.89729152  2.88709408 11.          0.55121781  3.          0.58381748]. \t  -0.5217676759230677 \t -0.4321567975765851\n",
            "6      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6682393310586452 \t -0.4321567975765851\n",
            "7      \t [ 0.52039992  0.56380532 14.          0.70324781  2.          0.77427978]. \t  -0.44119318049659856 \t -0.4321567975765851\n",
            "8      \t [ 9.45607821  0.96438893 14.          0.56821928 16.          0.48995458]. \t  -0.560546633196372 \t -0.4321567975765851\n",
            "9      \t [ 0.56432836  7.79494932 13.          0.55871992 19.          0.10266045]. \t  -0.6660467600439527 \t -0.4321567975765851\n",
            "10     \t [ 3.16676683  1.78770811 13.          0.5650927  13.          0.7417697 ]. \t  -0.4472490143246649 \t -0.4321567975765851\n",
            "11     \t [5.56939336 3.95670769 5.         0.95243816 3.         0.31115527]. \t  -0.6118696715045784 \t -0.4321567975765851\n",
            "12     \t [0.         0.         6.79132524 0.5        9.79132524 0.1       ]. \t  -0.6680142576361826 \t -0.4321567975765851\n",
            "13     \t [ 7.46605559  8.55652968 13.          0.99452706  1.          0.43764631]. \t  -0.5573240867828692 \t -0.4321567975765851\n",
            "14     \t [ 9.00192632  0.12142981  6.          0.72039779 17.          0.26695117]. \t  -0.6635472276398457 \t -0.4321567975765851\n",
            "15     \t [ 1.20113738  5.88016015  6.          0.57839075 15.          0.60752162]. \t  -0.5267223663659426 \t -0.4321567975765851\n",
            "16     \t [1.7958895  9.19371155 5.         0.54448974 5.         0.93868386]. \t  -0.4669880318809955 \t -0.4321567975765851\n",
            "17     \t [ 9.62487731  6.86253449  5.          0.66465947 12.          0.21107909]. \t  -0.6659019290689656 \t -0.4321567975765851\n",
            "18     \t [ 7.01475491  4.53933393 10.          0.82075044 14.          0.83058729]. \t  -0.4517750017757683 \t -0.4321567975765851\n",
            "19     \t [ 5.75277105  1.25995511 14.          0.97274397  8.          0.39753437]. \t  -0.6135363407765908 \t -0.4321567975765851\n",
            "20     \t [ 2.00664037  5.88682269 14.          0.94696775  4.          0.80981462]. \t  -0.4349577325456077 \t -0.4321567975765851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2307134915487135"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWGPYRJhVKsO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5693bd64-d376-464c-ad94-dbd9d15320ef"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\n",
        "\n",
        "np.random.seed(run_num_20)\n",
        "surrogate_loser_20 = dGaussianProcess(cov_func)\n",
        "\n",
        "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
        "\n",
        "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
        "    return operator * score\n",
        "\n",
        "loser_20 = dGPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity20, param, n_jobs = -1) # define BayesOpt\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_loser_20 = loser_20.getResult()[0]\n",
        "params_loser_20['max_depth'] = int(params_loser_20['max_depth'])\n",
        "params_loser_20['min_child_weight'] = int(params_loser_20['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_loser_train20 = xgb.DMatrix(X_train20, y_train20)\n",
        "dX_loser_test20 = xgb.DMatrix(X_test20, y_test20)\n",
        "model_loser_20 = xgb.train(params_loser_20, dX_loser_train20)\n",
        "pred_loser_20 = model_loser_20.predict(dX_loser_test20)\n",
        "\n",
        "rmse_loser_20 = np.sqrt(mean_squared_error(pred_loser_20, y_test20))\n",
        "rmse_loser_20"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  -0.5510634045248362 \t -0.4667910180825121\n",
            "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  -0.4667910180825121 \t -0.4667910180825121\n",
            "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  -0.6541702754539586 \t -0.4667910180825121\n",
            "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  -0.5813476654605825 \t -0.4667910180825121\n",
            "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  -0.5078849923055367 \t -0.4667910180825121\n",
            "1      \t [ 9.25190708  0.65043072 10.          0.56676233  5.          0.41289847]. \t  -0.6164858434894066 \t -0.4667910180825121\n",
            "2      \t [8.65689736 8.11684598 5.         0.53907072 1.         0.87574215]. \t  -0.47576906935129115 \t -0.4667910180825121\n",
            "3      \t [6.49728107e-01 7.72862043e-04 7.00000000e+00 5.71905117e-01\n",
            " 2.00000000e+00 1.02014940e-01]. \t  -0.6533990377801203 \t -0.4667910180825121\n",
            "4      \t [ 9.51806562  0.44900923  6.          0.62997227 16.          0.40028974]. \t  -0.6257141325107729 \t -0.4667910180825121\n",
            "5      \t [ 0.48808143  1.97030839  7.          0.79892377 16.          0.98477167]. \t  \u001b[92m-0.4602371335347518\u001b[0m \t -0.4602371335347518\n",
            "6      \t [ 9.98029391  8.23632437  6.          0.63606408 13.          0.22908429]. \t  -0.6532745897130707 \t -0.4602371335347518\n",
            "7      \t [ 3.9245879   0.87058505 14.          0.68269183  1.          0.34048922]. \t  -0.623283664424315 \t -0.4602371335347518\n",
            "8      \t [ 0.45468431  0.13267263 10.          0.667214    9.          0.47071141]. \t  -0.5807933165122651 \t -0.4602371335347518\n",
            "9      \t [ 4.64792903  9.65609988  6.          0.78696753 17.          0.90406502]. \t  -0.47227152594951016 \t -0.4602371335347518\n",
            "10     \t [ 9.73713042  9.77875019 14.          0.8552366  13.          0.6588042 ]. \t  -0.5450528828222597 \t -0.4602371335347518\n",
            "11     \t [ 9.74564452  1.02954501 14.          0.98964301 12.          0.47743205]. \t  -0.5773481317621234 \t -0.4602371335347518\n",
            "12     \t [ 7.76906477  2.6060444   5.          0.53491145 10.          0.41560504]. \t  -0.627993339841767 \t -0.4602371335347518\n",
            "13     \t [6.49958993 1.98622034 6.         0.83635209 1.         0.2931143 ]. \t  -0.6276302072456058 \t -0.4602371335347518\n",
            "14     \t [1.36051555 9.95957797 7.         0.9242116  2.         0.87097462]. \t  \u001b[92m-0.449167771194887\u001b[0m \t -0.449167771194887\n",
            "15     \t [ 0.20978943  8.45447688 14.          0.77351134 17.          0.2656195 ]. \t  -0.6586945947582249 \t -0.449167771194887\n",
            "16     \t [7.55121074 9.86496533 6.         0.88828535 7.         0.3097795 ]. \t  -0.6258969685789838 \t -0.449167771194887\n",
            "17     \t [ 1.88681738  3.46035586 14.          0.68070687  6.          0.9944693 ]. \t  \u001b[92m-0.42905917541157645\u001b[0m \t -0.42905917541157645\n",
            "18     \t [ 1.00455473  4.86452269 10.          0.65242434  1.          0.22603818]. \t  -0.6579989052523156 \t -0.42905917541157645\n",
            "19     \t [ 0.87144588  0.14777143 14.          0.88891044 13.          0.11582186]. \t  -0.6586044677275812 \t -0.42905917541157645\n",
            "20     \t [ 8.17732484  5.22697425 11.          0.61303379  1.          0.32672413]. \t  -0.6191817873624627 \t -0.42905917541157645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2900830531756915"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1d_1LyydIfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5edc1121-e2f8-4dac-d3ab-af4c5b079a74"
      },
      "source": [
        "end_lose = time.time()\n",
        "end_lose\n",
        "\n",
        "time_lose = end_lose - start_lose\n",
        "time_lose\n",
        "\n",
        "start_win = time.time()\n",
        "start_win"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1614595863.6794083"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAyOw7XYVwAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab14a3b8-16eb-481f-9fb5-28ace67e7620"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \n",
        "\n",
        "np.random.seed(run_num_1)\n",
        "surrogate_winner_1 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=test_perc, random_state=run_num_1)\n",
        "\n",
        "def f_syn_polarity1(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_1, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train1, y=y_train1).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_1 = dGPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity1, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_1 = winner_1.getResult()[0]\n",
        "params_winner_1['max_depth'] = int(params_winner_1['max_depth'])\n",
        "params_winner_1['min_child_weight'] = int(params_winner_1['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train1 = xgb.DMatrix(X_train1, y_train1)\n",
        "dX_winner_test1 = xgb.DMatrix(X_test1, y_test1)\n",
        "model_winner_1 = xgb.train(params_winner_1, dX_winner_train1)\n",
        "pred_winner_1 = model_winner_1.predict(dX_winner_test1)\n",
        "\n",
        "rmse_winner_1 = np.sqrt(mean_squared_error(pred_winner_1, y_test1))\n",
        "rmse_winner_1"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.12170176  1.69069754 11.          0.92695323  3.          0.36579277]. \t  -0.5875926438135084 \t -0.45245711231879204\n",
            "init   \t [ 1.49162957  0.22478325  6.          0.85613654 15.          0.19363322]. \t  -0.6811472770543097 \t -0.45245711231879204\n",
            "init   \t [ 5.27957805  2.81819356 11.          0.81062146 14.          0.74826845]. \t  -0.46117277972616416 \t -0.45245711231879204\n",
            "init   \t [3.81060005 6.24236569 7.         0.95038709 5.         0.9861745 ]. \t  -0.45245711231879204 \t -0.45245711231879204\n",
            "init   \t [4.77531134 7.87990376 6.         0.78863691 6.         0.47918609]. \t  -0.5612893767574513 \t -0.45245711231879204\n",
            "1      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6826322236836013 \t -0.45245711231879204\n",
            "2      \t [ 9.60774341  8.94642175  5.          0.60295867 15.          0.76784922]. \t  -0.5001571385184761 \t -0.45245711231879204\n",
            "3      \t [ 0.4810761   9.31563532  8.          0.92845546 14.          0.70014982]. \t  -0.5056468669267833 \t -0.45245711231879204\n",
            "4      \t [ 9.85849625  0.83439907  7.          0.7705582  19.          0.15487995]. \t  -0.6815452883308839 \t -0.45245711231879204\n",
            "5      \t [ 9.1348132   9.54667443 13.          0.95557113 18.          0.11016711]. \t  -0.6791252728059957 \t -0.45245711231879204\n",
            "6      \t [ 9.97122318  8.75901621 13.          0.9904996   9.          0.60161775]. \t  -0.4895864123043516 \t -0.45245711231879204\n",
            "7      \t [7.19079936 0.87565629 5.         0.54470564 8.         0.4449021 ]. \t  -0.5736147518216788 \t -0.45245711231879204\n",
            "8      \t [ 5.24989998  8.90398408 13.          0.54165135  1.          0.74771929]. \t  -0.46141851807096074 \t -0.45245711231879204\n",
            "9      \t [ 1.7157918   5.89141512 14.          0.5296881  10.          0.1803042 ]. \t  -0.6825568503758982 \t -0.45245711231879204\n",
            "10     \t [ 1.33491429  4.9488224  14.          0.57825982 18.          0.42075517]. \t  -0.5921600742966667 \t -0.45245711231879204\n",
            "11     \t [8.99095022 4.91206514 6.         0.87212346 1.         0.53198502]. \t  -0.5638457176592562 \t -0.45245711231879204\n",
            "12     \t [0.51753753 1.68511981 9.         0.80360039 9.         0.45200858]. \t  -0.5426731188677099 \t -0.45245711231879204\n",
            "13     \t [ 0.47819106  0.69361363 14.          0.7271902   3.          0.48615024]. \t  -0.5388370338312534 \t -0.45245711231879204\n",
            "14     \t [ 3.39786838  7.15897283  5.          0.7138544  19.          0.22047835]. \t  -0.6821252760743777 \t -0.45245711231879204\n",
            "15     \t [0.4939583  9.37390762 8.         0.7641092  1.         0.4917045 ]. \t  -0.5481894484993145 \t -0.45245711231879204\n",
            "16     \t [ 8.70209222  0.21363429 14.          0.98391211 18.          0.50653374]. \t  -0.5341942124085062 \t -0.45245711231879204\n",
            "17     \t [ 5.30693639  9.58195731  9.          0.53796245 11.          0.46099251]. \t  -0.5549909911648074 \t -0.45245711231879204\n",
            "18     \t [ 3.83524595  9.61883757 12.          0.81445794 17.          0.17700713]. \t  -0.6797728963073484 \t -0.45245711231879204\n",
            "19     \t [ 0.25234021  8.87254669 14.          0.7101496   5.          0.12345742]. \t  -0.684133223230854 \t -0.45245711231879204\n",
            "20     \t [9.81917647 9.19103216 5.         0.82619792 9.         0.10869763]. \t  -0.6822789922076578 \t -0.45245711231879204\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.87018129255363"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrDQbChpZ48F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b75f7d5-5dfb-4429-9199-8ca5e041e582"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \n",
        "\n",
        "np.random.seed(run_num_2)\n",
        "surrogate_winner_2 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=test_perc, random_state=run_num_2)\n",
        "\n",
        "def f_syn_polarity2(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_2, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train2, y=y_train2).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_2 = dGPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity2, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_2 = winner_2.getResult()[0]\n",
        "params_winner_2['max_depth'] = int(params_winner_2['max_depth'])\n",
        "params_winner_2['min_child_weight'] = int(params_winner_2['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train2 = xgb.DMatrix(X_train2, y_train2)\n",
        "dX_winner_test2 = xgb.DMatrix(X_test2, y_test2)\n",
        "model_winner_2 = xgb.train(params_winner_2, dX_winner_train2)\n",
        "pred_winner_2 = model_winner_2.predict(dX_winner_test2)\n",
        "\n",
        "rmse_winner_2 = np.sqrt(mean_squared_error(pred_winner_2, y_test2))\n",
        "rmse_winner_2"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.51985493  0.73903599  7.          0.71824677 17.          0.61827209]. \t  -0.5233872966482411 \t -0.42848969661986275\n",
            "init   \t [ 8.4047092   4.35120551 12.          0.84231905 16.          0.80172484]. \t  -0.42848969661986275 \t -0.42848969661986275\n",
            "init   \t [ 6.42749368  9.61026168 10.          0.73363448 15.          0.84932207]. \t  -0.43419399571353007 \t -0.42848969661986275\n",
            "init   \t [ 2.66008363  8.36685389  6.          0.92082163 12.          0.69999264]. \t  -0.5353675801921922 \t -0.42848969661986275\n",
            "init   \t [ 1.74779478  2.17691512 12.          0.71794562  2.          0.48990646]. \t  -0.5346620615324694 \t -0.42848969661986275\n",
            "1      \t [0.65899411 0.08911785 5.         0.57513718 8.         0.6649358 ]. \t  -0.5435492302872315 \t -0.42848969661986275\n",
            "2      \t [7.50154303 9.49874736 7.         0.83842364 4.         0.5714185 ]. \t  -0.5422231033003287 \t -0.42848969661986275\n",
            "3      \t [9.88263606 1.71181353 7.         0.97275193 1.         0.74825108]. \t  -0.441747506473179 \t -0.42848969661986275\n",
            "4      \t [ 0.38425009  3.26044865 12.          0.89101602 19.          0.732816  ]. \t  \u001b[92m-0.4225097924337229\u001b[0m \t -0.4225097924337229\n",
            "5      \t [ 0.17098677  4.24340552 14.          0.58045842 10.          0.64319766]. \t  -0.5100599626019375 \t -0.4225097924337229\n",
            "6      \t [ 8.24807017  4.40295761 13.          0.99966982  5.          0.29909156]. \t  -0.5674503952477116 \t -0.4225097924337229\n",
            "7      \t [8.17228996 0.99614058 7.         0.60509314 8.         0.9040458 ]. \t  -0.44411068565064227 \t -0.4225097924337229\n",
            "8      \t [ 2.33543052  9.65234501 14.          0.6896847  19.          0.38329235]. \t  -0.5709606841295877 \t -0.4225097924337229\n",
            "9      \t [ 2.26995028  7.26113474  5.          0.89244153 19.          0.72822627]. \t  -0.46107668663747675 \t -0.4225097924337229\n",
            "10     \t [0.15708054 7.33475875 5.         0.73448573 4.         0.78960732]. \t  -0.45554380802299876 \t -0.4225097924337229\n",
            "11     \t [ 2.40520679  9.030061   11.          0.61309393  6.          0.56126501]. \t  -0.5355680728892867 \t -0.4225097924337229\n",
            "12     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6860882831213788 \t -0.4225097924337229\n",
            "13     \t [ 1.232075    0.38392574  6.          0.82892322 17.          0.89031195]. \t  -0.4492334705885589 \t -0.4225097924337229\n",
            "14     \t [ 9.36121058  6.35176728  6.          0.60864885 19.          0.85644281]. \t  -0.46989433163634564 \t -0.4225097924337229\n",
            "15     \t [ 9.59276769  7.37641605  6.          0.89538811 12.          0.94983532]. \t  -0.45071664883790197 \t -0.4225097924337229\n",
            "16     \t [ 3.73257763  0.5989952  11.          0.5927604   8.          0.12999526]. \t  -0.6870220318894334 \t -0.4225097924337229\n",
            "17     \t [ 7.48756016  0.13555104 14.          0.75269236 12.          0.41177876]. \t  -0.5678014017572689 \t -0.4225097924337229\n",
            "18     \t [ 8.11913722  7.7931672  14.          0.50393458 11.          0.23511725]. \t  -0.6881787024693147 \t -0.4225097924337229\n",
            "19     \t [ 5.58220629  9.80183415 14.          0.65958042  3.          0.18872667]. \t  -0.6874927850540977 \t -0.4225097924337229\n",
            "20     \t [4.48522304 4.53396218 8.         0.58675824 5.         0.98490977]. \t  -0.43730907484181936 \t -0.4225097924337229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.316024710268154"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpUPyXRfZ95Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "810fd2a2-ac10-4ee4-a7ee-2c8466935dcf"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \n",
        "\n",
        "np.random.seed(run_num_3)\n",
        "surrogate_winner_3 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=test_perc, random_state=run_num_3)\n",
        "\n",
        "def f_syn_polarity3(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_3, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train3, y=y_train3).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_3 = dGPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity3, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_3 = winner_3.getResult()[0]\n",
        "params_winner_3['max_depth'] = int(params_winner_3['max_depth'])\n",
        "params_winner_3['min_child_weight'] = int(params_winner_3['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train3 = xgb.DMatrix(X_train3, y_train3)\n",
        "dX_winner_test3 = xgb.DMatrix(X_test3, y_test3)\n",
        "model_winner_3 = xgb.train(params_winner_3, dX_winner_train3)\n",
        "pred_winner_3 = model_winner_3.predict(dX_winner_test3)\n",
        "\n",
        "rmse_winner_3 = np.sqrt(mean_squared_error(pred_winner_3, y_test3))\n",
        "rmse_winner_3"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 7.51575607  1.09251275 13.          0.80377147 16.          0.31229624]. \t  -0.6858957949546655 \t -0.5081673732303724\n",
            "init   \t [6.12794891 1.19652403 5.         0.74822174 8.         0.33695838]. \t  -0.6849895205987827 \t -0.5081673732303724\n",
            "init   \t [ 6.21631223  4.4909976  10.          0.66764529  9.          0.77207323]. \t  -0.5081673732303724 \t -0.5081673732303724\n",
            "init   \t [ 0.58127233  4.79918477 11.          0.64526552 15.          0.62431773]. \t  -0.5106501747238601 \t -0.5081673732303724\n",
            "init   \t [ 5.76228258  0.67219724  7.          0.88228691 13.          0.29867219]. \t  -0.6839256266327581 \t -0.5081673732303724\n",
            "1      \t [9.79994489 9.57810833 8.         0.93763368 1.         0.1772549 ]. \t  -0.7316297465421655 \t -0.5081673732303724\n",
            "2      \t [ 3.27075931  5.07207678 14.          0.99587284  2.          0.51564135]. \t  -0.6620507374994331 \t -0.5081673732303724\n",
            "3      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7323904210578609 \t -0.5081673732303724\n",
            "4      \t [ 8.88346709  9.03427619  8.          0.525814   19.          0.22866902]. \t  -0.732406939137863 \t -0.5081673732303724\n",
            "5      \t [1.21637479 8.34226514 5.         0.57939054 5.         0.55479603]. \t  -0.6531491980452168 \t -0.5081673732303724\n",
            "6      \t [ 2.93278685  9.15897164 14.          0.67343746 10.          0.85376406]. \t  \u001b[92m-0.504811636744922\u001b[0m \t -0.504811636744922\n",
            "7      \t [ 1.1995358   9.87930749  5.          0.84448568 18.          0.26022822]. \t  -0.7297224033867351 \t -0.504811636744922\n",
            "8      \t [ 1.96525323  0.16048396 14.          0.65133015  9.          0.3003508 ]. \t  -0.6860931128073492 \t -0.504811636744922\n",
            "9      \t [6.19566593 0.54575662 9.         0.53247577 1.         0.58149275]. \t  -0.5171483404397559 \t -0.504811636744922\n",
            "10     \t [ 3.97093318  9.58939295  6.          0.5381492  12.          0.74760437]. \t  -0.5217629748454513 \t -0.504811636744922\n",
            "11     \t [ 9.94719165  9.26136843 13.          0.64663072 11.          0.47018827]. \t  -0.6546346492088932 \t -0.504811636744922\n",
            "12     \t [9.55919323 7.66342182 6.         0.63144136 7.         0.82664124]. \t  -0.5195220027397864 \t -0.504811636744922\n",
            "13     \t [ 2.80079796  0.61761702  5.          0.62606225 19.          0.71334647]. \t  -0.5274647167409906 \t -0.504811636744922\n",
            "14     \t [ 9.59599852  3.11576364  5.          0.59256945 17.          0.56621848]. \t  -0.6538676177721936 \t -0.504811636744922\n",
            "15     \t [ 8.91533927  0.16704192 13.          0.66647549  6.          0.25014   ]. \t  -0.7314177593008601 \t -0.504811636744922\n",
            "16     \t [ 4.33253274  9.14516752 14.          0.94398901 19.          0.74581708]. \t  \u001b[92m-0.5007721901039625\u001b[0m \t -0.5007721901039625\n",
            "17     \t [9.75857066 2.84550968 5.         0.7181157  3.         0.67923616]. \t  -0.5274197984747595 \t -0.5007721901039625\n",
            "18     \t [ 0.          0.          5.          0.5        10.43409208  0.1       ]. \t  -0.7323784626096901 \t -0.5007721901039625\n",
            "19     \t [ 9.94497871  9.34276152 14.          0.66308198  5.          0.66061477]. \t  -0.5102482930096841 \t -0.5007721901039625\n",
            "20     \t [ 0.45956381  8.89338741 11.          0.67604128  4.          0.20043998]. \t  -0.7307540586635348 \t -0.5007721901039625\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.6249009343932785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKX_nfEaaAwm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57f4fcb5-ec0e-4cd6-e753-6c438de0200f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \n",
        "\n",
        "np.random.seed(run_num_4)\n",
        "surrogate_winner_4 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=test_perc, random_state=run_num_4)\n",
        "\n",
        "def f_syn_polarity4(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_4, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train4, y=y_train4).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_4 = dGPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity4, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_4 = winner_4.getResult()[0]\n",
        "params_winner_4['max_depth'] = int(params_winner_4['max_depth'])\n",
        "params_winner_4['min_child_weight'] = int(params_winner_4['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train4 = xgb.DMatrix(X_train4, y_train4)\n",
        "dX_winner_test4 = xgb.DMatrix(X_test4, y_test4)\n",
        "model_winner_4 = xgb.train(params_winner_4, dX_winner_train4)\n",
        "pred_winner_4 = model_winner_4.predict(dX_winner_test4)\n",
        "\n",
        "rmse_winner_4 = np.sqrt(mean_squared_error(pred_winner_4, y_test4))\n",
        "rmse_winner_4"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.42798506  1.23319667 14.          0.81910755  7.          0.7021549 ]. \t  -0.5581090891563811 \t -0.5581090891563811\n",
            "init   \t [ 5.54158317  7.71233962 10.          0.59117644  6.          0.31476458]. \t  -0.6204181120564479 \t -0.5581090891563811\n",
            "init   \t [2.18282634 2.93794127 7.         0.84739327 5.         0.62716229]. \t  -0.5626656925564042 \t -0.5581090891563811\n",
            "init   \t [ 5.66159581  4.68678814 11.          0.54147808  9.          0.58609221]. \t  -0.5595885941852147 \t -0.5581090891563811\n",
            "init   \t [ 3.02160958  6.07432614  6.          0.92038559 14.          0.12778609]. \t  -0.6361772582153176 \t -0.5581090891563811\n",
            "1      \t [ 1.89460697  0.10324856 13.          0.70356349 16.          0.67776197]. \t  \u001b[92m-0.5568243993303088\u001b[0m \t -0.5568243993303088\n",
            "2      \t [ 9.69969     4.32607677 14.          0.87739158 16.          0.66014279]. \t  -0.5573098211239071 \t -0.5568243993303088\n",
            "3      \t [ 2.2210328   8.31054196 13.          0.70317243 19.          0.56953182]. \t  -0.573504425404327 \t -0.5568243993303088\n",
            "4      \t [ 0.14706294  0.75702217 14.          0.56309891  8.          0.29887971]. \t  -0.6254087880261816 \t -0.5568243993303088\n",
            "5      \t [ 6.94218243  0.32144048  6.          0.59286079 19.          0.70636825]. \t  -0.5706574487524134 \t -0.5568243993303088\n",
            "6      \t [ 9.24159749  9.43575685  7.          0.98380459 19.          0.34937346]. \t  -0.6187589419936593 \t -0.5568243993303088\n",
            "7      \t [7.57184449 0.0324327  9.         0.86318723 1.         0.6413689 ]. \t  -0.5611723012436254 \t -0.5568243993303088\n",
            "8      \t [ 1.62755426  6.7921099  14.          0.91903975  1.          0.61791343]. \t  -0.5680865264437001 \t -0.5568243993303088\n",
            "9      \t [ 9.43074486  0.98044049  5.          0.96290856 10.          0.27397556]. \t  -0.6390681992417737 \t -0.5568243993303088\n",
            "10     \t [ 8.73128599  9.22658128 14.          0.98344346  2.          0.13489277]. \t  -0.6368164613179 \t -0.5568243993303088\n",
            "11     \t [ 0.          0.          7.37255492  0.5        12.37255492  0.1       ]. \t  -0.6353530029272869 \t -0.5568243993303088\n",
            "12     \t [8.91398595 7.00476796 6.         0.72156795 1.         0.41673037]. \t  -0.6210807206819127 \t -0.5568243993303088\n",
            "13     \t [ 9.58851274  9.75423983 14.          0.60815214 12.          0.97572229]. \t  \u001b[92m-0.44045254560081915\u001b[0m \t -0.44045254560081915\n",
            "14     \t [ 9.87497198  6.68701692  6.          0.55980787 13.          0.32605223]. \t  -0.6200123103666009 \t -0.44045254560081915\n",
            "15     \t [ 2.16464782  9.35692886 11.          0.97431128 12.          0.43257419]. \t  -0.5747879420555002 \t -0.44045254560081915\n",
            "16     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6351353738423114 \t -0.44045254560081915\n",
            "17     \t [0.93838584 7.15564229 8.47367908 0.5        1.47367908 0.1       ]. \t  -0.6346196916676865 \t -0.44045254560081915\n",
            "18     \t [7.36355448 5.3077281  5.         0.5709887  7.         0.87569884]. \t  -0.467633753337906 \t -0.44045254560081915\n",
            "19     \t [ 7.93069008  0.11830338 11.          0.97110397 13.          0.22887984]. \t  -0.6356672520315936 \t -0.44045254560081915\n",
            "20     \t [ 6.39787933  3.84205635 14.          0.72880814  3.          0.21974221]. \t  -0.6364956155454153 \t -0.44045254560081915\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.469425637772456"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJmI9saAaEG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b782dec-b9f2-48ce-e69c-eed0da880e26"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \n",
        "\n",
        "np.random.seed(run_num_5)\n",
        "surrogate_winner_5 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=test_perc, random_state=run_num_5)\n",
        "\n",
        "def f_syn_polarity5(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_5, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train5, y=y_train5).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_5 = dGPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity5, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_5 = winner_5.getResult()[0]\n",
        "params_winner_5['max_depth'] = int(params_winner_5['max_depth'])\n",
        "params_winner_5['min_child_weight'] = int(params_winner_5['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train5 = xgb.DMatrix(X_train5, y_train5)\n",
        "dX_winner_test5 = xgb.DMatrix(X_test5, y_test5)\n",
        "model_winner_5 = xgb.train(params_winner_5, dX_winner_train5)\n",
        "pred_winner_5 = model_winner_5.predict(dX_winner_test5)\n",
        "\n",
        "rmse_winner_5 = np.sqrt(mean_squared_error(pred_winner_5, y_test5))\n",
        "rmse_winner_5"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.57353274  5.41760627  8.          0.53710825 19.          0.26034376]. \t  -0.6523797617249512 \t -0.466901759574539\n",
            "init   \t [5.17826437 9.78817462 5.         0.84048437 6.         0.79712615]. \t  -0.4886603987288415 \t -0.466901759574539\n",
            "init   \t [ 9.84476201  9.65484856  5.          0.59464714 19.          0.34528747]. \t  -0.564220001422124 \t -0.466901759574539\n",
            "init   \t [ 9.17476842  4.53262239 13.          0.64368565 10.          0.515308  ]. \t  -0.5049042831858346 \t -0.466901759574539\n",
            "init   \t [ 8.66719465  7.99685372 12.          0.79833571 17.          0.72443466]. \t  -0.466901759574539 \t -0.466901759574539\n",
            "1      \t [ 3.31480734  6.81602635 12.          0.95330947  1.          0.27334927]. \t  -0.6531167190113543 \t -0.466901759574539\n",
            "2      \t [2.53140694 0.29740057 9.         0.7505917  7.         0.82287404]. \t  -0.47475119282304395 \t -0.466901759574539\n",
            "3      \t [ 9.84728748  0.28222034 13.          0.93969018  3.          0.22339517]. \t  -0.6522247903981228 \t -0.466901759574539\n",
            "4      \t [ 0.68116737  3.94603776 14.          0.80380753 14.          0.77317996]. \t  \u001b[92m-0.4613270217842209\u001b[0m \t -0.4613270217842209\n",
            "5      \t [ 9.6928838   0.38736829  6.          0.56167539 11.          0.4775038 ]. \t  -0.5239028011959734 \t -0.4613270217842209\n",
            "6      \t [ 1.62699639  9.0444625   7.          0.60549869 13.          0.13190699]. \t  -0.6526723866694732 \t -0.4613270217842209\n",
            "7      \t [ 7.31108123  0.10391064 14.          0.53552151 15.          0.19462546]. \t  -0.6531977730432595 \t -0.4613270217842209\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6542193437616992 \t -0.4613270217842209\n",
            "9      \t [ 1.05461775  1.17661042  5.          0.5648338  17.          0.90649901]. \t  -0.4702337390488589 \t -0.4613270217842209\n",
            "10     \t [ 0.29281635  9.83144868 11.          0.57517626 19.          0.13846438]. \t  -0.6543975191191339 \t -0.4613270217842209\n",
            "11     \t [9.66604659 1.51730123 5.         0.59597224 4.         0.66643448]. \t  -0.5016316198301449 \t -0.4613270217842209\n",
            "12     \t [0.57140531 6.2823043  5.         0.70475062 2.         0.3328708 ]. \t  -0.5594485185267623 \t -0.4613270217842209\n",
            "13     \t [ 2.5240907   6.84169344 14.          0.61623097  8.          0.50182056]. \t  -0.5037582468108619 \t -0.4613270217842209\n",
            "14     \t [0.0858834  4.38043443 5.         0.57114514 9.         0.11102852]. \t  -0.6531285980012218 \t -0.4613270217842209\n",
            "15     \t [ 9.08851157  7.7443441   6.          0.74314397 12.          0.52508293]. \t  -0.5238407378055661 \t -0.4613270217842209\n",
            "16     \t [ 0.62266475  1.9018111  14.          0.63329124  3.          0.94054341]. \t  \u001b[92m-0.42529491633232686\u001b[0m \t -0.42529491633232686\n",
            "17     \t [ 9.23494676  7.58606552 12.          0.78487568  2.          0.96478853]. \t  -0.42939677763278594 \t -0.42529491633232686\n",
            "18     \t [ 5.11817103  4.38089559  9.          0.53776664 13.          0.7209118 ]. \t  -0.4792507830025482 \t -0.42529491633232686\n",
            "19     \t [8.44899175 7.77761284 6.         0.53217155 1.         0.55958377]. \t  -0.5238817589404079 \t -0.42529491633232686\n",
            "20     \t [ 3.67612744  5.42599858 14.          0.93140269 19.          0.80853077]. \t  -0.4626498801811607 \t -0.42529491633232686\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.31722690080873"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulhEolsxaG4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bc655e9-a428-4d73-c654-cc67b74992cd"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \n",
        "\n",
        "np.random.seed(run_num_6)\n",
        "surrogate_winner_6 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=test_perc, random_state=run_num_6)\n",
        "\n",
        "def f_syn_polarity6(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=int(min_child_weight),\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_6, objective = 'reg:squarederror', eval_metric = 'rmse')\n",
        "    score = np.array(cross_val_score(reg, X=X_train6, y=y_train6).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_6 = dGPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity6, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_6 = winner_6.getResult()[0]\n",
        "params_winner_6['max_depth'] = int(params_winner_6['max_depth'])\n",
        "params_winner_6['min_child_weight'] = int(params_winner_6['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train6 = xgb.DMatrix(X_train6, y_train6)\n",
        "dX_winner_test6 = xgb.DMatrix(X_test6, y_test6)\n",
        "model_winner_6 = xgb.train(params_winner_6, dX_winner_train6)\n",
        "pred_winner_6 = model_winner_6.predict(dX_winner_test6)\n",
        "\n",
        "rmse_winner_6 = np.sqrt(mean_squared_error(pred_winner_6, y_test6))\n",
        "rmse_winner_6"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.92860151 3.31979805 5.         0.99251441 2.         0.57683563]. \t  -0.5471181110640053 \t -0.5219867521874206\n",
            "init   \t [4.18807429 3.35407849 9.         0.87750649 3.         0.56623277]. \t  -0.629286701398377 \t -0.5219867521874206\n",
            "init   \t [ 5.788586    6.45355096 14.          0.70660047 12.          0.82154882]. \t  -0.5219867521874206 \t -0.5219867521874206\n",
            "init   \t [4.58184578 6.73834679 5.         0.90108528 3.         0.65482895]. \t  -0.5476166257785141 \t -0.5219867521874206\n",
            "init   \t [ 4.42510505  5.75952352 14.          0.97882365 15.          0.29525604]. \t  -0.6655867879161768 \t -0.5219867521874206\n",
            "1      \t [ 7.09628606  0.82458553  6.          0.52978945 17.          0.78827589]. \t  -0.5368551114917748 \t -0.5219867521874206\n",
            "2      \t [ 1.88885215  8.62524025  6.          0.90916563 14.          0.40379086]. \t  -0.6680072912825885 \t -0.5219867521874206\n",
            "3      \t [ 0.5654966   9.52584762 14.          0.82016688  4.          0.10717254]. \t  -0.7389555129670606 \t -0.5219867521874206\n",
            "4      \t [ 9.63033822  0.03793409 13.          0.68076302  7.          0.91391681]. \t  \u001b[92m-0.4301920669254681\u001b[0m \t -0.4301920669254681\n",
            "5      \t [ 9.97469028  9.15579642 12.          0.66745021 19.          0.28959158]. \t  -0.6668248751122167 \t -0.4301920669254681\n",
            "6      \t [ 9.85880187  8.48475629 13.          0.76006171  2.          0.58751173]. \t  -0.5403277210407594 \t -0.4301920669254681\n",
            "7      \t [ 9.51844     9.62786679  5.          0.56989798 15.          0.51068013]. \t  -0.6345288616058514 \t -0.4301920669254681\n",
            "8      \t [ 0.15151883  3.04875397  5.          0.61495161 19.          0.79420245]. \t  -0.5357132607573185 \t -0.4301920669254681\n",
            "9      \t [ 3.08077333  0.89569015  6.          0.84149914 11.          0.94992477]. \t  -0.4472711154368675 \t -0.4301920669254681\n",
            "10     \t [ 0.19816257  1.95434851 14.          0.59292238  7.          0.17971262]. \t  -0.7383261448869979 \t -0.4301920669254681\n",
            "11     \t [ 0.56219975  9.42928268 11.          0.51903033 19.          0.52214278]. \t  -0.6326946461691906 \t -0.4301920669254681\n",
            "12     \t [9.58176304 9.94093363 6.         0.96280567 8.         0.32855082]. \t  -0.6677341620025121 \t -0.4301920669254681\n",
            "13     \t [ 5.43877488  0.17504551 13.          0.58721411 19.          0.60115325]. \t  -0.5401916158723044 \t -0.4301920669254681\n",
            "14     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7382709735940614 \t -0.4301920669254681\n",
            "15     \t [9.18163606 3.89469538 8.         0.59772622 8.         0.18901946]. \t  -0.7380580836797348 \t -0.4301920669254681\n",
            "16     \t [0.52725495 9.82259272 6.         0.62080595 7.         0.68770749]. \t  -0.5497290569747493 \t -0.4301920669254681\n",
            "17     \t [ 6.71573436  6.18076517  8.          0.50736725 18.          0.99692636]. \t  -0.4548470613295324 \t -0.4301920669254681\n",
            "18     \t [ 7.23381367  0.21465732 13.          0.75484136 13.          0.44717401]. \t  -0.6306144837329356 \t -0.4301920669254681\n",
            "19     \t [ 0.17525712  4.19183624 13.          0.93093846  1.          0.77496282]. \t  -0.5254340278392874 \t -0.4301920669254681\n",
            "20     \t [ 0.22896089  2.69806987 14.          0.53297649 18.          0.65056758]. \t  -0.5430966302037052 \t -0.4301920669254681\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.1451215046465775"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYebx3RVaJ1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3662c902-0fff-4605-ed98-e25e74a0d617"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \n",
        "\n",
        "np.random.seed(run_num_7)\n",
        "surrogate_winner_7 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=test_perc, random_state=run_num_7)\n",
        "\n",
        "def f_syn_polarity7(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_7, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train7, y=y_train7).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_7 = dGPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity7, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_7 = winner_7.getResult()[0]\n",
        "params_winner_7['max_depth'] = int(params_winner_7['max_depth'])\n",
        "params_winner_7['min_child_weight'] = int(params_winner_7['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train7 = xgb.DMatrix(X_train7, y_train7)\n",
        "dX_winner_test7 = xgb.DMatrix(X_test7, y_test7)\n",
        "model_winner_7 = xgb.train(params_winner_7, dX_winner_train7)\n",
        "pred_winner_7 = model_winner_7.predict(dX_winner_test7)\n",
        "\n",
        "rmse_winner_7 = np.sqrt(mean_squared_error(pred_winner_7, y_test7))\n",
        "rmse_winner_7"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.11505126  6.92324143 13.          0.94046175 12.          0.3650384 ]. \t  -0.612852517751432 \t -0.4284992540085738\n",
            "init   \t [ 5.15357029  9.0066636   6.          0.74104227 15.          0.8966337 ]. \t  -0.45500673865987984 \t -0.4284992540085738\n",
            "init   \t [ 2.63548913  9.6610934  14.          0.64712408  1.          0.80820948]. \t  -0.43730487111474525 \t -0.4284992540085738\n",
            "init   \t [ 1.40821426  1.72758589 12.          0.70495069  5.          0.86632715]. \t  -0.4284992540085738 \t -0.4284992540085738\n",
            "init   \t [2.85126987 2.73434054 7.         0.88394733 8.         0.65545939]. \t  -0.48253390492139586 \t -0.4284992540085738\n",
            "1      \t [1.03697999e-02 2.51695028e+00 1.40000000e+01 6.19497248e-01\n",
            " 1.90000000e+01 7.01827417e-01]. \t  -0.47758423778274467 \t -0.4284992540085738\n",
            "2      \t [9.9089961  7.50235155 7.         0.67925503 5.         0.2471192 ]. \t  -0.6736712023667494 \t -0.4284992540085738\n",
            "3      \t [ 7.21196079  0.11384945  8.          0.67755731 17.          0.74323626]. \t  -0.4647919397201027 \t -0.4284992540085738\n",
            "4      \t [0.35283087 9.2950614  5.         0.93054416 3.         0.27625531]. \t  -0.670508947897148 \t -0.4284992540085738\n",
            "5      \t [ 0.19352731  8.09930152 12.          0.94917595  9.          0.75994611]. \t  -0.4324409379892192 \t -0.4284992540085738\n",
            "6      \t [9.76759527 0.48761621 7.         0.96362191 1.         0.137157  ]. \t  -0.6715547907624335 \t -0.4284992540085738\n",
            "7      \t [ 8.53387117  0.35946342 13.          0.65332712  6.          0.68302293]. \t  -0.4712802345283902 \t -0.4284992540085738\n",
            "8      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6745888440506937 \t -0.4284992540085738\n",
            "9      \t [ 0.30506512  9.51761383 13.          0.5277377  16.          0.92234588]. \t  -0.4390633386106087 \t -0.4284992540085738\n",
            "10     \t [ 9.21287749  3.65759887  5.          0.85687073 12.          0.88459516]. \t  -0.45867733996321497 \t -0.4284992540085738\n",
            "11     \t [ 7.34212739  6.00942781 13.          0.89177574 19.          0.18238365]. \t  -0.6705926103380092 \t -0.4284992540085738\n",
            "12     \t [ 0.08020415  5.86819932  5.          0.75400569 19.          0.89491758]. \t  -0.46498007849528483 \t -0.4284992540085738\n",
            "13     \t [3.90747565e-03 0.00000000e+00 8.90161620e+00 5.00000000e-01\n",
            " 1.39016162e+01 1.00000000e-01]. \t  -0.6739594152904902 \t -0.4284992540085738\n",
            "14     \t [ 4.28784881  0.51490206 14.          0.52848198 14.          0.29231035]. \t  -0.6206000490464539 \t -0.4284992540085738\n",
            "15     \t [ 7.56572451  6.67046024 14.          0.88731871  4.          0.29940932]. \t  -0.6117537685654454 \t -0.4284992540085738\n",
            "16     \t [4.54036829 9.23672573 7.         0.78071654 7.         0.925943  ]. \t  -0.44422074369565967 \t -0.4284992540085738\n",
            "17     \t [ 9.52566208  5.8884151   6.          0.78376025 19.          0.40148749]. \t  -0.6258103990077785 \t -0.4284992540085738\n",
            "18     \t [ 2.59261818  5.09994005  9.          0.90295051 14.          0.32748669]. \t  -0.6164460174367136 \t -0.4284992540085738\n",
            "19     \t [6.33023693 8.46072362 6.         0.94176984 1.         0.70789652]. \t  -0.4820071465062707 \t -0.4284992540085738\n",
            "20     \t [ 2.45273961  5.23216272 10.          0.89291144  1.          0.75188209]. \t  -0.43728627888824734 \t -0.4284992540085738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.492050628419268"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk0IPTSTbIl3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38cd1547-d868-4700-cbfc-577d304e3c0a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \n",
        "\n",
        "np.random.seed(run_num_8)\n",
        "surrogate_winner_8 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=test_perc, random_state=run_num_8)\n",
        "\n",
        "def f_syn_polarity8(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_8, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train8, y=y_train8).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_8 = dGPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity8, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_8 = winner_8.getResult()[0]\n",
        "params_winner_8['max_depth'] = int(params_winner_8['max_depth'])\n",
        "params_winner_8['min_child_weight'] = int(params_winner_8['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train8 = xgb.DMatrix(X_train8, y_train8)\n",
        "dX_winner_test8 = xgb.DMatrix(X_test8, y_test8)\n",
        "model_winner_8 = xgb.train(params_winner_8, dX_winner_train8)\n",
        "pred_winner_8 = model_winner_8.predict(dX_winner_test8)\n",
        "\n",
        "rmse_winner_8 = np.sqrt(mean_squared_error(pred_winner_8, y_test8))\n",
        "rmse_winner_8"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.29221489  0.77396898 11.          0.56062468  3.          0.25451879]. \t  -0.6231794271723858 \t -0.4674926150198253\n",
            "init   \t [ 0.10611429  1.37648061  8.          0.6050375  15.          0.790205  ]. \t  -0.4674926150198253 \t -0.4674926150198253\n",
            "init   \t [ 8.35239571  4.16908043  7.          0.67902746 14.          0.16683749]. \t  -0.6176439249709439 \t -0.4674926150198253\n",
            "init   \t [ 6.06301777  7.27127491 12.          0.87295531 11.          0.37694382]. \t  -0.5951520828762356 \t -0.4674926150198253\n",
            "init   \t [8.85209637 7.89269955 6.         0.72444854 3.         0.47766986]. \t  -0.5688519156580504 \t -0.4674926150198253\n",
            "1      \t [ 1.47825581  9.16010046 11.          0.96606546  1.          0.92358292]. \t  \u001b[92m-0.43518063190798095\u001b[0m \t -0.43518063190798095\n",
            "2      \t [ 7.52273592  2.30141271 14.          0.75505047 19.          0.77804046]. \t  -0.45158957529537797 \t -0.43518063190798095\n",
            "3      \t [1.64420908 9.47353882 5.         0.79993338 7.         0.15673599]. \t  -0.6175838614360663 \t -0.43518063190798095\n",
            "4      \t [ 1.02399475  9.65113789  9.          0.53947972 17.          0.8292164 ]. \t  -0.4719352875928771 \t -0.43518063190798095\n",
            "5      \t [8.14212583 0.61314553 6.         0.86488725 6.         0.28574491]. \t  -0.5954481959734638 \t -0.43518063190798095\n",
            "6      \t [ 8.76387285  7.83054608 13.          0.54159783  4.          0.24710422]. \t  -0.6207082177098945 \t -0.43518063190798095\n",
            "7      \t [ 9.0517509   9.45530828 11.          0.85305105 19.          0.79961763]. \t  -0.4589725747508552 \t -0.43518063190798095\n",
            "8      \t [ 7.77762087  0.97608919 14.          0.56979605  8.          0.73436998]. \t  -0.4483480686509709 \t -0.43518063190798095\n",
            "9      \t [2.67676373 3.95611485 5.         0.79739163 2.         0.89987835]. \t  -0.45674643004535687 \t -0.43518063190798095\n",
            "10     \t [ 0.62854908  4.77207773 14.          0.78182185 18.          0.73284936]. \t  -0.4461225555243119 \t -0.43518063190798095\n",
            "11     \t [3.06290827 1.66002331 6.         0.51411222 9.         0.98529356]. \t  -0.461628022517463 \t -0.43518063190798095\n",
            "12     \t [ 0.55545334  6.5757175   9.          0.88529579 12.          0.24203797]. \t  -0.6187020960077592 \t -0.43518063190798095\n",
            "13     \t [ 9.10053278  1.48753729 11.          0.9741735   1.          0.65977303]. \t  -0.5223704878111852 \t -0.43518063190798095\n",
            "14     \t [ 9.48329103  0.2076913   7.          0.76655383 19.          0.88164364]. \t  -0.4616094756962891 \t -0.43518063190798095\n",
            "15     \t [ 0.81390458  8.42248516 12.          0.78729971  7.          0.71328057]. \t  -0.5176932464043367 \t -0.43518063190798095\n",
            "16     \t [ 5.51535579  8.63063044  6.          0.64760098 18.          0.14185561]. \t  -0.6172074564902792 \t -0.43518063190798095\n",
            "17     \t [ 0.81210087  0.8533868  13.          0.66456412 12.          0.37769434]. \t  -0.5971747957799627 \t -0.43518063190798095\n",
            "18     \t [ 8.74769008  9.46041181  5.          0.76888057 12.          0.28611188]. \t  -0.5996998823178632 \t -0.43518063190798095\n",
            "19     \t [ 6.67918913  1.91467598 12.          0.83062216 13.          0.72788563]. \t  -0.45035357506572193 \t -0.43518063190798095\n",
            "20     \t [ 4.53564763  5.14157265 14.          0.52558575  1.          0.72718907]. \t  -0.45119962298136185 \t -0.43518063190798095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.711175290813375"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UroEj_RbLSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd9034a8-af04-44af-ba27-366048fc5748"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \n",
        "\n",
        "np.random.seed(run_num_9)\n",
        "surrogate_winner_9 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train9, X_test9, y_train9, y_test9 = train_test_split(X, y, test_size=test_perc, random_state=run_num_9)\n",
        "\n",
        "def f_syn_polarity9(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_9, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train9, y=y_train9).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_9 = dGPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity9, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_9 = winner_9.getResult()[0]\n",
        "params_winner_9['max_depth'] = int(params_winner_9['max_depth'])\n",
        "params_winner_9['min_child_weight'] = int(params_winner_9['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train9 = xgb.DMatrix(X_train9, y_train9)\n",
        "dX_winner_test9 = xgb.DMatrix(X_test9, y_test9)\n",
        "model_winner_9 = xgb.train(params_winner_9, dX_winner_train9)\n",
        "pred_winner_9 = model_winner_9.predict(dX_winner_test9)\n",
        "\n",
        "rmse_winner_9 = np.sqrt(mean_squared_error(pred_winner_9, y_test9))\n",
        "rmse_winner_9"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.0342804   5.27522296 13.          0.60741904 17.          0.15336222]. \t  -0.6890593508433319 \t -0.4886355541929722\n",
            "init   \t [7.56063644 1.79876369 7.         0.84869647 5.         0.55956942]. \t  -0.5532305414240943 \t -0.4886355541929722\n",
            "init   \t [2.43675539 0.20225128 6.         0.51669963 7.         0.4646446 ]. \t  -0.5609155518755553 \t -0.4886355541929722\n",
            "init   \t [ 7.6043547   4.73758376  9.          0.87564624 15.          0.65751286]. \t  -0.4886355541929722 \t -0.4886355541929722\n",
            "init   \t [ 4.70349314  6.71029586  6.          0.97680389 11.          0.41971652]. \t  -0.6548660487332898 \t -0.4886355541929722\n",
            "1      \t [ 0.20910387  7.25545521 14.          0.59407892  3.          0.95410586]. \t  \u001b[92m-0.4312356520914486\u001b[0m \t -0.4312356520914486\n",
            "2      \t [ 2.14892068  9.25577985 14.          0.84828472 11.          0.2793841 ]. \t  -0.6883959263608518 \t -0.4312356520914486\n",
            "3      \t [ 0.37305557  1.02756836 13.          0.57348052 13.          0.68410192]. \t  -0.4836872799099695 \t -0.4312356520914486\n",
            "4      \t [ 7.98252507  8.04957695 12.          0.6367811   3.          0.85775382]. \t  -0.4368210491589304 \t -0.4312356520914486\n",
            "5      \t [ 1.2588195   0.04438454  7.          0.70342864 18.          0.58554745]. \t  -0.4951281025154505 \t -0.4312356520914486\n",
            "6      \t [ 9.17710587  9.71573287  5.          0.9584376  19.          0.78072441]. \t  -0.4682275209845145 \t -0.4312356520914486\n",
            "7      \t [6.11416909 9.54959201 5.         0.91943284 1.         0.10968146]. \t  -0.6901756354808697 \t -0.4312356520914486\n",
            "8      \t [ 0.31270412  9.94818438  8.          0.84397658 19.          0.43974503]. \t  -0.5483333357303245 \t -0.4312356520914486\n",
            "9      \t [ 8.76504058  0.46155195 14.          0.61459938 10.          0.19095511]. \t  -0.6902913672662703 \t -0.4312356520914486\n",
            "10     \t [ 0.77001066  0.48822065 12.          0.89190754  5.          0.37116504]. \t  -0.6599680642282422 \t -0.4312356520914486\n",
            "11     \t [ 0.76412088  5.02527282 14.          0.53794557 19.          0.6779987 ]. \t  -0.4862552398836468 \t -0.4312356520914486\n",
            "12     \t [1.11781249 4.65858905 8.         0.79271199 1.         0.71342306]. \t  -0.4899393997261333 \t -0.4312356520914486\n",
            "13     \t [ 4.60583171  0.06752    13.          0.94036906 18.          0.58293565]. \t  -0.47784115580365294 \t -0.4312356520914486\n",
            "14     \t [6.09359952e-03 8.65908473e+00 6.00000000e+00 6.67169539e-01\n",
            " 6.00000000e+00 4.18236274e-01]. \t  -0.6570878867409317 \t -0.4312356520914486\n",
            "15     \t [ 7.69267031  0.52709121  5.          0.97712342 11.          0.47111142]. \t  -0.5671609865381374 \t -0.4312356520914486\n",
            "16     \t [9.94406911 8.10389604 7.         0.8844164  7.         0.60247845]. \t  -0.4916963401121463 \t -0.4312356520914486\n",
            "17     \t [ 4.9384323   0.37731064 14.          0.9492051   1.          0.8118423 ]. \t  -0.4371623420164621 \t -0.4312356520914486\n",
            "18     \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6893514282293631 \t -0.4312356520914486\n",
            "19     \t [ 9.68084543  9.58294825 13.          0.63062284  9.          0.21596151]. \t  -0.689113084484204 \t -0.4312356520914486\n",
            "20     \t [ 5.24998729  5.27861916  5.          0.87944065 19.          0.38196712]. \t  -0.656916604582956 \t -0.4312356520914486\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.130636454660636"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VgaJOoJbOIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "616c275d-ab10-4403-a9ef-9e0a5bd3d040"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \n",
        "\n",
        "np.random.seed(run_num_10)\n",
        "surrogate_winner_10 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train10, X_test10, y_train10, y_test10 = train_test_split(X, y, test_size=test_perc, random_state=run_num_10)\n",
        "\n",
        "def f_syn_polarity10(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_10, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train10, y=y_train10).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_10 = dGPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity10, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_10 = winner_10.getResult()[0]\n",
        "params_winner_10['max_depth'] = int(params_winner_10['max_depth'])\n",
        "params_winner_10['min_child_weight'] = int(params_winner_10['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train10 = xgb.DMatrix(X_train10, y_train10)\n",
        "dX_winner_test10 = xgb.DMatrix(X_test10, y_test10)\n",
        "model_winner_10 = xgb.train(params_winner_10, dX_winner_train10)\n",
        "pred_winner_10 = model_winner_10.predict(dX_winner_test10)\n",
        "\n",
        "rmse_winner_10 = np.sqrt(mean_squared_error(pred_winner_10, y_test10))\n",
        "rmse_winner_10"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 6.53589585  1.15006943  6.          0.93623727 14.          0.13663866]. \t  -0.6929852048256443 \t -0.4483221221388197\n",
            "init   \t [ 3.97194461  2.33132197 14.          0.9533253  19.          0.26403087]. \t  -0.6932702114662359 \t -0.4483221221388197\n",
            "init   \t [ 7.43539415  0.69582081  9.          0.9763222  11.          0.12608349]. \t  -0.6931242497450955 \t -0.4483221221388197\n",
            "init   \t [ 9.82027485  3.39637684  9.          0.78522537 19.          0.87618843]. \t  -0.4483221221388197 \t -0.4483221221388197\n",
            "init   \t [ 0.57576207  5.82646405 12.          0.77704362 18.          0.11865044]. \t  -0.6946096327074635 \t -0.4483221221388197\n",
            "1      \t [ 6.01651788  9.70919163  7.          0.6223146  15.          0.10507665]. \t  -0.6924325879252461 \t -0.4483221221388197\n",
            "2      \t [1.70339296 0.4436921  5.         0.5844373  2.         0.68058688]. \t  -0.5784474315368291 \t -0.4483221221388197\n",
            "3      \t [ 0.26043995  4.89348176 14.          0.51660447  9.          0.97374739]. \t  \u001b[92m-0.43451777076695297\u001b[0m \t -0.43451777076695297\n",
            "4      \t [ 9.51459367  2.97626838 10.          0.78292126  1.          0.96515375]. \t  -0.43758762459692707 \t -0.43451777076695297\n",
            "5      \t [4.33454338 9.11291731 6.         0.7883404  2.         0.49230158]. \t  -0.5779880165341125 \t -0.43451777076695297\n",
            "6      \t [ 7.90697916  9.14312267 14.          0.75613081  9.          0.87208929]. \t  \u001b[92m-0.4282600941244823\u001b[0m \t -0.4282600941244823\n",
            "7      \t [8.72554417 8.43289928 5.         0.78366846 8.         0.24269244]. \t  -0.6948360255852956 \t -0.4282600941244823\n",
            "8      \t [ 0.          6.07074054  5.          0.5        12.5056365   0.1       ]. \t  -0.6917431976232338 \t -0.4282600941244823\n",
            "9      \t [ 0.76839003  7.96408221 14.          0.73589275  1.          0.91984177]. \t  \u001b[92m-0.4267572960312064\u001b[0m \t -0.4267572960312064\n",
            "10     \t [0.         0.         5.         0.5        8.52054182 0.1       ]. \t  -0.6921741272697253 \t -0.4267572960312064\n",
            "11     \t [ 2.37424324  2.33069167 12.          0.80835547  2.          0.90917589]. \t  -0.43025305601156233 \t -0.4267572960312064\n",
            "12     \t [9.43529673 0.18357483 5.         0.85562643 4.         0.49300261]. \t  -0.5821244948315509 \t -0.4267572960312064\n",
            "13     \t [ 7.07869654  9.30947966 13.          0.76949873  1.          0.94513615]. \t  -0.4306670978270799 \t -0.4267572960312064\n",
            "14     \t [ 6.13591409  7.09849216 14.          0.57093785 15.          0.85036412]. \t  -0.5137107424858349 \t -0.4267572960312064\n",
            "15     \t [ 2.94327639  9.86430569 10.          0.52453661 10.          0.45562946]. \t  -0.581934467726267 \t -0.4267572960312064\n",
            "16     \t [ 0.60051063  0.93658443 10.          0.71787415 13.          0.3650692 ]. \t  -0.6037929489655063 \t -0.4267572960312064\n",
            "17     \t [ 3.18354686  4.79790696  5.          0.81878631 19.          0.2934667 ]. \t  -0.6119259457756776 \t -0.4267572960312064\n",
            "18     \t [ 8.58082048  3.39962732 14.          0.7151638   9.          0.76214186]. \t  -0.5049104315367501 \t -0.4267572960312064\n",
            "19     \t [ 8.52646717  0.12245704 14.          0.80155281 15.          0.7100538 ]. \t  -0.5709119331337676 \t -0.4267572960312064\n",
            "20     \t [3.00455642 4.71521339 7.         0.63232687 7.         0.15232121]. \t  -0.6913279505261656 \t -0.4267572960312064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.501711564413504"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51z87uHWbRGr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5730f98b-8356-4e38-dc1c-8c49c902052a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \n",
        "\n",
        "np.random.seed(run_num_11)\n",
        "surrogate_winner_11 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train11, X_test11, y_train11, y_test11 = train_test_split(X, y, test_size=test_perc, random_state=run_num_11)\n",
        "\n",
        "def f_syn_polarity11(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_11, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train11, y=y_train11).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_11 = dGPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity11, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_11 = winner_11.getResult()[0]\n",
        "params_winner_11['max_depth'] = int(params_winner_11['max_depth'])\n",
        "params_winner_11['min_child_weight'] = int(params_winner_11['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train11 = xgb.DMatrix(X_train11, y_train11)\n",
        "dX_winner_test11 = xgb.DMatrix(X_test11, y_test11)\n",
        "model_winner_11 = xgb.train(params_winner_11, dX_winner_train11)\n",
        "pred_winner_11 = model_winner_11.predict(dX_winner_test11)\n",
        "\n",
        "rmse_winner_11 = np.sqrt(mean_squared_error(pred_winner_11, y_test11))\n",
        "rmse_winner_11"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 9.81035663  8.2124785   9.          0.68487173 15.          0.28207988]. \t  -0.7133787013189323 \t -0.47317045732943896\n",
            "init   \t [ 4.91906771  1.38558332  6.          0.618878   19.          0.10263204]. \t  -0.7112023662943636 \t -0.47317045732943896\n",
            "init   \t [ 5.23426175  1.89971159 13.          0.76313779 17.          0.46597853]. \t  -0.47317045732943896 \t -0.47317045732943896\n",
            "init   \t [ 4.0204056   3.75167577  9.          0.86204818 17.          0.26560056]. \t  -0.7132057194891379 \t -0.47317045732943896\n",
            "init   \t [8.8704459  5.64917815 5.         0.51406171 1.         0.82826731]. \t  -0.48048685248123163 \t -0.47317045732943896\n",
            "1      \t [0.08349406 1.89250335 6.         0.66386636 1.         0.88641723]. \t  \u001b[92m-0.4484239383130805\u001b[0m \t -0.4484239383130805\n",
            "2      \t [ 5.35837408  0.31510164 14.          0.97519815  6.          0.3034681 ]. \t  -0.607905230502132 \t -0.4484239383130805\n",
            "3      \t [ 4.66315557  9.30076713 11.          0.83534598  8.          0.21303816]. \t  -0.7135282416096518 \t -0.4484239383130805\n",
            "4      \t [9.02046973 1.16865322 7.         0.6807515  8.         0.2962635 ]. \t  -0.6068266615066733 \t -0.4484239383130805\n",
            "5      \t [0.79988529 9.85424348 5.         0.5743044  3.         0.67901433]. \t  -0.48209943859810334 \t -0.4484239383130805\n",
            "6      \t [ 0.02057794  7.14089873 13.          0.90855163  1.          0.61823707]. \t  -0.45805892074172583 \t -0.4484239383130805\n",
            "7      \t [ 0.97603689  3.49871064  9.          0.91846837 10.          0.35158251]. \t  -0.6085109103402477 \t -0.4484239383130805\n",
            "8      \t [ 9.27240029  9.57413463 14.          0.58544186  3.          0.73779027]. \t  -0.45267877796490286 \t -0.4484239383130805\n",
            "9      \t [ 0.4027375   9.38447167 12.          0.94595612 15.          0.85310792]. \t  -0.451136347669006 \t -0.4484239383130805\n",
            "10     \t [ 1.3440527   8.97080429  5.          0.80277597 12.          0.63284111]. \t  -0.48349651102568225 \t -0.4484239383130805\n",
            "11     \t [ 7.94225687  3.68352192 13.          0.94902199 11.          0.95471054]. \t  \u001b[92m-0.422771965919715\u001b[0m \t -0.422771965919715\n",
            "12     \t [9.20332934 9.98167579 5.         0.57575366 6.         0.3717594 ]. \t  -0.6046860081027627 \t -0.422771965919715\n",
            "13     \t [5.68758712 2.62033511 9.         0.67614771 3.         0.67632008]. \t  -0.47052528509931824 \t -0.422771965919715\n",
            "14     \t [2.94143112e-03 7.03829535e-01 1.30000000e+01 5.98722168e-01\n",
            " 2.00000000e+00 1.01335355e-01]. \t  -0.7122743790062083 \t -0.422771965919715\n",
            "15     \t [ 0.81561387  9.31451412  5.          0.70594876 19.          0.98284163]. \t  -0.45988817082889427 \t -0.422771965919715\n",
            "16     \t [ 9.8968071   1.89203294 13.          0.7888373   2.          0.82539259]. \t  -0.45294282860990764 \t -0.422771965919715\n",
            "17     \t [ 8.37835592  2.52983026  5.          0.97806207 14.          0.6500905 ]. \t  -0.48098936825483635 \t -0.422771965919715\n",
            "18     \t [ 0.56769151  4.18340522 14.          0.91967306 13.          0.92813967]. \t  \u001b[92m-0.41912566738485735\u001b[0m \t -0.41912566738485735\n",
            "19     \t [3.64172872 0.         5.         0.5        9.68342711 0.1       ]. \t  -0.7114862606727149 \t -0.41912566738485735\n",
            "20     \t [ 8.98906297  7.01351387 13.          0.77197065 19.          0.47265223]. \t  -0.475345282554151 \t -0.41912566738485735\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.3224670398414355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8jZUeoWbTvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b3767cb-3ff0-4fa7-e934-520a29983270"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\n",
        "\n",
        "np.random.seed(run_num_12)\n",
        "surrogate_winner_12 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train12, X_test12, y_train12, y_test12 = train_test_split(X, y, test_size=test_perc, random_state=run_num_12)\n",
        "\n",
        "def f_syn_polarity12(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_12, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train12, y=y_train12).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_12 = dGPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity12, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_12 = winner_12.getResult()[0]\n",
        "params_winner_12['max_depth'] = int(params_winner_12['max_depth'])\n",
        "params_winner_12['min_child_weight'] = int(params_winner_12['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train12 = xgb.DMatrix(X_train12, y_train12)\n",
        "dX_winner_test12 = xgb.DMatrix(X_test12, y_test12)\n",
        "model_winner_12 = xgb.train(params_winner_12, dX_winner_train12)\n",
        "pred_winner_12 = model_winner_12.predict(dX_winner_test12)\n",
        "\n",
        "rmse_winner_12 = np.sqrt(mean_squared_error(pred_winner_12, y_test12))\n",
        "rmse_winner_12"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.9151945   6.22108771  9.          0.89267929 16.          0.87460279]. \t  -0.4420077321695894 \t -0.4420077321695894\n",
            "init   \t [ 1.50636966  1.98518759 11.          0.67890863 17.          0.5381501 ]. \t  -0.5998539491151165 \t -0.4420077321695894\n",
            "init   \t [ 3.31015428  8.02639569  7.          0.75154158 13.          0.11992952]. \t  -0.7324503600335561 \t -0.4420077321695894\n",
            "init   \t [ 2.90728553  2.46394443  5.          0.94461307 17.          0.43194161]. \t  -0.5995790338913064 \t -0.4420077321695894\n",
            "init   \t [ 9.33140102  6.51378143  8.          0.76907392 18.          0.38515251]. \t  -0.5982523495604211 \t -0.4420077321695894\n",
            "1      \t [5.68098653 8.6912739  6.         0.52027807 1.         0.51631894]. \t  -0.5999713561677879 \t -0.4420077321695894\n",
            "2      \t [ 4.61089026  7.78540446 14.          0.61560224  6.          0.60910898]. \t  -0.5310432089168232 \t -0.4420077321695894\n",
            "3      \t [9.12195032 0.87787126 9.         0.87292886 5.         0.2622847 ]. \t  -0.7326988083055951 \t -0.4420077321695894\n",
            "4      \t [ 0.42901773  1.02682351 10.          0.62582208  7.          0.40519792]. \t  -0.600033100066767 \t -0.4420077321695894\n",
            "5      \t [ 9.48743388  2.23858916 14.          0.84020064 13.          0.41361313]. \t  -0.5921503481987865 \t -0.4420077321695894\n",
            "6      \t [ 6.85696089  9.7830661  14.          0.97897908 13.          0.89524334]. \t  \u001b[92m-0.43162480213383303\u001b[0m \t -0.43162480213383303\n",
            "7      \t [ 7.4487954   0.25501799  6.          0.59353564 11.          0.79320372]. \t  -0.5151049017617342 \t -0.43162480213383303\n",
            "8      \t [0.97885925 5.30354005 9.60639944 0.5        1.         0.1       ]. \t  -0.7336679261714426 \t -0.43162480213383303\n",
            "9      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7334076855722197 \t -0.43162480213383303\n",
            "10     \t [9.69324843 9.42428289 7.         0.6384364  6.         0.83589131]. \t  -0.509004724458299 \t -0.43162480213383303\n",
            "11     \t [ 6.57352389  3.08967805 14.          0.73179937  1.          0.49930476]. \t  -0.6024722906486825 \t -0.43162480213383303\n",
            "12     \t [ 0.          0.          5.          0.5        11.32853237  0.1       ]. \t  -0.7334775686570922 \t -0.43162480213383303\n",
            "13     \t [ 8.76460845  0.13227845  6.          0.75995694 17.          0.20887306]. \t  -0.7329728586443053 \t -0.43162480213383303\n",
            "14     \t [0.41721862 8.87551388 5.         0.69088851 6.         0.86973069]. \t  -0.468169334397084 \t -0.43162480213383303\n",
            "15     \t [ 1.07765263  6.20138171 14.          0.59898972 11.          0.65973368]. \t  -0.5325207293005967 \t -0.43162480213383303\n",
            "16     \t [ 4.36197463  9.75001875 13.          0.72270526 18.          0.55588964]. \t  -0.5980036083013376 \t -0.43162480213383303\n",
            "17     \t [ 8.95792842  8.74686238 12.          0.60340754  1.          0.64076061]. \t  -0.5341583767040248 \t -0.43162480213383303\n",
            "18     \t [ 9.82623366  8.33878675  6.          0.79015982 12.          0.42024491]. \t  -0.5979516918871897 \t -0.43162480213383303\n",
            "19     \t [ 5.54847359  0.06497189 14.          0.98018975  7.          0.10144988]. \t  -0.7326985019066897 \t -0.43162480213383303\n",
            "20     \t [ 7.25143535  1.86402411 14.          0.59310957 19.          0.53879904]. \t  -0.5987977809908415 \t -0.43162480213383303\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.07295168965299"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snTrqE2RbWbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d96fe782-bacd-475c-ec15-860f5680ee72"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \n",
        "\n",
        "np.random.seed(run_num_13)\n",
        "surrogate_winner_13 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train13, X_test13, y_train13, y_test13 = train_test_split(X, y, test_size=test_perc, random_state=run_num_13)\n",
        "\n",
        "def f_syn_polarity13(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_13, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train13, y=y_train13).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_13 = dGPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity13, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_13 = winner_13.getResult()[0]\n",
        "params_winner_13['max_depth'] = int(params_winner_13['max_depth'])\n",
        "params_winner_13['min_child_weight'] = int(params_winner_13['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train13 = xgb.DMatrix(X_train13, y_train13)\n",
        "dX_winner_test13 = xgb.DMatrix(X_test13, y_test13)\n",
        "model_winner_13 = xgb.train(params_winner_13, dX_winner_train13)\n",
        "pred_winner_13 = model_winner_13.predict(dX_winner_test13)\n",
        "\n",
        "rmse_winner_13 = np.sqrt(mean_squared_error(pred_winner_13, y_test13))\n",
        "rmse_winner_13"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.30967343  2.54282073  6.          0.9850871  17.          0.50848087]. \t  -0.5693403578256062 \t -0.4459810811310791\n",
            "init   \t [ 6.95119021  3.25593423 14.          0.8602375   8.          0.82055713]. \t  -0.5112217679187058 \t -0.4459810811310791\n",
            "init   \t [ 5.09252675  8.19921021 10.          0.78792002 16.          0.382158  ]. \t  -0.581693558072188 \t -0.4459810811310791\n",
            "init   \t [ 5.4465191   3.19841462 14.          0.90654581  5.          0.10359415]. \t  -0.6818818523119694 \t -0.4459810811310791\n",
            "init   \t [ 9.8393864   9.89619708  9.          0.63387763 10.          0.88979174]. \t  -0.4459810811310791 \t -0.4459810811310791\n",
            "1      \t [ 9.27118046  9.4344906  10.          0.71578295  1.          0.5463837 ]. \t  -0.5604994598662556 \t -0.4459810811310791\n",
            "2      \t [0.21910824 0.53051294 5.         0.62754372 4.         0.55299122]. \t  -0.578556985871312 \t -0.4459810811310791\n",
            "3      \t [0.32904954 9.80442032 8.         0.7285653  2.         0.94646442]. \t  \u001b[92m-0.43885135786496293\u001b[0m \t -0.43885135786496293\n",
            "4      \t [ 8.54553338  0.49573541 12.          0.81406546 19.          0.20999807]. \t  -0.6795516887212301 \t -0.43885135786496293\n",
            "5      \t [9.40145549 1.43629395 5.         0.63655396 1.         0.63002732]. \t  -0.5662893508870628 \t -0.43885135786496293\n",
            "6      \t [ 8.7238127   0.85665784  6.          0.75815556 12.          0.87180469]. \t  -0.4644522650882914 \t -0.43885135786496293\n",
            "7      \t [ 0.11400844  9.5774521   6.          0.90472821 11.          0.18513309]. \t  -0.6793968950126906 \t -0.43885135786496293\n",
            "8      \t [ 1.95513735  0.12334702 13.          0.54467307 14.          0.86324917]. \t  \u001b[92m-0.43794830902637205\u001b[0m \t -0.43794830902637205\n",
            "9      \t [ 2.89959158  8.10209185 14.          0.9094105   1.          0.86003598]. \t  \u001b[92m-0.42582758559630707\u001b[0m \t -0.42582758559630707\n",
            "10     \t [ 0.63533588  7.88973169 13.          0.71777205  8.          0.22258282]. \t  -0.6847473450330067 \t -0.42582758559630707\n",
            "11     \t [4.59853484 7.19189873 5.         0.92777191 6.         0.5760847 ]. \t  -0.5604008171822471 \t -0.42582758559630707\n",
            "12     \t [ 9.78022669  6.34740254  5.          0.81941991 15.          0.17866883]. \t  -0.6810440437950237 \t -0.42582758559630707\n",
            "13     \t [ 0.44924058  6.01817024 13.          0.62113095 19.          0.22629481]. \t  -0.6838258323776948 \t -0.42582758559630707\n",
            "14     \t [ 2.87751807  1.16984879  6.          0.80421316 10.          0.54045166]. \t  -0.5677361264662384 \t -0.42582758559630707\n",
            "15     \t [ 8.93459678  5.75013258 14.          0.86506851 14.          0.11026757]. \t  -0.6798789215383618 \t -0.42582758559630707\n",
            "16     \t [ 1.2199304   9.32862427  5.          0.98694875 18.          0.69275841]. \t  -0.5593626212561581 \t -0.42582758559630707\n",
            "17     \t [7.30733696 1.06961871 9.         0.6583612  6.         0.41488291]. \t  -0.5857440675365031 \t -0.42582758559630707\n",
            "18     \t [ 0.25964514  2.37009305 11.          0.85577938  4.          0.42985865]. \t  -0.556481072440868 \t -0.42582758559630707\n",
            "19     \t [ 5.54919942  8.87232522  5.          0.84416975 12.          0.31830005]. \t  -0.5998464916577737 \t -0.42582758559630707\n",
            "20     \t [ 9.03439965  9.39442441 14.          0.98025678 19.          0.55921955]. \t  -0.550861248380592 \t -0.42582758559630707\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.159065314760995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23OXr6XLbY7_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7e79a53-1d0b-48d6-d359-da058ab116f6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \n",
        "\n",
        "np.random.seed(run_num_14)\n",
        "surrogate_winner_14 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train14, X_test14, y_train14, y_test14 = train_test_split(X, y, test_size=test_perc, random_state=run_num_14)\n",
        "\n",
        "def f_syn_polarity14(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_14, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train14, y=y_train14).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_14 = dGPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity14, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_14 = winner_14.getResult()[0]\n",
        "params_winner_14['max_depth'] = int(params_winner_14['max_depth'])\n",
        "params_winner_14['min_child_weight'] = int(params_winner_14['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train14 = xgb.DMatrix(X_train14, y_train14)\n",
        "dX_winner_test14 = xgb.DMatrix(X_test14, y_test14)\n",
        "model_winner_14 = xgb.train(params_winner_14, dX_winner_train14)\n",
        "pred_winner_14 = model_winner_14.predict(dX_winner_test14)\n",
        "\n",
        "rmse_winner_14 = np.sqrt(mean_squared_error(pred_winner_14, y_test14))\n",
        "rmse_winner_14"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.59560606  1.64569495 11.          0.89021195 15.          0.4857008 ]. \t  -0.6102441482204597 \t -0.4517949441804544\n",
            "init   \t [ 0.57460093  9.25007434  5.          0.56647642 17.          0.7533211 ]. \t  -0.5614002279921607 \t -0.4517949441804544\n",
            "init   \t [1.90674396 4.32019547 9.         0.93708225 3.         0.91775905]. \t  -0.4517949441804544 \t -0.4517949441804544\n",
            "init   \t [ 0.36770475  8.43368681 14.          0.6826844  15.          0.6291966 ]. \t  -0.5407924410004543 \t -0.4517949441804544\n",
            "init   \t [ 5.33840324  0.449182   14.          0.52045439  2.          0.48604556]. \t  -0.6242722768101235 \t -0.4517949441804544\n",
            "1      \t [ 9.54492219  9.07631824  8.          0.54579372 12.          0.38295996]. \t  -0.6262431675183743 \t -0.4517949441804544\n",
            "2      \t [9.53270461 0.83608685 6.         0.99976104 3.         0.60747265]. \t  -0.5533922160484371 \t -0.4517949441804544\n",
            "3      \t [ 1.15250646  0.51569303  5.          0.69322415 17.          0.16291682]. \t  -0.6917729046487754 \t -0.4517949441804544\n",
            "4      \t [ 6.37447116  9.98862868 14.          0.72259558  4.          0.61899393]. \t  -0.5434760859463281 \t -0.4517949441804544\n",
            "5      \t [ 8.08070558  7.65123431 14.          0.59104644 19.          0.9302005 ]. \t  -0.45873776977159153 \t -0.4517949441804544\n",
            "6      \t [ 6.26901123  2.37239677  5.          0.95063922 11.          0.52876485]. \t  -0.6158458956422692 \t -0.4517949441804544\n",
            "7      \t [1.0907871  9.11150389 5.         0.58744161 9.         0.87439163]. \t  -0.4933476055445382 \t -0.4517949441804544\n",
            "8      \t [ 2.21919917  0.31865167 14.          0.54399684 14.          0.18056849]. \t  -0.6942842535038787 \t -0.4517949441804544\n",
            "9      \t [ 7.34435618  8.39108681  7.          0.81547994 19.          0.62767067]. \t  -0.546522525570801 \t -0.4517949441804544\n",
            "10     \t [ 6.57539862  4.15362663 14.          0.96053826  9.          0.2080482 ]. \t  -0.691194495047777 \t -0.4517949441804544\n",
            "11     \t [6.00529466 8.33461243 6.         0.97581801 4.         0.31582778]. \t  -0.6226835813168086 \t -0.4517949441804544\n",
            "12     \t [0.         0.         5.         0.5        6.96881104 0.1       ]. \t  -0.694383982648781 \t -0.4517949441804544\n",
            "13     \t [ 2.98654692  5.09732719 10.          0.79852532 12.          0.69096265]. \t  -0.5376334643821391 \t -0.4517949441804544\n",
            "14     \t [ 0.84209312  7.86749962 14.          0.8648448   1.          0.1570582 ]. \t  -0.6937301125121068 \t -0.4517949441804544\n",
            "15     \t [ 1.22436035  9.67689861 12.          0.7460835   9.          0.69284382]. \t  -0.5399272231860202 \t -0.4517949441804544\n",
            "16     \t [ 4.4386325   0.37972087 10.          0.61118217  8.          0.15198669]. \t  -0.6912722224579347 \t -0.4517949441804544\n",
            "17     \t [ 7.0496565   9.86456792 14.          0.62416475 11.          0.68843039]. \t  -0.5429606831910581 \t -0.4517949441804544\n",
            "18     \t [3.58944068 0.         5.         0.5        1.         0.1       ]. \t  -0.6947083862270318 \t -0.4517949441804544\n",
            "19     \t [ 9.66203958  6.20819976 11.          0.71875858  5.          0.43480687]. \t  -0.6129974118264951 \t -0.4517949441804544\n",
            "20     \t [ 0.13684155  3.86104617 14.          0.82673561  9.          0.20283653]. \t  -0.6919355943160154 \t -0.4517949441804544\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.334202643456426"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgxvE7Irbbj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32a42fd7-eed7-4158-f469-90fb511b5d98"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \n",
        "\n",
        "np.random.seed(run_num_15)\n",
        "surrogate_winner_15 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train15, X_test15, y_train15, y_test15 = train_test_split(X, y, test_size=test_perc, random_state=run_num_15)\n",
        "\n",
        "def f_syn_polarity15(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_15, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train15, y=y_train15).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_15 = dGPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity15, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_15 = winner_15.getResult()[0]\n",
        "params_winner_15['max_depth'] = int(params_winner_15['max_depth'])\n",
        "params_winner_15['min_child_weight'] = int(params_winner_15['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train15 = xgb.DMatrix(X_train15, y_train15)\n",
        "dX_winner_test15 = xgb.DMatrix(X_test15, y_test15)\n",
        "model_winner_15 = xgb.train(params_winner_15, dX_winner_train15)\n",
        "pred_winner_15 = model_winner_15.predict(dX_winner_test15)\n",
        "\n",
        "rmse_winner_15 = np.sqrt(mean_squared_error(pred_winner_15, y_test15))\n",
        "rmse_winner_15"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.64315424  8.97297685  6.          0.81493635 11.          0.33008564]. \t  -0.5946480910132583 \t -0.4392277798535851\n",
            "init   \t [9.51455383 9.60803032 5.         0.68485728 1.         0.30145081]. \t  -0.5968379804084254 \t -0.4392277798535851\n",
            "init   \t [ 8.11809889  6.32508536  7.          0.61766329 19.          0.51383159]. \t  -0.5092318398204285 \t -0.4392277798535851\n",
            "init   \t [ 7.28571251  5.36071389 10.          0.76240297 12.          0.12035683]. \t  -0.6346120183758519 \t -0.4392277798535851\n",
            "init   \t [ 6.80847768  3.59767194 14.          0.51365585 16.          0.89730673]. \t  -0.4392277798535851 \t -0.4392277798535851\n",
            "1      \t [ 2.28792065  8.58421878 12.          0.67551126  1.          0.37971264]. \t  -0.596161946051267 \t -0.4392277798535851\n",
            "2      \t [ 9.35473751  1.48895841 12.          0.55362474  4.          0.11106606]. \t  -0.6327865627642436 \t -0.4392277798535851\n",
            "3      \t [3.91572323 2.23174129 6.         0.50937407 1.         0.51336364]. \t  -0.5160141940687206 \t -0.4392277798535851\n",
            "4      \t [ 0.74184801  3.17830864 14.          0.90502664  8.          0.31107615]. \t  -0.5905073342805591 \t -0.4392277798535851\n",
            "5      \t [ 1.33182259  0.8125478   9.          0.94558338 18.          0.71777254]. \t  -0.4467705016632985 \t -0.4392277798535851\n",
            "6      \t [ 0.48538268  1.39730148  6.          0.8238236  11.          0.45741795]. \t  -0.5144696373796631 \t -0.4392277798535851\n",
            "7      \t [ 0.57885492  9.35143843  6.          0.69337529 17.          0.15510814]. \t  -0.6331051904120661 \t -0.4392277798535851\n",
            "8      \t [0.24960251 9.45540541 5.         0.62890215 3.         0.73551484]. \t  -0.47327438015853385 \t -0.4392277798535851\n",
            "9      \t [ 8.23428085  9.77240559 13.          0.85977854  8.          0.93876208]. \t  \u001b[92m-0.4281095031928478\u001b[0m \t -0.4281095031928478\n",
            "10     \t [ 1.35750397  9.83931966 14.          0.92698791 15.          0.2272101 ]. \t  -0.6341880539033447 \t -0.4281095031928478\n",
            "11     \t [ 9.85965179  9.04589828 13.          0.95564786 19.          0.1426314 ]. \t  -0.632071954329094 \t -0.4281095031928478\n",
            "12     \t [9.06175259 4.11518452 5.         0.87621161 6.         0.52300377]. \t  -0.5192107686497183 \t -0.4281095031928478\n",
            "13     \t [ 0.15053668  0.43786928 13.          0.62231098  1.          0.88145445]. \t  -0.43523183945633176 \t -0.4281095031928478\n",
            "14     \t [ 7.18775342  0.2062489   5.          0.98239776 17.          0.28043211]. \t  -0.634545784625454 \t -0.4281095031928478\n",
            "15     \t [0.10178544 7.49949902 9.         0.60008085 8.         0.35520416]. \t  -0.5902884239300654 \t -0.4281095031928478\n",
            "16     \t [0.22657563 0.19081685 8.21923751 0.5        5.21923751 0.1       ]. \t  -0.632681987586384 \t -0.4281095031928478\n",
            "17     \t [ 8.4324691   8.27251391 11.          0.71354715  2.          0.54685464]. \t  -0.498601956775111 \t -0.4281095031928478\n",
            "18     \t [ 2.12400437  9.06769045 14.          0.70585245  9.          0.90846764]. \t  -0.4286407197787245 \t -0.4281095031928478\n",
            "19     \t [9.58390631 0.99765082 6.         0.72021045 1.         0.39555686]. \t  -0.5960083204162412 \t -0.4281095031928478\n",
            "20     \t [ 9.96011127  1.55235537 14.          0.63004724 11.          0.14483121]. \t  -0.6331081546428745 \t -0.4281095031928478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.189953800077167"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye4UEpNabeCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "886fc574-2d96-4a22-eca5-1e029f182779"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \n",
        "\n",
        "np.random.seed(run_num_16)\n",
        "surrogate_winner_16 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train16, X_test16, y_train16, y_test16 = train_test_split(X, y, test_size=test_perc, random_state=run_num_16)\n",
        "\n",
        "def f_syn_polarity16(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_16, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train16, y=y_train16).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_16 = dGPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity16, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_16 = winner_16.getResult()[0]\n",
        "params_winner_16['max_depth'] = int(params_winner_16['max_depth'])\n",
        "params_winner_16['min_child_weight'] = int(params_winner_16['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train16 = xgb.DMatrix(X_train16, y_train16)\n",
        "dX_winner_test16 = xgb.DMatrix(X_test16, y_test16)\n",
        "model_winner_16 = xgb.train(params_winner_16, dX_winner_train16)\n",
        "pred_winner_16 = model_winner_16.predict(dX_winner_test16)\n",
        "\n",
        "rmse_winner_16 = np.sqrt(mean_squared_error(pred_winner_16, y_test16))\n",
        "rmse_winner_16"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [8.63948332 3.16366002 7.         0.75395583 1.         0.56999262]. \t  -0.5713220948397237 \t -0.4331621293825035\n",
            "init   \t [ 3.3426621   9.77029134 14.          0.93751218 17.          0.80549151]. \t  -0.4331621293825035 \t -0.4331621293825035\n",
            "init   \t [ 3.36871073  4.8308288  13.          0.91984982 14.          0.48829226]. \t  -0.5561611494046279 \t -0.4331621293825035\n",
            "init   \t [ 8.84540736  1.59826081  8.          0.75772888 14.          0.29029227]. \t  -0.6835926126526815 \t -0.4331621293825035\n",
            "init   \t [ 7.74981136  8.34377636 13.          0.83182409 15.          0.19378622]. \t  -0.7677243885128009 \t -0.4331621293825035\n",
            "1      \t [0.90186209 9.9119065  9.         0.96247707 9.         0.82473059]. \t  -0.44000666714577197 \t -0.4331621293825035\n",
            "2      \t [ 3.6191694   8.20490707  5.          0.81280746 16.          0.99306688]. \t  -0.4581574120389309 \t -0.4331621293825035\n",
            "3      \t [2.15072525 1.72325427 6.         0.7282133  7.         0.66792915]. \t  -0.510524353390639 \t -0.4331621293825035\n",
            "4      \t [ 4.49483609  7.05102179 14.          0.67776057  3.          0.50727602]. \t  -0.5621481895640063 \t -0.4331621293825035\n",
            "5      \t [ 4.63470447  0.29792913 13.          0.70911522  7.          0.30290504]. \t  -0.6845759859311331 \t -0.4331621293825035\n",
            "6      \t [9.17745543 7.46026312 5.         0.6284814  9.         0.72652494]. \t  -0.47418011145069017 \t -0.4331621293825035\n",
            "7      \t [2.7669192  6.70200041 7.         0.83888371 1.         0.66194747]. \t  -0.49144662381331905 \t -0.4331621293825035\n",
            "8      \t [ 1.86481894  0.99601669  6.          0.66208635 14.          0.74317901]. \t  -0.4660455507597825 \t -0.4331621293825035\n",
            "9      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.7670392049994392 \t -0.4331621293825035\n",
            "10     \t [ 9.07329721  6.04353062  6.          0.62940057 19.          0.1206801 ]. \t  -0.7674777671186626 \t -0.4331621293825035\n",
            "11     \t [ 8.54895835  0.69961229 14.          0.57654595 19.          0.20454627]. \t  -0.7677953049378372 \t -0.4331621293825035\n",
            "12     \t [8.53167413 9.15372778 5.         0.57519522 2.         0.50414261]. \t  -0.5950876842055789 \t -0.4331621293825035\n",
            "13     \t [ 9.26465723  9.86116065 13.          0.52207502  6.          0.17735657]. \t  -0.7670798777253309 \t -0.4331621293825035\n",
            "14     \t [ 0.3330471   0.01629553 13.          0.91520431 11.          0.22019979]. \t  -0.7679362913445089 \t -0.4331621293825035\n",
            "15     \t [ 0.76287087  0.49621071 13.          0.81150873 18.          0.76762748]. \t  -0.4363142295126112 \t -0.4331621293825035\n",
            "16     \t [9.9506446  0.41258268 8.         0.70130417 7.         0.77986813]. \t  -0.4500722951786756 \t -0.4331621293825035\n",
            "17     \t [ 8.84742705  5.08354098 12.          0.54989851 10.          0.27761013]. \t  -0.7666143119340576 \t -0.4331621293825035\n",
            "18     \t [ 6.47050729  9.70570023  8.          0.55783027 12.          0.88747479]. \t  -0.4508283906225067 \t -0.4331621293825035\n",
            "19     \t [ 9.46331313  2.81887466 13.          0.76076385  3.          0.42087971]. \t  -0.6854869575909893 \t -0.4331621293825035\n",
            "20     \t [ 0.          4.21232703 11.63777748  0.5         1.63777748  0.1       ]. \t  -0.7669723054879378 \t -0.4331621293825035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.501874502356394"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Biq2Uaa5bg3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e964b1-90dc-4c6c-b10e-dda50a75857a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \n",
        "\n",
        "np.random.seed(run_num_17)\n",
        "surrogate_winner_17 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train17, X_test17, y_train17, y_test17 = train_test_split(X, y, test_size=test_perc, random_state=run_num_17)\n",
        "\n",
        "def f_syn_polarity17(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_17, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train17, y=y_train17).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_17 = dGPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity17, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_17 = winner_17.getResult()[0]\n",
        "params_winner_17['max_depth'] = int(params_winner_17['max_depth'])\n",
        "params_winner_17['min_child_weight'] = int(params_winner_17['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train17 = xgb.DMatrix(X_train17, y_train17)\n",
        "dX_winner_test17 = xgb.DMatrix(X_test17, y_test17)\n",
        "model_winner_17 = xgb.train(params_winner_17, dX_winner_train17)\n",
        "pred_winner_17 = model_winner_17.predict(dX_winner_test17)\n",
        "\n",
        "rmse_winner_17 = np.sqrt(mean_squared_error(pred_winner_17, y_test17))\n",
        "rmse_winner_17"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.85575275  3.86599303 13.          0.85645882  2.          0.78699813]. \t  -0.4597056260580034 \t -0.4597056260580034\n",
            "init   \t [1.97780831 2.92527676 8.         0.92268607 9.         0.39017299]. \t  -0.5687207024308397 \t -0.4597056260580034\n",
            "init   \t [ 7.16656007  1.028324    5.          0.96975483 15.          0.90932849]. \t  -0.4688025715067707 \t -0.4597056260580034\n",
            "init   \t [ 2.46007304  9.53596657  7.          0.71704714 11.          0.17659118]. \t  -0.6983036523024195 \t -0.4597056260580034\n",
            "init   \t [3.44161135 9.92427612 8.         0.96331165 2.         0.29160186]. \t  -0.5675253504862205 \t -0.4597056260580034\n",
            "1      \t [ 7.66483352  5.40306801 14.          0.87670581  9.          0.9260652 ]. \t  \u001b[92m-0.43020599740044335\u001b[0m \t -0.43020599740044335\n",
            "2      \t [ 6.95619362  6.95953721 12.          0.92317057 19.          0.34981799]. \t  -0.5628121282495753 \t -0.43020599740044335\n",
            "3      \t [0.  0.  5.  0.5 1.  0.1]. \t  -0.6990710246949499 \t -0.43020599740044335\n",
            "4      \t [ 1.79054908  0.97767001 14.          0.72051649 17.          0.75055942]. \t  -0.4586950539387324 \t -0.43020599740044335\n",
            "5      \t [7.85364213 1.7463908  7.         0.80931735 2.         0.57315987]. \t  -0.5442127017186587 \t -0.43020599740044335\n",
            "6      \t [ 1.77308987  6.92082513  5.          0.52059222 19.          0.18788647]. \t  -0.69931956875667 \t -0.43020599740044335\n",
            "7      \t [ 8.95440724  5.31304705 13.          0.73043408  1.          0.56398539]. \t  -0.5373247826064305 \t -0.43020599740044335\n",
            "8      \t [ 8.26790477  9.9524702   5.          0.59716297 16.          0.88254342]. \t  -0.47691281251538375 \t -0.43020599740044335\n",
            "9      \t [ 0.90443942  7.22834774 14.          0.92580232 12.          0.13465218]. \t  -0.6965996897404226 \t -0.43020599740044335\n",
            "10     \t [ 8.52285971  0.31262108 12.          0.8797653  16.          0.34954268]. \t  -0.5627660085466504 \t -0.43020599740044335\n",
            "11     \t [7.50153168 8.37519947 6.         0.6544959  7.         0.90121291]. \t  -0.46115116499634456 \t -0.43020599740044335\n",
            "12     \t [ 1.83063475  0.7056051   5.          0.62427541 19.          0.60134826]. \t  -0.5515025691718822 \t -0.43020599740044335\n",
            "13     \t [ 1.78559475  9.94726594 14.          0.84124706  5.          0.41412983]. \t  -0.5643771257102934 \t -0.43020599740044335\n",
            "14     \t [ 0.8513654   7.30808802 13.          0.68493838 18.          0.30819132]. \t  -0.5655524008056683 \t -0.43020599740044335\n",
            "15     \t [9.52955352 8.15137211 8.         0.76863454 2.         0.57854032]. \t  -0.5395532171313935 \t -0.43020599740044335\n",
            "16     \t [ 0.          0.          5.          0.5        13.01971605  0.1       ]. \t  -0.699276175504395 \t -0.43020599740044335\n",
            "17     \t [ 9.6713951   2.53929081  8.          0.55813478 10.          0.24505208]. \t  -0.7003519360876083 \t -0.43020599740044335\n",
            "18     \t [ 7.02372752  0.82996782 13.          0.91924474  5.          0.61544084]. \t  -0.530630307048183 \t -0.43020599740044335\n",
            "19     \t [ 5.57429295  9.9484983  10.          0.99835426 15.          0.79220597]. \t  -0.4602719911516614 \t -0.43020599740044335\n",
            "20     \t [ 3.87116535  3.46167946  9.          0.68739534 15.          0.44731298]. \t  -0.5417289076052462 \t -0.43020599740044335\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.734749153689431"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H4MWSXFcZjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14906e34-de48-4398-f0b5-a1d37e6277b3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \n",
        "\n",
        "np.random.seed(run_num_18)\n",
        "surrogate_winner_18 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train18, X_test18, y_train18, y_test18 = train_test_split(X, y, test_size=test_perc, random_state=run_num_18)\n",
        "\n",
        "def f_syn_polarity18(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_18, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train18, y=y_train18).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_18 = dGPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity18, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_18 = winner_18.getResult()[0]\n",
        "params_winner_18['max_depth'] = int(params_winner_18['max_depth'])\n",
        "params_winner_18['min_child_weight'] = int(params_winner_18['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train18 = xgb.DMatrix(X_train18, y_train18)\n",
        "dX_winner_test18 = xgb.DMatrix(X_test18, y_test18)\n",
        "model_winner_18 = xgb.train(params_winner_18, dX_winner_train18)\n",
        "pred_winner_18 = model_winner_18.predict(dX_winner_test18)\n",
        "\n",
        "rmse_winner_18 = np.sqrt(mean_squared_error(pred_winner_18, y_test18))\n",
        "rmse_winner_18"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.73429403  9.68540663 10.          0.68875849  9.          0.48011572]. \t  -0.536396330012702 \t -0.4337279026393176\n",
            "init   \t [ 6.12033333  7.66062926  8.          0.76133734 13.          0.93379456]. \t  -0.4433380408289477 \t -0.4337279026393176\n",
            "init   \t [ 1.46524679  7.01527914  7.          0.90913299 10.          0.36016753]. \t  -0.5541638729833303 \t -0.4337279026393176\n",
            "init   \t [ 9.73855241  3.33774046 14.          0.53290419  7.          0.7088681 ]. \t  -0.5062350384654568 \t -0.4337279026393176\n",
            "init   \t [ 3.00618018  1.82702795 11.          0.75681389 14.          0.98627449]. \t  -0.4337279026393176 \t -0.4337279026393176\n",
            "1      \t [ 1.46586883  1.13810325 14.          0.90052794  5.          0.16182112]. \t  -0.6179729230954069 \t -0.4337279026393176\n",
            "2      \t [ 9.17850108  5.22481201 14.          0.60105853 18.          0.63442006]. \t  -0.5052903785308961 \t -0.4337279026393176\n",
            "3      \t [6.20612396 4.37552923 6.         0.71941082 3.         0.90943147]. \t  -0.45137007433293086 \t -0.4337279026393176\n",
            "4      \t [ 0.42107725  9.92938812 12.          0.92462675 18.          0.88352347]. \t  \u001b[92m-0.4255403331751115\u001b[0m \t -0.4255403331751115\n",
            "5      \t [ 1.45418057  8.89399    11.          0.93175096  1.          0.52391385]. \t  -0.5358666580451181 \t -0.4255403331751115\n",
            "6      \t [0.         0.         5.         0.5        5.65360202 0.1       ]. \t  -0.6237072180254504 \t -0.4255403331751115\n",
            "7      \t [ 8.67626106  0.1397848   6.          0.55771681 18.          0.89120888]. \t  -0.4650266593401632 \t -0.4255403331751115\n",
            "8      \t [ 7.20807715  7.41236348  5.          0.52425285 19.          0.24651851]. \t  -0.6221585758998005 \t -0.4255403331751115\n",
            "9      \t [7.94412856 1.24068931 7.         0.96649935 9.         0.47928692]. \t  -0.5420539933608051 \t -0.4255403331751115\n",
            "10     \t [ 2.66410938  9.83743919 13.          0.58899688  8.          0.29675269]. \t  -0.5550068432497505 \t -0.4255403331751115\n",
            "11     \t [ 8.62753755  4.44899956 12.          0.98231134  1.          0.88389967]. \t  -0.4267991891390327 \t -0.4255403331751115\n",
            "12     \t [ 1.2277143   1.05411485  5.          0.50198686 13.          0.89258454]. \t  -0.470676137340884 \t -0.4255403331751115\n",
            "13     \t [ 3.86727486  0.04582369 10.          0.54629234  1.          0.59406244]. \t  -0.5128905589861811 \t -0.4255403331751115\n",
            "14     \t [9.89654007 7.91459554 5.         0.64703423 6.         0.95875063]. \t  -0.46098420452454364 \t -0.4255403331751115\n",
            "15     \t [ 9.49381141  1.31696272 11.          0.51283555 13.          0.25364349]. \t  -0.6228089112997249 \t -0.4255403331751115\n",
            "16     \t [0.         3.85719544 5.71731711 0.5        1.         0.1       ]. \t  -0.6229556599815081 \t -0.4255403331751115\n",
            "17     \t [ 1.04974938  7.3117691   5.          0.60120794 17.          0.83587336]. \t  -0.5008428213738166 \t -0.4255403331751115\n",
            "18     \t [5.83203328 9.4812958  5.         0.60479193 1.         0.24995758]. \t  -0.6202582020135203 \t -0.4255403331751115\n",
            "19     \t [ 3.86883296  7.31704712 14.          0.75047101 14.          0.80828494]. \t  -0.44933473824030495 \t -0.4255403331751115\n",
            "20     \t [ 4.0233419   3.91847735 10.          0.87935081  7.          0.57035954]. \t  -0.5324028319840026 \t -0.4255403331751115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.9005482179018887"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EgxA8k4ccL6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6da5b3c4-7cf7-497b-eff1-216fd2ad65a9"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \n",
        "\n",
        "np.random.seed(run_num_19)\n",
        "surrogate_winner_19 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train19, X_test19, y_train19, y_test19 = train_test_split(X, y, test_size=test_perc, random_state=run_num_19)\n",
        "\n",
        "def f_syn_polarity19(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_19, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train19, y=y_train19).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_19 = dGPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity19, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_19 = winner_19.getResult()[0]\n",
        "params_winner_19['max_depth'] = int(params_winner_19['max_depth'])\n",
        "params_winner_19['min_child_weight'] = int(params_winner_19['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train19 = xgb.DMatrix(X_train19, y_train19)\n",
        "dX_winner_test19 = xgb.DMatrix(X_test19, y_test19)\n",
        "model_winner_19 = xgb.train(params_winner_19, dX_winner_train19)\n",
        "pred_winner_19 = model_winner_19.predict(dX_winner_test19)\n",
        "\n",
        "rmse_winner_19 = np.sqrt(mean_squared_error(pred_winner_19, y_test19))\n",
        "rmse_winner_19"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 8.24520174  9.84626759 12.          0.81846905 12.          0.27594309]. \t  -0.6624515538441494 \t -0.4321567975765851\n",
            "init   \t [ 8.6021676   8.64300514 14.          0.95067208 18.          0.13363564]. \t  -0.663790961968593 \t -0.4321567975765851\n",
            "init   \t [ 3.02486657  6.06229509 10.          0.50494482  9.          0.10218675]. \t  -0.668875700942164 \t -0.4321567975765851\n",
            "init   \t [ 6.57586195  8.01231848  7.          0.63945407 18.          0.49206406]. \t  -0.5640092362987262 \t -0.4321567975765851\n",
            "init   \t [ 6.05008856  8.15558548 14.          0.64718943  8.          0.90611106]. \t  -0.4321567975765851 \t -0.4321567975765851\n",
            "1      \t [ 0.85655458  0.69294498  9.          0.90992265 19.          0.25605959]. \t  -0.6632879756016564 \t -0.4321567975765851\n",
            "2      \t [ 1.16239725  9.89254371 11.          0.98469188  1.          0.20271996]. \t  -0.6659068700895453 \t -0.4321567975765851\n",
            "3      \t [9.26425961 9.717621   5.         0.8856331  5.         0.65589573]. \t  -0.5256636565447839 \t -0.4321567975765851\n",
            "4      \t [8.38158931 1.39953008 5.         0.6836482  9.         0.46934447]. \t  -0.5657479189361881 \t -0.4321567975765851\n",
            "5      \t [ 9.89729152  2.88709408 11.          0.55121781  3.          0.58381748]. \t  -0.5217676759230677 \t -0.4321567975765851\n",
            "6      \t [0.         1.11454853 5.         0.5        1.         0.1       ]. \t  -0.6682393310586452 \t -0.4321567975765851\n",
            "7      \t [ 0.52039992  0.56380532 14.          0.70324781  2.          0.77427978]. \t  -0.44119318049659856 \t -0.4321567975765851\n",
            "8      \t [ 9.45607821  0.96438893 14.          0.56821928 16.          0.48995458]. \t  -0.560546633196372 \t -0.4321567975765851\n",
            "9      \t [ 0.56432836  7.79494932 13.          0.55871992 19.          0.10266045]. \t  -0.6660467600439527 \t -0.4321567975765851\n",
            "10     \t [ 3.16676683  1.78770811 13.          0.5650927  13.          0.7417697 ]. \t  -0.4472490143246649 \t -0.4321567975765851\n",
            "11     \t [ 0.64379529  1.54731928  6.          0.82512479 11.          0.6319422 ]. \t  -0.5209201684825245 \t -0.4321567975765851\n",
            "12     \t [ 7.73804622  1.66772104  5.          0.66266078 17.          0.87441258]. \t  -0.4732994211333518 \t -0.4321567975765851\n",
            "13     \t [ 7.46605559  8.55652968 13.          0.99452706  1.          0.43764631]. \t  -0.5573240867828692 \t -0.4321567975765851\n",
            "14     \t [ 0.36172735  9.41325373  5.          0.88367823 18.          0.31289693]. \t  -0.610279476214515 \t -0.4321567975765851\n",
            "15     \t [6.50533986 6.11396147 7.         0.78114466 1.         0.75288314]. \t  -0.45838856364045305 \t -0.4321567975765851\n",
            "16     \t [1.7958895  9.19371155 5.         0.54448974 5.         0.93868386]. \t  -0.4669880318809955 \t -0.4321567975765851\n",
            "17     \t [ 9.62487731  6.86253449  5.          0.66465947 12.          0.21107909]. \t  -0.6659019290689656 \t -0.4321567975765851\n",
            "18     \t [ 1.7944313   9.07405715  5.          0.89658957 12.          0.48452475]. \t  -0.5646254130385107 \t -0.4321567975765851\n",
            "19     \t [4.43453509 0.79656434 9.         0.94541308 4.         0.56406861]. \t  -0.5574336760699456 \t -0.4321567975765851\n",
            "20     \t [ 8.84762988  1.25680743 14.          0.66615751  9.          0.97141347]. \t  -0.4335880467055418 \t -0.4321567975765851\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2307134915487135"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08y-IGpyceje",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "391f09da-1abb-4415-e6db-683727e84c76"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \n",
        "\n",
        "np.random.seed(run_num_20)\n",
        "surrogate_winner_20 = dGaussianProcess(d_cov_func)\n",
        "\n",
        "X_train20, X_test20, y_train20, y_test20 = train_test_split(X, y, test_size=test_perc, random_state=run_num_20)\n",
        "\n",
        "def f_syn_polarity20(alpha, gamma, max_depth, subsample, min_child_weight, colsample):\n",
        "    reg = XGBRegressor(reg_alpha=alpha, gamma=gamma, max_depth=int(max_depth), subsample=subsample, min_child_weight=min_child_weight,\n",
        "          colsample_bytree=colsample, n_estimators = n_est, random_state=run_num_20, objective = 'reg:squarederror')\n",
        "    score = np.array(cross_val_score(reg, X=X_train20, y=y_train20).mean())\n",
        "    return operator * score\n",
        "\n",
        "winner_20 = dGPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity20, param, n_jobs = -1) # Define BayesOpt\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run\n",
        "\n",
        "### Return optimal parameters' set:\n",
        "params_winner_20 = winner_20.getResult()[0]\n",
        "params_winner_20['max_depth'] = int(params_winner_20['max_depth'])\n",
        "params_winner_20['min_child_weight'] = int(params_winner_20['min_child_weight'])\n",
        "\n",
        "### Re-train with optimal parameters, run predictons:\n",
        "dX_winner_train20 = xgb.DMatrix(X_train20, y_train20)\n",
        "dX_winner_test20 = xgb.DMatrix(X_test20, y_test20)\n",
        "model_winner_20 = xgb.train(params_winner_20, dX_winner_train20)\n",
        "pred_winner_20 = model_winner_20.predict(dX_winner_test20)\n",
        "\n",
        "rmse_winner_20 = np.sqrt(mean_squared_error(pred_winner_20, y_test20))\n",
        "rmse_winner_20"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 5.70517285  5.64528755 11.          0.51213999 19.          0.57883228]. \t  -0.5510634045248362 \t -0.4667910180825121\n",
            "init   \t [0.68106288 5.8452906  6.         0.58037829 8.         0.87974899]. \t  -0.4667910180825121 \t -0.4667910180825121\n",
            "init   \t [ 4.04121426  7.17971417 11.          0.57603584 10.          0.19954225]. \t  -0.6541702754539586 \t -0.4667910180825121\n",
            "init   \t [ 5.27556761  8.67655329 13.          0.54252273  4.          0.48570796]. \t  -0.5813476654605825 \t -0.4667910180825121\n",
            "init   \t [ 5.30003889  5.73946822 12.          0.52666698 16.          0.77467019]. \t  -0.5078849923055367 \t -0.4667910180825121\n",
            "1      \t [ 9.25190708  0.65043072 10.          0.56676233  5.          0.41289847]. \t  -0.6164858434894066 \t -0.4667910180825121\n",
            "2      \t [8.65689736 8.11684598 5.         0.53907072 1.         0.87574215]. \t  -0.47576906935129115 \t -0.4667910180825121\n",
            "3      \t [6.49728107e-01 7.72862043e-04 7.00000000e+00 5.71905117e-01\n",
            " 2.00000000e+00 1.02014940e-01]. \t  -0.6533990377801203 \t -0.4667910180825121\n",
            "4      \t [ 9.51806562  0.44900923  6.          0.62997227 16.          0.40028974]. \t  -0.6257141325107729 \t -0.4667910180825121\n",
            "5      \t [ 0.48808143  1.97030839  7.          0.79892377 16.          0.98477167]. \t  \u001b[92m-0.4602371335347518\u001b[0m \t -0.4602371335347518\n",
            "6      \t [ 9.98029391  8.23632437  6.          0.63606408 13.          0.22908429]. \t  -0.6532745897130707 \t -0.4602371335347518\n",
            "7      \t [ 3.9245879   0.87058505 14.          0.68269183  1.          0.34048922]. \t  -0.623283664424315 \t -0.4602371335347518\n",
            "8      \t [ 0.45468431  0.13267263 10.          0.667214    9.          0.47071141]. \t  -0.5807933165122651 \t -0.4602371335347518\n",
            "9      \t [ 4.64792903  9.65609988  6.          0.78696753 17.          0.90406502]. \t  -0.47227152594951016 \t -0.4602371335347518\n",
            "10     \t [ 9.73713042  9.77875019 14.          0.8552366  13.          0.6588042 ]. \t  -0.5450528828222597 \t -0.4602371335347518\n",
            "11     \t [ 9.74564452  1.02954501 14.          0.98964301 12.          0.47743205]. \t  -0.5773481317621234 \t -0.4602371335347518\n",
            "12     \t [ 7.76906477  2.6060444   5.          0.53491145 10.          0.41560504]. \t  -0.627993339841767 \t -0.4602371335347518\n",
            "13     \t [6.49958993 1.98622034 6.         0.83635209 1.         0.2931143 ]. \t  -0.6276302072456058 \t -0.4602371335347518\n",
            "14     \t [1.36051555 9.95957797 7.         0.9242116  2.         0.87097462]. \t  \u001b[92m-0.449167771194887\u001b[0m \t -0.449167771194887\n",
            "15     \t [ 0.20978943  8.45447688 14.          0.77351134 17.          0.2656195 ]. \t  -0.6586945947582249 \t -0.449167771194887\n",
            "16     \t [7.55121074 9.86496533 6.         0.88828535 7.         0.3097795 ]. \t  -0.6258969685789838 \t -0.449167771194887\n",
            "17     \t [ 1.88681738  3.46035586 14.          0.68070687  6.          0.9944693 ]. \t  \u001b[92m-0.42905917541157645\u001b[0m \t -0.42905917541157645\n",
            "18     \t [ 1.00455473  4.86452269 10.          0.65242434  1.          0.22603818]. \t  -0.6579989052523156 \t -0.42905917541157645\n",
            "19     \t [ 0.87144588  0.14777143 14.          0.88891044 13.          0.11582186]. \t  -0.6586044677275812 \t -0.42905917541157645\n",
            "20     \t [ 8.17732484  5.22697425 11.          0.61303379  1.          0.32672413]. \t  -0.6191817873624627 \t -0.42905917541157645\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.2900830531756915"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78QysHAlchIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a65ca93-0c25-4f81-cc2f-f34c2ee21a76"
      },
      "source": [
        "end_win = time.time()\n",
        "end_win\n",
        "\n",
        "time_win = end_win - start_win\n",
        "time_win"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1555.3646206855774"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU2FlhY4vHUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b78e448f-07fd-41aa-ac61-421b550f43b8"
      },
      "source": [
        "rmse_loser = [rmse_loser_1,\n",
        "rmse_loser_2,\n",
        "rmse_loser_3,\n",
        "rmse_loser_4,\n",
        "rmse_loser_5,\n",
        "rmse_loser_6,\n",
        "rmse_loser_7,\n",
        "rmse_loser_8,\n",
        "rmse_loser_9,\n",
        "rmse_loser_10,\n",
        "rmse_loser_11,\n",
        "rmse_loser_12,\n",
        "rmse_loser_13,\n",
        "rmse_loser_14,\n",
        "rmse_loser_15,\n",
        "rmse_loser_16,\n",
        "rmse_loser_17,\n",
        "rmse_loser_18,\n",
        "rmse_loser_19,\n",
        "rmse_loser_20]\n",
        "\n",
        "np.mean(rmse_loser)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.335494497141253"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ1lotm7vJi9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84bebbb7-9629-4a7b-f181-1f5e4ceb05d6"
      },
      "source": [
        "rmse_winner = [rmse_winner_1,\n",
        "rmse_winner_2,\n",
        "rmse_winner_3,\n",
        "rmse_winner_4,\n",
        "rmse_winner_5,\n",
        "rmse_winner_6,\n",
        "rmse_winner_7,\n",
        "rmse_winner_8,\n",
        "rmse_winner_9,\n",
        "rmse_winner_10,\n",
        "rmse_winner_11,\n",
        "rmse_winner_12,\n",
        "rmse_winner_13,\n",
        "rmse_winner_14,\n",
        "rmse_winner_15,\n",
        "rmse_winner_16,\n",
        "rmse_winner_17,\n",
        "rmse_winner_18,\n",
        "rmse_winner_19,\n",
        "rmse_winner_20]\n",
        "\n",
        "np.mean(rmse_winner)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.315753191260537"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yU4s1GRvMdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c36cfae-21d7-4aae-b0b9-c86f933a32ab"
      },
      "source": [
        "min_rmse_loser = min_max_array(rmse_loser)\n",
        "min_rmse_loser, len(min_rmse_loser)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4.87018129255363,\n",
              "  4.343487868410211,\n",
              "  4.343487868410211,\n",
              "  4.343487868410211,\n",
              "  4.31722690080873,\n",
              "  4.1451215046465775,\n",
              "  4.1451215046465775,\n",
              "  4.1451215046465775,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  3.9962203731848227,\n",
              "  3.9962203731848227,\n",
              "  3.9962203731848227,\n",
              "  3.9962203731848227,\n",
              "  3.9962203731848227,\n",
              "  3.9962203731848227,\n",
              "  3.937438334950126,\n",
              "  3.937438334950126,\n",
              "  3.937438334950126],\n",
              " 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unXOpKHcvO15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3800cb84-a5ab-4d6c-e595-d9c6fb5d3ea8"
      },
      "source": [
        "min_rmse_winner = min_max_array(rmse_winner)\n",
        "min_rmse_winner, len(min_rmse_winner)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([4.87018129255363,\n",
              "  4.316024710268154,\n",
              "  4.316024710268154,\n",
              "  4.316024710268154,\n",
              "  4.316024710268154,\n",
              "  4.1451215046465775,\n",
              "  4.1451215046465775,\n",
              "  4.1451215046465775,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.130636454660636,\n",
              "  4.07295168965299,\n",
              "  4.07295168965299,\n",
              "  4.07295168965299,\n",
              "  4.07295168965299,\n",
              "  4.07295168965299,\n",
              "  3.734749153689431,\n",
              "  3.734749153689431,\n",
              "  3.734749153689431,\n",
              "  3.734749153689431],\n",
              " 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxo85-HEvRPi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "73ac1fd9-5853-4b30-df8b-aaf1f6e355bc"
      },
      "source": [
        "### Visualise!\n",
        "\n",
        "title = obj_func\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(min_rmse_loser, color = 'Green', label='RMSE: GP dEI ')\n",
        "plt.plot(min_rmse_winner, color = 'Yellow', label='RMSE: GP dERM ')# r'($\\nu$' ' = {})'.format(df))\n",
        "\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\n",
        "plt.xlabel('Experiment(s)', weight = 'bold', family = 'Arial') # x-axis label\n",
        "plt.ylabel('RMSE ($)', weight = 'bold', family = 'Arial') # y-axis label\n",
        "plt.legend(loc=0) # add plot legend\n",
        "\n",
        "### Make the x-ticks integers, not floats:\n",
        "count = len(min_rmse_loser)\n",
        "plt.xticks(np.arange(count), np.arange(1, count + 1))\n",
        "plt.grid(b=None)\n",
        "plt.show() #visualize!\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAETCAYAAAA1Rb1FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deVxUdfv/8dewCyiCsiiioua+YK6QphK476XiglppC7lkailGmuZCX+tHYuFyZ7mbGmLZnVYqaeYGmonmAm6ACKm4ICDb/P6YmFsTkNEZDsxcz8fDBzIzn3OuGcc3h8/5zHVUarVajRBCCKNnpnQBQgghyoYEvhBCmAgJfCGEMBES+EIIYSIk8IUQwkRI4AshhImQwBdCCBMhgS+Mwq5du2jUqBHt27cnLS0NgPz8fIYOHUqjRo1YsGABAKmpqcyePRtfX1+aN29Ohw4dePHFF1m+fLl2W4GBgTRq1IhGjRrRuHFjOnbsyCuvvEJcXFyZPZ/C/SclJZXZPoXxk8AXRqFHjx707NmT27dvM3v2bAC+/vprTpw4Qe3atZkyZQoXL15kwIABbNq0iaysLHr06EHXrl3Jz8/nq6++emSb7dq1Y9SoUdSsWZMDBw4wefLksn5aQuiVhdIFCKEvs2fP5siRI+zZs4elS5eycuVKVCoV8+fPp1KlSsyfP5/09HQ8PT3ZtGkTVatW1Y49e/bsI9vz8/Nj7NixnD17lv79+5OUlEROTg5WVlZkZmYSHh7Ozz//zI0bN6hduzYvv/wyAwcOBECtVrN582bWrVtHYmIizs7O9O7dm6CgIKytrbl9+zYhISEcPnyYzMxMnJ2d6dSpE3PnzqVRo0baGl544QUA1qxZQ4cOHQz8CgpjJ0f4wmg4OTkREhICQHh4ONnZ2YwYMYL27duTnZ3NwYMHARgzZsxDYQ88FLKFfvnlFz766COCg4MB6NatG1ZWVgDMnDmTVatWYW5uTs+ePbl8+TLvvfceO3bsAGDDhg188MEHpKSk0KtXL/Lz81m2bBnz588HYNWqVezatYu6desyePBg6tevz/HjxwEYPXq0tobBgwczevRo3Nzc9PlSCRMlR/jCqPTo0QNXV1dSU1MBGDVqFAC3b98mLy8PAHd3dwD27dvH+PHjtWP/fRR99OhRjh49CoBKpaJ169YA3Lhxg507dwKa4HZ3d6dx48YsWLCAdevW0bdvX9avXw/ArFmzGDRoEGfOnGHAgAFs2bKFWbNmaWtp2bIl/fr1o379+tjY2GjHrFmzBoC33nqLWrVqGeCVEqZIjvCFUfnqq69ITU1FpVIBEBoaCoCDgwMWFprjm2vXrgGa4B89ejSWlpZFbmvmzJmcPXuWnTt34uDgwKeffsrRo0dJTk4GwMbGRvvDo169egDa+wq/1q9f/6H7CwoKSElJYcyYMXTq1ImNGzcyZMgQ2rVrx7vvvktBQYF+XxAhHiCBL4zGhQsXWLJkCSqVis8++wwnJyeio6OJiorCxsaGjh07ArB27VoyMjKoX78+s2bN0h5ZF8fT0xMXFxcALl26pA357Oxsrl69CsDFixeB//32UPj1woULD301MzOjRo0aVK1alS+//JJjx46xfft2GjRowI4dOzh27Jj2caA5FyCEvsiUjjAKBQUFBAcHc//+fUaOHEmPHj0oKCjg7bffZuHChTz33HMEBwczYsQIzp07R+/evfH29kalUpGVlVXkNn/55ReSk5O5dOkS586dw8zMjBYtWlCtWjV69OjBrl27ePnll3n22We1UzwjR47Ufp07dy7z58/nyJEjHDp0CICXXnoJa2trli5dyp49e2jYsCGWlpba3wjs7e0BqFGjBsnJycydO5e6desyZcoUbG1tDf0yCiNnPmfOnDlKFyHE01q9ejVbtmzB3d2d8PBwrKyseOaZZzh//jynTp3iypUrjBo1it69e3Pv3j2uXLnCn3/+SUpKCg0aNGDkyJF069YNa2trtm3bRnJyMlevXuXEiRNcv36dhg0bEhwcjLe3NwCdO3cmJyeH8+fPc/LkSTw8PJg2bZp2lU7hD4aLFy8SExODvb09w4cPZ+rUqVhYWJCRkUFsbCzHjh3j1KlTuLq6MmHCBO2qHGdnZ06cOMHp06c5ceIEY8eOpVKlSoq9vsI4qOQCKEIIYRpkDl8IIUyEBL4QQpgICXwhhDAREvhCCGEiJPCFEMJElOt1+LGxsUqXIIQQFVKbNm0eua1cBz4UXbQQQojiFXewLFM6QghhIiTwhRDCREjgCyGEiZDAF0IIEyGBL4QQJkICXwghTIQEvhBCmIhyvw7/Sey/8gz5+TXp6vmr0qUIIYCkpCT69etH8+bNAcjJyaFhw4bMmTMHc3NzfH19CQgI4LXXXtOOCQ0NZdeuXezZs4fc3FzmzZvHuXPnMDc3x9zcnEWLFlGzZk0CAwPJzMx86AIxQ4cOpV+/fsXWs337dtauXYuVlRXZ2dn079+fsWPHAjy0vdzcXBo2bMjs2bMxNzcvclt79+5l165dLFq0CF9fX9zc3B56bFBQEB4eHkyaNInIyMineRmfmlEGfiWLW9hWSla6DCHEAzw9PVm7dq32+xkzZvD9998zcOBAnJ2d2b17tzbw1Wo1cXFx2sfu2LEDMzMzNm3aBMC2bdvYsGED06ZNA2DhwoU0bNiwVHXExsayceNGvv76a+zt7cnIyODll1+mQYMGdOrU6ZHtzZw5kx07djBgwIBSbX/lypXY2dk9dFtSUlKpxhqaUQZ+Zm5dmjrHkFeQg4WZldLlCCGK0LJlSy5fvgyAlZUVdnZ2xMfH06BBA2JjY6lfv7720o937tzh3r172rGDBg0q1T7efPNNIiIiHrpt3bp1TJw4UXs5SXt7ezZs2FDsxewfrLPQ2bNnee+993BwcKB27dqle8LlgFEGvqVZK2wtY4i/EU2Dat2VLkeIcmXNiTWsOr5Kr9t8pfUrjG41utSPz83NZffu3QwfPlx7W48ePfj++++ZMmUK//3vf+nevTv79u0DoH///mzbto0ePXrQpUsXunfvTtu2bR+7n3+HPWguKP/v3waKC/v8/Hz279/P0KFDH7r9iy++YMKECfj5+TF79uzH1lFeGGXgO1XqDHxJSsZuCXwhyomLFy8SGBgIaI6Qx40bh5+fn/b+F154gYCAACZNmsSRI0cIDg7W3ufo6Mi2bduIjY3lt99+Y+rUqbz44otMmjQJ0Ey7PDiHv2DBAjw8PIqsw8zMjPz8fACOHz/Op59+yv3792natCmFl/gu3F5BQQGdO3ema9euD20jISGBZ599FoAOHTpofzABjB8//qE5/JUrV+r6UhmMUQZ+nao9AcjKi1G4EiHKn9GtRut0NK4vD87hT5o0CU9Pz4fur1KlCrVq1eLrr7+mVatWWFj8L55ycnKwsLCgbdu2tG3bliFDhhAYGKgNfF3m8Bs0aMDJkydxc3OjdevWrF27lsOHD7N+/XrtYx63PbVajUqlAqCgoOCh+4qawy8vjHJZpo2FKyl3LbA0P690KUKIIkyfPp3FixeTlZX10O09e/ZkxYoVdO/+8G/mwcHBfPvtt9rvr127VuwR/OOMHj2aJUuWcOPGDUAT2IcOHcLKqvTn+zw9PbUnlQ8fPvxEdSjBKI/wAVLvVaN6pTSlyxBCFMHDw4MePXoQERHBO++8o73dz8+PxYsX4+Pj89Djg4OD+eCDD4iMjMTKygoLCwvt9As8OqXToUMHJkyYUORJ2xYtWvDee+/x+uuvY2lpyf379/Hy8iIkJKTU9b/55pvMnDmTNWvW4OHhQW5urva+f0/p9O3bl+eee67U2zYklVqtVitdRHFiY2OfuB/+oSQfWrgcBO5gZ1VZv4UJIUQ5Vlx2GnxKJzs7Gz8/v0c+cLB+/XqGDRvG8OHDmT9/vt73a2XuhZ0VxN/crfdtCyFERWTwwI+IiMDBweGh2zIyMvjyyy9Zv349GzduJCEhgT/++EOv+3Wx6wZA2r29et2uEEJUVAYN/ISEBOLj4x9Z0mRpaYmlpSWZmZnk5eWRlZX1yA+Fp1Wz8gsAZOcd0+t2hRCiojJo4IeGhjJjxoxHbre2tuatt97Cz8+Pbt260apVq0eWaD0tM5UTqRmWWFsk6HW7QghRURks8KOiovDy8ipy6VRGRgbLly9n586d7N69mxMnTnDmzBm91/B3pjMudn/rfbtCCFERGWxZZnR0NImJiURHR3Pt2jWsrKxwc3PDx8eHhIQEPDw8cHJyAqBt27bExcXRuHFjvdaQk9eARtWv8ve9VJztXPW6bSGEqGgMFvhhYWHav4eHh+Pu7q5dW+vu7k5CQgLZ2dnY2NgQFxdHly5d9F6DjcWz2Fnt40TqbpztRuh9+0KI0pH2yB5P9fz1pUw/eBUZGUnlypXx9/fn1VdfZfTo0Zibm9O6detSNULSlau9LxDG9cxfAQl8IZRk6u2Rn+b560uZBP7EiRMfuS0gIICAgACD7tepkuYf736efpd8CiGenqm3R9bl+euLUfbSKaRSOXI90wpbywtKlyJEObIG6KrnP2t0qqCwPXKzZs20txW2Rwa07ZEL9e/fn/Pnz9OjRw8WLFhATEzpGiPqqz1yy5YtH7q9sD3y6tWrMTPTPUZ1ff76YrS9dApdz3TF1T6ZAnUBZiqj/vkmRLlm6u2Rn+b564vRB35ufkMaV0/k8q2LeDrWV7ocIcqB0f/8KVum3h75aZ6/vhj9Ia+dVVvsrSD+prRYEKK8MPX2yLo+f30x+iN8N7sXgFBuZO4DxildjhACaY+s6/PXF6Ntj/w/6YAT6/70YlTL4/ooSwghyjXF2iMrz5EbmdbYWV5SuhAhhFCUCQQ+pGfXwL3KLXLyc5QuRQghFGMSgZ9f0Igm1eHM9b+ULkUIIRRjEoFvb9WeytZwIT1a6VKEEEIxJhH4rna+AKRnHVC4EiGEUI5JBL6FueZj0QXqkwpXIoQQyjGJwAcn0rNsqGydqHQhQgihGBMJfLh9vyZ1HO5x5/4dpUsRQghFmEzgq9VNaeoMcWkyrSOEME0mE/hVrDtS2Roupu97/IOFEMIImUzgO1XqDMDt7IMKVyKEEMowmcBXqf650IDqlLKFCCGEQkwm8KEat7MrUcU6iXLcL04IIQzGhAIf7tyvRQOnHFIyUpQuRQghypxJBb5K1VxW6gghTJZJBX5VG2+qWMOl9N+ULkUIIcqcSQW+vVUHAO7kHFK4EiGEKHsmFfjQFABz1RmF6xBCiLJnYoFfnbv3balaKYX8gnylixFCiDJlYoEPGTm1aVwtn4T0BKVLEUKIMmXQwM/OzsbPz4/IyMiHbk9JSWH48OG89NJLfPDBB4Ys4RHmZi1o6gwnU/8s0/0KIYTSDBr4ERERODg4PHL7okWLeOWVV9i6dSvm5uZcvXrVkGU8pKqNDw42cPn272W2TyGEKA8MFvgJCQnEx8fTtWvXh24vKCggNjYWX1/NVahmz55NzZo1DVXGI6zMWwNwL+dome1TCCHKA4MFfmhoKDNmzHjk9ps3b2JnZ8fChQsZPnw4n3zyiaFKKIamp4652dky3q8QQijLIIEfFRWFl5cXHh4ej9ynVqtJTU1l9OjRrFu3jtOnTxMdHW2IMopRnYwcW1zs/iYrN6sM9yuEEMqyMMRGo6OjSUxMJDo6mmvXrmFlZYWbmxs+Pj44OjpSs2ZNateuDYC3tzfnz59/ZOrHkDJzPWnqfIq/rv/FszWeLbP9CiGEkgwS+GFhYdq/h4eH4+7ujo+Pj2aHFhZ4eHhw6dIl6taty6lTp+jTp48hyiiWpVkrmjqfYvuZPyXwhRAmo8zW4UdGRvLzzz8DEBwczMyZMwkICKBy5craE7hlpYp1R6rawOXbcjEUIYTpMMgR/oMmTpz4yG116tRh48aNht51sczNWgCQlRurWA1CCFHWTO6TthqalTpW5ucVrkMIIcqOiQa+M5m5drhXucPNrJtKFyOEEGXCRAMfsvPq0cwZ4tLilC5FCCHKhMkGvpV5a+mpI4QwKSYb+HaW7XCsBFduH1a6FCGEKBMmG/gqlebE7f38YwpXIoQQZcNkA79wpY61RQJqtVrhWoQQwvBMOPCdycq1o77jfRLvJCpdjBBCGJwJB76KnPxn/jlxe1LpYoQQwuBMOPDBxqI1zZzhZJqs1BFCGD+TDnxri2dxrARJd2KULkUIIQzOpAMfmgKQk39c4TqEEMLwTDzwNSt17Cwvk5ufq3AtQghhWCYe+C7cz7OnsXMB529KIzUhhHEz8cBXkVvQkKbVZaWOEML4mXjgQyWLZ2nmIit1hBDGz+QD39ysJU6VIFFW6gghjJzJB37hSp2CApnSEUIYNwn8f1bqVK2UQkZOhsK1CCGE4Ujg40pOvj3NnOFU2imlixFCCIORwEdFfkFjmsrVr4QQRk4CH+mpI4QwDRL4gErVnGq2kHRHLoYihDBeFiXdefjwYf773/8SGxtLcnIyADVr1qRdu3b06dOHdu3alUmRhvfPSh21rNQRQhivYgN/0KBBnDlzBltbW5o0aULDhg1Rq9WkpaXx/fffs2nTJpo2bUpkZGRZ1msgmpU6NSvfJu1eGi52LgrXI4QQ+lds4NepU4d3332X9u3bY25u/tB9+fn5HD58mM2bNxu8wLLhRm6+Pc1cMjiZepIX6r2gdEFCCKF3xc7hh4WF4e3t/UjYA5ibm+Pj40NYWFiJG8/OzsbPz6/Y3wI++eQTAgMDdSzZEFSoaarpqZMm0zpCCONU4knbmJgY/vxTs3Ll2LFjTJo0ienTp3Pp0qVSbTwiIgIHB4ci74uPj+fo0aO6VWtAVuataO6i4mSqrNQRQhinEgN/0qRJHDt2jJycHN58801iYmL49ddfmTFjxmM3nJCQQHx8PF27di3y/kWLFjFlypQnKtowmlHNVk3y3T+ULkQIIQyi2MDfsGEDN2/e5OrVqyxfvpzbt2/Tt29funbtyunTp4mKiiIqKqrYDYeGhhb7gyEyMpL27dvj7u7+9M9AbzQrdcxUf1GgLlC4FiGE0L9iA9/CQnM+Nysri1OnTmFubk7Dhg2xtLRErVYDaL/+W1RUFF5eXnh4eDxy361bt4iMjOTll1/WR/16pFmp4+mYzcX0iwrXIoQQ+lfsKp2hQ4eyevVqvv/+e/Ly8mjTpg0vvfQSiYmJeHh4MHDgwGI3Gh0dTWJiItHR0Vy7dg0rKyvc3Nzw8fHh0KFD3Lx5k5EjR5KTk8OVK1dYsGABwcHBBnmCpVeDvAJ7mjlnEJcWR32n+grXI4QQ+lXiB68+//xzVq1ahUql4vXXXwfg/v37jz06f3D1Tnh4OO7u7vj4+ADQs2dPevbsCUBSUhIzZ84sB2EPoEJFM5o6H+a3KycZ0HiA0gUJIYRelRj4devWZe7cuQ/dVpoTtkWJjIykcuXK+Pv7P9H4smBu1oIWrkeJiJGlmUII41Ns4H/yyScMHTq0yHl4gMTERDZv3szUqVNL3MHEiROLva9WrVqsXbu2lKWWhWZUq1RA0u3jShcihBB6V2zgb9u2jf/85z/Ur1+fFi1a4OLiom2tEBcXR0JCAs7Ozo8N/IpFs1LHxiKB+3n3sbawVrgeIYTQn2IDf8+ePWzfvp0ffviBnTt3kpWVBYCNjQ1eXl68/PLL9OvXr8wKLRualTqNqhdw5voZWrm1UrgeIYTQn2ID38rKiiFDhjBkyBAKCgpIT08HwNHRETMzY+2qXJP8gn966qSdlMAXQhiVEk/aFjIzM6NatWqGrqUcUKFSNae582E+P/od9lb2ilVSx6EOXm5eqFQqxWoQQhiXUgW+KTFTNaOFawxbTm9hy+ktitbiZu9GzwY96dWgF/71/HGs5KhoPUKIik0C/xHNcKqUx59v7CZf7aRIBWq1mpNpJ/kx/ke2n9nO1398jZnKDO9a3vRq0Itez/TCy80LM5WxTq0JIQxBpS6uP0I5EBsbS5s2bcp4r7uAnkA00KWM9/2o/IJ8jiQf4cf4H/kx/kdirsYA4Grnqj36716/uxz9CyG0isvOYg8RJ0yYwLFjx8jOzmbp0qUkJSUB8NtvvzFo0CDDVaq4Zv98PaVoFYXMzczx9vBmbre5HB1/lGtTr7F64Gq6eXbj+3PfE/BtANX/rzrPrXqOj/Z9ROzVWGn+JoQoUrFTOr/88gu9e/fG09OTzz//nDZt2lCrVi3u3LnDmTNnyrLGMuYOVAFWA1cUruVRrvYwuhWMblWHAvWrpNxN4UL6BS6kXyAlI4SfL4Twe6ItNSq/zktNP1W6XCFEOVKqOfxyPOtjACqgL/AtcELhWkpmpgL3Kpo/neuAWm1NgboANZncvf//uHQrkLpVWytdphCinCgx8H/99Vft1a127tzJmTNnOH36dFnUpbD1//ypWFQqMFdB2r1fqG7rz29XRlG3avmYmhJCKK/EwN++fbv27998843277I2vHxzsfPjWEoz/Ouf4nTaLpq69FC6JCFEOVBs4C9cuLAs6xB61sBpAypakXhnHE1dEpUuRwhRDhQb+Ma9Esf4VbFuSUxyZ/zq7edo8mrauY9RuiQhhMKKXZa5Y8cObevilJQUhg0bRuvWrQkICCA+Pr7MChRPrpnLeu7lqsjMnWJiJ96FEEUpNvC/+OILEhM1UwFhYWGcOHECS0tL4uLiHrkoiiifKll6cPb6ALrUTWffZZmiE8LUFRv4KSkpNG7cGNBco9ba2pqff/6Zt99+m1OnZOVHRdG6xtf8fc8ce6uPyCvIVbocIYSCig18S0tLLl++zMGDB7l9+zZeXl44ODhgb28vq3QqEAszB5LujKNNzSz2XjSmi9UIIXRVbOB7e3uzfPlyXnnlFVQqFX379gXg+PHj1K5du8wKFE/Py20Jl2/ZULtqBFm5GUqXI4RQSLGrdObNm4ebmxsXL16kbdu2DBkyhNzcXHJycggICCjLGsVTUqmsuJszg+Yuc9gZ/yo9G3zz+EFCCKMj3TJNhpoz1x2pbHUHO6tkqtrUULogIYSBFJedxR7hz5w5s9iNqVQqFixYoJ/KRBlRYa5ajHuV8fx4fiS9ntmjdEFCiDJW7BF+48aNtSdn//0QlUrFX3/9ZfDi5Ahf//64VpM6Dilk552iRuWmSpcjhDAAnY/wbW1tyczMpE6dOgwaNAgfHx8jvni56ahWaQUONv04kjCCGpX/ULocIUQZKjbBDxw4wIIFC3B2diYsLIxJkybxyy+/4OzsTPPmzcuyRqFHHg59OZrciOfrnOBC+j6lyxFClKFiA79SpUoMHjyYdevW8eGHH3Lz5k2WL1/Od999V5b1CQOo57gWMxVcujVW6VKEEGWo2Cmda9eu8e2337Jt2zaSk5Np1aoVL774In369Cn1xrOzs+nbty9BQUEMHjxYe/uhQ4f49NNPMTMzw9PTk/nz58t0URlytmvH74kd6FLnMKfSttDMZYjSJQkhykCxge/r64tarcbDw4PJkydTr149QHNNW4Du3bs/duMRERE4ODg8cvsHH3zAmjVrcHNzY9KkSezfv58uXZS/YLgpaeGykXu59bl1/y1AAl8IU1Bs4BcUaC6EfeXKFT777DPt7Wq1ulSrdBISEoiPj6dr166P3BcZGYm9vT0ATk5OpKenP0nt4ilUtvbkwJWePFf7R44mh9HO/W2lSxJCGFixgT9hwoSn2nBoaCghISFERUU9cl9h2KelpXHgwAEmT578VPsST6ZNzbWk3XPB0vx9CtQTMVOZK12SEMKAnijwz507V+JGo6Ki8PLywsPDo9jH3LhxgzfeeIPZs2fj6OhYilKFvtlYVON4SiDeHqvZfzmYznVClS5JCGFAJV7TdteuXSQmJtKyZUvat2/P2bNnWbJkCdHR0SW2SI6OjiYxMZHo6GiuXbuGlZUVbm5u+Pj4AJCRkcH48eN5++236dSpk36fkdBJe/dlXLy1ETf7MHLzP8TS3EbpkoQQBlJs4H/00UesX79eO2c/ZswY1q9fT25uLs2aNStxo2FhYdq/h4eH4+7urg17gEWLFjFmzBief/55PTwF8TTMzWy4cW8Kbd1D2XvxNbp5rlG6JCGEgRTbWuG5557Dw8ODkSNHcvjwYbZu3Yq7uzuzZs3C19e31DsoDHyAypUr06lTJ9q1a0fr1q21j+nbty/Dhg17ZKy0VigbanUBf113wMEmk6rWqdhZVVe6JCHEU9C5tcLNmzeZMWMG/fr1w8fHh61btzJt2jSdwh5g4sSJj9wWFxen0zaEYalUZuQXLMC98iT2XAjEt96PSpckhDCAEpunNW3aFBcXF/Ly8jhw4ACtWrWiatWqqFQqIiIiDF6cHOGXrZirLtR3vA7E41ipntLlCCGekM5H+ACnT5/m9OnT2u//+EPTbEsucWicHKy/wMFmCNGXhuPreVjpcoQQelZs4O/evbss6xDlwDPVXuJAYj28ax3h6p0j1KzSXumShBB6VGzgF55oFaaljsNqzFSdyVM/x8k05ZZo3s+z5l6OBypVE6pYdcC9ygtUt22KSiU9l4R4UiVO6QjTU6tKJ367Mh4bi0hF67CzzKRJ9T+ws/oD2AhAepaKpDt23MquQb76GWwtnsXF7nlqOXTGwkw+PyDE40jgi0d0qr0CWKF0GRSo80i+c5RrGdFk5MSg4gyVra/SsFoCrvbngf8CcD8PLqZbcz2zOvfzPLE0b4GtZX1AuXNNtap0xtmunWL7F6IoEvii3DJTWeBexRv3Kt6P3Hcr6yJJd3dzK/sQeQVxVLK4jJt9Gh4OyViY/aZAtQ/LzYeM+5Owt14EVFK6HCEACXxRQVWt5EnVSuOAcQ/dnpOfQfzNPWTcv6hMYUBG7l0ups8hsNUS1OofUKkiAH/F6hGikAS+MCpW5vY0cOqvdBkcS6mC7+rJbBt2Fweb7sBI4FPAReHKhCmTJQ9CGMCE9hOAbtRfco/0rInAZqAx8CVQoGhtwnRJ4AthAGYqM74a8BU5+WYM+uZPCtTHgeZopqC6AiVfQEgIQ5DAF8JA6lStw5JeS/j18q98dugnIBr4DxAHtAI+ALIVrFCYGgl8IQxoTKsx9G/Un5m7Z3L67zPAq8AZYCgwD2gJ7FGyRGFCJEjQGwYAABieSURBVPCFMCCVSsWKviuobF2Z0dtGk5ufi+bE7TrgJzTz+S8AY4HryhUqTIIEvhAG5mrvyrI+y4hNiWXB/gUP3OMPnASCgfVoTuquBopsYCvEU5NlmUKUgRebvsiolqOYt28efRr2oW3Ntv/cUwmYDwwHXkdzpP81mvl9WyVKLSfcgVpKF2F0JPCFKCPhvcLZe3Evo7eNJva1WCpZPvgJ3ObAfmAl8B6g24WGjI8jcA2wUroQoyKBL0QZqWpTlVUDVtFjXQ/e3/M+n/T45F+PMENzlD8YiCn7AsuNY8D7wCFArnutTxL4QpSh7vW782bbN/l/h/4f/Rv1p0vdLkU8yhnoVdallSM+wGw0J7Ul8PVJTtoKUcb+z///qOdYj7Hbx3L3/l2lyymHHICOaAJf6JMEvhBlzM7KjjWD1nDl9hXe2fWO0uWUU/5oprVuKl2IUZHAF0IBPh4+TPeZzn+O/4cfzv2gdDnlUHc0y1PlUqv6JIEvhEI+7PohLVxaMO77cdzIvKF0OeVMOzRTOzKto08S+EIoxNrCmrWD1nIj8wZB/w1SupxyxgLN0tSfkQ+i6Y8EvhAKauXWijld57D51GY2xW1SupxypjtwGTivdCFGQwJfCIW9+9y7dKzVkaAfgrh696rS5ZQjhVcJk2kdfTFo4GdnZ+Pn50dkZORDt//++++89NJLDBs2jM8//9yQJQhR7lmYWbB64Gqy87IZ99041GqZwtCoD9RDM60j9MGggR8REYGDg8Mjt3/00UeEh4ezceNGDhw4QHx8vCHLEKLca1itIR/7f8yP8T+y8thKpcspR7qjaR+dq3QhRsFggZ+QkEB8fDxdu3Z96PbExEQcHByoUaMGZmZmdOnShYMHDxqqDCEqjKB2Qbzg+QLv7HqHC+kXlC6nnPAHMtC0WRBPy2CtFUJDQwkJCSEqKuqh2//++2+cnJy03zs5OZGYmGioMoSoMAovi9g8ojntVrbDqZLT4wcZucpW+RwdDxExA/jscLVSj6tVpRaL/RfTpmYbA1ZX8Rgk8KOiovDy8sLDw8MQmxfCaHk4eLBt2DZWHV+FWpYjApCQfpeeDdQcTGpf6jF7Lu6h/X/a807Hd/iw24fYWppyq+n/MUjgR0dHk5iYSHR0NNeuXcPKygo3Nzd8fHxwcXHh+vX/XdknNTUVFxcXQ5QhRIXk6+mLr6ept0d+0GzgI9YPXoqmbfLj3cq+xbs/v8vig4v59q9vWd53Of71/R8/0MgZZA4/LCyMb7/9ls2bNzNkyBCCgoLw8fEBoFatWmRkZJCUlEReXh579+7lueeeM0QZQgij0B3NpSBLf+3fqjZVWdFvBdFjorEws6D7uu6MjRpr8p9oLrN1+JGRkfz8s2Z51Zw5c5g6dSojR46kd+/eeHp6llUZQogKpz1QhSdZj9+lbhf+fPNPgjsFs/7kepp83oQNJzeY7NJXlbocP/PY2FjatJGTLkKIgcAJ4AKgeqIt/Jn6J+O/H8+R5CP0atCLiD4R1KlaR59FlhvFZad80lYIUQF0By4BCU+8hZauLfn9ld8J6xHGvsv7aPZFMz479Bn5Bfn6KrLck8AXQlQA+mmzYG5mzuSOkzkVdIrn6zzP27vexmeVDydTTz59iRWABL4QogJoANRFX20W6lStww8jfmD94PVcSL/Asyue5f0975Odl62X7ZdXEvhCiApAhb7bLKhUKka0GMFfb/3FiBYjmL9/Pq2WtWLf5X162X55JBcxF0JUEP7ACuAIoL+l3NVtq7N64GpGtRjF6ztep8vXXejfqL+in3RuWr0p05+brvftSuALISoIXzSTEj+jz8Av5F/fn5NvnuTDXz9ky+ktFKgL9L6P0rqeed0ggS/LMoUQFUhHNKH/u9KFlGuyLFMIYQT80Uzp3FK6kApJAl8IUYF0B/KBvUoXUiFJ4AshKpCOgD1y2cMnI4EvhKhALIFuyGUPn4wEvhCigumOpsXCk7dZMFUS+EKICqawzYIc5etKAl8IUcE0BGojga87CXwhRAVT2GZhN5CncC0ViwS+EKIC8gduA0eVLqRCkcAXQlRAL6A50pdpHV1I4AshKqBqQFtkPb5uJPCFEBWUP3AIzdSOKA0JfCFEBVXYZiFa4ToqDgl8IUQF5Q3YIdM6pSeBL4SooKyArsiJ29KTwBdCVGDdgfPARaULqRAk8IUQFVj3f77KUX5pSOALISqwRkAtJPBLx2DXtM3KymLGjBncuHGD+/fvExQURLdu3bT3r1+/nu+++w4zMzOaN2/OrFmzDFWKEMJoFbZZiESzYsdc2XLKOYMd4e/du5fmzZuzbt06wsLCWLRokfa+jIwMvvzyS9avX8/GjRtJSEjgjz/+MFQpQgij5o/mkocxShdS7hnsCL93797av6ekpODq6qr93tLSEktLSzIzM7G1tSUrKwsHBwdDlSKEMGp+/K/NQgeFaynfDBb4hQICArh27RrLli3T3mZtbc1bb72Fn58f1tbW9OnTB09PT0OXIoQwStWBZ9Gsx39f4VrKN4OftN20aRMRERFMnz4dtVoNaKZ0li9fzs6dO9m9ezcnTpzgzJkzhi5FCGG0/IGDwF2lCynXDBb4cXFxpKSkANCkSRPy8/O5efMmAAkJCXh4eODk5ISVlRVt27YlLi7OUKUIIYxedzS98aMVrqN8M1jgx8TEsGrVKgCuX79OZmYmjo6OALi7u5OQkEB2djag+eFQt25dQ5UihDB6PoAt0mahZAabww8ICGDWrFmMGDGC7OxsPvjgA6KioqhcuTL+/v68+uqrjB49GnNzc1q3bk3btm0NVYoQwuhZA12Q9fglU6kLJ9bLodjYWNq0aaN0GUKICiEMmAJcAuooW4rCistO+aStEMJISJuFx5HAF0IYiSZATSTwiyeBL4QwEoVtFn5B02ZB/JsEvhDCiHQHbgLHlC6kXJLAF0IYkRf++SrTOkUxeGsFIYQoOy5Aa2Ar8IzCtTyNBmieh35J4AshjEw/YC4wVOlCnkJNIFnvW5XAF0IYmRAgAChQupCn4GaQrUrgCyGMjAWaJZri3+SkrRBCmAgJfCGEMBES+EIIYSIk8IUQwkRI4AshhImQwBdCCBMhgS+EECai3K/Dj42NVboEIYQwCuX6ildCCCH0R6Z0hBDCREjgCyGEiTDKwD937hx+fn6sW7fuicZ//PHHDBs2jBdffJGffvpJp7FZWVlMnjyZUaNGMWTIEPbu3avz/rOzs/Hz8yMyMlLnsYcPH6Zjx44EBgYSGBjIvHnzdN7Gd999R//+/Rk8eDDR0dE6jd2yZYt234GBgbRurVuL13v37jFhwgQCAwMJCAhg//79Oo0vKCggJCSEgIAAAgMDSUhIKNW4f79nUlJSCAwMZMSIEUyePJmcnBydxgOsWbOGZs2ace/evSfa/9ixYxk1ahRjx47l77//1mn88ePHGT58OIGBgbz66qvcvHlT5/oB9u/fT6NGjXSuf8aMGfTr10/7Pnjc++jf43Nzc5k6dSovvfQSY8aM4fbt2zqNnzRpknbf/fr1IyQkRKfxR48e1b5+r7/+us77T0hIYOTIkYwaNYr333+fvLy8Esf/O3N0ff+VVrk/aaurzMxM5s2bh7e39xONP3ToEOfPn+ebb74hPT2dQYMG0b1798cP/MfevXtp3rw548ePJzk5mVdeeYVu3brpVENERAQODg66lq7Vvn17lixZ8kRj09PT+fzzz/n222/JzMwkPDycrl27lnr8kCFDGDJkCABHjhzhxx9/1Gn/27Ztw9PTk6lTp5KamsqYMWPYuXNnqcfv3r2bu3fvsmnTJq5cucL8+fNZvnx5iWOKes8sWbKEESNG0KtXLz799FO2bt3KiBEjSj0+KiqKGzdu4OLi8tiaixofFhbG0KFD6d27N+vXr+err77i3XffLfX4r776io8//hgPDw+WLl3K5s2beeONN0o9HuD+/fusWLECZ2dnnesHeOedd0r13i9q/ObNm3F0dOSTTz7hm2++ISYmhhdeeKHU4x98/8+cOVP7nizt+IULF7J48WLq1avHsmXL+Oabb3jttddKPX7x4sW89tprdOnShc8//5wff/yRfv36FTm+qMzx9vYu9ftPF0Z3hG9lZcXKlStL9R+tKO3ateOzzz4DoEqVKmRlZZGfX/rrY/bu3Zvx48cDmqM0V1dXnfafkJBAfHy8TiGrTwcPHsTb2xt7e3tcXFye6DeEQp9//jlBQUE6jXF0dOTWrVsA3LlzB0dHR53GX7p0iZYtWwJQu3Ztrl69+th/v6LeM4cPH9YGTLdu3Th48KBO4/38/JgyZQoqleqxNRc1fvbs2fTo0QN4+DUp7fglS5bg4eGBWq0mNTUVN7fi2+0W939m2bJljBgxAisrK53r10VR4/fu3Uv//v0BGDZsWLFh/7j9X7hwgbt372rfE6Ud/+Brfvv27RLfh0WNv3z5snafnTt35sCBA8WOLypzdHn/6cLoAt/CwgIbG5snHm9ubo6trS0AW7du5fnnn8fc3Fzn7QQEBDBt2jSCg4N1GhcaGsqMGTN03t+D4uPjeeONNxg+fHiJb7SiJCUlkZ2dzRtvvMGIESOe+I32559/UqNGjcceHf5bnz59uHr1Kv7+/owaNYr33ntPp/ENGzbkt99+Iz8/nwsXLpCYmEh6enqJY4p6z2RlZWmDrlq1aiVOqRQ13t7evtQ1FzXe1tYWc3Nz8vPz2bBhQ7FHh8WNB9i3bx89e/bk+vXr2vAs7fiLFy9y5swZevXq9UT1A6xbt47Ro0czZcqUEqeUihqfnJzMvn37CAwMZMqUKSX+wCvp//yaNWsYNWqUzvUHBwfz1ltv0aNHD2JjYxk0aJBO4xs2bMivv/4KaKbFrl+/Xuz4ojJHl/efLowu8PXll19+YevWrXzwwQdPNH7Tpk1EREQwffp0SrvyNSoqCi8vLzw8PJ5onwB169ZlwoQJREREEBoayqxZs3Se/7t16xZLly5l0aJFzJw5s9T1P2jr1q0l/icpzvbt26lZsyY///wzq1evZu7cuTqN79KlCy1atGDkyJGsXr2aevXqPVH9D1Jq5XJ+fj7vvvsuHTt2fKIpyueff56dO3dSr149VqxYodPYhQsXMnPmTJ33WWjAgAFMmzaNNWvW0KRJE5YuXarTeLVajaenJ2vXruWZZ5557LRcUXJycoiNjaVjx446j503bx5Lly5l165dtGnThg0bNug0/r333uPHH39k9OjRqNXqUr2Hisscfb7/JPCLsH//fpYtW8bKlSupXLmyTmPj4uJISUkBoEmTJuTn5z/2hFmh6Ohodu/ezdChQ9myZQtffPEFv//+u077d3V1pXfv3qhUKmrXrk316tVJTU0t9fhq1arRunVrLCwsqF27NnZ2dqWu/0GHDx/W+YQtwLFjx+jUqRMAjRs3Ji0tTacpNYApU6awadMmPvzwQ+7cuUO1atV0rsPW1pbs7GwAUlNTn3i64mnMnDmTOnXqMGHCBJ3H/vyz5iLeKpVKe5RaWqmpqVy4cIFp06YxdOhQ0tLSHnuU/G/e3t40aaK5CImvry/nzp3TaXz16tVp164dAJ06dSI+Pl6n8aA58VrSVE5Jzp49S5s2bQDw8fEhLi5Op/E1atRg+fLlrFmzhlatWuHu7l7i4/+dOYZ6/0ng/8vdu3f5+OOPWb58OVWrVtV5fExMDKtWrQLg+vXrZGZmlnoeOiwsjG+//ZbNmzczZMgQgoKC8PHx0Wn/3333HV9++SUAf//9Nzdu3NDpPEKnTp04dOgQBQUFpKen61R/odTUVOzs7B4791uUOnXqcOLECUDza72dnZ1OU2pnzpzRHpnu27ePpk2bYmam+9vcx8eHXbt2AfDTTz/RuXNnnbfxNL777jssLS2ZNGnSE40PDw/nr7/+AuDEiRN4enqWeqyrqyu//PILmzdvZvPmzbi4uOi84m3ixIkkJiYCmh/+zzyj2wXFn3/+ee0KrVOnTulUf6GTJ0/SuHFjnceB5gdO4Q+ZkydPUqdOHZ3GL1myRLsyKTIyEl9f32IfW1TmGOr9Z3SftI2LiyM0NJTk5GQsLCxwdXUlPDy81OH9zTffEB4e/tAbLDQ0lJo1a5ZqfHZ2NrNmzSIlJYXs7GwmTJhQ4j92ccLDw3F3d2fw4ME6jcvIyGDatGncuXOH3NxcJkyYQJcuXXTaxqZNm9i6dSsAb775ZoknzIoSFxdHWFgY//nPf3QaB5plmcHBwdy4cYO8vDwmT56s03RGQUEBwcHBxMfHY21tzeLFi6lRo8Zj6/33e2bx4sXMmDGD+/fvU7NmTRYuXIilpWWpx/v4+PD777/zxx9/0KJFC7y8vIpdZVPU+Bs3bmBtba09F1C/fn3mzJlT6vHTp09nwYIFmJubY2Njw8cff1zsbzqP+z/j6+vLnj17dHr9Ro0axYoVK6hUqRK2trYsXLhQp/0vXryY+fPn8/fff2Nra0toaCjVq1fXqf7w8HDatGlD7969i629uPFTpkzh448/xtLSEgcHBxYsWECVKlVKPX7atGnMmzcPtVpN27ZtS5weKypzFi1axPvvv1+q958ujC7whRBCFE2mdIQQwkRI4AshhImQwBdCCBMhgS+EECZCAl8IIUyEBL4ot5KSkmjUqNFDf9q2bVtm+/f19X2iD489qWXLlvH1118/dNv169dp1arVYz/p+dJLL+nct0iYHqPrlimMT9OmTRk3bhyAXtYil0Z+fj7vv/8+ubm5ZbI/gOXLl+Po6MjYsWO1t61btw61Ws2AAQNKHDts2DBCQkK4cuUKtWvXNnCloqKSI3xR7jk5OeHt7a39M3nyZJo1a8bZs2f5448/aNKkibZJXeFR+cKFC+nQoQMBAQFcvXoV0HwCeOLEibRr145OnTqxePFibdsGX19fvLy8mDNnDm3atOHcuXN89NFH2kZ2kZGRNGrUiHfeeYfevXvj7e3Nrl27mDp1Kl5eXgQFBWl7nh8/fpxhw4bRunVrevTowY4dO4D//cYSEBDAuHHjePbZZ5k6dSpqtZrAwEAyMzNJTk6mUaNG2v3u2LGDDh06YGdnB2g+kOfj40OLFi3w9/fn+++/BzQdFdVqtc7tqIVpkcAX5d5vv/2mDfugoCBmz56Ng4MDISEhhISE4Orq+lBX0szMTDIzMwkICOD48eMsWLAAgGnTpnHgwAFGjx6Nr68vK1eufGiqJCsri7S0NN577z2cnJyKrOXYsWMMHz6c9PR03n77bapUqUKbNm3YvXs30dHR3Lp1izfeeIM7d+7wxhtv4O7uzvTp07VtDkDT6qBdu3Z4enqyY8cOYmNjCQoKwsrKCkdHRz799FOGDx9OWloaiYmJtGjRAtC06V26dCkNGjRg3rx59O/fn4KCAkDTCqBGjRrExMTo/fUXxkOmdES516pVK95++21A0y/cycmJOXPmMHHiRAC+/PLLh9oRm5mZERISgpWVFVFRURw5coR79+5x9OhR1Gr1Q50bDxw4QGBgoPb70NDQEhvmDRgwgMDAQFasWMH169eZOXMm27dv57fffiMpKQkLCwtu3brFrVu3+PTTT7XjDh06hL+/v/b5vP7666hUKuLi4khKSmLgwIFYWFhga2tLnz59ALQ9hQobZ9na2uLs7MzFixeJjY2lZcuWD12cx8XFheTk5Cd7kYVJkMAX5Z6jo+MjTeQe7A9eUq/xB6nVaho3bvxQj/0Hf1DY2to+tjtqYT8VS0tLbGxssLKy0jZ3e7Cr58CBAx+ad3+wW2Lh1cwKxxUepZdUd+E+t2/fzq5du/jrr7+YPXs2hw8fZvHixQ89TojiSOCLci8tLY0ffvhB+32TJk1YvHgxnTt3JiMjg/nz5+Pt7a3tClpQUMC8efNwcnLi2rVr+Pv7Y2dnR/v27YmJiSEmJgZXV1diY2OpV6/eE7fQLYqXlxdVq1Zl//79tGjRgry8PKKjowkKCnpsAz4HBwdu3rzJtm3baNGihbbpW1paGqBpjPfxxx/TunVrmjdvzo4dO7T3FT5O166UwrRI4Ity7/Tp07zzzjva7wtb3s6dO5esrCwGDRpESEiI9iIftra22Nvbs2nTJry8vLTz+4UdGNevX09ubi4NGzZk4MCBeq21atWqLFu2jNDQUD755BOsra3x8vLC3d39sUfg48aN47PPPmPGjBlMnjyZoKAgPDw8tL3YLSwsuHr1Knv27CE7O5v69etrp7quX7/OtWvX9HLdU2G8pFumMCq+vr6kp6dz/PhxpUvRi88++4wvv/ySgwcPalfqFGXLli2EhITw008/ybJMUSxZpSNEOTZy5EhUKhXbt28v8XHffPMNvr6+EvaiRHKEL4QQJkKO8IUQwkRI4AshhImQwBdCCBMhgS+EECZCAl8IIUyEBL4QQpiI/w+rmruhzChQQQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwyO7_iZvT7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eabc4322-7623-43e9-aae5-357ef54684a8"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1534.7303249835968, 1555.3646206855774)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    }
  ]
}