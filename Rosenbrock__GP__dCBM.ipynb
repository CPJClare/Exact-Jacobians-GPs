{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rosenbrock__GP__dCBM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaQo_XsusPbm"
      },
      "source": [
        "Rosenbrock synthetic function:\r\n",
        "\r\n",
        "GP EI: (exact GP EI gradients) vs. GP CBM: (exact GP CBM gradients)\r\n",
        "\r\n",
        "https://www.sfu.ca/~ssurjano/rosen.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhgucjJgsYfm",
        "outputId": "571fe97b-53be-456b-bf77-7157ca620032"
      },
      "source": [
        "!pip install pyGPGO"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyGPGO\n",
            "  Downloading https://files.pythonhosted.org/packages/8d/87/a113c91ba014708114f7635d5c0f6a5e5c773480c5f0a537b257a02d180d/pyGPGO-0.4.0.dev1.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (0.22.2.post1)\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (1.0.5)\n",
            "Requirement already satisfied: pyMC3 in /usr/local/lib/python3.7/dist-packages (from pyGPGO) (3.7)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from theano->pyGPGO) (1.15.0)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.8.4 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (4.41.1)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (2.10.0)\n",
            "Requirement already satisfied: pandas>=0.18.0 in /usr/local/lib/python3.7/dist-packages (from pyMC3->pyGPGO) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.18.0->pyMC3->pyGPGO) (2018.9)\n",
            "Building wheels for collected packages: pyGPGO\n",
            "  Building wheel for pyGPGO (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyGPGO: filename=pyGPGO-0.4.0.dev1-cp37-none-any.whl size=19866 sha256=4598329f45ebb3681df7f3b06b83dbaf3262cdeb70eab003afc5d476708e1c52\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/27/04/c4fa3bfe194d36e3cd51542132f43415a6813114a5e8301acb\n",
            "Successfully built pyGPGO\n",
            "Installing collected packages: pyGPGO\n",
            "Successfully installed pyGPGO-0.4.0.dev1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E99qMZverfrP"
      },
      "source": [
        "### Import modules:\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import scipy as sp\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import warnings\r\n",
        "import time\r\n",
        "\r\n",
        "from pyGPGO.logger import EventLogger\r\n",
        "from pyGPGO.GPGO import GPGO\r\n",
        "from pyGPGO.surrogates.GaussianProcess import GaussianProcess\r\n",
        "from pyGPGO.surrogates.tStudentProcess import tStudentProcess\r\n",
        "from pyGPGO.acquisition import Acquisition\r\n",
        "from pyGPGO.covfunc import squaredExponential\r\n",
        "\r\n",
        "from collections import OrderedDict\r\n",
        "from joblib import Parallel, delayed\r\n",
        "from numpy.linalg import slogdet, inv, cholesky, solve\r\n",
        "from scipy.optimize import minimize\r\n",
        "from scipy.spatial.distance import cdist\r\n",
        "from scipy.special import gamma\r\n",
        "from scipy.stats import norm, t\r\n",
        "from matplotlib.pyplot import rc\r\n",
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset\r\n",
        "\r\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiJSpz2P9qXK"
      },
      "source": [
        "n_start_AcqFunc = 250 #multi-start iterations to avoid local optima in AcqFunc optimization"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzPUbCs3s9nu"
      },
      "source": [
        "### Inputs:\r\n",
        "\r\n",
        "obj_func = 'Rosenbrock'\r\n",
        "n_test = n_start_AcqFunc # test points\r\n",
        "df = 3 # nu\r\n",
        "Beta_CBM = 1.5 # Default UCB Acquisition function parameter in pyGPGO https://github.com/josejimenezluna/pyGPGO/blob/master/pyGPGO/acquisition.py#L83\r\n",
        "\r\n",
        "util_loser = 'dEI_GP'\r\n",
        "util_winner = 'dCBM_GP'\r\n",
        "n_init = 5 # random initialisations"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AMqjmpbtAb1"
      },
      "source": [
        "### Objective function:\r\n",
        "\r\n",
        "if obj_func == 'Rosenbrock':\r\n",
        "            \r\n",
        "    # True y bounds:\r\n",
        "    y_lb = 0\r\n",
        "    operator = -1 # targets global minimum \r\n",
        "    y_global_orig = y_lb * operator # targets global minimum\r\n",
        "            \r\n",
        "# Constraints:\r\n",
        "    lb = -2.048 \r\n",
        "    ub = +2.048 \r\n",
        "    \r\n",
        "# Input array dimension(s):\r\n",
        "    dim = 2\r\n",
        "\r\n",
        "# 2-D inputs' parameter bounds:\r\n",
        "    param = {'x1_training': ('cont', [lb, ub]),\r\n",
        "             'x2_training': ('cont', [lb, ub])}\r\n",
        "    \r\n",
        "    max_iter = 100  # iterations of Bayesian optimisation\r\n",
        "    \r\n",
        "# Test data:\r\n",
        "    x1_test = np.linspace(lb, ub, n_test)\r\n",
        "    x2_test = np.linspace(lb, ub, n_test)\r\n",
        "    Xstar_d = np.column_stack((x1_test, x2_test))\r\n",
        "    \r\n",
        "    def f_syn_polarity(x1_training, x2_training):\r\n",
        "        return operator * (100 * (x2_training - x1_training ** 2) ** 2 + (x1_training - 1) ** 2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyQX-59DtC7-"
      },
      "source": [
        "### Cumulative Regret Calculator:\r\n",
        "\r\n",
        "def min_max_array(x):\r\n",
        "    new_list = []\r\n",
        "    for i, num in enumerate(x):\r\n",
        "            new_list.append(np.min(x[0:i+1]))\r\n",
        "    return new_list"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE_I_YSntHxl"
      },
      "source": [
        "### Set-seeds:\r\n",
        "\r\n",
        "run_num_1 = 111\r\n",
        "run_num_2 = 113\r\n",
        "run_num_3 = 3333\r\n",
        "run_num_4 = 444\r\n",
        "run_num_5 = 5555\r\n",
        "run_num_6 = 6\r\n",
        "run_num_7 = 777\r\n",
        "run_num_8 = 887\r\n",
        "run_num_9 = 999\r\n",
        "run_num_10 = 1000\r\n",
        "run_num_11 = 1113\r\n",
        "run_num_12 = 1234\r\n",
        "run_num_13 = 2345\r\n",
        "run_num_14 = 88\r\n",
        "run_num_15 = 1556\r\n",
        "run_num_16 = 1666\r\n",
        "run_num_17 = 717\r\n",
        "run_num_18 = 8\r\n",
        "run_num_19 = 1998\r\n",
        "run_num_20 = 2000"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oe29v5N5tKjF"
      },
      "source": [
        "### Derivatives - Squared-exponential covariance function:\r\n",
        "\r\n",
        "def l2norm_(X, Xstar):\r\n",
        "    \r\n",
        "    return cdist(X, Xstar)\r\n",
        "\r\n",
        "def kronDelta(X, Xstar):\r\n",
        "\r\n",
        "    return cdist(X, Xstar) < np.finfo(np.float32).eps\r\n",
        "\r\n",
        "class squaredExponentialDeriv(squaredExponential):\r\n",
        "    \r\n",
        "    def K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r **2) + self.sigman * kronDelta(X, Xstar)\r\n",
        "        return K\r\n",
        "    \r\n",
        "    def dK(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        dK = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * l2norm_(X, Xstar)\r\n",
        "        return dK\r\n",
        "    \r\n",
        "        \r\n",
        "    def d2K(self, X, Xstar):\r\n",
        "        \r\n",
        "        r = (l2norm_(X, Xstar)/self.l)\r\n",
        "        d2K = self.sigmaf/self.l**2 * np.exp(-1/2 * r **2) * (r**2-1)\r\n",
        "        return d2K\r\n",
        "    \r\n",
        "cov_func = squaredExponential()\r\n",
        "d_cov_func = squaredExponentialDeriv()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEBX5l7TtNSN"
      },
      "source": [
        "class Acquisition_new(Acquisition):    \r\n",
        "    def __init__(self, mode, eps=1e-08, **params):\r\n",
        "        \r\n",
        "        self.params = params\r\n",
        "        self.eps = eps\r\n",
        "\r\n",
        "        mode_dict = {\r\n",
        "            'dEI_GP': self.dEI_GP,\r\n",
        "            'dCBM_GP': self.dCBM_GP\r\n",
        "        }\r\n",
        "\r\n",
        "        self.f = mode_dict[mode]\r\n",
        "    \r\n",
        "    def dEI_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "        z = -1 * (tau - mean - self.eps) / (std + self.eps)\r\n",
        "        \r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        d2sdx = -dsdx**2 / ((std + self.eps)) - dvdv / (std + self.eps) - d2v / (std + self.eps)\r\n",
        "        dmdx = (dm - z * dsdx) / (std + self.eps)\r\n",
        "        d2mdx = (d2m - (z * d2sdx + 2 * dmdx * dsdx)) / (std + self.eps)\r\n",
        "        \r\n",
        "        f = (std + self.eps) * (z * norm.cdf(z) + norm.pdf(z)[0])\r\n",
        "        df = (f / (std + self.eps) * dsdx + (std + self.eps) * norm.cdf(z) * dmdx)\r\n",
        "        d2f = (f / (std + self.eps) * d2sdx + dsdx * dmdx * norm.cdf(z) \\\r\n",
        "            + d2mdx * (std + self.eps) * norm.cdf(z) + dsdx * norm.cdf(z) * dmdx \\\r\n",
        "            + norm.pdf(z)[0] * (std + self.eps) * dmdx)\r\n",
        "            \r\n",
        "        return f, df, d2f\r\n",
        "\r\n",
        "    def dCBM_GP(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "    \r\n",
        "        gamma = -1 * (y_global_orig - mean - self.eps) / (std + self.eps)\r\n",
        "        dsdx = ds / 2 * (std + self.eps)\r\n",
        "        dmdx = (dm - gamma * dsdx) / (std + self.eps)\r\n",
        "\r\n",
        "        f = (std + self.eps) * (gamma + np.sqrt(Beta_CBM))\r\n",
        "        df = dsdx * (gamma + np.sqrt(Beta_CBM)) + (std + self.eps) * (dmdx + np.sqrt(Beta_CBM))\r\n",
        "        return f, df\r\n",
        "    \r\n",
        "    def _eval(self, tau, mean, std):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, **self.params)\r\n",
        "    \r\n",
        "    def d_eval(self, tau, mean, std, ds, dm, dvdv, d2v, d2m):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, **self.params)\r\n",
        "\r\n",
        "    def d_eval_stp(self, tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0):\r\n",
        "    \r\n",
        "        return self.f(tau, mean, std, ds, dm, dvdv, d2v, d2m, nu=3.0, **self.params)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SdY8rHwtRJ_"
      },
      "source": [
        "### Surrogate derivatives: \r\n",
        "\r\n",
        "from scipy.linalg import cholesky, solve\r\n",
        "\r\n",
        "class dGaussianProcess(GaussianProcess):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1\r\n",
        "    sigman = 1e-6\r\n",
        "\r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\r\n",
        "        L = cholesky(K).T\r\n",
        "        alpha = solve(L.T, solve(L, self.y))\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(self.L, Kstar.T)\r\n",
        "        dv = solve(self.L, dKstar.T)\r\n",
        "        d2v = solve(self.L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -2 * np.dot(dv.T, v)\r\n",
        "        dvdv = np.dot(dv.T, dv)\r\n",
        "        d2s = -2 * (dvdv + d2v)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha)\r\n",
        "        d2m = np.dot(d2Kstar, alpha)\r\n",
        "        return ds, dm, dvdv, d2v, d2m\r\n",
        "\r\n",
        "class dtStudentProcess(tStudentProcess):\r\n",
        "    l = 1\r\n",
        "    sigmaf = 1\r\n",
        "    sigman = 1e-6\r\n",
        "    \r\n",
        "    def AcqGrad(self, Xstar, return_std=False):\r\n",
        "        r_X = l2norm_(self.X, self.X)/self.l\r\n",
        "        K = self.sigmaf * np.exp(-1/2*r_X **2) + self.sigman * kronDelta(self.X, self.X)\r\n",
        "        L = cholesky(self.K11).T\r\n",
        "        alpha = solve(L.T, solve(L, self.y))\r\n",
        "        Xstar = np.atleast_2d(Xstar)\r\n",
        "        Kstar = squaredExponentialDeriv.K(self, self.X, Xstar).T\r\n",
        "        dKstar = squaredExponentialDeriv.dK(self, self.X, Xstar).T\r\n",
        "        d2Kstar = squaredExponentialDeriv.d2K(self, self.X, Xstar).T\r\n",
        "        v = solve(L, Kstar.T)\r\n",
        "        dv = solve(L, dKstar.T)\r\n",
        "        d2v = solve(L, d2Kstar.T)\r\n",
        "        \r\n",
        "        ds = -2 * np.dot(dv.T, v)\r\n",
        "        dvdv = np.dot(dv.T, dv)\r\n",
        "        d2s = -2 * (dvdv + d2v)\r\n",
        "        \r\n",
        "        dm = np.dot(dKstar, alpha)\r\n",
        "        d2m = np.dot(d2Kstar, alpha)\r\n",
        "        return ds, dm, dvdv, d2v, d2m"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNv4Z0EqtUi2"
      },
      "source": [
        "class dGPGO(GPGO):  \r\n",
        "    n_start = n_start_AcqFunc\r\n",
        "    eps = 1e-08\r\n",
        "        \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7vea4uj-GOi"
      },
      "source": [
        "## dGPGO_stp - BayesOpt derivatives' class: Student's-t\r\n",
        "\r\n",
        "class dGPGO_stp(GPGO):  \r\n",
        "    n_start = 100\r\n",
        "        \r\n",
        "    def func_stp(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2v, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[0]\r\n",
        "        df = -self.A.d_eval_stp(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2v=d2v, d2m=d2m, nu=df)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "        \r\n",
        "    def d_optimizeAcq_stp(self, method='L-BFGS-B', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func_stp,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.x_best = x_best\r\n",
        "        self.f_best = f_best\r\n",
        "        self.best = x_best[np.argmin(f_best)]\r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq_stp()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "expMSaSUtcB-"
      },
      "source": [
        "### d2GPGO - BayesOpt class: \r\n",
        "\r\n",
        "class d2GPGO(GPGO):  \r\n",
        "    n_start = 100\r\n",
        "    p = np.full((n_start,1),1)\r\n",
        "    \r\n",
        "    def func(self, xnew):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2s, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        f  = np.empty((self.n_start,))\r\n",
        "        df = np.empty((self.n_start,))\r\n",
        "        f  = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[0]\r\n",
        "        df = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[1]\r\n",
        "        df_array = np.full((len(xnew),),df)\r\n",
        "        return f, df_array\r\n",
        "    \r\n",
        "    def hessp_nonzero(self, xnew, p):\r\n",
        "        new_mean, new_var = self.GP.predict(xnew, return_std=True)\r\n",
        "        new_std = np.sqrt(new_var + 1e-6)\r\n",
        "        ds, dm, dvdv, d2s, d2m = self.GP.AcqGrad(xnew, return_std=True)\r\n",
        "        df2 = np.empty((self.n_start,))\r\n",
        "        df2 = -self.A.d_eval(self.tau, new_mean, new_std, ds=ds, dm=dm, dvdv=dvdv, d2s=d2s, d2m=d2m)[2]\r\n",
        "        H2 = np.empty((self.n_start,))\r\n",
        "        df2 = np.asarray(df2)\r\n",
        "        p = np.asarray(p)\r\n",
        "        H2 = np.multiply(df2,p)\r\n",
        "        return H2\r\n",
        "    \r\n",
        "    def d_optimizeAcq(self, method='Newton-CG', n_start=n_start):\r\n",
        "        start_points_dict = [self._sampleParam() for i in range(n_start)]\r\n",
        "        start_points_arr = np.array([list(s.values())\r\n",
        "                                     for s in start_points_dict])\r\n",
        "        x_best = np.empty((n_start, len(self.parameter_key)))\r\n",
        "        f_best = np.empty((n_start,))\r\n",
        "        opt = Parallel(n_jobs=self.n_jobs)(delayed(minimize)(self.func,\r\n",
        "                                                                 x0=start_point,\r\n",
        "                                                                 method=method,\r\n",
        "                                                                 jac = True,\r\n",
        "                                                                 hessp = self.hessp_nonzero,\r\n",
        "                                                                 bounds=self.parameter_range) for start_point in\r\n",
        "                                               start_points_arr)\r\n",
        "        \r\n",
        "        x_best = np.array([res.x for res in opt])\r\n",
        "        f_best = np.array([np.atleast_1d(res.fun)[0] for res in opt])\r\n",
        "\r\n",
        "        self.best = x_best[np.argmin(f_best)]     \r\n",
        "    \r\n",
        "    def run(self, max_iter=10, init_evals=3, resume=False):\r\n",
        "        \r\n",
        "        if not resume:\r\n",
        "            self.init_evals = init_evals\r\n",
        "            self._firstRun(self.init_evals)\r\n",
        "            self.logger._printInit(self)\r\n",
        "        for iteration in range(max_iter):\r\n",
        "            self.d_optimizeAcq()\r\n",
        "            self.updateGP()\r\n",
        "            self.logger._printCurrent(self)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umKPgDuEtcxH",
        "outputId": "dd3b93ed-c9b6-4a26-969a-39117d0434b7"
      },
      "source": [
        "start_lose = time.time()\r\n",
        "start_lose"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1615129976.871753"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfEuNIqvtdSv",
        "outputId": "90db52d2-7a07-45a0-f739-d45b588ba01a"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 1\r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_loser_1 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_1 = dGPGO(surrogate_loser_1, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.45944904 -1.35549029]. \t  -245.71064611316496 \t -108.5713485785257\n",
            "init   \t [-0.26190226  1.10289909]. \t  -108.5713485785257 \t -108.5713485785257\n",
            "init   \t [-0.83834755 -1.43702853]. \t  -461.2775269355244 \t -108.5713485785257\n",
            "init   \t [-1.95592878 -0.32676048]. \t  -1732.9949421003257 \t -108.5713485785257\n",
            "init   \t [-1.07035795 -0.66496024]. \t  -332.12317010404104 \t -108.5713485785257\n",
            "1      \t [-0.16418989 -0.35655496]. \t  \u001b[92m-16.063581388629835\u001b[0m \t -16.063581388629835\n",
            "2      \t [-0.46725478 -0.31805013]. \t  -30.922881903337014 \t -16.063581388629835\n",
            "3      \t [0.36410567 1.99516816]. \t  -347.33045841529025 \t -16.063581388629835\n",
            "4      \t [ 1.88433773 -0.57897707]. \t  -1706.2290269490966 \t -16.063581388629835\n",
            "5      \t [-0.2797263  -0.65083633]. \t  -54.79392119360623 \t -16.063581388629835\n",
            "6      \t [-0.36967736 -0.35543443]. \t  -26.09184200624848 \t -16.063581388629835\n",
            "7      \t [ 0.02629338 -0.17265926]. \t  \u001b[92m-3.953147754267091\u001b[0m \t -3.953147754267091\n",
            "8      \t [0.115618   0.25596482]. \t  -6.667476486235974 \t -3.953147754267091\n",
            "9      \t [ 0.08916241 -0.04709341]. \t  \u001b[92m-1.132602051393015\u001b[0m \t -1.132602051393015\n",
            "10     \t [-1.62320868  1.91504656]. \t  -58.686647613096916 \t -1.132602051393015\n",
            "11     \t [-0.08235995  0.39477545]. \t  -16.22530470798019 \t -1.132602051393015\n",
            "12     \t [-0.73826796  0.85450126]. \t  -12.598228932592956 \t -1.132602051393015\n",
            "13     \t [-1.9487096   1.65113376]. \t  -469.37043159536927 \t -1.132602051393015\n",
            "14     \t [-1.12645133  1.928153  ]. \t  -47.984224062261326 \t -1.132602051393015\n",
            "15     \t [-0.52027153  0.35508588]. \t  -3.0236191451316823 \t -1.132602051393015\n",
            "16     \t [-1.38434577  2.00911604]. \t  -6.544486382798326 \t -1.132602051393015\n",
            "17     \t [0.11931237 0.03484829]. \t  \u001b[92m-0.8180996773800518\u001b[0m \t -0.8180996773800518\n",
            "18     \t [-0.28710502  1.95755726]. \t  -353.26712743307354 \t -0.8180996773800518\n",
            "19     \t [-2.01047749 -0.74297611]. \t  -2298.681505002316 \t -0.8180996773800518\n",
            "20     \t [1.82737506 1.96720994]. \t  -188.947559440074 \t -0.8180996773800518\n",
            "21     \t [-0.01733915  0.01617608]. \t  -1.0601818852729488 \t -0.8180996773800518\n",
            "22     \t [-1.21732315  1.76122934]. \t  -12.72037112638002 \t -0.8180996773800518\n",
            "23     \t [-0.95070309  1.48236624]. \t  -37.27492459488175 \t -0.8180996773800518\n",
            "24     \t [-1.41896608  0.20289253]. \t  -333.66856468368997 \t -0.8180996773800518\n",
            "25     \t [-0.54492688 -0.24007982]. \t  -31.226397664342578 \t -0.8180996773800518\n",
            "26     \t [-0.7271155   0.48673977]. \t  -3.1589684286668533 \t -0.8180996773800518\n",
            "27     \t [-0.05750687 -0.24363503]. \t  -7.216359370591485 \t -0.8180996773800518\n",
            "28     \t [1.97325415 0.90319927]. \t  -895.2757944749616 \t -0.8180996773800518\n",
            "29     \t [1.27468998 0.27710306]. \t  -181.71346766613016 \t -0.8180996773800518\n",
            "30     \t [0.75854931 0.56201665]. \t  \u001b[92m-0.07620196038707869\u001b[0m \t -0.07620196038707869\n",
            "31     \t [0.69211926 0.40956748]. \t  -0.5772817795001087 \t -0.07620196038707869\n",
            "32     \t [0.9018657  0.56261731]. \t  -6.296907159700716 \t -0.07620196038707869\n",
            "33     \t [ 0.66521893 -0.23347934]. \t  -45.80907842624932 \t -0.07620196038707869\n",
            "34     \t [-0.08322602 -1.64656607]. \t  -274.5771703903309 \t -0.07620196038707869\n",
            "35     \t [ 0.74271154 -1.83825245]. \t  -571.2154314512803 \t -0.07620196038707869\n",
            "36     \t [0.03658015 0.7696289 ]. \t  -59.95525251309522 \t -0.07620196038707869\n",
            "37     \t [0.26039215 0.32972883]. \t  -7.407477600251035 \t -0.07620196038707869\n",
            "38     \t [ 0.37248426 -1.61622801]. \t  -308.38663721223264 \t -0.07620196038707869\n",
            "39     \t [-1.65372101 -1.45526157]. \t  -1762.698113551249 \t -0.07620196038707869\n",
            "40     \t [-0.47088606 -0.46262524]. \t  -48.99821853353074 \t -0.07620196038707869\n",
            "41     \t [-0.3125598   0.45186613]. \t  -14.266629745783442 \t -0.07620196038707869\n",
            "42     \t [0.38598463 0.51471049]. \t  -13.752591818790338 \t -0.07620196038707869\n",
            "43     \t [0.72190023 0.52150295]. \t  -0.07735265867424165 \t -0.07620196038707869\n",
            "44     \t [ 1.73016164 -1.2227373 ]. \t  -1778.1645165098817 \t -0.07620196038707869\n",
            "45     \t [ 1.64398144 -1.05553562]. \t  -1412.8294011552719 \t -0.07620196038707869\n",
            "46     \t [0.27098297 1.72492339]. \t  -273.27392385300084 \t -0.07620196038707869\n",
            "47     \t [-1.96679809  0.3621967 ]. \t  -1238.0742281602288 \t -0.07620196038707869\n",
            "48     \t [0.6870367  0.46579552]. \t  -0.10181973365367795 \t -0.07620196038707869\n",
            "49     \t [1.23422419 1.1855391 ]. \t  -11.463734691460713 \t -0.07620196038707869\n",
            "50     \t [1.16602856 1.44326202]. \t  -0.7271208806517206 \t -0.07620196038707869\n",
            "51     \t [1.01272581 1.19254546]. \t  -2.78678767066262 \t -0.07620196038707869\n",
            "52     \t [-0.08642683  1.66645759]. \t  -276.40443835653934 \t -0.07620196038707869\n",
            "53     \t [ 0.2793997 -1.2676269]. \t  -181.60771680116625 \t -0.07620196038707869\n",
            "54     \t [-1.52130991  1.28473634]. \t  -112.37440040662757 \t -0.07620196038707869\n",
            "55     \t [ 0.89555147 -0.5238285 ]. \t  -175.79632639733111 \t -0.07620196038707869\n",
            "56     \t [0.59567427 0.35122253]. \t  -0.16477912069810416 \t -0.07620196038707869\n",
            "57     \t [-0.2340737   0.17313989]. \t  -2.923595782136139 \t -0.07620196038707869\n",
            "58     \t [-1.13926972  1.3171951 ]. \t  -4.613568196613076 \t -0.07620196038707869\n",
            "59     \t [1.1589209  1.28792886]. \t  -0.329615494928996 \t -0.07620196038707869\n",
            "60     \t [1.54581875 1.85457943]. \t  -28.917870132042722 \t -0.07620196038707869\n",
            "61     \t [1.34128166 1.87657041]. \t  -0.717623902851532 \t -0.07620196038707869\n",
            "62     \t [-1.24554527 -0.53065282]. \t  -438.5297977056196 \t -0.07620196038707869\n",
            "63     \t [1.18809787 1.37499203]. \t  -0.1692234542156798 \t -0.07620196038707869\n",
            "64     \t [1.7002555  0.27668319]. \t  -683.8869845466562 \t -0.07620196038707869\n",
            "65     \t [1.99383884 1.10766416]. \t  -823.3747770666051 \t -0.07620196038707869\n",
            "66     \t [1.37808234 1.81106111]. \t  -0.9182236498656533 \t -0.07620196038707869\n",
            "67     \t [0.35317376 0.11662955]. \t  -0.4249486786488104 \t -0.07620196038707869\n",
            "68     \t [1.03606703 1.05837255]. \t  \u001b[92m-0.023988235066214003\u001b[0m \t -0.023988235066214003\n",
            "69     \t [-1.50630826 -0.16398703]. \t  -598.206928062259 \t -0.023988235066214003\n",
            "70     \t [ 1.68333128 -1.97788986]. \t  -2315.5144414799224 \t -0.023988235066214003\n",
            "71     \t [1.34551784 1.78423272]. \t  -0.1879508149799955 \t -0.023988235066214003\n",
            "72     \t [1.29225532 1.67688582]. \t  -0.09026012074033712 \t -0.023988235066214003\n",
            "73     \t [ 0.74592129 -0.2301352 ]. \t  -61.928093339755414 \t -0.023988235066214003\n",
            "74     \t [-1.87109385  1.76343759]. \t  -310.15278416471114 \t -0.023988235066214003\n",
            "75     \t [-0.93266155 -1.55003675]. \t  -589.324028504928 \t -0.023988235066214003\n",
            "76     \t [-1.68024478  0.26353533]. \t  -662.383562847581 \t -0.023988235066214003\n",
            "77     \t [0.4746603  0.23465048]. \t  -0.28472045823755326 \t -0.023988235066214003\n",
            "78     \t [1.32834068 1.7406204 ]. \t  -0.1647784105535939 \t -0.023988235066214003\n",
            "79     \t [-0.05339432 -1.69392365]. \t  -289.01404418626623 \t -0.023988235066214003\n",
            "80     \t [-0.94283968 -1.63797583]. \t  -642.3083516225248 \t -0.023988235066214003\n",
            "81     \t [ 1.05683909 -0.15691659]. \t  -162.26635865176712 \t -0.023988235066214003\n",
            "82     \t [0.55274964 0.23507388]. \t  -0.6964698420642352 \t -0.023988235066214003\n",
            "83     \t [1.3296437  0.28016115]. \t  -221.46093307441774 \t -0.023988235066214003\n",
            "84     \t [-2.02364883 -1.77656229]. \t  -3456.8483534725187 \t -0.023988235066214003\n",
            "85     \t [-1.27770636  1.26153307]. \t  -18.95208179070812 \t -0.023988235066214003\n",
            "86     \t [0.51038623 0.20598007]. \t  -0.5368996443132265 \t -0.023988235066214003\n",
            "87     \t [ 0.50612368 -1.16357471]. \t  -201.8089132793395 \t -0.023988235066214003\n",
            "88     \t [-0.88460821  0.61773558]. \t  -6.267523692053022 \t -0.023988235066214003\n",
            "89     \t [0.50228244 0.23501356]. \t  -0.2775621902900523 \t -0.023988235066214003\n",
            "90     \t [-0.44738973 -1.32343723]. \t  -234.22904880534765 \t -0.023988235066214003\n",
            "91     \t [-0.63994283  0.08808947]. \t  -13.021610016776167 \t -0.023988235066214003\n",
            "92     \t [1.0383734  1.04010109]. \t  -0.1467723720377292 \t -0.023988235066214003\n",
            "93     \t [1.06284503 1.13140222]. \t  \u001b[92m-0.004260196964201298\u001b[0m \t -0.004260196964201298\n",
            "94     \t [-1.15969297  0.44127987]. \t  -86.31500129402497 \t -0.004260196964201298\n",
            "95     \t [-0.60324118  1.28584672]. \t  -87.56897214586364 \t -0.004260196964201298\n",
            "96     \t [0.40836656 0.16078733]. \t  -0.35360128622997594 \t -0.004260196964201298\n",
            "97     \t [1.4073171 2.0264264]. \t  -0.3764504237923546 \t -0.004260196964201298\n",
            "98     \t [1.35693448 1.64384049]. \t  -4.0252899496503955 \t -0.004260196964201298\n",
            "99     \t [0.88133044 0.79074803]. \t  -0.03369559239437238 \t -0.004260196964201298\n",
            "100    \t [0.90711595 0.79505496]. \t  -0.08593583231557217 \t -0.004260196964201298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1j6UBhkdtdVe",
        "outputId": "7547b14a-5fac-4a7e-d01e-007cf55517eb"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 2\r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_loser_2 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_2 = dGPGO(surrogate_loser_2, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.44173258 -1.74529086]. \t  -1462.4037722675753 \t -1.3013277264983028\n",
            "init   \t [ 1.6176405  -0.26012243]. \t  -828.0271838688934 \t -1.3013277264983028\n",
            "init   \t [-1.525032    0.31071385]. \t  -412.40181251788414 \t -1.3013277264983028\n",
            "init   \t [ 1.39456889 -0.26574622]. \t  -488.8170397507174 \t -1.3013277264983028\n",
            "init   \t [0.80244966 0.75627765]. \t  -1.3013277264983028 \t -1.3013277264983028\n",
            "1      \t [ 0.33906095 -0.26492231]. \t  -14.868074192108132 \t -1.3013277264983028\n",
            "2      \t [ 0.63761197 -0.00466461]. \t  -17.040990776479866 \t -1.3013277264983028\n",
            "3      \t [0.97082692 1.75663077]. \t  -66.28094057138775 \t -1.3013277264983028\n",
            "4      \t [ 0.2824696  -0.71012934]. \t  -62.91195966330248 \t -1.3013277264983028\n",
            "5      \t [0.83023894 1.20141882]. \t  -26.25572497486854 \t -1.3013277264983028\n",
            "6      \t [1.1566147  1.12418388]. \t  -4.5859003313837405 \t -1.3013277264983028\n",
            "7      \t [-0.74554464 -0.41245325]. \t  -96.80549233062297 \t -1.3013277264983028\n",
            "8      \t [0.97465444 0.82884679]. \t  -1.4672722002650613 \t -1.3013277264983028\n",
            "9      \t [1.61837733 2.00595213]. \t  -37.98296228972681 \t -1.3013277264983028\n",
            "10     \t [0.82477171 0.79542092]. \t  -1.3571765467732686 \t -1.3013277264983028\n",
            "11     \t [-0.29464919  0.72430263]. \t  -42.31476276292781 \t -1.3013277264983028\n",
            "12     \t [0.8943991  0.84073397]. \t  \u001b[92m-0.1774868517993569\u001b[0m \t -0.1774868517993569\n",
            "13     \t [ 0.99179266 -0.2276715 ]. \t  -146.7306964172385 \t -0.1774868517993569\n",
            "14     \t [-0.56286493 -1.72824109]. \t  -420.66877543293583 \t -0.1774868517993569\n",
            "15     \t [-0.10093385 -0.19015513]. \t  -5.225777817449663 \t -0.1774868517993569\n",
            "16     \t [-1.06905002  1.88119765]. \t  -58.79404261214023 \t -0.1774868517993569\n",
            "17     \t [-0.34106247 -0.75024631]. \t  -76.89279157666216 \t -0.1774868517993569\n",
            "18     \t [0.17184324 0.13281523]. \t  -1.7526255309303058 \t -0.1774868517993569\n",
            "19     \t [0.79806246 0.64724868]. \t  \u001b[92m-0.051480636842797745\u001b[0m \t -0.051480636842797745\n",
            "20     \t [-0.74325245 -0.41707328]. \t  -97.03146624563828 \t -0.051480636842797745\n",
            "21     \t [ 0.14093629 -0.04595891]. \t  -1.1712433634712784 \t -0.051480636842797745\n",
            "22     \t [0.81522103 0.6769212 ]. \t  \u001b[92m-0.04936063201465491\u001b[0m \t -0.04936063201465491\n",
            "23     \t [0.72451938 0.50584602]. \t  -0.11230301877260293 \t -0.04936063201465491\n",
            "24     \t [ 0.56866235 -0.90474152]. \t  -151.01353122800091 \t -0.04936063201465491\n",
            "25     \t [0.44397812 0.30176866]. \t  -1.4043662848067613 \t -0.04936063201465491\n",
            "26     \t [0.88264132 0.79574464]. \t  \u001b[92m-0.04162511363239053\u001b[0m \t -0.04162511363239053\n",
            "27     \t [0.68156913 0.41441561]. \t  -0.35260827284530105 \t -0.04162511363239053\n",
            "28     \t [0.51158158 0.29053007]. \t  -0.3215792826944577 \t -0.04162511363239053\n",
            "29     \t [-1.69076328  0.3014827 ]. \t  -661.1662444475091 \t -0.04162511363239053\n",
            "30     \t [-0.92530255  0.67130482]. \t  -7.1248506505111795 \t -0.04162511363239053\n",
            "31     \t [-0.43040611  1.99670407]. \t  -330.18285815833576 \t -0.04162511363239053\n",
            "32     \t [-0.6314424   0.18749216]. \t  -7.123303427193068 \t -0.04162511363239053\n",
            "33     \t [-1.58293138  2.02745107]. \t  -29.54103678051261 \t -0.04162511363239053\n",
            "34     \t [0.32747641 0.09066291]. \t  -0.4797706335825145 \t -0.04162511363239053\n",
            "35     \t [1.46932108 1.16443248]. \t  -99.11770622554333 \t -0.04162511363239053\n",
            "36     \t [0.93821415 0.86251887]. \t  \u001b[92m-0.035241894498354374\u001b[0m \t -0.035241894498354374\n",
            "37     \t [-0.98007518  1.8084631 ]. \t  -75.81680806136724 \t -0.035241894498354374\n",
            "38     \t [ 1.32130641 -1.83018897]. \t  -1278.909164298691 \t -0.035241894498354374\n",
            "39     \t [1.16111548 1.37459576]. \t  -0.09568906685019224 \t -0.035241894498354374\n",
            "40     \t [-0.79520927  0.44741328]. \t  -6.643223185924455 \t -0.035241894498354374\n",
            "41     \t [-1.33829421  1.62006885]. \t  -8.39043860228765 \t -0.035241894498354374\n",
            "42     \t [-1.0567372  -0.45304023]. \t  -250.63656771094728 \t -0.035241894498354374\n",
            "43     \t [ 2.02301052 -1.55702126]. \t  -3192.836445995665 \t -0.035241894498354374\n",
            "44     \t [1.0177217  1.05191344]. \t  \u001b[92m-0.026415621071020308\u001b[0m \t -0.026415621071020308\n",
            "45     \t [-1.15938681  1.23036401]. \t  -5.958308524783243 \t -0.026415621071020308\n",
            "46     \t [1.16378936 1.38101067]. \t  -0.09760953665351375 \t -0.026415621071020308\n",
            "47     \t [1.50263101 1.01997912]. \t  -153.4974360773491 \t -0.026415621071020308\n",
            "48     \t [1.84746402 0.69827684]. \t  -737.75732880183 \t -0.026415621071020308\n",
            "49     \t [-0.79787615 -1.07553245]. \t  -296.3742858915344 \t -0.026415621071020308\n",
            "50     \t [1.32469683 1.95092639]. \t  -3.9511331739041573 \t -0.026415621071020308\n",
            "51     \t [1.35307803 1.778157  ]. \t  -0.4020048738638888 \t -0.026415621071020308\n",
            "52     \t [-0.75825752  1.91406987]. \t  -182.4144761163101 \t -0.026415621071020308\n",
            "53     \t [-0.80497207  1.85963067]. \t  -150.06764795856756 \t -0.026415621071020308\n",
            "54     \t [1.26328001 1.56576959]. \t  -0.1599583044778113 \t -0.026415621071020308\n",
            "55     \t [-1.90268391  0.96809939]. \t  -711.7925471201771 \t -0.026415621071020308\n",
            "56     \t [1.25385    1.52764835]. \t  -0.26238898360948615 \t -0.026415621071020308\n",
            "57     \t [0.3879084  1.76656536]. \t  -261.55013351865534 \t -0.026415621071020308\n",
            "58     \t [-0.18045394 -1.23520567]. \t  -162.11737031666118 \t -0.026415621071020308\n",
            "59     \t [1.54251158 0.31288154]. \t  -427.32018454078906 \t -0.026415621071020308\n",
            "60     \t [1.85648927 1.71578724]. \t  -300.2883765100222 \t -0.026415621071020308\n",
            "61     \t [-1.24963833 -1.51014022]. \t  -948.6171836162908 \t -0.026415621071020308\n",
            "62     \t [-1.69061122 -0.69050267]. \t  -1266.5445430312402 \t -0.026415621071020308\n",
            "63     \t [ 0.2998765  -1.89503628]. \t  -394.4976661783674 \t -0.026415621071020308\n",
            "64     \t [1.19907741 1.45971508]. \t  -0.08771754024096773 \t -0.026415621071020308\n",
            "65     \t [ 0.12167434 -1.732526  ]. \t  -306.08789281014197 \t -0.026415621071020308\n",
            "66     \t [0.80727433 0.58756804]. \t  -0.4483292996493487 \t -0.026415621071020308\n",
            "67     \t [ 0.32900738 -0.38466485]. \t  -24.746327383062937 \t -0.026415621071020308\n",
            "68     \t [ 0.88967876 -1.29655175]. \t  -436.0199975541458 \t -0.026415621071020308\n",
            "69     \t [1.1981682  1.44214661]. \t  -0.04354724987751669 \t -0.026415621071020308\n",
            "70     \t [0.79004918 0.58112711]. \t  -0.22941477244356115 \t -0.026415621071020308\n",
            "71     \t [-1.84477672  1.67970458]. \t  -305.1367924344605 \t -0.026415621071020308\n",
            "72     \t [1.24686003 1.56358902]. \t  -0.06891275469397362 \t -0.026415621071020308\n",
            "73     \t [-0.84108376  1.7411032 ]. \t  -110.23929448243285 \t -0.026415621071020308\n",
            "74     \t [0.75483372 0.56410819]. \t  -0.06331658876375948 \t -0.026415621071020308\n",
            "75     \t [-1.57882819 -0.776232  ]. \t  -1075.2409896985714 \t -0.026415621071020308\n",
            "76     \t [ 1.34076278 -1.96365453]. \t  -1414.853401573814 \t -0.026415621071020308\n",
            "77     \t [0.75337646 0.53567887]. \t  -0.16256640192346394 \t -0.026415621071020308\n",
            "78     \t [1.28609931 1.66618478]. \t  -0.09657462710522058 \t -0.026415621071020308\n",
            "79     \t [-2.0382743   0.10376635]. \t  -1650.1257476316432 \t -0.026415621071020308\n",
            "80     \t [1.21830229 1.46689116]. \t  -0.07782520260960916 \t -0.026415621071020308\n",
            "81     \t [-0.19118443  0.05462748]. \t  -1.4515944904185003 \t -0.026415621071020308\n",
            "82     \t [0.81707117 0.64759812]. \t  -0.07349168672564534 \t -0.026415621071020308\n",
            "83     \t [-0.35186844  1.22744145]. \t  -123.62747676309935 \t -0.026415621071020308\n",
            "84     \t [0.02188484 1.06933684]. \t  -115.20242968324177 \t -0.026415621071020308\n",
            "85     \t [-1.41357471 -1.37068528]. \t  -1140.7597357766265 \t -0.026415621071020308\n",
            "86     \t [0.0417646  0.31435926]. \t  -10.69102757382465 \t -0.026415621071020308\n",
            "87     \t [1.29509121 1.6760604 ]. \t  -0.08722302883666529 \t -0.026415621071020308\n",
            "88     \t [ 0.78679977 -0.75523225]. \t  -188.91169055958073 \t -0.026415621071020308\n",
            "89     \t [ 2.04698854 -0.95916468]. \t  -2652.652812167113 \t -0.026415621071020308\n",
            "90     \t [-0.86567468 -1.04638044]. \t  -325.96084184091194 \t -0.026415621071020308\n",
            "91     \t [-0.31919473 -0.90157516]. \t  -102.43355936094454 \t -0.026415621071020308\n",
            "92     \t [1.78776824 1.57337201]. \t  -263.9501550654721 \t -0.026415621071020308\n",
            "93     \t [1.28798869 1.65471811]. \t  -0.08469875407276624 \t -0.026415621071020308\n",
            "94     \t [0.82881531 0.64333576]. \t  -0.21939204741303808 \t -0.026415621071020308\n",
            "95     \t [-0.20726081  0.57599578]. \t  -29.87050810289775 \t -0.026415621071020308\n",
            "96     \t [1.37586746 1.95449868]. \t  -0.5193465183474737 \t -0.026415621071020308\n",
            "97     \t [0.41936547 0.26065522]. \t  -1.056033835945708 \t -0.026415621071020308\n",
            "98     \t [-1.70982569  0.65088814]. \t  -523.821384062963 \t -0.026415621071020308\n",
            "99     \t [ 0.23906176 -1.33358848]. \t  -193.99452545400055 \t -0.026415621071020308\n",
            "100    \t [0.65717454 1.15221083]. \t  -52.00541436633597 \t -0.026415621071020308\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yw0oJ-TpiN_",
        "outputId": "4ff37027-68a7-4f58-9bd9-726b9336a03e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 3\r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_loser_3 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_3 = dGPGO(surrogate_loser_3, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.03045369 -1.60050678]. \t  -708.8071948417274 \t -1.118465165857483\n",
            "init   \t [-0.05684724 -0.0006915 ]. \t  -1.118465165857483 \t -1.118465165857483\n",
            "init   \t [ 0.64352256 -1.08181622]. \t  -223.90997740226942 \t -1.118465165857483\n",
            "init   \t [ 0.46200787 -1.55790376]. \t  -314.05929962166397 \t -1.118465165857483\n",
            "init   \t [ 0.86918061 -0.52199202]. \t  -163.20929387450548 \t -1.118465165857483\n",
            "1      \t [-0.47032198 -0.98469614]. \t  -147.5810642040628 \t -1.118465165857483\n",
            "2      \t [ 0.19510124 -0.39571949]. \t  -19.464716969441064 \t -1.118465165857483\n",
            "3      \t [-0.48374833  1.21060744]. \t  -97.57528626382148 \t -1.118465165857483\n",
            "4      \t [-0.04142004  0.27493655]. \t  -8.549523360099188 \t -1.118465165857483\n",
            "5      \t [-0.75508265  0.36868271]. \t  -7.139214187351264 \t -1.118465165857483\n",
            "6      \t [-0.37659969  0.1941843 ]. \t  -2.169151970106639 \t -1.118465165857483\n",
            "7      \t [-2.00120028  0.75454632]. \t  -1065.4237754889868 \t -1.118465165857483\n",
            "8      \t [0.29432639 0.0138672 ]. \t  \u001b[92m-1.0273890250980249\u001b[0m \t -1.0273890250980249\n",
            "9      \t [-1.93971307  0.59809255]. \t  -1009.9810140546972 \t -1.0273890250980249\n",
            "10     \t [-0.49691934  0.33305786]. \t  -2.9825885043834797 \t -1.0273890250980249\n",
            "11     \t [0.11487674 0.05912955]. \t  \u001b[92m-0.9944261838126743\u001b[0m \t -0.9944261838126743\n",
            "12     \t [1.20192189 0.99338095]. \t  -20.402100015337346 \t -0.9944261838126743\n",
            "13     \t [0.74861108 0.64386018]. \t  \u001b[92m-0.7594469688952155\u001b[0m \t -0.7594469688952155\n",
            "14     \t [2.048 2.048]. \t  -461.7603900415353 \t -0.7594469688952155\n",
            "15     \t [1.76285049 0.63025447]. \t  -614.3267727264987 \t -0.7594469688952155\n",
            "16     \t [0.96375751 1.56754617]. \t  -40.79733469171215 \t -0.7594469688952155\n",
            "17     \t [1.03601665 1.15966006]. \t  \u001b[92m-0.7465766926129433\u001b[0m \t -0.7465766926129433\n",
            "18     \t [0.51055878 0.3178585 ]. \t  \u001b[92m-0.5666020698356665\u001b[0m \t -0.5666020698356665\n",
            "19     \t [0.95991252 0.86549378]. \t  \u001b[92m-0.3145160316585125\u001b[0m \t -0.3145160316585125\n",
            "20     \t [0.93855127 0.88859853]. \t  \u001b[92m-0.009735844765363858\u001b[0m \t -0.009735844765363858\n",
            "21     \t [-1.54677484 -1.42960587]. \t  -1467.344875545267 \t -0.009735844765363858\n",
            "22     \t [1.13269135 1.26759145]. \t  -0.04131755720586214 \t -0.009735844765363858\n",
            "23     \t [0.38547819 0.21072274]. \t  -0.7636421454044093 \t -0.009735844765363858\n",
            "24     \t [-0.51379233  0.32424217]. \t  -2.6546893059535472 \t -0.009735844765363858\n",
            "25     \t [-0.53104766 -1.92376744]. \t  -488.89022912611455 \t -0.009735844765363858\n",
            "26     \t [0.98391655 0.95875632]. \t  \u001b[92m-0.008973762773523547\u001b[0m \t -0.008973762773523547\n",
            "27     \t [1.04668274 1.04339526]. \t  -0.2741362116411211 \t -0.008973762773523547\n",
            "28     \t [1.08242906 1.20393354]. \t  -0.11100004184662911 \t -0.008973762773523547\n",
            "29     \t [-0.33250964 -0.39777081]. \t  -27.615873630731052 \t -0.008973762773523547\n",
            "30     \t [1.07504385 1.15794208]. \t  \u001b[92m-0.006125661922954601\u001b[0m \t -0.006125661922954601\n",
            "31     \t [1.06176258 1.1392823 ]. \t  -0.018077009257239162 \t -0.006125661922954601\n",
            "32     \t [ 0.52575715 -0.44393616]. \t  -52.11629023706623 \t -0.006125661922954601\n",
            "33     \t [-0.28444708 -0.11423451]. \t  -5.457947591478899 \t -0.006125661922954601\n",
            "34     \t [-1.28321751  0.98886312]. \t  -48.48106860740266 \t -0.006125661922954601\n",
            "35     \t [-1.24396408  1.61208391]. \t  -5.4531725633735615 \t -0.006125661922954601\n",
            "36     \t [-1.1282022  1.2002165]. \t  -5.056665030592668 \t -0.006125661922954601\n",
            "37     \t [0.51069402 0.31521316]. \t  -0.5354083256716863 \t -0.006125661922954601\n",
            "38     \t [-1.31746446  1.97848047]. \t  -11.264265296218156 \t -0.006125661922954601\n",
            "39     \t [ 1.7242307  -0.10450261]. \t  -947.6092010408778 \t -0.006125661922954601\n",
            "40     \t [-0.84831635  0.64795332]. \t  -3.9301804066432773 \t -0.006125661922954601\n",
            "41     \t [ 0.69436079 -1.97932458]. \t  -605.9726815722216 \t -0.006125661922954601\n",
            "42     \t [1.04146909 1.03730469]. \t  -0.22595209065942243 \t -0.006125661922954601\n",
            "43     \t [1.03419323 1.05570699]. \t  -0.02034766142365846 \t -0.006125661922954601\n",
            "44     \t [-1.42549166  1.8991367 ]. \t  -7.648979004438684 \t -0.006125661922954601\n",
            "45     \t [-0.25567451 -0.60585552]. \t  -46.63101579411829 \t -0.006125661922954601\n",
            "46     \t [1.62153724 1.77767364]. \t  -72.9271970608067 \t -0.006125661922954601\n",
            "47     \t [1.14907196 1.24566204]. \t  -0.5802961480496691 \t -0.006125661922954601\n",
            "48     \t [-0.14167054 -1.84125797]. \t  -347.75779274568123 \t -0.006125661922954601\n",
            "49     \t [1.1378145  1.25246812]. \t  -0.19668640197016854 \t -0.006125661922954601\n",
            "50     \t [0.08799344 0.14240424]. \t  -2.6451251959418776 \t -0.006125661922954601\n",
            "51     \t [-1.86423292  1.96849273]. \t  -235.27004747386428 \t -0.006125661922954601\n",
            "52     \t [-1.14761215 -0.61437996]. \t  -377.64036514287966 \t -0.006125661922954601\n",
            "53     \t [1.08165771 1.16910576]. \t  -0.006745005607482398 \t -0.006125661922954601\n",
            "54     \t [0.8669972  0.78603518]. \t  -0.13568908083082817 \t -0.006125661922954601\n",
            "55     \t [-1.17155945  1.30088295]. \t  -5.229309276185716 \t -0.006125661922954601\n",
            "56     \t [1.45691194 0.1092011 ]. \t  -405.5832286366827 \t -0.006125661922954601\n",
            "57     \t [0.67907159 0.23444503]. \t  -5.241975604509687 \t -0.006125661922954601\n",
            "58     \t [1.35079852 1.737578  ]. \t  -0.881328652925095 \t -0.006125661922954601\n",
            "59     \t [ 0.82191645 -0.39087832]. \t  -113.7579362329471 \t -0.006125661922954601\n",
            "60     \t [1.12075149 1.24792408]. \t  -0.021239188950571825 \t -0.006125661922954601\n",
            "61     \t [1.28769073 1.63789832]. \t  -0.12376855714934631 \t -0.006125661922954601\n",
            "62     \t [1.26599189 1.58100526]. \t  -0.11797189318565926 \t -0.006125661922954601\n",
            "63     \t [-1.89418234 -1.62116101]. \t  -2721.8358303256878 \t -0.006125661922954601\n",
            "64     \t [1.26056682 1.57394954]. \t  -0.09063321711292399 \t -0.006125661922954601\n",
            "65     \t [-0.33670335  1.87999363]. \t  -313.88298237180817 \t -0.006125661922954601\n",
            "66     \t [0.9427676  0.89343169]. \t  \u001b[92m-0.0054108541640425275\u001b[0m \t -0.0054108541640425275\n",
            "67     \t [0.20204697 0.38969872]. \t  -12.80815708669491 \t -0.0054108541640425275\n",
            "68     \t [1.50003658 0.3734927 ]. \t  -352.4191905238935 \t -0.0054108541640425275\n",
            "69     \t [-0.29527149  1.70664909]. \t  -263.9440407563509 \t -0.0054108541640425275\n",
            "70     \t [1.05446509 1.13105095]. \t  -0.03965521490137981 \t -0.0054108541640425275\n",
            "71     \t [-2.00348166  0.78190776]. \t  -1053.623340234436 \t -0.0054108541640425275\n",
            "72     \t [0.99356435 0.96510804]. \t  -0.04871492118397859 \t -0.0054108541640425275\n",
            "73     \t [0.17664647 0.85788352]. \t  -69.01781709494013 \t -0.0054108541640425275\n",
            "74     \t [0.99287609 0.96134527]. \t  -0.05986847761864009 \t -0.0054108541640425275\n",
            "75     \t [0.83096098 0.66821642]. \t  -0.07821283108443079 \t -0.0054108541640425275\n",
            "76     \t [0.88918186 0.77418265]. \t  -0.03937953542494712 \t -0.0054108541640425275\n",
            "77     \t [-0.76123896 -0.88227645]. \t  -216.7765445007591 \t -0.0054108541640425275\n",
            "78     \t [0.34422813 0.4797669 ]. \t  -13.481919620506176 \t -0.0054108541640425275\n",
            "79     \t [-0.3496611   1.99775184]. \t  -353.5674665544358 \t -0.0054108541640425275\n",
            "80     \t [0.92627648 0.83861158]. \t  -0.04298017453543089 \t -0.0054108541640425275\n",
            "81     \t [ 0.39394077 -1.05235458]. \t  -146.1835367236264 \t -0.0054108541640425275\n",
            "82     \t [1.31075222 1.7167406 ]. \t  -0.09674403923068131 \t -0.0054108541640425275\n",
            "83     \t [0.30981187 0.87976317]. \t  -61.907432340113104 \t -0.0054108541640425275\n",
            "84     \t [0.98621654 0.99359697]. \t  -0.04418048304730919 \t -0.0054108541640425275\n",
            "85     \t [ 1.51087379 -1.52307762]. \t  -1448.6854653269693 \t -0.0054108541640425275\n",
            "86     \t [ 0.04136922 -0.11238201]. \t  -2.2207039332235077 \t -0.0054108541640425275\n",
            "87     \t [ 2.00030005 -1.1263103 ]. \t  -2630.137104728218 \t -0.0054108541640425275\n",
            "88     \t [1.42267845 0.33450296]. \t  -285.62340308224816 \t -0.0054108541640425275\n",
            "89     \t [0.99560463 0.98250413]. \t  -0.007630923643092643 \t -0.0054108541640425275\n",
            "90     \t [ 0.85116426 -0.33166719]. \t  -111.56696821035466 \t -0.0054108541640425275\n",
            "91     \t [-0.55705471  1.56959819]. \t  -161.0051040713034 \t -0.0054108541640425275\n",
            "92     \t [1.34185465 0.11229551]. \t  -285.14525980682635 \t -0.0054108541640425275\n",
            "93     \t [ 0.34866918 -0.0873866 ]. \t  -4.7905262893301686 \t -0.0054108541640425275\n",
            "94     \t [0.88414022 0.7804986 ]. \t  -0.013568770554377583 \t -0.0054108541640425275\n",
            "95     \t [-1.0219889  -0.74463384]. \t  -324.17458727864135 \t -0.0054108541640425275\n",
            "96     \t [-1.97904207 -1.1990717 ]. \t  -2625.8920644414125 \t -0.0054108541640425275\n",
            "97     \t [0.88678747 0.76523517]. \t  -0.05757829370212176 \t -0.0054108541640425275\n",
            "98     \t [-0.84088724  1.7432666 ]. \t  -110.75478073212808 \t -0.0054108541640425275\n",
            "99     \t [ 0.25338118 -0.64784029]. \t  -51.25786484737453 \t -0.0054108541640425275\n",
            "100    \t [-1.33438605  0.77689646]. \t  -106.18865265597901 \t -0.0054108541640425275\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zciLjgNltda2",
        "outputId": "f4bad395-cb07-4279-99ec-cd5dcf1adac7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 4\r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_loser_4 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_4 = dGPGO(surrogate_loser_4, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.39002285 1.31602418]. \t  -38.11488737154776 \t -12.122423820878506\n",
            "init   \t [0.58248055 0.68494384]. \t  -12.122423820878506 \t -12.122423820878506\n",
            "init   \t [-1.88768295 -1.7790059 ]. \t  -2862.4120613667637 \t -12.122423820878506\n",
            "init   \t [-0.91545338 -0.75218738]. \t  -256.5560079635026 \t -12.122423820878506\n",
            "init   \t [-1.25504548 -1.24070756]. \t  -797.98450090518 \t -12.122423820878506\n",
            "1      \t [-0.07724009 -1.60241953]. \t  -259.85085705484 \t -12.122423820878506\n",
            "2      \t [0.50582555 1.31540574]. \t  -112.5080347477725 \t -12.122423820878506\n",
            "3      \t [1.38124473 0.39077049]. \t  -230.29442671843938 \t -12.122423820878506\n",
            "4      \t [1.97887267 1.83898365]. \t  -432.3317389383738 \t -12.122423820878506\n",
            "5      \t [0.84233915 0.9518293 ]. \t  \u001b[92m-5.89549813715424\u001b[0m \t -5.89549813715424\n",
            "6      \t [1.0556916  1.08009465]. \t  \u001b[92m-0.12136948890890542\u001b[0m \t -0.12136948890890542\n",
            "7      \t [-0.18308617 -1.34190572]. \t  -190.579434904392 \t -0.12136948890890542\n",
            "8      \t [1.05222041 1.13392694]. \t  \u001b[92m-0.0743321857598434\u001b[0m \t -0.0743321857598434\n",
            "9      \t [-0.13107399 -0.84726098]. \t  -76.00521578930851 \t -0.0743321857598434\n",
            "10     \t [ 0.37177912 -0.23395445]. \t  -14.2460219313725 \t -0.0743321857598434\n",
            "11     \t [1.057716   1.11667387]. \t  \u001b[92m-0.003767644645393139\u001b[0m \t -0.003767644645393139\n",
            "12     \t [-1.78443008  1.4136824 ]. \t  -321.22301968113425 \t -0.003767644645393139\n",
            "13     \t [ 1.48633283 -0.77964418]. \t  -893.5466803786309 \t -0.003767644645393139\n",
            "14     \t [0.25372219 0.06764368]. \t  -0.5579990295176024 \t -0.003767644645393139\n",
            "15     \t [0.3352143  0.15908183]. \t  -0.6601523487531911 \t -0.003767644645393139\n",
            "16     \t [0.29189838 0.01864103]. \t  -0.9444796585289186 \t -0.003767644645393139\n",
            "17     \t [-0.25533525 -0.95263609]. \t  -105.17410174748937 \t -0.003767644645393139\n",
            "18     \t [-1.22702171  0.03516077]. \t  -221.173568600708 \t -0.003767644645393139\n",
            "19     \t [-0.23860944 -1.69672867]. \t  -309.06759363010303 \t -0.003767644645393139\n",
            "20     \t [1.43289088 1.93136973]. \t  -1.6710779543216412 \t -0.003767644645393139\n",
            "21     \t [1.07946734 2.048     ]. \t  -77.93111768811308 \t -0.003767644645393139\n",
            "22     \t [-0.84972588  0.45175101]. \t  -10.726779538532615 \t -0.003767644645393139\n",
            "23     \t [-0.67310233  0.83762191]. \t  -17.587539091298325 \t -0.003767644645393139\n",
            "24     \t [1.28079797 1.82400176]. \t  -3.448213660017752 \t -0.003767644645393139\n",
            "25     \t [-0.81246147  0.63327293]. \t  -3.356951636991144 \t -0.003767644645393139\n",
            "26     \t [-0.71350369  0.59260605]. \t  -3.633629427727572 \t -0.003767644645393139\n",
            "27     \t [-0.75099975 -0.36212237]. \t  -88.83637904884068 \t -0.003767644645393139\n",
            "28     \t [-0.49949511  0.06754364]. \t  -5.559128586272248 \t -0.003767644645393139\n",
            "29     \t [-0.60496585  0.88407464]. \t  -29.41773966484951 \t -0.003767644645393139\n",
            "30     \t [-1.39511098 -1.07364137]. \t  -917.7620618067973 \t -0.003767644645393139\n",
            "31     \t [1.44351854 2.00896715]. \t  -0.7558928709733288 \t -0.003767644645393139\n",
            "32     \t [0.59607989 0.36541864]. \t  -0.17336742519789472 \t -0.003767644645393139\n",
            "33     \t [1.08616483 1.14274229]. \t  -0.14441133113668123 \t -0.003767644645393139\n",
            "34     \t [-0.96255788 -0.02208449]. \t  -93.83624093464555 \t -0.003767644645393139\n",
            "35     \t [1.37553468 1.96569251]. \t  -0.6826760286825106 \t -0.003767644645393139\n",
            "36     \t [1.22974468 1.44907648]. \t  -0.4521496094356799 \t -0.003767644645393139\n",
            "37     \t [2.04206484 1.39503674]. \t  -771.1439943589678 \t -0.003767644645393139\n",
            "38     \t [0.97817863 0.96346351]. \t  -0.0048719716429606475 \t -0.003767644645393139\n",
            "39     \t [ 1.35705638 -0.0890798 ]. \t  -372.880721250297 \t -0.003767644645393139\n",
            "40     \t [1.24019056 1.50517118]. \t  -0.165942040598321 \t -0.003767644645393139\n",
            "41     \t [1.32821362 1.71954028]. \t  -0.3067397043779968 \t -0.003767644645393139\n",
            "42     \t [-0.48263409 -0.17790873]. \t  -19.077516014712117 \t -0.003767644645393139\n",
            "43     \t [0.42976678 0.13721056]. \t  -0.550685701238865 \t -0.003767644645393139\n",
            "44     \t [ 1.9474966  -0.04153141]. \t  -1471.0637656833974 \t -0.003767644645393139\n",
            "45     \t [-0.63915313 -1.52523522]. \t  -376.62647934729335 \t -0.003767644645393139\n",
            "46     \t [0.59967352 0.26573875]. \t  -1.0414111735529614 \t -0.003767644645393139\n",
            "47     \t [ 0.16957906 -1.30817191]. \t  -179.42750437290832 \t -0.003767644645393139\n",
            "48     \t [-0.02410094 -0.51242169]. \t  -27.365943875563282 \t -0.003767644645393139\n",
            "49     \t [1.39700274 1.91958205]. \t  -0.2602327845162954 \t -0.003767644645393139\n",
            "50     \t [ 0.50426436 -1.43702746]. \t  -286.2987090020451 \t -0.003767644645393139\n",
            "51     \t [ 1.1489613  -0.40819872]. \t  -298.7280091007735 \t -0.003767644645393139\n",
            "52     \t [ 1.30973476 -1.95863185]. \t  -1349.9507199683198 \t -0.003767644645393139\n",
            "53     \t [-0.61209944  1.81939613]. \t  -211.32345951362092 \t -0.003767644645393139\n",
            "54     \t [0.87875814 0.75969817]. \t  -0.030368838694919645 \t -0.003767644645393139\n",
            "55     \t [1.08383245 1.1236438 ]. \t  -0.26762777491811934 \t -0.003767644645393139\n",
            "56     \t [1.41162934 1.99620681]. \t  -0.17067032175005348 \t -0.003767644645393139\n",
            "57     \t [1.9173578  0.61140306]. \t  -940.1769340714045 \t -0.003767644645393139\n",
            "58     \t [ 1.94491488 -1.47738315]. \t  -2767.733922681714 \t -0.003767644645393139\n",
            "59     \t [0.61405767 2.03367284]. \t  -274.5833037339654 \t -0.003767644645393139\n",
            "60     \t [-0.11891331  0.13662719]. \t  -2.752269057151917 \t -0.003767644645393139\n",
            "61     \t [-0.10786281  1.00686775]. \t  -100.27630475842861 \t -0.003767644645393139\n",
            "62     \t [1.97698163 1.70213882]. \t  -487.7381985083474 \t -0.003767644645393139\n",
            "63     \t [0.53734859 1.65356498]. \t  -186.48780951709446 \t -0.003767644645393139\n",
            "64     \t [-1.14297065  1.86649181]. \t  -35.964633455673976 \t -0.003767644645393139\n",
            "65     \t [-0.96246214 -0.02709468]. \t  -94.75376211511318 \t -0.003767644645393139\n",
            "66     \t [1.25126118 0.0571559 ]. \t  -227.6199426633258 \t -0.003767644645393139\n",
            "67     \t [-0.6937863  -1.40106436]. \t  -357.2133150989895 \t -0.003767644645393139\n",
            "68     \t [0.95549784 0.86604783]. \t  -0.2222068591982532 \t -0.003767644645393139\n",
            "69     \t [1.28225655 1.6353635 ]. \t  -0.08744509852916786 \t -0.003767644645393139\n",
            "70     \t [-1.80768649 -1.70179769]. \t  -2477.50407993395 \t -0.003767644645393139\n",
            "71     \t [ 0.30088331 -1.44707546]. \t  -236.9120555068589 \t -0.003767644645393139\n",
            "72     \t [0.80834368 0.6273686 ]. \t  -0.10459707926781348 \t -0.003767644645393139\n",
            "73     \t [0.61443815 1.81916878]. \t  -207.97967276656087 \t -0.003767644645393139\n",
            "74     \t [1.33216717 1.71759687]. \t  -0.4360620630523842 \t -0.003767644645393139\n",
            "75     \t [ 1.11552486 -0.4361074 ]. \t  -282.4224218613643 \t -0.003767644645393139\n",
            "76     \t [0.98267101 1.95397144]. \t  -97.67974536163663 \t -0.003767644645393139\n",
            "77     \t [-0.19775693  1.97289223]. \t  -375.38684111000026 \t -0.003767644645393139\n",
            "78     \t [-1.23013135  0.9694735 ]. \t  -34.53985251506643 \t -0.003767644645393139\n",
            "79     \t [ 1.6256887 -0.6896703]. \t  -1110.969801925948 \t -0.003767644645393139\n",
            "80     \t [0.93273914 0.86062769]. \t  -0.013312362338648295 \t -0.003767644645393139\n",
            "81     \t [2.04120098 0.77758227]. \t  -1149.5614262260044 \t -0.003767644645393139\n",
            "82     \t [ 0.3824406  -0.99422429]. \t  -130.45200646480208 \t -0.003767644645393139\n",
            "83     \t [-0.82565706 -0.22225113]. \t  -85.04752133209 \t -0.003767644645393139\n",
            "84     \t [1.24169556 0.05195789]. \t  -222.02371025462764 \t -0.003767644645393139\n",
            "85     \t [0.33993996 1.91512803]. \t  -324.2804853053065 \t -0.003767644645393139\n",
            "86     \t [0.98453716 0.98003788]. \t  -0.011740524325372582 \t -0.003767644645393139\n",
            "87     \t [ 0.30211268 -0.94085493]. \t  -107.01566089528842 \t -0.003767644645393139\n",
            "88     \t [1.80504874 0.0468575 ]. \t  -1031.9207747589403 \t -0.003767644645393139\n",
            "89     \t [ 1.9476981  -1.06044231]. \t  -2357.0008076212116 \t -0.003767644645393139\n",
            "90     \t [1.17513285 0.41271901]. \t  -93.77532296883037 \t -0.003767644645393139\n",
            "91     \t [-1.98501009 -0.04967342]. \t  -1600.8712085231891 \t -0.003767644645393139\n",
            "92     \t [-0.17324745 -0.76910253]. \t  -65.23534049934628 \t -0.003767644645393139\n",
            "93     \t [0.58963501 0.34054937]. \t  -0.17346897888556995 \t -0.003767644645393139\n",
            "94     \t [0.67907186 0.43314691]. \t  -0.18134828203992925 \t -0.003767644645393139\n",
            "95     \t [-0.83907006  0.07181086]. \t  -43.35336521664036 \t -0.003767644645393139\n",
            "96     \t [0.76718452 0.61299269]. \t  -0.11383958913844518 \t -0.003767644645393139\n",
            "97     \t [0.67066997 0.44211967]. \t  -0.11435426825749294 \t -0.003767644645393139\n",
            "98     \t [-0.63542945  1.47879097]. \t  -118.24151322059333 \t -0.003767644645393139\n",
            "99     \t [0.76584676 0.57666316]. \t  -0.06454594618863316 \t -0.003767644645393139\n",
            "100    \t [0.75534477 0.5600731 ]. \t  -0.0708237557011021 \t -0.003767644645393139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5Hnw4war1af",
        "outputId": "2003629c-1a5a-449b-9330-ddccb3b900d5"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 5\r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_loser_5 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_5 = dGPGO(surrogate_loser_5, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.23491901 0.17105153]. \t  -1.9278091788796494 \t -1.9278091788796494\n",
            "init   \t [ 0.24749582 -0.48313101]. \t  -30.201785940713883 \t -1.9278091788796494\n",
            "init   \t [-1.56019024  1.41721238]. \t  -109.97965348801837 \t -1.9278091788796494\n",
            "init   \t [0.98198164 1.77093658]. \t  -65.06852657321838 \t -1.9278091788796494\n",
            "init   \t [1.47180752 0.85239655]. \t  -172.83511538815276 \t -1.9278091788796494\n",
            "1      \t [-0.22001069  0.22846888]. \t  -4.730736857945067 \t -1.9278091788796494\n",
            "2      \t [-0.04631006  0.34471263]. \t  -12.830048576914852 \t -1.9278091788796494\n",
            "3      \t [-1.24874396 -0.40718067]. \t  -391.7856478877513 \t -1.9278091788796494\n",
            "4      \t [ 0.02295425 -0.00498953]. \t  \u001b[92m-0.9576614962030955\u001b[0m \t -0.9576614962030955\n",
            "5      \t [ 0.9852452  -0.41754642]. \t  -192.72527827698983 \t -0.9576614962030955\n",
            "6      \t [ 0.17833193 -1.94470778]. \t  -391.33433945839664 \t -0.9576614962030955\n",
            "7      \t [-1.02601459  0.70105493]. \t  -16.470578116788527 \t -0.9576614962030955\n",
            "8      \t [0.1645955  0.02668441]. \t  \u001b[92m-0.6979172642698882\u001b[0m \t -0.6979172642698882\n",
            "9      \t [ 0.43346255 -0.75667882]. \t  -89.54194982904822 \t -0.6979172642698882\n",
            "10     \t [0.05775992 0.01737802]. \t  -0.9075335979803729 \t -0.6979172642698882\n",
            "11     \t [-1.29511157  1.01406091]. \t  -49.258002513003866 \t -0.6979172642698882\n",
            "12     \t [-0.10514936 -0.21837704]. \t  -6.485324879184702 \t -0.6979172642698882\n",
            "13     \t [-0.7467318   0.65803919]. \t  -4.059706617026702 \t -0.6979172642698882\n",
            "14     \t [-0.60173605  1.09331657]. \t  -56.0353319781476 \t -0.6979172642698882\n",
            "15     \t [-0.5033492  -1.35122097]. \t  -259.7282014742476 \t -0.6979172642698882\n",
            "16     \t [1.80992487 0.13819377]. \t  -985.1308628648286 \t -0.6979172642698882\n",
            "17     \t [1.10914658 1.20798517]. \t  \u001b[92m-0.06129011369415375\u001b[0m \t -0.06129011369415375\n",
            "18     \t [2.048 2.048]. \t  -461.7603900415999 \t -0.06129011369415375\n",
            "19     \t [0.87491994 0.83113376]. \t  -0.446622240028497 \t -0.06129011369415375\n",
            "20     \t [1.08286286 1.1766518 ]. \t  \u001b[92m-0.008514483059359458\u001b[0m \t -0.008514483059359458\n",
            "21     \t [1.04133418 1.11839936]. \t  -0.1174615126135806 \t -0.008514483059359458\n",
            "22     \t [0.49963123 0.23086254]. \t  -0.2855958144241081 \t -0.008514483059359458\n",
            "23     \t [0.93196297 0.90382006]. \t  -0.1289916174950372 \t -0.008514483059359458\n",
            "24     \t [1.03662994 0.97811476]. \t  -0.9323134917241241 \t -0.008514483059359458\n",
            "25     \t [-0.81535936  0.58043406]. \t  -4.007474413946899 \t -0.008514483059359458\n",
            "26     \t [-0.78857247 -1.59982509]. \t  -496.78147157734134 \t -0.008514483059359458\n",
            "27     \t [0.99036749 0.33968709]. \t  -41.10622914377682 \t -0.008514483059359458\n",
            "28     \t [0.63718644 0.30046434]. \t  -1.245549493064055 \t -0.008514483059359458\n",
            "29     \t [ 1.47354817 -0.45096802]. \t  -687.8763878924361 \t -0.008514483059359458\n",
            "30     \t [-0.11339345 -0.71322107]. \t  -53.95873790462344 \t -0.008514483059359458\n",
            "31     \t [ 0.25685491 -1.73171831]. \t  -323.72218821205206 \t -0.008514483059359458\n",
            "32     \t [-1.94162085  1.78946102]. \t  -400.8636249015539 \t -0.008514483059359458\n",
            "33     \t [-1.23235411  1.84002276]. \t  -15.308451918145744 \t -0.008514483059359458\n",
            "34     \t [-1.12852027  1.37561898]. \t  -5.572243123919866 \t -0.008514483059359458\n",
            "35     \t [0.80808532 0.58624571]. \t  -0.4824698690405001 \t -0.008514483059359458\n",
            "36     \t [-1.40731081 -1.74373723]. \t  -1392.8070946925925 \t -0.008514483059359458\n",
            "37     \t [1.06391531 1.16313568]. \t  -0.10155338780814412 \t -0.008514483059359458\n",
            "38     \t [1.36081877 0.32800021]. \t  -232.33521879013418 \t -0.008514483059359458\n",
            "39     \t [1.62807101 0.40241991]. \t  -505.8326830375496 \t -0.008514483059359458\n",
            "40     \t [0.72551944 0.50675545]. \t  -0.11384581870317573 \t -0.008514483059359458\n",
            "41     \t [ 1.45800013 -0.78186987]. \t  -845.6434516684446 \t -0.008514483059359458\n",
            "42     \t [-0.95981259  1.55133123]. \t  -43.54233524581199 \t -0.008514483059359458\n",
            "43     \t [0.70930071 0.50522015]. \t  -0.08495240605275833 \t -0.008514483059359458\n",
            "44     \t [0.7620922  0.60467104]. \t  -0.11365667830682971 \t -0.008514483059359458\n",
            "45     \t [0.18326765 0.89863262]. \t  -75.49743886501514 \t -0.008514483059359458\n",
            "46     \t [ 1.2578951  -0.78115751]. \t  -558.6596858541424 \t -0.008514483059359458\n",
            "47     \t [-0.70498112  1.35567505]. \t  -76.63952263722884 \t -0.008514483059359458\n",
            "48     \t [ 1.59433827 -1.56934331]. \t  -1690.597343715668 \t -0.008514483059359458\n",
            "49     \t [1.51693909 0.89691712]. \t  -197.4413596981743 \t -0.008514483059359458\n",
            "50     \t [1.00746689 1.02233169]. \t  \u001b[92m-0.005446487424419977\u001b[0m \t -0.005446487424419977\n",
            "51     \t [-1.42388075  0.1146593 ]. \t  -371.7468143613834 \t -0.005446487424419977\n",
            "52     \t [ 0.1085647  -0.58892905]. \t  -36.880548666447616 \t -0.005446487424419977\n",
            "53     \t [1.01245249 1.03492609]. \t  -0.009888952603299361 \t -0.005446487424419977\n",
            "54     \t [1.01931537 1.03345034]. \t  \u001b[92m-0.0034571917858046007\u001b[0m \t -0.0034571917858046007\n",
            "55     \t [-1.65362243  1.55016785]. \t  -147.29819343735738 \t -0.0034571917858046007\n",
            "56     \t [-1.45956907 -0.7097167 ]. \t  -812.6427464783716 \t -0.0034571917858046007\n",
            "57     \t [ 0.54610405 -0.11064588]. \t  -16.923939619141667 \t -0.0034571917858046007\n",
            "58     \t [ 0.98175078 -1.36234828]. \t  -541.1130116542306 \t -0.0034571917858046007\n",
            "59     \t [1.0751185  1.20090496]. \t  -0.20836943972817076 \t -0.0034571917858046007\n",
            "60     \t [1.0504432  1.11387854]. \t  -0.013459781296602602 \t -0.0034571917858046007\n",
            "61     \t [1.35617493 0.55961046]. \t  -163.86447197354195 \t -0.0034571917858046007\n",
            "62     \t [0.45552316 0.21897706]. \t  -0.30962423274284356 \t -0.0034571917858046007\n",
            "63     \t [-0.16196487 -1.7443089 ]. \t  -314.83188763669534 \t -0.0034571917858046007\n",
            "64     \t [1.37328284 1.6831954 ]. \t  -4.248488633261746 \t -0.0034571917858046007\n",
            "65     \t [1.23348108 1.77303889]. \t  -6.382923344952589 \t -0.0034571917858046007\n",
            "66     \t [-0.62266337  1.87911137]. \t  -225.06093923446917 \t -0.0034571917858046007\n",
            "67     \t [1.18500214 1.46119513]. \t  -0.3587274565223503 \t -0.0034571917858046007\n",
            "68     \t [0.19064963 0.18377161]. \t  -2.8284411978450037 \t -0.0034571917858046007\n",
            "69     \t [-1.33577761  0.72523685]. \t  -117.61771581899634 \t -0.0034571917858046007\n",
            "70     \t [0.42332497 1.62320704]. \t  -208.8470227456658 \t -0.0034571917858046007\n",
            "71     \t [-2.02702739 -0.26349559]. \t  -1920.894768467468 \t -0.0034571917858046007\n",
            "72     \t [0.39183501 0.50543384]. \t  -12.753166403029297 \t -0.0034571917858046007\n",
            "73     \t [1.27394533 1.66709006]. \t  -0.26999797717969853 \t -0.0034571917858046007\n",
            "74     \t [ 0.76614337 -1.31814465]. \t  -363.00302733841585 \t -0.0034571917858046007\n",
            "75     \t [1.28292561 1.63333802]. \t  -0.09582253221044269 \t -0.0034571917858046007\n",
            "76     \t [1.29637884 1.65048082]. \t  -0.17854547250380065 \t -0.0034571917858046007\n",
            "77     \t [0.79176731 1.25677505]. \t  -39.71818790076598 \t -0.0034571917858046007\n",
            "78     \t [0.99025131 1.00097006]. \t  -0.04159853630295643 \t -0.0034571917858046007\n",
            "79     \t [0.85512775 0.72113603]. \t  -0.031203980078682414 \t -0.0034571917858046007\n",
            "80     \t [-0.59155645 -0.47791717]. \t  -71.06764146316972 \t -0.0034571917858046007\n",
            "81     \t [ 1.53401241 -1.94544776]. \t  -1848.117337600054 \t -0.0034571917858046007\n",
            "82     \t [-1.26090414 -1.76207818]. \t  -1128.6735459557462 \t -0.0034571917858046007\n",
            "83     \t [-1.68651501 -1.61570474]. \t  -1996.4109351925408 \t -0.0034571917858046007\n",
            "84     \t [1.46877044 0.82530088]. \t  -177.6383431187912 \t -0.0034571917858046007\n",
            "85     \t [-1.16358746 -0.31670177]. \t  -283.784094866158 \t -0.0034571917858046007\n",
            "86     \t [1.03028438 1.04660832]. \t  -0.023051409914086236 \t -0.0034571917858046007\n",
            "87     \t [0.3867384  0.02623288]. \t  -1.897210205768112 \t -0.0034571917858046007\n",
            "88     \t [-1.48794505  1.79929581]. \t  -23.386208377647066 \t -0.0034571917858046007\n",
            "89     \t [1.24054662 1.52465116]. \t  -0.07832531223517429 \t -0.0034571917858046007\n",
            "90     \t [ 0.26647502 -0.99000205]. \t  -113.11248998845508 \t -0.0034571917858046007\n",
            "91     \t [0.9410594  0.87881259]. \t  -0.008071103041009397 \t -0.0034571917858046007\n",
            "92     \t [-1.42508924 -1.98741794]. \t  -1620.5523613714354 \t -0.0034571917858046007\n",
            "93     \t [-1.25859021  1.86885188]. \t  -13.212480352712507 \t -0.0034571917858046007\n",
            "94     \t [-1.65480693  0.49966047]. \t  -508.2371864994538 \t -0.0034571917858046007\n",
            "95     \t [-0.00691463  0.37379988]. \t  -14.9829378826118 \t -0.0034571917858046007\n",
            "96     \t [1.24404156 1.55830734]. \t  -0.07093675417988167 \t -0.0034571917858046007\n",
            "97     \t [-0.73715664 -1.36234614]. \t  -366.2045166787597 \t -0.0034571917858046007\n",
            "98     \t [1.27097076 1.63619241]. \t  -0.1167962764405455 \t -0.0034571917858046007\n",
            "99     \t [ 0.77014904 -0.30280835]. \t  -80.32330169562528 \t -0.0034571917858046007\n",
            "100    \t [-0.40242189  0.70870106]. \t  -31.861182802358318 \t -0.0034571917858046007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fh_Amb8TtdgO",
        "outputId": "e2984aa0-8477-4cca-9fa7-ccd564c67011"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 6\r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_loser_6 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_6 = dGPGO(surrogate_loser_6, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.60915518 -0.68821072]. \t  -1074.6314195925434 \t -3.0269049669752817\n",
            "init   \t [ 1.31575449 -1.87721062]. \t  -1302.169546932896 \t -3.0269049669752817\n",
            "init   \t [-1.60703824  0.38933325]. \t  -487.8262244570432 \t -3.0269049669752817\n",
            "init   \t [ 0.12213192 -0.33256477]. \t  -12.844955340992902 \t -3.0269049669752817\n",
            "init   \t [-0.67416945  0.50183959]. \t  -3.0269049669752817 \t -3.0269049669752817\n",
            "1      \t [-0.06756934  0.5044222 ]. \t  -26.125364719936726 \t -3.0269049669752817\n",
            "2      \t [-0.46927273 -0.51516379]. \t  -56.23723640401884 \t -3.0269049669752817\n",
            "3      \t [-0.27427447  0.04468444]. \t  \u001b[92m-1.7170570344409537\u001b[0m \t -1.7170570344409537\n",
            "4      \t [-0.7005749   1.50074021]. \t  -104.88882839307404 \t -1.7170570344409537\n",
            "5      \t [-0.46613674  0.64777826]. \t  -20.68213437104052 \t -1.7170570344409537\n",
            "6      \t [1.16259188 1.02707415]. \t  -10.559428954318465 \t -1.7170570344409537\n",
            "7      \t [0.26282348 0.02154315]. \t  \u001b[92m-0.7693680794170482\u001b[0m \t -0.7693680794170482\n",
            "8      \t [0.13742794 0.05100134]. \t  -0.8471672326655502 \t -0.7693680794170482\n",
            "9      \t [0.76291309 0.57541653]. \t  \u001b[92m-0.06059244450758462\u001b[0m \t -0.06059244450758462\n",
            "10     \t [0.9717123  1.30814843]. \t  -13.244841950407265 \t -0.06059244450758462\n",
            "11     \t [0.74995669 0.75504438]. \t  -3.7723574144839005 \t -0.06059244450758462\n",
            "12     \t [1.86881989 2.02915792]. \t  -214.88827624604227 \t -0.06059244450758462\n",
            "13     \t [-0.46121134  0.22456258]. \t  -2.1491729768093766 \t -0.06059244450758462\n",
            "14     \t [-1.64886576  2.02848959]. \t  -54.663578807527045 \t -0.06059244450758462\n",
            "15     \t [0.55463831 0.35702693]. \t  -0.44241535558223744 \t -0.06059244450758462\n",
            "16     \t [0.78138218 1.84221429]. \t  -151.74548715531026 \t -0.06059244450758462\n",
            "17     \t [ 0.05896132 -0.01172031]. \t  -0.9086479056578121 \t -0.06059244450758462\n",
            "18     \t [-1.11205613  1.15532179]. \t  -5.12251527619631 \t -0.06059244450758462\n",
            "19     \t [0.6083543  0.34598655]. \t  -0.21150784087130964 \t -0.06059244450758462\n",
            "20     \t [0.60534978 0.36004684]. \t  -0.15984674139762628 \t -0.06059244450758462\n",
            "21     \t [-0.77698697  1.56001318]. \t  -94.60949982977071 \t -0.06059244450758462\n",
            "22     \t [-0.55078469 -0.83989102]. \t  -133.10808530049047 \t -0.06059244450758462\n",
            "23     \t [-1.31227773  2.03619975]. \t  -15.214200315819033 \t -0.06059244450758462\n",
            "24     \t [ 0.80570692 -1.02979778]. \t  -281.92889771006395 \t -0.06059244450758462\n",
            "25     \t [-0.94696839  0.86842292]. \t  -3.8709233696267025 \t -0.06059244450758462\n",
            "26     \t [-0.15693252  0.26323614]. \t  -7.031886120412313 \t -0.06059244450758462\n",
            "27     \t [1.47166778 0.25534753]. \t  -365.20765281558084 \t -0.06059244450758462\n",
            "28     \t [0.68127202 0.48442364]. \t  -0.142764383238895 \t -0.06059244450758462\n",
            "29     \t [-1.52580274 -1.09844367]. \t  -1180.4820266939364 \t -0.06059244450758462\n",
            "30     \t [0.48343844 0.21728772]. \t  -0.29381391000716506 \t -0.06059244450758462\n",
            "31     \t [1.59467551 0.79854923]. \t  -304.6609988132639 \t -0.06059244450758462\n",
            "32     \t [1.0002996  1.10496461]. \t  -1.0892119646126375 \t -0.06059244450758462\n",
            "33     \t [0.66975161 0.45602927]. \t  -0.11463222060463235 \t -0.06059244450758462\n",
            "34     \t [0.85061931 0.72472308]. \t  \u001b[92m-0.022451450323285087\u001b[0m \t -0.022451450323285087\n",
            "35     \t [0.8763174  0.77042807]. \t  \u001b[92m-0.015920326692339105\u001b[0m \t -0.015920326692339105\n",
            "36     \t [1.0304445  1.05936354]. \t  \u001b[92m-0.0015282576420380743\u001b[0m \t -0.0015282576420380743\n",
            "37     \t [-1.68156932 -1.93210104]. \t  -2272.737961281629 \t -0.0015282576420380743\n",
            "38     \t [1.18306743 1.43030098]. \t  -0.12747094498765982 \t -0.0015282576420380743\n",
            "39     \t [1.05524058 1.18053815]. \t  -0.4520248243555432 \t -0.0015282576420380743\n",
            "40     \t [1.16802336 1.32788128]. \t  -0.16070816763035162 \t -0.0015282576420380743\n",
            "41     \t [0.09480507 1.95906405]. \t  -381.0990390354115 \t -0.0015282576420380743\n",
            "42     \t [-1.16850434  1.4108355 ]. \t  -4.908827732761238 \t -0.0015282576420380743\n",
            "43     \t [1.03157812 1.04914732]. \t  -0.02351549960770606 \t -0.0015282576420380743\n",
            "44     \t [ 1.22316586 -1.80538469]. \t  -1090.0528432604024 \t -0.0015282576420380743\n",
            "45     \t [ 0.18623279 -1.93678777]. \t  -389.3317792113233 \t -0.0015282576420380743\n",
            "46     \t [-1.2323997  -1.29908191]. \t  -799.0345393152076 \t -0.0015282576420380743\n",
            "47     \t [1.16545637 1.32089235]. \t  -0.16722330840851782 \t -0.0015282576420380743\n",
            "48     \t [0.65243029 1.06414155]. \t  -40.88599954011427 \t -0.0015282576420380743\n",
            "49     \t [1.0706259  1.12852903]. \t  -0.03635522814658004 \t -0.0015282576420380743\n",
            "50     \t [-0.03454204 -0.1886721 ]. \t  -4.675158818828568 \t -0.0015282576420380743\n",
            "51     \t [0.77334622 0.61511475]. \t  -0.08044346238140163 \t -0.0015282576420380743\n",
            "52     \t [1.39524932 1.49131097]. \t  -20.896020069571428 \t -0.0015282576420380743\n",
            "53     \t [-1.45946305  0.86133243]. \t  -167.00891750251822 \t -0.0015282576420380743\n",
            "54     \t [1.15947831 1.31691205]. \t  -0.10093679401279845 \t -0.0015282576420380743\n",
            "55     \t [ 1.0966438  -1.78210956]. \t  -890.8749525146312 \t -0.0015282576420380743\n",
            "56     \t [0.81645276 0.67938207]. \t  -0.05004022079805938 \t -0.0015282576420380743\n",
            "57     \t [1.10644041 1.25330942]. \t  -0.09600497454499114 \t -0.0015282576420380743\n",
            "58     \t [ 0.05426691 -2.01085777]. \t  -406.43452858749674 \t -0.0015282576420380743\n",
            "59     \t [1.16675717 1.36857024]. \t  -0.03306121760283245 \t -0.0015282576420380743\n",
            "60     \t [0.8648525  0.74363049]. \t  -0.020147848026052854 \t -0.0015282576420380743\n",
            "61     \t [-0.89282563  0.73921926]. \t  -3.918242355093483 \t -0.0015282576420380743\n",
            "62     \t [ 1.23043616 -1.71564361]. \t  -1043.095545084003 \t -0.0015282576420380743\n",
            "63     \t [0.85032242 0.70890977]. \t  -0.04239296622262545 \t -0.0015282576420380743\n",
            "64     \t [1.98349663 0.05164635]. \t  -1508.4352567550156 \t -0.0015282576420380743\n",
            "65     \t [0.71146786 0.39093788]. \t  -1.4114756191988476 \t -0.0015282576420380743\n",
            "66     \t [ 0.15070243 -0.35797581]. \t  -15.213567966194207 \t -0.0015282576420380743\n",
            "67     \t [-0.84196325 -0.03742189]. \t  -59.09278066675081 \t -0.0015282576420380743\n",
            "68     \t [0.73029918 1.53562134]. \t  -100.53015091470867 \t -0.0015282576420380743\n",
            "69     \t [-0.93643438 -1.58272987]. \t  -608.7322847330377 \t -0.0015282576420380743\n",
            "70     \t [1.3120196  2.00453818]. \t  -8.114337895616032 \t -0.0015282576420380743\n",
            "71     \t [0.9293458  0.85768369]. \t  -0.008591935989947641 \t -0.0015282576420380743\n",
            "72     \t [-1.64153317 -0.05783136]. \t  -764.5826832498624 \t -0.0015282576420380743\n",
            "73     \t [0.0951594  1.35191321]. \t  -181.1454700941001 \t -0.0015282576420380743\n",
            "74     \t [-0.47475672 -0.9289252 ]. \t  -135.4201751947115 \t -0.0015282576420380743\n",
            "75     \t [-0.67246471 -0.39538125]. \t  -74.63802490155976 \t -0.0015282576420380743\n",
            "76     \t [-0.54687729 -1.66262826]. \t  -387.2207058781848 \t -0.0015282576420380743\n",
            "77     \t [0.18670875 0.77461356]. \t  -55.38495183637282 \t -0.0015282576420380743\n",
            "78     \t [-1.87549053 -0.06118832]. \t  -1288.944200275795 \t -0.0015282576420380743\n",
            "79     \t [1.82501194 1.48588699]. \t  -341.0025557765546 \t -0.0015282576420380743\n",
            "80     \t [ 0.05802756 -1.76728631]. \t  -314.4086978048547 \t -0.0015282576420380743\n",
            "81     \t [1.32318137 1.80428107]. \t  -0.39037303414753044 \t -0.0015282576420380743\n",
            "82     \t [1.14603764 1.3159491 ]. \t  -0.021975630904855405 \t -0.0015282576420380743\n",
            "83     \t [1.16723617 1.96839226]. \t  -36.74574817607512 \t -0.0015282576420380743\n",
            "84     \t [1.11620229 1.25273114]. \t  -0.018159103179929793 \t -0.0015282576420380743\n",
            "85     \t [-1.20320127 -0.35699766]. \t  -330.54503505181793 \t -0.0015282576420380743\n",
            "86     \t [1.82389133 1.28038113]. \t  -419.37161019000627 \t -0.0015282576420380743\n",
            "87     \t [ 0.88625922 -0.3468704 ]. \t  -128.2291097907271 \t -0.0015282576420380743\n",
            "88     \t [0.85864023 0.04165531]. \t  -48.406994899042076 \t -0.0015282576420380743\n",
            "89     \t [0.46181679 0.9575149 ]. \t  -55.678982222462196 \t -0.0015282576420380743\n",
            "90     \t [-0.7935673   1.42080154]. \t  -65.79328558574495 \t -0.0015282576420380743\n",
            "91     \t [ 2.04234106 -0.12966452]. \t  -1850.7930646007785 \t -0.0015282576420380743\n",
            "92     \t [1.23923152 1.54383138]. \t  -0.06385218695013092 \t -0.0015282576420380743\n",
            "93     \t [1.27932336 1.66748409]. \t  -0.1729830223847136 \t -0.0015282576420380743\n",
            "94     \t [ 2.02538509 -0.76140625]. \t  -2366.5031536246265 \t -0.0015282576420380743\n",
            "95     \t [1.25603595 1.60487613]. \t  -0.13980967564426916 \t -0.0015282576420380743\n",
            "96     \t [-1.95689662 -1.63522269]. \t  -2995.0018570387247 \t -0.0015282576420380743\n",
            "97     \t [0.95008464 0.89464807]. \t  -0.00891196172657524 \t -0.0015282576420380743\n",
            "98     \t [-2.01222674  0.53070006]. \t  -1246.9566845983406 \t -0.0015282576420380743\n",
            "99     \t [0.70205516 0.48460947]. \t  -0.09561367871903 \t -0.0015282576420380743\n",
            "100    \t [-0.90499114  1.91485776]. \t  -123.71745106321148 \t -0.0015282576420380743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNmoDTuUtdi3",
        "outputId": "8c98923a-915c-45f5-fdad-b20a7384349f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 7\r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_loser_7 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_7 = dGPGO(surrogate_loser_7, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.42268934 -0.80954733]. \t  -808.7939502616841 \t -2.0077595729598063\n",
            "init   \t [-1.79389885 -0.16441204]. \t  -1151.9264213711547 \t -2.0077595729598063\n",
            "init   \t [1.37319786 1.74897991]. \t  -2.0077595729598063 \t -2.0077595729598063\n",
            "init   \t [0.92974688 1.09976052]. \t  -5.543015908052858 \t -2.0077595729598063\n",
            "init   \t [-0.94533605  0.58994398]. \t  -13.008689168416776 \t -2.0077595729598063\n",
            "1      \t [-0.37625117  1.06266898]. \t  -86.73733152643135 \t -2.0077595729598063\n",
            "2      \t [ 0.03555281 -0.06681774]. \t  \u001b[92m-1.3936707114630122\u001b[0m \t -1.3936707114630122\n",
            "3      \t [1.70981563 0.62552629]. \t  -528.5581328002694 \t -1.3936707114630122\n",
            "4      \t [1.00342219 1.66585077]. \t  -43.427410777261855 \t -1.3936707114630122\n",
            "5      \t [-0.28030133  0.4011523 ]. \t  -12.045180746113228 \t -1.3936707114630122\n",
            "6      \t [-1.54963585  1.50677465]. \t  -86.5309531618843 \t -1.3936707114630122\n",
            "7      \t [ 0.11233166 -1.46148808]. \t  -218.0869481450979 \t -1.3936707114630122\n",
            "8      \t [-0.95822245  0.95071318]. \t  -3.940409222089954 \t -1.3936707114630122\n",
            "9      \t [1.92585728 1.93941244]. \t  -313.9751288778981 \t -1.3936707114630122\n",
            "10     \t [-0.7289408   0.54875501]. \t  -3.019513411687114 \t -1.3936707114630122\n",
            "11     \t [1.20067738 1.581771  ]. \t  -2.0043289107835234 \t -1.3936707114630122\n",
            "12     \t [0.46213398 0.26553063]. \t  \u001b[92m-0.5593132411126935\u001b[0m \t -0.5593132411126935\n",
            "13     \t [1.31763586 1.87891628]. \t  -2.138706571027679 \t -0.5593132411126935\n",
            "14     \t [0.45604776 0.59219017]. \t  -15.057663906077446 \t -0.5593132411126935\n",
            "15     \t [-0.22276621  0.11703406]. \t  -1.9495582474432847 \t -0.5593132411126935\n",
            "16     \t [ 0.29565231 -0.03211627]. \t  -1.9247653511248326 \t -0.5593132411126935\n",
            "17     \t [0.17355264 0.02926456]. \t  -0.6830885034792047 \t -0.5593132411126935\n",
            "18     \t [0.3951644  0.16699149]. \t  \u001b[92m-0.37756925347923925\u001b[0m \t -0.37756925347923925\n",
            "19     \t [1.27246343 1.60397822]. \t  \u001b[92m-0.09729457969826462\u001b[0m \t -0.09729457969826462\n",
            "20     \t [-0.94093813  0.72810265]. \t  -6.2403716851369 \t -0.09729457969826462\n",
            "21     \t [-0.54832657  1.90584102]. \t  -260.05727709114376 \t -0.09729457969826462\n",
            "22     \t [0.63186756 0.48432176]. \t  -0.8591294559071819 \t -0.09729457969826462\n",
            "23     \t [ 0.96780316 -0.78044245]. \t  -294.8392690826092 \t -0.09729457969826462\n",
            "24     \t [-1.97561109 -1.33580864]. \t  -2753.4069170851544 \t -0.09729457969826462\n",
            "25     \t [-0.49292243 -0.7916302 ]. \t  -109.26909728905902 \t -0.09729457969826462\n",
            "26     \t [0.81115052 0.70978983]. \t  -0.3042436276168672 \t -0.09729457969826462\n",
            "27     \t [-0.588465    0.05940153]. \t  -10.753781252257108 \t -0.09729457969826462\n",
            "28     \t [0.72600312 0.60635585]. \t  -0.7035319805615363 \t -0.09729457969826462\n",
            "29     \t [1.55375794 0.04422647]. \t  -561.9669176444024 \t -0.09729457969826462\n",
            "30     \t [1.32791237 1.75857392]. \t  -0.10980881641314008 \t -0.09729457969826462\n",
            "31     \t [ 1.84142169 -0.68915833]. \t  -1665.3415997891302 \t -0.09729457969826462\n",
            "32     \t [0.70587209 0.46014561]. \t  -0.2317468799219714 \t -0.09729457969826462\n",
            "33     \t [1.29468303 1.72357606]. \t  -0.31124794977871817 \t -0.09729457969826462\n",
            "34     \t [ 2.01308506 -1.1081972 ]. \t  -2664.317705524392 \t -0.09729457969826462\n",
            "35     \t [-1.15538548 -0.32404392]. \t  -279.8603611900302 \t -0.09729457969826462\n",
            "36     \t [1.27476812 1.62866836]. \t  \u001b[92m-0.0768185504021595\u001b[0m \t -0.0768185504021595\n",
            "37     \t [ 0.9721382  -0.41205174]. \t  -184.17401640929336 \t -0.0768185504021595\n",
            "38     \t [-0.02246719  1.02914007]. \t  -106.85449638551168 \t -0.0768185504021595\n",
            "39     \t [-1.20940169 -1.15883084]. \t  -692.0989167941194 \t -0.0768185504021595\n",
            "40     \t [-1.75043739 -0.96288583]. \t  -1629.1708668895085 \t -0.0768185504021595\n",
            "41     \t [ 0.18646567 -0.01091241]. \t  -0.8705212881455077 \t -0.0768185504021595\n",
            "42     \t [-1.57895555  1.65991668]. \t  -76.07055806729942 \t -0.0768185504021595\n",
            "43     \t [ 1.22473655 -0.02664716]. \t  -233.10943775648116 \t -0.0768185504021595\n",
            "44     \t [ 0.35130358 -0.51042541]. \t  -40.596073343757254 \t -0.0768185504021595\n",
            "45     \t [1.31169037 1.71205742]. \t  -0.10433210379994093 \t -0.0768185504021595\n",
            "46     \t [-1.59405765 -0.72663747]. \t  -1074.487542263908 \t -0.0768185504021595\n",
            "47     \t [-0.28651539 -0.574408  ]. \t  -44.75422470822018 \t -0.0768185504021595\n",
            "48     \t [ 1.17786294 -1.63653741]. \t  -914.4278621791691 \t -0.0768185504021595\n",
            "49     \t [-1.73635555  0.31082404]. \t  -738.7068672809747 \t -0.0768185504021595\n",
            "50     \t [-1.59047368  0.71679317]. \t  -335.3397783815587 \t -0.0768185504021595\n",
            "51     \t [1.24558232 1.59362825]. \t  -0.23799774150302205 \t -0.0768185504021595\n",
            "52     \t [-0.74435884  0.34039411]. \t  -7.6085296637269195 \t -0.0768185504021595\n",
            "53     \t [0.7140417  0.44870733]. \t  -0.45568258475893825 \t -0.0768185504021595\n",
            "54     \t [0.70850424 0.45035585]. \t  -0.3514570134726549 \t -0.0768185504021595\n",
            "55     \t [1.12644203 1.83016155]. \t  -31.520623750753167 \t -0.0768185504021595\n",
            "56     \t [0.66971685 0.45191973]. \t  -0.11024233161666784 \t -0.0768185504021595\n",
            "57     \t [-1.24954666  1.78525415]. \t  -10.073012518900768 \t -0.0768185504021595\n",
            "58     \t [0.75999203 0.54763979]. \t  -0.14729271932259913 \t -0.0768185504021595\n",
            "59     \t [1.26920987 1.64225928]. \t  -0.17085392230467644 \t -0.0768185504021595\n",
            "60     \t [1.26662319 1.61163542]. \t  \u001b[92m-0.07641853550900016\u001b[0m \t -0.07641853550900016\n",
            "61     \t [-1.37789349 -0.22158635]. \t  -455.1693559042023 \t -0.07641853550900016\n",
            "62     \t [-1.95087914  0.75283349]. \t  -940.8471583586562 \t -0.07641853550900016\n",
            "63     \t [0.61700267 0.30621819]. \t  -0.7013262445509651 \t -0.07641853550900016\n",
            "64     \t [0.67047068 0.44595488]. \t  -0.10986838832854515 \t -0.07641853550900016\n",
            "65     \t [0.71131079 0.51730102]. \t  -0.09619642912615073 \t -0.07641853550900016\n",
            "66     \t [-1.98057804  0.15332739]. \t  -1429.6928273167414 \t -0.07641853550900016\n",
            "67     \t [-1.21300802  1.51226535]. \t  -5.06449648828758 \t -0.07641853550900016\n",
            "68     \t [ 0.37605334 -0.813153  ]. \t  -91.50952887424619 \t -0.07641853550900016\n",
            "69     \t [-0.99219442  1.58641447]. \t  -40.204989906806524 \t -0.07641853550900016\n",
            "70     \t [-0.86162263 -0.11474692]. \t  -76.93461829347547 \t -0.07641853550900016\n",
            "71     \t [-1.05647768 -0.98787153]. \t  -446.9176886955735 \t -0.07641853550900016\n",
            "72     \t [-0.75778259 -0.25072129]. \t  -71.14499647851899 \t -0.07641853550900016\n",
            "73     \t [0.71481062 0.50334247]. \t  -0.08712687188637122 \t -0.07641853550900016\n",
            "74     \t [0.76422544 0.59494797]. \t  \u001b[92m-0.06748687884423658\u001b[0m \t -0.06748687884423658\n",
            "75     \t [1.65163192 0.53075901]. \t  -483.1621969903884 \t -0.06748687884423658\n",
            "76     \t [1.01202069 0.95936413]. \t  -0.42033022406164844 \t -0.06748687884423658\n",
            "77     \t [0.84517478 0.73987668]. \t  -0.08928319617884767 \t -0.06748687884423658\n",
            "78     \t [0.75157005 0.54179966]. \t  -0.11488399313826303 \t -0.06748687884423658\n",
            "79     \t [-1.02331335 -1.98831652]. \t  -925.511763560693 \t -0.06748687884423658\n",
            "80     \t [-1.76585423  0.51846661]. \t  -683.5327188082996 \t -0.06748687884423658\n",
            "81     \t [0.75010484 0.5213793 ]. \t  -0.232834681612964 \t -0.06748687884423658\n",
            "82     \t [-0.34050634 -1.25170618]. \t  -188.8438156755429 \t -0.06748687884423658\n",
            "83     \t [-0.06717539 -0.55957888]. \t  -32.958775452655466 \t -0.06748687884423658\n",
            "84     \t [0.94887324 0.87272592]. \t  -0.07898054832118488 \t -0.06748687884423658\n",
            "85     \t [-1.0565167   0.00792408]. \t  -127.06291560199584 \t -0.06748687884423658\n",
            "86     \t [0.84915878 0.7528633 ]. \t  -0.12383040631323093 \t -0.06748687884423658\n",
            "87     \t [-1.93157144 -1.29004162]. \t  -2529.648099431939 \t -0.06748687884423658\n",
            "88     \t [-0.56364059  1.63132091]. \t  -175.00740333734777 \t -0.06748687884423658\n",
            "89     \t [1.30276769 1.72131991]. \t  -0.14982766937968078 \t -0.06748687884423658\n",
            "90     \t [-0.80306651 -1.303614  ]. \t  -382.92789483486376 \t -0.06748687884423658\n",
            "91     \t [ 1.98490407 -1.38419481]. \t  -2835.5091279274216 \t -0.06748687884423658\n",
            "92     \t [ 0.24002072 -1.48468418]. \t  -238.44468467100532 \t -0.06748687884423658\n",
            "93     \t [1.30623684 1.71433512]. \t  -0.1003103558526618 \t -0.06748687884423658\n",
            "94     \t [ 1.46606788 -1.9522138 ]. \t  -1682.5038950182677 \t -0.06748687884423658\n",
            "95     \t [1.38696035 1.33125903]. \t  -35.24351314917995 \t -0.06748687884423658\n",
            "96     \t [0.98938011 0.98397349]. \t  \u001b[92m-0.002714288849242109\u001b[0m \t -0.002714288849242109\n",
            "97     \t [-0.33165955  0.04899537]. \t  -2.1454500004474717 \t -0.002714288849242109\n",
            "98     \t [-0.33682144 -0.00609563]. \t  -3.2161758712036335 \t -0.002714288849242109\n",
            "99     \t [0.03997086 1.39923583]. \t  -196.26089887888634 \t -0.002714288849242109\n",
            "100    \t [0.95960873 0.93171848]. \t  -0.01344619104426845 \t -0.002714288849242109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_21yVprtdlu",
        "outputId": "6c94c2cd-2f7c-43e7-ebbf-dbc2c9fe0999"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 8\r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_loser_8 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_8 = dGPGO(surrogate_loser_8, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.59948851 -1.8750873 ]. \t  -1972.3059423071145 \t -7.238275380208551\n",
            "init   \t [ 0.40834083 -0.81972959]. \t  -97.66272647293047 \t -7.238275380208551\n",
            "init   \t [ 1.21492186 -0.54806067]. \t  -409.7425700197761 \t -7.238275380208551\n",
            "init   \t [ 1.55897178 -0.82804067]. \t  -1062.051446884513 \t -7.238275380208551\n",
            "init   \t [-0.69999303  0.28146449]. \t  -7.238275380208551 \t -7.238275380208551\n",
            "1      \t [0.61360238 0.32771785]. \t  \u001b[92m-0.38734981524274686\u001b[0m \t -0.38734981524274686\n",
            "2      \t [ 0.44957683 -0.14950718]. \t  -12.667085696146009 \t -0.38734981524274686\n",
            "3      \t [-1.39188014  0.88003045]. \t  -117.50939285923893 \t -0.38734981524274686\n",
            "4      \t [1.41566408 0.84930114]. \t  -133.52992403681057 \t -0.38734981524274686\n",
            "5      \t [-0.6916621   0.64393699]. \t  -5.60208758511781 \t -0.38734981524274686\n",
            "6      \t [0.75922499 0.1685607 ]. \t  -16.69310442272994 \t -0.38734981524274686\n",
            "7      \t [0.35625505 1.43304671]. \t  -171.0117160875193 \t -0.38734981524274686\n",
            "8      \t [-0.01553477  0.50788672]. \t  -26.801695313336594 \t -0.38734981524274686\n",
            "9      \t [-0.8962121   0.44792833]. \t  -16.217141274308645 \t -0.38734981524274686\n",
            "10     \t [0.71806711 0.70823025]. \t  -3.7893425127670963 \t -0.38734981524274686\n",
            "11     \t [-1.45841171  2.02404169]. \t  -7.103102879159884 \t -0.38734981524274686\n",
            "12     \t [0.5643111  0.40904053]. \t  -1.0105432891995718 \t -0.38734981524274686\n",
            "13     \t [-0.56707816  0.41821546]. \t  -3.3896207753230425 \t -0.38734981524274686\n",
            "14     \t [-1.01385586  1.62704583]. \t  -39.952744159584476 \t -0.38734981524274686\n",
            "15     \t [0.58428637 0.40839102]. \t  -0.6217238959189965 \t -0.38734981524274686\n",
            "16     \t [-2.03615452  1.53171002]. \t  -692.6303498825225 \t -0.38734981524274686\n",
            "17     \t [1.4484407  1.88703496]. \t  -4.650898799351691 \t -0.38734981524274686\n",
            "18     \t [-1.16952629  1.98801246]. \t  -43.1742175698741 \t -0.38734981524274686\n",
            "19     \t [-1.02906948  1.10283435]. \t  -4.309408343334651 \t -0.38734981524274686\n",
            "20     \t [1.14713505 1.38075619]. \t  -0.4420372642961823 \t -0.38734981524274686\n",
            "21     \t [1.03799728 1.22229839]. \t  -2.0998868809733406 \t -0.38734981524274686\n",
            "22     \t [1.23015975 1.56784857]. \t  \u001b[92m-0.3506044600393513\u001b[0m \t -0.3506044600393513\n",
            "23     \t [1.22577866 1.45245473]. \t  \u001b[92m-0.30176262008602805\u001b[0m \t -0.30176262008602805\n",
            "24     \t [-1.90600432  0.6011805 ]. \t  -927.5483607203025 \t -0.30176262008602805\n",
            "25     \t [1.22988235 1.51959968]. \t  \u001b[92m-0.05773061435510794\u001b[0m \t -0.05773061435510794\n",
            "26     \t [-1.17538182  1.46656695]. \t  -5.45554343087837 \t -0.05773061435510794\n",
            "27     \t [-2.02223264 -0.51849968]. \t  -2132.4307428489506 \t -0.05773061435510794\n",
            "28     \t [1.92951462 1.09425467]. \t  -691.9082143721105 \t -0.05773061435510794\n",
            "29     \t [0.83603421 0.69123358]. \t  \u001b[92m-0.03284404679566778\u001b[0m \t -0.03284404679566778\n",
            "30     \t [1.37616    1.87961861]. \t  -0.16165392097749176 \t -0.03284404679566778\n",
            "31     \t [0.99454833 0.91627153]. \t  -0.5308124556940027 \t -0.03284404679566778\n",
            "32     \t [1.4028137  2.04755223]. \t  -0.796925196137526 \t -0.03284404679566778\n",
            "33     \t [1.05311543 1.08099774]. \t  -0.08152592616659253 \t -0.03284404679566778\n",
            "34     \t [ 0.42324543 -2.01100952]. \t  -480.00669093860853 \t -0.03284404679566778\n",
            "35     \t [ 0.190941   -0.04878897]. \t  -1.3812889957609409 \t -0.03284404679566778\n",
            "36     \t [-0.18657001 -0.10636657]. \t  -3.400984813924212 \t -0.03284404679566778\n",
            "37     \t [-1.33951156  1.72886835]. \t  -5.901329482203336 \t -0.03284404679566778\n",
            "38     \t [1.28676527 0.96379415]. \t  -47.96458146455062 \t -0.03284404679566778\n",
            "39     \t [1.131244   1.31148382]. \t  -0.11816351914173326 \t -0.03284404679566778\n",
            "40     \t [-0.75348494  1.99447316]. \t  -206.631586007862 \t -0.03284404679566778\n",
            "41     \t [-0.83236489 -1.3406397 ]. \t  -416.8579921277826 \t -0.03284404679566778\n",
            "42     \t [ 2.01538058 -0.33779095]. \t  -1936.6348722404737 \t -0.03284404679566778\n",
            "43     \t [0.33010469 0.19124598]. \t  -1.1257081671619646 \t -0.03284404679566778\n",
            "44     \t [1.40714957 1.94921237]. \t  -0.26098948963885904 \t -0.03284404679566778\n",
            "45     \t [1.19510526 1.45322224]. \t  -0.10029464005430952 \t -0.03284404679566778\n",
            "46     \t [1.13987265 1.29452607]. \t  \u001b[92m-0.021852623826453442\u001b[0m \t -0.021852623826453442\n",
            "47     \t [ 1.04555863 -0.33677927]. \t  -204.48410237553725 \t -0.021852623826453442\n",
            "48     \t [-0.4324462   1.50517663]. \t  -175.80830243387348 \t -0.021852623826453442\n",
            "49     \t [-0.23047777  1.43019956]. \t  -191.14888552417455 \t -0.021852623826453442\n",
            "50     \t [-1.17614757  1.22149043]. \t  -7.354599934031947 \t -0.021852623826453442\n",
            "51     \t [-0.27721953  1.68451558]. \t  -260.0899357890036 \t -0.021852623826453442\n",
            "52     \t [-0.68574931 -1.57082399]. \t  -419.44092006851207 \t -0.021852623826453442\n",
            "53     \t [1.2542339  1.61970987]. \t  -0.28185796598021506 \t -0.021852623826453442\n",
            "54     \t [ 0.21553377 -1.99468377]. \t  -417.2400550622069 \t -0.021852623826453442\n",
            "55     \t [1.91542278 1.13366555]. \t  -643.5511920107601 \t -0.021852623826453442\n",
            "56     \t [ 0.3149918  -0.37616106]. \t  -23.06793576005066 \t -0.021852623826453442\n",
            "57     \t [-0.34460371  1.04071351]. \t  -86.80931352169951 \t -0.021852623826453442\n",
            "58     \t [-1.98374815  1.75520272]. \t  -484.16630206685227 \t -0.021852623826453442\n",
            "59     \t [-1.8061815  -0.50959029]. \t  -1430.583972383175 \t -0.021852623826453442\n",
            "60     \t [0.78421657 0.57042093]. \t  -0.245252848489033 \t -0.021852623826453442\n",
            "61     \t [0.70564023 0.50051481]. \t  -0.08731675873544986 \t -0.021852623826453442\n",
            "62     \t [ 0.64167029 -0.50889445]. \t  -84.88531997530704 \t -0.021852623826453442\n",
            "63     \t [1.01977412 1.05765385]. \t  -0.03177169933083389 \t -0.021852623826453442\n",
            "64     \t [0.21991589 1.36604332]. \t  -174.23667298663855 \t -0.021852623826453442\n",
            "65     \t [0.99899032 1.43852549]. \t  -19.40788724532415 \t -0.021852623826453442\n",
            "66     \t [0.93405663 0.88546978]. \t  \u001b[92m-0.02126932659159693\u001b[0m \t -0.02126932659159693\n",
            "67     \t [-0.34263104 -1.21563687]. \t  -179.50032890794742 \t -0.02126932659159693\n",
            "68     \t [ 1.10108511 -0.64982308]. \t  -346.793388504788 \t -0.02126932659159693\n",
            "69     \t [ 1.55417844 -1.95204842]. \t  -1907.8293870643543 \t -0.02126932659159693\n",
            "70     \t [ 0.45966197 -0.08303173]. \t  -8.954441874760606 \t -0.02126932659159693\n",
            "71     \t [-0.69710604  0.61700195]. \t  -4.5974511549804715 \t -0.02126932659159693\n",
            "72     \t [-2.02537435  0.93380206]. \t  -1012.9902227465138 \t -0.02126932659159693\n",
            "73     \t [ 1.23113088 -1.6068377 ]. \t  -975.0671205123277 \t -0.02126932659159693\n",
            "74     \t [0.82086512 0.65526865]. \t  -0.0665028936378506 \t -0.02126932659159693\n",
            "75     \t [-1.44041432  1.4358205 ]. \t  -46.7842596086021 \t -0.02126932659159693\n",
            "76     \t [-1.33185859  0.89627932]. \t  -82.45012243198344 \t -0.02126932659159693\n",
            "77     \t [1.69438806 1.87624578]. \t  -99.42600253272379 \t -0.02126932659159693\n",
            "78     \t [1.39078969 0.55250285]. \t  -191.08793337017656 \t -0.02126932659159693\n",
            "79     \t [-0.51996186  0.82887034]. \t  -33.50362549718966 \t -0.02126932659159693\n",
            "80     \t [-0.38195512  1.28093926]. \t  -130.7435472768706 \t -0.02126932659159693\n",
            "81     \t [-1.238806    0.24087278]. \t  -172.39569306997012 \t -0.02126932659159693\n",
            "82     \t [0.80841935 0.6439636 ]. \t  -0.04587743648830525 \t -0.02126932659159693\n",
            "83     \t [ 0.51250767 -0.81361155]. \t  -116.07457999615696 \t -0.02126932659159693\n",
            "84     \t [0.79209725 0.62267504]. \t  -0.045473168314503036 \t -0.02126932659159693\n",
            "85     \t [0.97075693 0.9335548 ]. \t  \u001b[92m-0.008624202437991018\u001b[0m \t -0.008624202437991018\n",
            "86     \t [-0.66521432  0.24617109]. \t  -6.627839046105652 \t -0.008624202437991018\n",
            "87     \t [1.24207642 1.55105691]. \t  -0.06549511174024994 \t -0.008624202437991018\n",
            "88     \t [0.94742611 1.29214479]. \t  -15.568041789437807 \t -0.008624202437991018\n",
            "89     \t [1.35748863 1.84057215]. \t  -0.12828354308515053 \t -0.008624202437991018\n",
            "90     \t [-1.71028388  0.04698636]. \t  -835.6827274435514 \t -0.008624202437991018\n",
            "91     \t [0.33503966 1.87471175]. \t  -311.0687599752305 \t -0.008624202437991018\n",
            "92     \t [-1.71383293  1.05001275]. \t  -363.5212559628972 \t -0.008624202437991018\n",
            "93     \t [-0.11151438 -0.26141094]. \t  -8.734648863820025 \t -0.008624202437991018\n",
            "94     \t [0.93555506 0.47855305]. \t  -15.742052906702769 \t -0.008624202437991018\n",
            "95     \t [0.6228158  0.32330682]. \t  -0.5594896262848823 \t -0.008624202437991018\n",
            "96     \t [-1.58165864  1.13644603]. \t  -193.0415290939626 \t -0.008624202437991018\n",
            "97     \t [ 0.73390107 -1.9012    ]. \t  -595.3384730621333 \t -0.008624202437991018\n",
            "98     \t [ 0.54241415 -0.67884865]. \t  -94.89430414020869 \t -0.008624202437991018\n",
            "99     \t [1.01065966 1.0096387 ]. \t  -0.014024063131267398 \t -0.008624202437991018\n",
            "100    \t [1.36675047 1.90482644]. \t  -0.27007425547960934 \t -0.008624202437991018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkq32q1utdoO",
        "outputId": "bc1e1248-d45e-4724-a88c-9ec2ecef0f98"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 9\r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_loser_9 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_9 = dGPGO(surrogate_loser_9, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.24284125 0.11273132]. \t  -205.0993354093405 \t -3.486729021084656\n",
            "init   \t [-1.56011944  0.5721352 ]. \t  -353.19808767458017 \t -3.486729021084656\n",
            "init   \t [-1.67557012 -0.68720361]. \t  -1228.4786390381805 \t -3.486729021084656\n",
            "init   \t [-0.29744764  0.22276429]. \t  -3.486729021084656 \t -3.486729021084656\n",
            "init   \t [0.52480624 0.8085215 ]. \t  -28.645360944397154 \t -3.486729021084656\n",
            "1      \t [-0.42084996  1.1040236 ]. \t  -87.93482613667966 \t -3.486729021084656\n",
            "2      \t [0.09130205 0.2771249 ]. \t  -8.050476078634611 \t -3.486729021084656\n",
            "3      \t [1.34027586 1.76760892]. \t  \u001b[92m-0.19833152756144756\u001b[0m \t -0.19833152756144756\n",
            "4      \t [-0.18756391  0.42579889]. \t  -16.66860272011216 \t -0.19833152756144756\n",
            "5      \t [0.88246179 1.50452626]. \t  -52.69055737972176 \t -0.19833152756144756\n",
            "6      \t [2.048 2.048]. \t  -461.7603900415999 \t -0.19833152756144756\n",
            "7      \t [1.27982732 1.09230691]. \t  -29.851811719281347 \t -0.19833152756144756\n",
            "8      \t [-1.44807573  1.90157698]. \t  -9.809094279930342 \t -0.19833152756144756\n",
            "9      \t [1.25353455 2.01222292]. \t  -19.501272900917254 \t -0.19833152756144756\n",
            "10     \t [-0.08309162 -0.58835563]. \t  -36.606516095015934 \t -0.19833152756144756\n",
            "11     \t [1.17058719 1.45790505]. \t  -0.7970136470260041 \t -0.19833152756144756\n",
            "12     \t [-0.28673025  0.01206275]. \t  -2.147797915596625 \t -0.19833152756144756\n",
            "13     \t [-1.05613481  1.95822733]. \t  -75.25998743816882 \t -0.19833152756144756\n",
            "14     \t [-2.02019481  1.97518937]. \t  -452.6442141264078 \t -0.19833152756144756\n",
            "15     \t [-1.10078946  1.10156659]. \t  -5.627078020735778 \t -0.19833152756144756\n",
            "16     \t [-1.23057701  1.36250531]. \t  -7.2802368441914975 \t -0.19833152756144756\n",
            "17     \t [1.2635343 1.6311403]. \t  \u001b[92m-0.1893143427914994\u001b[0m \t -0.1893143427914994\n",
            "18     \t [-0.09037941 -0.14651037]. \t  -3.5814806229643863 \t -0.1893143427914994\n",
            "19     \t [-0.02731033 -1.89361341]. \t  -359.91506875937273 \t -0.1893143427914994\n",
            "20     \t [1.25599916 1.7389015 ]. \t  -2.6694859406391584 \t -0.1893143427914994\n",
            "21     \t [-0.11723884  0.01041056]. \t  -1.2493344301130864 \t -0.1893143427914994\n",
            "22     \t [1.05813244 0.2848522 ]. \t  -69.69115789460176 \t -0.1893143427914994\n",
            "23     \t [0.40881591 0.13015602]. \t  -0.486209457228743 \t -0.1893143427914994\n",
            "24     \t [0.54315476 0.38594919]. \t  -1.0355720404930138 \t -0.1893143427914994\n",
            "25     \t [0.48449582 0.22766394]. \t  -0.27074624249731616 \t -0.1893143427914994\n",
            "26     \t [-1.85090769 -0.12665691]. \t  -1270.1647887856454 \t -0.1893143427914994\n",
            "27     \t [0.60526501 0.37829678]. \t  \u001b[92m-0.1700984758265328\u001b[0m \t -0.1700984758265328\n",
            "28     \t [0.55059748 0.34801746]. \t  -0.40320350989624154 \t -0.1700984758265328\n",
            "29     \t [0.47891703 0.23013544]. \t  -0.2715873610350972 \t -0.1700984758265328\n",
            "30     \t [0.18541668 2.02241846]. \t  -395.8934976899818 \t -0.1700984758265328\n",
            "31     \t [0.52642259 0.28736236]. \t  -0.23476464372462252 \t -0.1700984758265328\n",
            "32     \t [0.75375726 1.95812448]. \t  -193.26353764808724 \t -0.1700984758265328\n",
            "33     \t [-1.33865525  1.99377279]. \t  -9.540619700456945 \t -0.1700984758265328\n",
            "34     \t [-0.73129346  0.5964421 ]. \t  -3.377473617663559 \t -0.1700984758265328\n",
            "35     \t [0.48521557 0.25912616]. \t  -0.3211341401470047 \t -0.1700984758265328\n",
            "36     \t [0.56146413 1.02155538]. \t  -50.080178024431504 \t -0.1700984758265328\n",
            "37     \t [0.82231768 0.67927012]. \t  \u001b[92m-0.03250966188882625\u001b[0m \t -0.03250966188882625\n",
            "38     \t [-1.26709962  1.64306065]. \t  -5.28050981937662 \t -0.03250966188882625\n",
            "39     \t [1.26642229 1.5848537 ]. \t  -0.10697346879458314 \t -0.03250966188882625\n",
            "40     \t [-0.33311469  0.41650083]. \t  -11.11238496663284 \t -0.03250966188882625\n",
            "41     \t [1.26354807 1.59274078]. \t  -0.07091144896219523 \t -0.03250966188882625\n",
            "42     \t [-0.0081328  -1.64996496]. \t  -273.2765953041409 \t -0.03250966188882625\n",
            "43     \t [0.24031165 0.7048205 ]. \t  -42.4471901057517 \t -0.03250966188882625\n",
            "44     \t [ 1.61631112 -1.48418493]. \t  -1678.6311613573498 \t -0.03250966188882625\n",
            "45     \t [-1.69413816  0.3032082 ]. \t  -666.1538340808597 \t -0.03250966188882625\n",
            "46     \t [0.83432269 0.70029004]. \t  \u001b[92m-0.029209353426092856\u001b[0m \t -0.029209353426092856\n",
            "47     \t [-0.84626561  0.11723745]. \t  -39.28017645872388 \t -0.029209353426092856\n",
            "48     \t [0.29733632 0.12212438]. \t  -0.6074096774597729 \t -0.029209353426092856\n",
            "49     \t [1.29295007 1.69839462]. \t  -0.15697385171704595 \t -0.029209353426092856\n",
            "50     \t [ 1.60159055 -1.12281436]. \t  -1360.4274655446186 \t -0.029209353426092856\n",
            "51     \t [0.67147936 0.42168057]. \t  -0.19321292091175873 \t -0.029209353426092856\n",
            "52     \t [1.91380379 0.04697279]. \t  -1308.1435369146268 \t -0.029209353426092856\n",
            "53     \t [0.49652228 0.17909307]. \t  -0.7083227934445969 \t -0.029209353426092856\n",
            "54     \t [1.1134421  1.25669822]. \t  -0.04158213219869683 \t -0.029209353426092856\n",
            "55     \t [-1.76315685  0.68887059]. \t  -593.2031575612671 \t -0.029209353426092856\n",
            "56     \t [1.23161183 1.54593794]. \t  -0.13815188777512266 \t -0.029209353426092856\n",
            "57     \t [1.30376312 1.67846651]. \t  -0.1377764649882903 \t -0.029209353426092856\n",
            "58     \t [-1.73618415 -0.26471173]. \t  -1082.701722128939 \t -0.029209353426092856\n",
            "59     \t [1.00231572 1.01679636]. \t  \u001b[92m-0.014790859610477337\u001b[0m \t -0.014790859610477337\n",
            "60     \t [1.11052663 1.22428929]. \t  -0.020280356071885546 \t -0.014790859610477337\n",
            "61     \t [0.59093498 0.33493549]. \t  -0.18769367281958008 \t -0.014790859610477337\n",
            "62     \t [-2.03594066 -1.14187835]. \t  -2804.382699699828 \t -0.014790859610477337\n",
            "63     \t [0.5867128  0.25919336]. \t  -0.8939618291042228 \t -0.014790859610477337\n",
            "64     \t [-1.24991045  1.5834155 ]. \t  -5.106784346627112 \t -0.014790859610477337\n",
            "65     \t [1.50125143 1.62555719]. \t  -39.714607462353506 \t -0.014790859610477337\n",
            "66     \t [-0.22080734  1.68418939]. \t  -268.9546463000948 \t -0.014790859610477337\n",
            "67     \t [1.08240709 1.4462156 ]. \t  -7.547883620562839 \t -0.014790859610477337\n",
            "68     \t [-0.19707086  1.02216595]. \t  -98.1265751377523 \t -0.014790859610477337\n",
            "69     \t [0.98686568 1.00639444]. \t  -0.10573630645673177 \t -0.014790859610477337\n",
            "70     \t [1.25117247 1.55795791]. \t  -0.06867464607512438 \t -0.014790859610477337\n",
            "71     \t [1.28082171 1.63700312]. \t  -0.08008663068279206 \t -0.014790859610477337\n",
            "72     \t [ 0.78084473 -1.0355336 ]. \t  -270.73347291450534 \t -0.014790859610477337\n",
            "73     \t [1.24548825 1.57593378]. \t  -0.12123790820979732 \t -0.014790859610477337\n",
            "74     \t [1.57236249 1.21634884]. \t  -158.0749092992291 \t -0.014790859610477337\n",
            "75     \t [0.94808783 0.88381763]. \t  -0.025353876616298517 \t -0.014790859610477337\n",
            "76     \t [-0.53765381 -0.87006796]. \t  -136.72483573616964 \t -0.014790859610477337\n",
            "77     \t [ 1.71771102 -1.06836806]. \t  -1615.6701892856786 \t -0.014790859610477337\n",
            "78     \t [1.99507809 0.19555372]. \t  -1433.4483271429306 \t -0.014790859610477337\n",
            "79     \t [0.96865643 0.9610416 ]. \t  -0.05272190712852584 \t -0.014790859610477337\n",
            "80     \t [1.2212461  1.46544867]. \t  -0.11651529469768873 \t -0.014790859610477337\n",
            "81     \t [0.39197243 0.75268142]. \t  -36.25447406670638 \t -0.014790859610477337\n",
            "82     \t [-1.39694674  1.90175093]. \t  -5.99245488403269 \t -0.014790859610477337\n",
            "83     \t [0.59889046 0.3338352 ]. \t  -0.22256449472436432 \t -0.014790859610477337\n",
            "84     \t [-0.03631004 -0.17967916]. \t  -4.349950937713276 \t -0.014790859610477337\n",
            "85     \t [0.96054307 0.9495561 ]. \t  -0.0739883614669761 \t -0.014790859610477337\n",
            "86     \t [ 0.74943733 -1.54421299]. \t  -443.5313342993164 \t -0.014790859610477337\n",
            "87     \t [1.06260063 1.13759807]. \t  \u001b[92m-0.01110642749342224\u001b[0m \t -0.01110642749342224\n",
            "88     \t [-1.45703678  0.68005051]. \t  -214.23470547517417 \t -0.01110642749342224\n",
            "89     \t [0.94694736 0.90103115]. \t  \u001b[92m-0.004682410240807627\u001b[0m \t -0.004682410240807627\n",
            "90     \t [ 0.23336998 -0.50785557]. \t  -32.207775776733314 \t -0.004682410240807627\n",
            "91     \t [1.25023007 1.52237014]. \t  -0.22830557401902302 \t -0.004682410240807627\n",
            "92     \t [1.23135719 1.48862354]. \t  -0.1297960097379552 \t -0.004682410240807627\n",
            "93     \t [0.83485889 0.71998434]. \t  -0.0801484523398668 \t -0.004682410240807627\n",
            "94     \t [-0.00623463  0.74020108]. \t  -55.796518282112864 \t -0.004682410240807627\n",
            "95     \t [-0.99623645 -0.80853576]. \t  -328.3532776023888 \t -0.004682410240807627\n",
            "96     \t [-0.92802601  0.40819344]. \t  -24.241702126559957 \t -0.004682410240807627\n",
            "97     \t [-0.17331047  1.9859159 ]. \t  -383.9230711749065 \t -0.004682410240807627\n",
            "98     \t [-0.21997478  1.33713612]. \t  -167.57527888398772 \t -0.004682410240807627\n",
            "99     \t [0.88425004 0.74788956]. \t  -0.12905644906559613 \t -0.004682410240807627\n",
            "100    \t [1.00490746 0.99039352]. \t  -0.03783678669409362 \t -0.004682410240807627\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Aur-aRtdq-",
        "outputId": "ee7e11ee-c2af-4444-8dc6-d9326cc6898e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 10\r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_loser_10 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_10 = dGPGO(surrogate_loser_10, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.62910294 -1.57693156]. \t  -389.29291138113445 \t -8.580376531587937\n",
            "init   \t [ 1.84435861 -0.07294402]. \t  -1207.999341247548 \t -8.580376531587937\n",
            "init   \t [ 1.5256557  -1.17828534]. \t  -1229.4172568797705 \t -8.580376531587937\n",
            "init   \t [-1.88125338 -0.42109149]. \t  -1576.6245828972787 \t -8.580376531587937\n",
            "init   \t [-1.09309052  1.39977001]. \t  -8.580376531587937 \t -8.580376531587937\n",
            "1      \t [-0.86019812  2.03750805]. \t  -171.82841453589126 \t -8.580376531587937\n",
            "2      \t [-0.09725394 -1.98440571]. \t  -398.75334498229967 \t -8.580376531587937\n",
            "3      \t [-0.71787532  1.26192598]. \t  -58.689416526596894 \t -8.580376531587937\n",
            "4      \t [-1.66940512  1.65929749]. \t  -134.27750112819044 \t -8.580376531587937\n",
            "5      \t [-0.21590304  1.35197276]. \t  -171.8745365573776 \t -8.580376531587937\n",
            "6      \t [ 0.55073144 -1.79055537]. \t  -438.6270149029219 \t -8.580376531587937\n",
            "7      \t [0.98226372 1.2697215 ]. \t  -9.295464012565606 \t -8.580376531587937\n",
            "8      \t [1.31261675 1.94600611]. \t  \u001b[92m-5.072563300257509\u001b[0m \t -5.072563300257509\n",
            "9      \t [1.33885437 1.58232492]. \t  \u001b[92m-4.533482903732608\u001b[0m \t -4.533482903732608\n",
            "10     \t [0.99101191 1.48541347]. \t  -25.332062695007775 \t -4.533482903732608\n",
            "11     \t [2.048 2.048]. \t  -461.7603900415999 \t -4.533482903732608\n",
            "12     \t [0.33393033 0.2144096 ]. \t  \u001b[92m-1.5024927073103536\u001b[0m \t -1.5024927073103536\n",
            "13     \t [ 0.19059185 -0.64759425]. \t  -47.42973036097653 \t -1.5024927073103536\n",
            "14     \t [0.33229456 0.37591818]. \t  -7.494776076220637 \t -1.5024927073103536\n",
            "15     \t [1.16362594 1.2521774 ]. \t  \u001b[92m-1.0640735852769558\u001b[0m \t -1.0640735852769558\n",
            "16     \t [0.46308579 0.16444882]. \t  \u001b[92m-0.5382731530325593\u001b[0m \t -0.5382731530325593\n",
            "17     \t [0.41415073 0.21195831]. \t  \u001b[92m-0.506738360174596\u001b[0m \t -0.506738360174596\n",
            "18     \t [ 0.39018098 -0.28753843]. \t  -19.7124915961744 \t -0.506738360174596\n",
            "19     \t [1.2346141 1.5506564]. \t  \u001b[92m-0.1246575074161505\u001b[0m \t -0.1246575074161505\n",
            "20     \t [-1.87856098  1.30352094]. \t  -503.5579671203956 \t -0.1246575074161505\n",
            "21     \t [0.41637339 0.15656252]. \t  -0.3688584008621456 \t -0.1246575074161505\n",
            "22     \t [-1.06254814  1.55142248]. \t  -22.09745810159913 \t -0.1246575074161505\n",
            "23     \t [1.20862894 1.43169714]. \t  -0.12813013041855423 \t -0.1246575074161505\n",
            "24     \t [0.43119568 0.17727596]. \t  -0.3310270984721544 \t -0.1246575074161505\n",
            "25     \t [0.76153104 0.72322349]. \t  -2.110183427272423 \t -0.1246575074161505\n",
            "26     \t [1.18456536 1.3504118 ]. \t  -0.3126719000090379 \t -0.1246575074161505\n",
            "27     \t [1.23787765 1.47346854]. \t  -0.4031832918887638 \t -0.1246575074161505\n",
            "28     \t [0.35048281 1.73211457]. \t  -259.39891582077297 \t -0.1246575074161505\n",
            "29     \t [-1.5112475   2.02381614]. \t  -13.069112565943167 \t -0.1246575074161505\n",
            "30     \t [-1.81413059 -1.87982595]. \t  -2681.735608840056 \t -0.1246575074161505\n",
            "31     \t [ 0.66620021 -1.06959045]. \t  -229.15336766252585 \t -0.1246575074161505\n",
            "32     \t [0.43068812 0.20771118]. \t  -0.3734840417386905 \t -0.1246575074161505\n",
            "33     \t [ 2.02131551 -1.66183259]. \t  -3304.4750197844164 \t -0.1246575074161505\n",
            "34     \t [ 0.25121189 -0.1424468 ]. \t  -4.785937184173313 \t -0.1246575074161505\n",
            "35     \t [ 0.22527611 -1.7089182 ]. \t  -310.24317612133376 \t -0.1246575074161505\n",
            "36     \t [1.18556672 1.35932009]. \t  -0.24832596302329496 \t -0.1246575074161505\n",
            "37     \t [1.22762998 1.53356027]. \t  \u001b[92m-0.12196035733472323\u001b[0m \t -0.12196035733472323\n",
            "38     \t [1.37993224 0.76530838]. \t  -129.85472110547894 \t -0.12196035733472323\n",
            "39     \t [-0.81057464  1.07467568]. \t  -20.720868440719848 \t -0.12196035733472323\n",
            "40     \t [ 1.36429407 -1.81891743]. \t  -1354.5315001039999 \t -0.12196035733472323\n",
            "41     \t [-0.98372483  0.52651721]. \t  -23.400672927577297 \t -0.12196035733472323\n",
            "42     \t [1.17305896 0.13752364]. \t  -153.42899481728517 \t -0.12196035733472323\n",
            "43     \t [1.22825442 1.45330983]. \t  -0.3578988943997122 \t -0.12196035733472323\n",
            "44     \t [-1.09851212 -1.00399318]. \t  -493.1329517742785 \t -0.12196035733472323\n",
            "45     \t [-0.51145232 -0.40888856]. \t  -47.2377621751766 \t -0.12196035733472323\n",
            "46     \t [-0.40623635 -1.12689812]. \t  -168.8848033846799 \t -0.12196035733472323\n",
            "47     \t [ 2.00666098 -0.03070804]. \t  -1647.2598439717965 \t -0.12196035733472323\n",
            "48     \t [-0.41532174 -0.47551646]. \t  -43.99465085536307 \t -0.12196035733472323\n",
            "49     \t [ 1.28532725 -1.43822542]. \t  -955.0715968483435 \t -0.12196035733472323\n",
            "50     \t [ 0.57893847 -1.48140375]. \t  -330.1712234659606 \t -0.12196035733472323\n",
            "51     \t [-0.95174761  0.94130511]. \t  -3.9352130961413714 \t -0.12196035733472323\n",
            "52     \t [-0.01548409  0.04620613]. \t  -1.242498702837128 \t -0.12196035733472323\n",
            "53     \t [-0.55717195  0.2036472 ]. \t  -3.5652671206950464 \t -0.12196035733472323\n",
            "54     \t [ 0.13208979 -0.01989839]. \t  -0.8927412743957782 \t -0.12196035733472323\n",
            "55     \t [0.35284375 0.14920397]. \t  -0.47984618984024335 \t -0.12196035733472323\n",
            "56     \t [0.55084584 1.55763767]. \t  -157.50514222288103 \t -0.12196035733472323\n",
            "57     \t [-1.98002668 -1.70712373]. \t  -3175.901794188777 \t -0.12196035733472323\n",
            "58     \t [0.18179443 0.00102903]. \t  -0.7719895520120091 \t -0.12196035733472323\n",
            "59     \t [1.93029386 0.8663766 ]. \t  -818.6297032102251 \t -0.12196035733472323\n",
            "60     \t [-0.58186118  0.32100713]. \t  -2.533103648209388 \t -0.12196035733472323\n",
            "61     \t [1.26917016 1.63733241]. \t  -0.14288716136459284 \t -0.12196035733472323\n",
            "62     \t [-1.53711804  0.59339743]. \t  -319.4914002029559 \t -0.12196035733472323\n",
            "63     \t [1.03984855 1.01302511]. \t  -0.46752924928229944 \t -0.12196035733472323\n",
            "64     \t [ 0.34578262 -0.99307802]. \t  -124.22558745727085 \t -0.12196035733472323\n",
            "65     \t [ 1.22527161 -0.36433001]. \t  -348.1047442233006 \t -0.12196035733472323\n",
            "66     \t [0.9582054  0.88846632]. \t  \u001b[92m-0.08990390927979725\u001b[0m \t -0.08990390927979725\n",
            "67     \t [0.97253808 0.96107658]. \t  \u001b[92m-0.023999031551614876\u001b[0m \t -0.023999031551614876\n",
            "68     \t [-0.37329274  1.59657522]. \t  -214.23720569315782 \t -0.023999031551614876\n",
            "69     \t [0.97148866 0.93859269]. \t  \u001b[92m-0.0035143294462421076\u001b[0m \t -0.0035143294462421076\n",
            "70     \t [0.87729541 0.79974483]. \t  -0.10564292944820287 \t -0.0035143294462421076\n",
            "71     \t [0.84160655 0.70851516]. \t  -0.025093047031433585 \t -0.0035143294462421076\n",
            "72     \t [-1.72421319 -0.65417702]. \t  -1322.9981744186289 \t -0.0035143294462421076\n",
            "73     \t [ 1.24898057 -1.24694444]. \t  -787.9290060785235 \t -0.0035143294462421076\n",
            "74     \t [1.24007303 1.54893508]. \t  -0.07007612892122705 \t -0.0035143294462421076\n",
            "75     \t [-1.2773907  -0.23007098]. \t  -351.8156843349986 \t -0.0035143294462421076\n",
            "76     \t [-1.28250973 -1.78120988]. \t  -1178.9856005689219 \t -0.0035143294462421076\n",
            "77     \t [0.93846077 0.86695317]. \t  -0.0227083060154697 \t -0.0035143294462421076\n",
            "78     \t [0.87519568 0.78968233]. \t  -0.0718155250039523 \t -0.0035143294462421076\n",
            "79     \t [-1.17896272  0.67580822]. \t  -55.74816867150558 \t -0.0035143294462421076\n",
            "80     \t [1.01591254 1.71707928]. \t  -46.92288984327722 \t -0.0035143294462421076\n",
            "81     \t [-0.09053564 -1.38041038]. \t  -194.0122314118666 \t -0.0035143294462421076\n",
            "82     \t [-1.44375349  0.00707512]. \t  -437.5098304110983 \t -0.0035143294462421076\n",
            "83     \t [-1.76670334 -0.90938185]. \t  -1632.2464505607536 \t -0.0035143294462421076\n",
            "84     \t [ 1.70713285 -0.89835968]. \t  -1454.1393775178587 \t -0.0035143294462421076\n",
            "85     \t [0.99343787 1.01420368]. \t  -0.07448954947368197 \t -0.0035143294462421076\n",
            "86     \t [-0.796771    1.73565395]. \t  -124.40663419788791 \t -0.0035143294462421076\n",
            "87     \t [1.19319732 1.40742892]. \t  -0.06386465037581994 \t -0.0035143294462421076\n",
            "88     \t [-0.32412935 -0.05427506]. \t  -4.292079571359494 \t -0.0035143294462421076\n",
            "89     \t [-1.45123758  0.69030709]. \t  -206.45283409422657 \t -0.0035143294462421076\n",
            "90     \t [-1.81082405 -1.93748806]. \t  -2729.162850805099 \t -0.0035143294462421076\n",
            "91     \t [1.45357638 0.68649884]. \t  -203.6632813385702 \t -0.0035143294462421076\n",
            "92     \t [ 1.11334257 -1.4045816 ]. \t  -699.1463474533937 \t -0.0035143294462421076\n",
            "93     \t [-1.84946302 -1.03142848]. \t  -1990.0981513159281 \t -0.0035143294462421076\n",
            "94     \t [1.54089267 1.39196953]. \t  -96.79974380386078 \t -0.0035143294462421076\n",
            "95     \t [ 0.56901041 -0.44080524]. \t  -58.64371641368977 \t -0.0035143294462421076\n",
            "96     \t [-0.84258106  1.8072523 ]. \t  -123.80390894280549 \t -0.0035143294462421076\n",
            "97     \t [-0.36971649 -1.66008565]. \t  -324.71649979111015 \t -0.0035143294462421076\n",
            "98     \t [1.42308353 0.96051045]. \t  -113.5282996267748 \t -0.0035143294462421076\n",
            "99     \t [0.67810219 1.92564012]. \t  -214.96572407533358 \t -0.0035143294462421076\n",
            "100    \t [1.43707224 0.96877898]. \t  -120.39981096582514 \t -0.0035143294462421076\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMu7U9NNtdtn",
        "outputId": "197dca0c-eb8b-4fd1-e238-f78f3102e0e6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 11\r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_loser_11 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_11 = dGPGO(surrogate_loser_11, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.97032207 1.31583119]. \t  -659.5505379977991 \t -45.00854175284071\n",
            "init   \t [ 0.59758024 -0.3125739 ]. \t  -45.00854175284071 \t -45.00854175284071\n",
            "init   \t [-1.21933423 -0.03314987]. \t  -235.94289844157058 \t -45.00854175284071\n",
            "init   \t [-1.48046507 -0.19447384]. \t  -575.5719351651762 \t -45.00854175284071\n",
            "init   \t [-1.58214297 -2.0360213 ]. \t  -2067.0989998073865 \t -45.00854175284071\n",
            "1      \t [-0.59782787  0.66030788]. \t  \u001b[92m-11.728483422473923\u001b[0m \t -11.728483422473923\n",
            "2      \t [-0.44273339  0.04021619]. \t  \u001b[92m-4.508739622784907\u001b[0m \t -4.508739622784907\n",
            "3      \t [-1.11955987  0.65043597]. \t  -40.85082114690448 \t -4.508739622784907\n",
            "4      \t [ 1.30792565 -1.05782521]. \t  -766.5511117868322 \t -4.508739622784907\n",
            "5      \t [0.38613963 0.10451944]. \t  \u001b[92m-0.5756012078214707\u001b[0m \t -0.5756012078214707\n",
            "6      \t [-0.80037406  0.38397636]. \t  -9.826845764091287 \t -0.5756012078214707\n",
            "7      \t [-0.36159767  1.94598938]. \t  -331.36230478725497 \t -0.5756012078214707\n",
            "8      \t [ 0.0915573  -0.33597592]. \t  -12.68355659131007 \t -0.5756012078214707\n",
            "9      \t [ 0.49826633 -0.0275608 ]. \t  -7.859963003684763 \t -0.5756012078214707\n",
            "10     \t [0.05276126 0.71526245]. \t  -51.659851309855156 \t -0.5756012078214707\n",
            "11     \t [ 0.20543015 -0.0638274 ]. \t  -1.7555549345227295 \t -0.5756012078214707\n",
            "12     \t [-1.45073686  1.55274697]. \t  -36.46442125185375 \t -0.5756012078214707\n",
            "13     \t [-1.09782657 -0.40581759]. \t  -263.94610895647355 \t -0.5756012078214707\n",
            "14     \t [-0.88857968  0.87009475]. \t  -4.215094951276874 \t -0.5756012078214707\n",
            "15     \t [-0.88702882 -0.79713069]. \t  -254.45089311807683 \t -0.5756012078214707\n",
            "16     \t [ 1.92259004 -0.29129619]. \t  -1590.985338494926 \t -0.5756012078214707\n",
            "17     \t [-0.52399875 -0.16859288]. \t  -21.962321500844066 \t -0.5756012078214707\n",
            "18     \t [ 0.22555161 -1.59901435]. \t  -272.81276944773344 \t -0.5756012078214707\n",
            "19     \t [-1.17547357  1.23700797]. \t  -6.827366201300569 \t -0.5756012078214707\n",
            "20     \t [0.10088775 1.46281324]. \t  -211.8232198151821 \t -0.5756012078214707\n",
            "21     \t [0.61794618 1.65841838]. \t  -163.10673822296363 \t -0.5756012078214707\n",
            "22     \t [ 0.0099395  -1.73764558]. \t  -302.9557719589955 \t -0.5756012078214707\n",
            "23     \t [0.29559594 0.04927424]. \t  -0.6413667749220707 \t -0.5756012078214707\n",
            "24     \t [0.72442873 0.75438542]. \t  -5.3470247263402015 \t -0.5756012078214707\n",
            "25     \t [-1.61216036 -0.94126898]. \t  -1260.2170238403203 \t -0.5756012078214707\n",
            "26     \t [-1.33775942 -1.84923898]. \t  -1329.5802276803001 \t -0.5756012078214707\n",
            "27     \t [0.64497691 0.54156911]. \t  -1.7029216576256063 \t -0.5756012078214707\n",
            "28     \t [ 1.75863482 -1.59610798]. \t  -2199.157970132603 \t -0.5756012078214707\n",
            "29     \t [-1.67254638  0.24145778]. \t  -660.4323974788158 \t -0.5756012078214707\n",
            "30     \t [-1.67066864 -1.91418671]. \t  -2221.136489962148 \t -0.5756012078214707\n",
            "31     \t [0.48590246 0.22234361]. \t  \u001b[92m-0.28322340436136556\u001b[0m \t -0.28322340436136556\n",
            "32     \t [-0.96901408 -0.41602442]. \t  -187.4829578926612 \t -0.28322340436136556\n",
            "33     \t [ 0.28557859 -0.87338187]. \t  -91.7008658277363 \t -0.28322340436136556\n",
            "34     \t [1.694842   1.68393527]. \t  -141.74889602888092 \t -0.28322340436136556\n",
            "35     \t [1.33505181 1.86729779]. \t  -0.8336459762839883 \t -0.28322340436136556\n",
            "36     \t [ 0.94980219 -0.49364355]. \t  -194.81928355041182 \t -0.28322340436136556\n",
            "37     \t [1.4656905  2.04725089]. \t  -1.2369219481070255 \t -0.28322340436136556\n",
            "38     \t [1.75118987 0.07716607]. \t  -894.2752412813545 \t -0.28322340436136556\n",
            "39     \t [0.77561893 0.44447291]. \t  -2.5187588695303353 \t -0.28322340436136556\n",
            "40     \t [1.18672693 1.5916093 ]. \t  -3.394333717925701 \t -0.28322340436136556\n",
            "41     \t [-0.84178006  1.97879725]. \t  -164.73386512539443 \t -0.28322340436136556\n",
            "42     \t [1.45709914 1.99815482]. \t  -1.7710167983531853 \t -0.28322340436136556\n",
            "43     \t [-0.0834664   1.76338305]. \t  -309.67376133433913 \t -0.28322340436136556\n",
            "44     \t [-0.56907761 -0.55733849]. \t  -80.11120082104294 \t -0.28322340436136556\n",
            "45     \t [ 0.13646305 -0.627146  ]. \t  -42.4473482539538 \t -0.28322340436136556\n",
            "46     \t [0.62933856 0.32434989]. \t  -0.6517246168226211 \t -0.28322340436136556\n",
            "47     \t [-0.61562099 -0.90126674]. \t  -166.51575971908775 \t -0.28322340436136556\n",
            "48     \t [0.55772191 0.29649741]. \t  \u001b[92m-0.21679854666071596\u001b[0m \t -0.21679854666071596\n",
            "49     \t [-1.1296874  -0.20714311]. \t  -224.56435014526477 \t -0.21679854666071596\n",
            "50     \t [0.59493351 0.34989462]. \t  \u001b[92m-0.16572013193456184\u001b[0m \t -0.16572013193456184\n",
            "51     \t [-0.02905022  0.00453234]. \t  -1.0603048036272553 \t -0.16572013193456184\n",
            "52     \t [-2.00016163  1.27511611]. \t  -751.8525811840013 \t -0.16572013193456184\n",
            "53     \t [-1.39269204  1.89928462]. \t  -5.887436572264647 \t -0.16572013193456184\n",
            "54     \t [-1.34635848  0.99460828]. \t  -72.42972137593217 \t -0.16572013193456184\n",
            "55     \t [0.71509222 0.4660443 ]. \t  -0.28649553412861256 \t -0.16572013193456184\n",
            "56     \t [ 0.78222375 -0.59035082]. \t  -144.58187812082383 \t -0.16572013193456184\n",
            "57     \t [-0.33202664 -1.0029603 ]. \t  -125.69616167866137 \t -0.16572013193456184\n",
            "58     \t [-0.99139494  1.52759432]. \t  -33.63877408109947 \t -0.16572013193456184\n",
            "59     \t [-1.20585533 -0.44728188]. \t  -366.38619263800217 \t -0.16572013193456184\n",
            "60     \t [ 1.36458872 -0.59491836]. \t  -603.8280138637097 \t -0.16572013193456184\n",
            "61     \t [0.30952834 0.29868557]. \t  -4.592690227278553 \t -0.16572013193456184\n",
            "62     \t [-0.02256099 -1.87738528]. \t  -353.69432248362006 \t -0.16572013193456184\n",
            "63     \t [-0.52472305  1.61408781]. \t  -181.55088101018254 \t -0.16572013193456184\n",
            "64     \t [0.87856461 0.76935759]. \t  \u001b[92m-0.01538067563656944\u001b[0m \t -0.01538067563656944\n",
            "65     \t [0.34149461 1.87892436]. \t  -311.0057994969181 \t -0.01538067563656944\n",
            "66     \t [-1.82360152 -0.2660919 ]. \t  -1297.942128509376 \t -0.01538067563656944\n",
            "67     \t [-1.13713511 -0.21115239]. \t  -230.83773110929698 \t -0.01538067563656944\n",
            "68     \t [-0.0171681   1.28735797]. \t  -166.6878061711111 \t -0.01538067563656944\n",
            "69     \t [-0.52678452  0.87257878]. \t  -37.74271649707644 \t -0.01538067563656944\n",
            "70     \t [0.04898973 0.33712364]. \t  -12.10841225009147 \t -0.01538067563656944\n",
            "71     \t [-0.01404161 -0.22677107]. \t  -6.179738497208102 \t -0.01538067563656944\n",
            "72     \t [0.83353528 0.68758315]. \t  -0.032891494022426415 \t -0.01538067563656944\n",
            "73     \t [0.82208687 1.0993916 ]. \t  -17.972365520672504 \t -0.01538067563656944\n",
            "74     \t [-0.49806933  1.27422427]. \t  -107.54284260181514 \t -0.01538067563656944\n",
            "75     \t [0.77319092 0.92358938]. \t  -10.663737077866921 \t -0.01538067563656944\n",
            "76     \t [ 0.72625056 -1.8458145 ]. \t  -563.308570745773 \t -0.01538067563656944\n",
            "77     \t [-1.44839121  0.77855851]. \t  -180.04421945941743 \t -0.01538067563656944\n",
            "78     \t [ 0.28985867 -1.50123496]. \t  -251.80701272330782 \t -0.01538067563656944\n",
            "79     \t [ 1.59307307 -1.7643583 ]. \t  -1851.2787304984117 \t -0.01538067563656944\n",
            "80     \t [-2.03817801  1.59554601]. \t  -663.8859957234097 \t -0.01538067563656944\n",
            "81     \t [ 0.50290089 -0.10287362]. \t  -12.90525671117939 \t -0.01538067563656944\n",
            "82     \t [ 1.519587   -0.68308356]. \t  -895.6129325439816 \t -0.01538067563656944\n",
            "83     \t [0.25956166 1.4313478 ]. \t  -186.59117783448178 \t -0.01538067563656944\n",
            "84     \t [ 1.256649   -1.63260162]. \t  -1031.6114516563475 \t -0.01538067563656944\n",
            "85     \t [-1.53789396 -0.57477465]. \t  -870.7376845511368 \t -0.01538067563656944\n",
            "86     \t [0.29759459 1.07304245]. \t  -97.4134417901508 \t -0.01538067563656944\n",
            "87     \t [-1.78556404 -1.65965977]. \t  -2357.9715377964612 \t -0.01538067563656944\n",
            "88     \t [-1.01750388 -0.1158688 ]. \t  -136.5925399122599 \t -0.01538067563656944\n",
            "89     \t [0.36842466 1.79975852]. \t  -277.29573833523074 \t -0.01538067563656944\n",
            "90     \t [-1.0695059   0.02320873]. \t  -129.8649439623663 \t -0.01538067563656944\n",
            "91     \t [0.78007427 0.62671844]. \t  -0.08150071301785675 \t -0.01538067563656944\n",
            "92     \t [-1.79594354  1.39508404]. \t  -342.82778427199776 \t -0.01538067563656944\n",
            "93     \t [0.84468995 0.67635742]. \t  -0.1620866730966448 \t -0.01538067563656944\n",
            "94     \t [ 0.10746123 -1.0786201 ]. \t  -119.64325677315229 \t -0.01538067563656944\n",
            "95     \t [0.68608758 1.26055941]. \t  -62.48377473932666 \t -0.01538067563656944\n",
            "96     \t [0.91835148 0.76807707]. \t  -0.5735606511488468 \t -0.01538067563656944\n",
            "97     \t [-1.81455988 -0.57823758]. \t  -1506.2814452342047 \t -0.01538067563656944\n",
            "98     \t [0.6486892  1.18761898]. \t  -58.92490993311827 \t -0.01538067563656944\n",
            "99     \t [-1.09863269  0.1074321 ]. \t  -125.3078511226666 \t -0.01538067563656944\n",
            "100    \t [-0.61278378 -0.73685194]. \t  -126.33463761962125 \t -0.01538067563656944\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEjKllHqtdwW",
        "outputId": "8a3f6912-38b2-43b8-9282-0c09e9e10da0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_loser_12 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_12 = dGPGO(surrogate_loser_12, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.26353633  0.50015753]. \t  -125.32555445527122 \t -19.52113145175031\n",
            "init   \t [-0.25506718  1.16882876]. \t  -123.40590278793995 \t -19.52113145175031\n",
            "init   \t [ 1.14678091 -0.93146069]. \t  -504.72793807789463 \t -19.52113145175031\n",
            "init   \t [-0.91560241  1.23646844]. \t  -19.52113145175031 \t -19.52113145175031\n",
            "init   \t [1.87653879 1.53982007]. \t  -393.43336554061835 \t -19.52113145175031\n",
            "1      \t [-1.50623477  1.94347507]. \t  \u001b[92m-16.861146488467053\u001b[0m \t -16.861146488467053\n",
            "2      \t [-1.17298972  1.69858754]. \t  \u001b[92m-15.134293586652838\u001b[0m \t -15.134293586652838\n",
            "3      \t [-1.88219337  1.81269391]. \t  -307.58250276319126 \t -15.134293586652838\n",
            "4      \t [-0.74187204  1.95815573]. \t  -201.21902316932247 \t -15.134293586652838\n",
            "5      \t [-0.46123599  0.48505636]. \t  \u001b[92m-9.550904750813473\u001b[0m \t -9.550904750813473\n",
            "6      \t [-0.71357184  0.79843935]. \t  -11.303149762215897 \t -9.550904750813473\n",
            "7      \t [-2.048 -2.048]. \t  -3905.9262268415996 \t -9.550904750813473\n",
            "8      \t [-1.53349463  2.03551595]. \t  -16.409873778429322 \t -9.550904750813473\n",
            "9      \t [-1.11646688  1.08871131]. \t  \u001b[92m-6.969105605243006\u001b[0m \t -6.969105605243006\n",
            "10     \t [-1.01357959  1.07085675]. \t  \u001b[92m-4.243842336959228\u001b[0m \t -4.243842336959228\n",
            "11     \t [-0.12044632 -0.03853364]. \t  \u001b[92m-1.5367342684524132\u001b[0m \t -1.5367342684524132\n",
            "12     \t [-0.14909959  0.03215739]. \t  \u001b[92m-1.3302838181961192\u001b[0m \t -1.3302838181961192\n",
            "13     \t [-0.38858978  0.14104994]. \t  -1.9380859667320358 \t -1.3302838181961192\n",
            "14     \t [-0.26504228  0.09323753]. \t  -1.6531865199500224 \t -1.3302838181961192\n",
            "15     \t [-1.53137776 -1.39676552]. \t  -1406.5769775535598 \t -1.3302838181961192\n",
            "16     \t [-0.85576725 -0.68931784]. \t  -205.55428566200078 \t -1.3302838181961192\n",
            "17     \t [-1.60659348  0.49208595]. \t  -443.21010900556564 \t -1.3302838181961192\n",
            "18     \t [-0.80151381  0.52969875]. \t  -4.516158736222404 \t -1.3302838181961192\n",
            "19     \t [ 0.03298517 -0.06661061]. \t  -1.3934281580334764 \t -1.3302838181961192\n",
            "20     \t [1.57587961 0.46549742]. \t  -407.5233249662723 \t -1.3302838181961192\n",
            "21     \t [-0.604194    0.34618626]. \t  -2.609023949170781 \t -1.3302838181961192\n",
            "22     \t [-0.46813722  0.56682228]. \t  -14.242857785593285 \t -1.3302838181961192\n",
            "23     \t [-0.00347882 -0.04844968]. \t  \u001b[92m-1.2418241604000273\u001b[0m \t -1.2418241604000273\n",
            "24     \t [1.15984121 0.29531421]. \t  -110.25821090265927 \t -1.2418241604000273\n",
            "25     \t [-0.64286013  0.47116103]. \t  -3.0341363514341926 \t -1.2418241604000273\n",
            "26     \t [ 0.01950453 -0.01516482]. \t  \u001b[92m-0.9855368393852267\u001b[0m \t -0.9855368393852267\n",
            "27     \t [ 0.62192199 -1.03710214]. \t  -202.88896043583838 \t -0.9855368393852267\n",
            "28     \t [-0.78944214  1.03138771]. \t  -19.862281627288276 \t -0.9855368393852267\n",
            "29     \t [0.57550973 0.16635393]. \t  -2.897992216024607 \t -0.9855368393852267\n",
            "30     \t [-0.19222577  0.17077657]. \t  -3.212337453204319 \t -0.9855368393852267\n",
            "31     \t [1.35840484 0.76388173]. \t  -117.06715361980729 \t -0.9855368393852267\n",
            "32     \t [0.86590902 0.71663559]. \t  \u001b[92m-0.12795775949515567\u001b[0m \t -0.12795775949515567\n",
            "33     \t [-0.05827082 -1.51731853]. \t  -232.3770506578901 \t -0.12795775949515567\n",
            "34     \t [0.79417208 0.57440766]. \t  -0.359352620047412 \t -0.12795775949515567\n",
            "35     \t [0.94324962 0.71135666]. \t  -3.18456323762495 \t -0.12795775949515567\n",
            "36     \t [ 2.04163797 -0.03123183]. \t  -1764.679667609521 \t -0.12795775949515567\n",
            "37     \t [-0.65059558 -0.51476499]. \t  -90.71629548434643 \t -0.12795775949515567\n",
            "38     \t [0.51607066 1.36921625]. \t  -121.8702319388875 \t -0.12795775949515567\n",
            "39     \t [ 0.21458814 -1.62712584]. \t  -280.5679665819224 \t -0.12795775949515567\n",
            "40     \t [-0.18489852  0.72966304]. \t  -49.77261283862049 \t -0.12795775949515567\n",
            "41     \t [0.81858808 0.63245375]. \t  -0.17453225259032354 \t -0.12795775949515567\n",
            "42     \t [-0.11661194 -1.42082376]. \t  -207.00350025199268 \t -0.12795775949515567\n",
            "43     \t [ 1.21019142 -0.02411951]. \t  -221.66182284279884 \t -0.12795775949515567\n",
            "44     \t [-1.5448088  -0.08848165]. \t  -618.996911885718 \t -0.12795775949515567\n",
            "45     \t [0.67551616 0.98433008]. \t  -27.984534946266628 \t -0.12795775949515567\n",
            "46     \t [1.63519227 0.98986453]. \t  -283.98544470693525 \t -0.12795775949515567\n",
            "47     \t [ 1.49332523 -0.11835829]. \t  -551.7315501101823 \t -0.12795775949515567\n",
            "48     \t [0.68674736 0.50070317]. \t  -0.1826990343353264 \t -0.12795775949515567\n",
            "49     \t [1.83379956 0.83301368]. \t  -640.6876431922506 \t -0.12795775949515567\n",
            "50     \t [1.03717996 1.56330399]. \t  -23.773025968055446 \t -0.12795775949515567\n",
            "51     \t [1.11283519 1.3223822 ]. \t  -0.7179965892568682 \t -0.12795775949515567\n",
            "52     \t [0.64973467 0.43817563]. \t  -0.14835138713011253 \t -0.12795775949515567\n",
            "53     \t [0.32815066 0.12835009]. \t  -0.49409502814461187 \t -0.12795775949515567\n",
            "54     \t [0.61157689 0.43333692]. \t  -0.5026475610597363 \t -0.12795775949515567\n",
            "55     \t [1.01104666 1.04797638]. \t  \u001b[92m-0.06648509470256328\u001b[0m \t -0.06648509470256328\n",
            "56     \t [1.0717823  1.22198261]. \t  -0.541933135027493 \t -0.06648509470256328\n",
            "57     \t [1.08715175 1.13027436]. \t  -0.2741049722866009 \t -0.06648509470256328\n",
            "58     \t [-1.16058688 -1.8209149 ]. \t  -1008.212481886966 \t -0.06648509470256328\n",
            "59     \t [0.95891728 0.94429174]. \t  \u001b[92m-0.06304005132943308\u001b[0m \t -0.06304005132943308\n",
            "60     \t [1.09426025 1.18271284]. \t  \u001b[92m-0.030472405474027683\u001b[0m \t -0.030472405474027683\n",
            "61     \t [0.92135239 0.85127538]. \t  \u001b[92m-0.006754344787098047\u001b[0m \t -0.006754344787098047\n",
            "62     \t [1.97180658 0.20350603]. \t  -1358.509591995981 \t -0.006754344787098047\n",
            "63     \t [0.94717848 0.87696423]. \t  -0.04352485045747276 \t -0.006754344787098047\n",
            "64     \t [0.62819858 0.44843553]. \t  -0.4277026147132267 \t -0.006754344787098047\n",
            "65     \t [1.01651248 1.00777454]. \t  -0.06541545522848757 \t -0.006754344787098047\n",
            "66     \t [-0.82463195 -1.82694224]. \t  -631.8141740341131 \t -0.006754344787098047\n",
            "67     \t [-0.88416439  0.69309902]. \t  -4.335916078337832 \t -0.006754344787098047\n",
            "68     \t [-0.20230161  1.30356964]. \t  -160.8724390811152 \t -0.006754344787098047\n",
            "69     \t [0.58790828 0.74704427]. \t  -16.28266764067562 \t -0.006754344787098047\n",
            "70     \t [-0.15007015 -1.21747465]. \t  -155.08159606040635 \t -0.006754344787098047\n",
            "71     \t [1.09669711 1.19545764]. \t  -0.014660256667027569 \t -0.006754344787098047\n",
            "72     \t [1.19891441 1.3948363 ]. \t  -0.22069774735029318 \t -0.006754344787098047\n",
            "73     \t [0.59368062 0.84720693]. \t  -24.64287629699933 \t -0.006754344787098047\n",
            "74     \t [1.09560986 1.22984276]. \t  -0.09605886646729314 \t -0.006754344787098047\n",
            "75     \t [0.84403032 0.72180358]. \t  -0.03319341543954549 \t -0.006754344787098047\n",
            "76     \t [-1.59583951 -1.82759243]. \t  -1920.1850770306987 \t -0.006754344787098047\n",
            "77     \t [0.12714667 0.02881252]. \t  -0.7778656902381174 \t -0.006754344787098047\n",
            "78     \t [1.10110496 1.15181613]. \t  -0.37765210595269305 \t -0.006754344787098047\n",
            "79     \t [0.277836   0.11895751]. \t  -0.6959495754691443 \t -0.006754344787098047\n",
            "80     \t [-0.19349692 -1.86509339]. \t  -363.38816596376915 \t -0.006754344787098047\n",
            "81     \t [1.01175657 1.00353431]. \t  -0.040607732046360645 \t -0.006754344787098047\n",
            "82     \t [1.42966273 1.46328414]. \t  -33.90021318514245 \t -0.006754344787098047\n",
            "83     \t [0.54479177 0.32961654]. \t  -0.3149197295522605 \t -0.006754344787098047\n",
            "84     \t [-1.08094176  1.62134052]. \t  -24.84265147854368 \t -0.006754344787098047\n",
            "85     \t [-0.18566239 -0.61609005]. \t  -43.72870176139577 \t -0.006754344787098047\n",
            "86     \t [1.16523437 1.33844182]. \t  -0.064664642663035 \t -0.006754344787098047\n",
            "87     \t [1.08539507 1.1961194 ]. \t  -0.03982544057882036 \t -0.006754344787098047\n",
            "88     \t [0.13530442 0.06395526]. \t  -0.9560722181291014 \t -0.006754344787098047\n",
            "89     \t [ 0.7760306  -1.36997905]. \t  -389.0084461128361 \t -0.006754344787098047\n",
            "90     \t [-1.22173971  1.70618719]. \t  -9.49602906924913 \t -0.006754344787098047\n",
            "91     \t [1.33601794 1.2571533 ]. \t  -27.969202793068458 \t -0.006754344787098047\n",
            "92     \t [-0.39760952  0.73843906]. \t  -35.633428818966976 \t -0.006754344787098047\n",
            "93     \t [-0.76961064  1.86473579]. \t  -165.04066698652198 \t -0.006754344787098047\n",
            "94     \t [0.74943408 0.58941087]. \t  -0.13984186202876087 \t -0.006754344787098047\n",
            "95     \t [-1.25939561 -0.5085482 ]. \t  -443.8504713348479 \t -0.006754344787098047\n",
            "96     \t [-1.85875106  1.24298154]. \t  -497.45533978245345 \t -0.006754344787098047\n",
            "97     \t [-1.13354993  2.00624636]. \t  -56.58097840260251 \t -0.006754344787098047\n",
            "98     \t [1.13592812 1.28181978]. \t  -0.02572342235004445 \t -0.006754344787098047\n",
            "99     \t [-1.88399263  0.56730864]. \t  -897.6211402026098 \t -0.006754344787098047\n",
            "100    \t [2.00461881 1.6776865 ]. \t  -548.9484404727244 \t -0.006754344787098047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoONg4VEtdy-",
        "outputId": "a05981f0-d15b-4f0a-d389-1bed9f0c4fb0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 13\r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_loser_13 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_13 = dGPGO(surrogate_loser_13, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.65799909 -0.35389173]. \t  -62.0309701572776 \t -62.0309701572776\n",
            "init   \t [ 0.68854808 -1.10615174]. \t  -249.81607110978882 \t -62.0309701572776\n",
            "init   \t [1.26025046 0.56040843]. \t  -105.7097019264073 \t -62.0309701572776\n",
            "init   \t [-1.34269398 -0.98145948]. \t  -780.713409392333 \t -62.0309701572776\n",
            "init   \t [ 1.70115068 -0.15230767]. \t  -928.4380405275942 \t -62.0309701572776\n",
            "1      \t [0.41169197 0.76902621]. \t  \u001b[92m-36.290440627725864\u001b[0m \t -36.290440627725864\n",
            "2      \t [1.56732557 1.62159952]. \t  -70.02931566657841 \t -36.290440627725864\n",
            "3      \t [0.44708238 0.08707663]. \t  \u001b[92m-1.578237817391173\u001b[0m \t -1.578237817391173\n",
            "4      \t [0.92702252 0.86460314]. \t  \u001b[92m-0.008063513247388177\u001b[0m \t -0.008063513247388177\n",
            "5      \t [-0.90131356  1.97423118]. \t  -138.6080292653433 \t -0.008063513247388177\n",
            "6      \t [0.67987852 0.5941335 ]. \t  -1.8422045489389927 \t -0.008063513247388177\n",
            "7      \t [ 0.85956863 -0.72063444]. \t  -213.0316067957737 \t -0.008063513247388177\n",
            "8      \t [1.19067678 1.85217997]. \t  -18.91266804836645 \t -0.008063513247388177\n",
            "9      \t [1.09067098 2.00432563]. \t  -66.3920042074066 \t -0.008063513247388177\n",
            "10     \t [-0.76612754  0.98864126]. \t  -19.254680648051 \t -0.008063513247388177\n",
            "11     \t [1.17575603 1.29780787]. \t  -0.7465109927057603 \t -0.008063513247388177\n",
            "12     \t [0.72586704 0.37811578]. \t  -2.2883160295173526 \t -0.008063513247388177\n",
            "13     \t [0.55322139 0.20508206]. \t  -1.2191423564326112 \t -0.008063513247388177\n",
            "14     \t [-1.33206651 -0.77282734]. \t  -654.2758580168237 \t -0.008063513247388177\n",
            "15     \t [-0.59369595  0.53069577]. \t  -5.716135202780976 \t -0.008063513247388177\n",
            "16     \t [-2.02415053  1.81206522]. \t  -531.3228965450492 \t -0.008063513247388177\n",
            "17     \t [0.58018287 0.31688201]. \t  -0.21517432057523567 \t -0.008063513247388177\n",
            "18     \t [1.19393231 1.53699067]. \t  -1.2811985513383701 \t -0.008063513247388177\n",
            "19     \t [-1.54062542  0.92282972]. \t  -216.90694503923066 \t -0.008063513247388177\n",
            "20     \t [0.99154575 0.94236248]. \t  -0.16653947694844695 \t -0.008063513247388177\n",
            "21     \t [-0.62056197 -1.76656905]. \t  -465.5929716936383 \t -0.008063513247388177\n",
            "22     \t [-1.17388695 -0.00878645]. \t  -197.0463823630232 \t -0.008063513247388177\n",
            "23     \t [0.85951184 0.66720746]. \t  -0.5317220809777736 \t -0.008063513247388177\n",
            "24     \t [ 0.53430913 -1.11507241]. \t  -196.37332171209164 \t -0.008063513247388177\n",
            "25     \t [-2.04602583 -0.00310691]. \t  -1764.3256972698305 \t -0.008063513247388177\n",
            "26     \t [-0.57281853 -0.60662065]. \t  -89.84796575492754 \t -0.008063513247388177\n",
            "27     \t [-0.52017856 -0.05716044]. \t  -13.052697998264273 \t -0.008063513247388177\n",
            "28     \t [-0.93027194  0.59447042]. \t  -11.066552780660691 \t -0.008063513247388177\n",
            "29     \t [0.79168256 1.72100923]. \t  -119.78125431659336 \t -0.008063513247388177\n",
            "30     \t [-0.41162842  0.99998671]. \t  -70.97381704737913 \t -0.008063513247388177\n",
            "31     \t [ 1.10650968 -0.76968109]. \t  -397.6327937907719 \t -0.008063513247388177\n",
            "32     \t [0.48552354 0.24405069]. \t  -0.27160425875247024 \t -0.008063513247388177\n",
            "33     \t [0.55486651 0.34667633]. \t  -0.3486838395435433 \t -0.008063513247388177\n",
            "34     \t [ 1.19751534 -0.68056059]. \t  -447.1938388605762 \t -0.008063513247388177\n",
            "35     \t [-1.77977978  0.80529652]. \t  -565.7825354395953 \t -0.008063513247388177\n",
            "36     \t [-2.00269429 -0.03686543]. \t  -1647.363110952309 \t -0.008063513247388177\n",
            "37     \t [-0.05723731 -0.00916061]. \t  -1.1332179262815876 \t -0.008063513247388177\n",
            "38     \t [0.83671297 1.25651821]. \t  -30.988055192015178 \t -0.008063513247388177\n",
            "39     \t [0.55016118 0.30517732]. \t  -0.20297995737585128 \t -0.008063513247388177\n",
            "40     \t [1.0275188  1.04328329]. \t  -0.016411293445117516 \t -0.008063513247388177\n",
            "41     \t [ 2.02097812 -0.56767553]. \t  -2165.1789393183 \t -0.008063513247388177\n",
            "42     \t [1.06708043 1.1464565 ]. \t  -0.010577331352798142 \t -0.008063513247388177\n",
            "43     \t [ 0.44738142 -1.75945227]. \t  -384.3095459218289 \t -0.008063513247388177\n",
            "44     \t [1.0461974  1.46347809]. \t  -13.614477131329513 \t -0.008063513247388177\n",
            "45     \t [ 0.23220122 -0.07927605]. \t  -2.363564553793089 \t -0.008063513247388177\n",
            "46     \t [1.04925422 1.13268521]. \t  -0.1032373244539513 \t -0.008063513247388177\n",
            "47     \t [1.02962678 1.11592012]. \t  -0.3121167904891954 \t -0.008063513247388177\n",
            "48     \t [ 0.19960659 -1.90024749]. \t  -377.0356578945621 \t -0.008063513247388177\n",
            "49     \t [0.35094864 0.10836632]. \t  -0.44316761714796343 \t -0.008063513247388177\n",
            "50     \t [1.01273991 1.08052579]. \t  -0.30138398593510035 \t -0.008063513247388177\n",
            "51     \t [-0.69694881  1.95944562]. \t  -220.0611550680011 \t -0.008063513247388177\n",
            "52     \t [-1.05415735 -1.03439792]. \t  -464.5990871560298 \t -0.008063513247388177\n",
            "53     \t [0.92359552 0.57127259]. \t  -7.944488047024072 \t -0.008063513247388177\n",
            "54     \t [-0.3029216   0.63837222]. \t  -31.575932970358767 \t -0.008063513247388177\n",
            "55     \t [0.88439215 0.80153632]. \t  -0.050950171115834816 \t -0.008063513247388177\n",
            "56     \t [ 0.11572348 -0.02973209]. \t  -0.9679130526096114 \t -0.008063513247388177\n",
            "57     \t [-0.54087444 -1.81978742]. \t  -448.5691858428467 \t -0.008063513247388177\n",
            "58     \t [0.98889748 0.97856586]. \t  \u001b[92m-0.00016520705189970985\u001b[0m \t -0.00016520705189970985\n",
            "59     \t [1.49541888 1.13688769]. \t  -121.1112632568946 \t -0.00016520705189970985\n",
            "60     \t [0.53232314 0.29460774]. \t  -0.23135499598464895 \t -0.00016520705189970985\n",
            "61     \t [ 1.18729071 -1.45128176]. \t  -818.5334177883941 \t -0.00016520705189970985\n",
            "62     \t [0.86514858 0.74837069]. \t  -0.01818614590869873 \t -0.00016520705189970985\n",
            "63     \t [-0.38430323 -0.14393198]. \t  -10.420573371876165 \t -0.00016520705189970985\n",
            "64     \t [-0.63342095  0.81723983]. \t  -19.975139704498243 \t -0.00016520705189970985\n",
            "65     \t [-1.07156387  1.22114234]. \t  -4.8227190361964345 \t -0.00016520705189970985\n",
            "66     \t [0.92379814 0.87338444]. \t  -0.045732531894914434 \t -0.00016520705189970985\n",
            "67     \t [ 0.51757545 -1.3693464 ]. \t  -268.28518421901623 \t -0.00016520705189970985\n",
            "68     \t [ 0.26761975 -0.19500353]. \t  -7.645209130738282 \t -0.00016520705189970985\n",
            "69     \t [0.11058556 0.28852397]. \t  -8.42493972861492 \t -0.00016520705189970985\n",
            "70     \t [-0.85830098 -1.40888248]. \t  -463.7973652642426 \t -0.00016520705189970985\n",
            "71     \t [ 1.79482222 -1.31531882]. \t  -2058.8015441070706 \t -0.00016520705189970985\n",
            "72     \t [-1.27769429  0.11797437]. \t  -234.5674956047715 \t -0.00016520705189970985\n",
            "73     \t [-0.19177921  1.56395293]. \t  -234.64627761659645 \t -0.00016520705189970985\n",
            "74     \t [ 0.79547643 -1.96309743]. \t  -673.9012232044952 \t -0.00016520705189970985\n",
            "75     \t [1.06935728 1.1365664 ]. \t  -0.009652645072618482 \t -0.00016520705189970985\n",
            "76     \t [-1.50435297 -0.48764829]. \t  -762.9212151557622 \t -0.00016520705189970985\n",
            "77     \t [-1.1056022   1.75222029]. \t  -32.509153433479376 \t -0.00016520705189970985\n",
            "78     \t [ 1.93059903 -0.51219088]. \t  -1798.120198187391 \t -0.00016520705189970985\n",
            "79     \t [1.37079816 1.93049847]. \t  -0.40179896032702306 \t -0.00016520705189970985\n",
            "80     \t [0.8268194  0.66066641]. \t  -0.08272558750586026 \t -0.00016520705189970985\n",
            "81     \t [1.40751444 2.00447791]. \t  -0.220735180708408 \t -0.00016520705189970985\n",
            "82     \t [-0.6357308  -1.23168039]. \t  -270.2709147453694 \t -0.00016520705189970985\n",
            "83     \t [-0.19146048 -0.65550324]. \t  -49.32817336411976 \t -0.00016520705189970985\n",
            "84     \t [1.34792586 0.78290787]. \t  -107.03587823357041 \t -0.00016520705189970985\n",
            "85     \t [0.97552344 0.9380648 ]. \t  -0.019043964394079054 \t -0.00016520705189970985\n",
            "86     \t [ 1.22373925 -0.18190517]. \t  -282.102907398439 \t -0.00016520705189970985\n",
            "87     \t [-1.95040851 -1.82662026]. \t  -3179.1984984099763 \t -0.00016520705189970985\n",
            "88     \t [-1.08662028  1.66416685]. \t  -27.723784719500078 \t -0.00016520705189970985\n",
            "89     \t [1.09151392 1.17762453]. \t  -0.027358400318382357 \t -0.00016520705189970985\n",
            "90     \t [0.05516837 0.74965998]. \t  -56.63631662900497 \t -0.00016520705189970985\n",
            "91     \t [-2.03381803 -0.29522442]. \t  -1973.147534557621 \t -0.00016520705189970985\n",
            "92     \t [0.88204445 0.54686205]. \t  -5.35650045213236 \t -0.00016520705189970985\n",
            "93     \t [-0.73415056 -0.96868031]. \t  -230.31034784815657 \t -0.00016520705189970985\n",
            "94     \t [0.89875558 1.62848863]. \t  -67.36953814857557 \t -0.00016520705189970985\n",
            "95     \t [-1.59789089 -1.06368423]. \t  -1314.9741894616814 \t -0.00016520705189970985\n",
            "96     \t [-0.07486746 -1.21481139]. \t  -150.0969901918518 \t -0.00016520705189970985\n",
            "97     \t [-0.10064113 -0.19189334]. \t  -5.292698725182635 \t -0.00016520705189970985\n",
            "98     \t [-0.40566945  0.12810339]. \t  -2.1088712429628327 \t -0.00016520705189970985\n",
            "99     \t [-1.95537468 -1.02301032]. \t  -2357.5909153464518 \t -0.00016520705189970985\n",
            "100    \t [ 1.8715921  -0.93847778]. \t  -1973.3051177606942 \t -0.00016520705189970985\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoQHSe2Dtd1m",
        "outputId": "3cfa7d14-6a5f-4bb2-b02a-78b22bee5f65"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 14\r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_loser_14 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_14 = dGPGO(surrogate_loser_14, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.6043691  0.02928512]. \t  -11.444570460525577 \t -4.306489127802793\n",
            "init   \t [0.11608629 1.6231842 ]. \t  -259.89734211112767 \t -4.306489127802793\n",
            "init   \t [0.81916392 0.87776093]. \t  -4.306489127802793 \t -4.306489127802793\n",
            "init   \t [ 0.89021801 -1.13533148]. \t  -371.66089065122344 \t -4.306489127802793\n",
            "init   \t [-1.33056707 -0.17677726]. \t  -384.58487399689074 \t -4.306489127802793\n",
            "1      \t [1.20014634 0.52224222]. \t  -84.33247643037394 \t -4.306489127802793\n",
            "2      \t [0.3990925  0.43403903]. \t  -7.910626836147134 \t -4.306489127802793\n",
            "3      \t [0.51527608 0.27463977]. \t  \u001b[92m-0.2432935527026396\u001b[0m \t -0.2432935527026396\n",
            "4      \t [1.53137544 1.8120952 ]. \t  -28.692917519939073 \t -0.2432935527026396\n",
            "5      \t [1.11969363 1.26054942]. \t  \u001b[92m-0.018999105716718055\u001b[0m \t -0.018999105716718055\n",
            "6      \t [1.02729802 1.2165308 ]. \t  -2.5989529034397827 \t -0.018999105716718055\n",
            "7      \t [1.44865337 1.35082366]. \t  -56.1177263844637 \t -0.018999105716718055\n",
            "8      \t [1.29740297 1.73971131]. \t  -0.40718596900658544 \t -0.018999105716718055\n",
            "9      \t [-1.24309755  1.69366197]. \t  -7.232865704168191 \t -0.018999105716718055\n",
            "10     \t [0.56958754 0.36912861]. \t  -0.3850517103361976 \t -0.018999105716718055\n",
            "11     \t [1.35949135 1.98432447]. \t  -1.981765697419812 \t -0.018999105716718055\n",
            "12     \t [-1.40611301  1.95482855]. \t  -5.839221403425711 \t -0.018999105716718055\n",
            "13     \t [-1.08945402 -0.16533092]. \t  -187.22138818008403 \t -0.018999105716718055\n",
            "14     \t [-0.57478424 -0.55700642]. \t  -81.22486524113543 \t -0.018999105716718055\n",
            "15     \t [-0.51640756  0.2882748 ]. \t  -2.346139372290676 \t -0.018999105716718055\n",
            "16     \t [-0.22249554 -0.07239387]. \t  -2.9804109652583604 \t -0.018999105716718055\n",
            "17     \t [-0.26347808  0.12501395]. \t  -1.905437828416229 \t -0.018999105716718055\n",
            "18     \t [-0.3753506  -0.00103897]. \t  -3.9059178913438064 \t -0.018999105716718055\n",
            "19     \t [0.05181263 0.00983458]. \t  -0.9041715685947159 \t -0.018999105716718055\n",
            "20     \t [-1.22536923  1.96285784]. \t  -26.234628732976525 \t -0.018999105716718055\n",
            "21     \t [-0.70708707  0.91569728]. \t  -20.196886272705065 \t -0.018999105716718055\n",
            "22     \t [-1.91539776  1.95894083]. \t  -300.8437993746776 \t -0.018999105716718055\n",
            "23     \t [ 0.33061555 -1.7446117 ]. \t  -344.14939823471684 \t -0.018999105716718055\n",
            "24     \t [ 0.2889832  -0.02511481]. \t  -1.6855078173979487 \t -0.018999105716718055\n",
            "25     \t [-0.98387027 -1.93324359]. \t  -845.6575912879017 \t -0.018999105716718055\n",
            "26     \t [-0.15521305  1.6408169 ]. \t  -262.7147527326001 \t -0.018999105716718055\n",
            "27     \t [-1.38319376 -0.62574996]. \t  -650.3189806290884 \t -0.018999105716718055\n",
            "28     \t [-0.22258774  0.77729379]. \t  -54.45650740111608 \t -0.018999105716718055\n",
            "29     \t [0.91423748 0.91937701]. \t  -0.705362561033248 \t -0.018999105716718055\n",
            "30     \t [-1.56710626  1.52314372]. \t  -93.57892023928483 \t -0.018999105716718055\n",
            "31     \t [ 1.68856139 -1.69422831]. \t  -2066.601950465006 \t -0.018999105716718055\n",
            "32     \t [0.7269338  0.53346647]. \t  -0.07709898912627434 \t -0.018999105716718055\n",
            "33     \t [-1.49436433  0.52273651]. \t  -298.7646462028464 \t -0.018999105716718055\n",
            "34     \t [ 1.21221907 -1.24380709]. \t  -736.2350470514691 \t -0.018999105716718055\n",
            "35     \t [1.01643062 0.60629059]. \t  -18.21956078873848 \t -0.018999105716718055\n",
            "36     \t [-1.71144866 -0.58798446]. \t  -1244.3096860073397 \t -0.018999105716718055\n",
            "37     \t [1.00698279 1.00630881]. \t  \u001b[92m-0.00598628680283637\u001b[0m \t -0.00598628680283637\n",
            "38     \t [0.95087013 0.82176906]. \t  -0.6811414992149321 \t -0.00598628680283637\n",
            "39     \t [ 1.19759156 -0.75635892]. \t  -479.90507167496264 \t -0.00598628680283637\n",
            "40     \t [0.62304424 0.3997443 ]. \t  -0.1554593865596887 \t -0.00598628680283637\n",
            "41     \t [-1.80520355  1.80372094]. \t  -219.58299236338456 \t -0.00598628680283637\n",
            "42     \t [0.93605721 0.85563527]. \t  -0.046392233932515146 \t -0.00598628680283637\n",
            "43     \t [0.77488254 0.55568842]. \t  -0.2509746947640763 \t -0.00598628680283637\n",
            "44     \t [0.46962302 0.20124042]. \t  -0.3185694417005745 \t -0.00598628680283637\n",
            "45     \t [0.73522446 0.54528247]. \t  -0.07234097869880866 \t -0.00598628680283637\n",
            "46     \t [1.01767863 1.05705422]. \t  -0.04604193359000726 \t -0.00598628680283637\n",
            "47     \t [0.95406635 0.9434474 ]. \t  -0.11236581250489598 \t -0.00598628680283637\n",
            "48     \t [0.47079852 0.21945611]. \t  -0.28053607208673287 \t -0.00598628680283637\n",
            "49     \t [ 1.85063588 -0.97466119]. \t  -1936.2962367024556 \t -0.00598628680283637\n",
            "50     \t [-1.94939435  1.05330205]. \t  -763.2098910205044 \t -0.00598628680283637\n",
            "51     \t [-0.92324205  0.71701112]. \t  -5.531221754433577 \t -0.00598628680283637\n",
            "52     \t [-1.60149383 -0.62832208]. \t  -1026.359445503927 \t -0.00598628680283637\n",
            "53     \t [0.88335428 0.76682488]. \t  -0.03180399498064435 \t -0.00598628680283637\n",
            "54     \t [-1.23617305 -1.44542477]. \t  -889.1995863710406 \t -0.00598628680283637\n",
            "55     \t [-0.78117471 -1.60706685]. \t  -494.8148560291321 \t -0.00598628680283637\n",
            "56     \t [ 1.71130173 -0.6721661 ]. \t  -1297.0242076040583 \t -0.00598628680283637\n",
            "57     \t [-0.29868956 -1.07522585]. \t  -137.27895007309056 \t -0.00598628680283637\n",
            "58     \t [0.67230061 0.46398472]. \t  -0.12177873235102721 \t -0.00598628680283637\n",
            "59     \t [ 0.36157136 -1.50268131]. \t  -267.2120990859399 \t -0.00598628680283637\n",
            "60     \t [0.74820506 0.55446216]. \t  -0.06626149664293017 \t -0.00598628680283637\n",
            "61     \t [0.92088371 0.844365  ]. \t  -0.0076002705336947985 \t -0.00598628680283637\n",
            "62     \t [-1.03233629 -1.32807551]. \t  -577.1552290395983 \t -0.00598628680283637\n",
            "63     \t [0.87139754 0.72822947]. \t  -0.11328576034017876 \t -0.00598628680283637\n",
            "64     \t [0.51171294 0.69297598]. \t  -18.825373352716998 \t -0.00598628680283637\n",
            "65     \t [ 0.01102164 -0.63187888]. \t  -40.92052333757982 \t -0.00598628680283637\n",
            "66     \t [ 1.08362544 -1.74324689]. \t  -851.1823555090484 \t -0.00598628680283637\n",
            "67     \t [-1.97621475  0.19571888]. \t  -1385.049603529039 \t -0.00598628680283637\n",
            "68     \t [-0.41089759  0.62934004]. \t  -23.196952287907333 \t -0.00598628680283637\n",
            "69     \t [ 0.39130643 -1.49916247]. \t  -273.37448175824903 \t -0.00598628680283637\n",
            "70     \t [-1.07184414 -0.90691365]. \t  -426.9089002026663 \t -0.00598628680283637\n",
            "71     \t [-1.07872013  1.14281955]. \t  -4.364414492511502 \t -0.00598628680283637\n",
            "72     \t [-0.84180355 -0.03393875]. \t  -58.53355289837961 \t -0.00598628680283637\n",
            "73     \t [0.8845345 0.779401 ]. \t  -0.014232449126398566 \t -0.00598628680283637\n",
            "74     \t [-1.3150643  -0.37131301]. \t  -446.6565677912945 \t -0.00598628680283637\n",
            "75     \t [-0.71785847  0.56052251]. \t  -3.1553573574022105 \t -0.00598628680283637\n",
            "76     \t [0.62777772 0.37392982]. \t  -0.17925267818251503 \t -0.00598628680283637\n",
            "77     \t [ 0.42264889 -0.6119495 ]. \t  -62.83525877699036 \t -0.00598628680283637\n",
            "78     \t [-0.74381035 -0.01336308]. \t  -35.14634796034531 \t -0.00598628680283637\n",
            "79     \t [1.01339524 1.03386855]. \t  \u001b[92m-0.004938559968799545\u001b[0m \t -0.004938559968799545\n",
            "80     \t [-1.01837067 -1.92221297]. \t  -879.8146046778985 \t -0.004938559968799545\n",
            "81     \t [-1.50431446 -1.46030715]. \t  -1392.5449028268836 \t -0.004938559968799545\n",
            "82     \t [0.27956274 0.79907521]. \t  -52.49157805656037 \t -0.004938559968799545\n",
            "83     \t [ 0.52086063 -1.67252572]. \t  -378.0737835953263 \t -0.004938559968799545\n",
            "84     \t [-0.94978618  0.4780828 ]. \t  -21.78019844628485 \t -0.004938559968799545\n",
            "85     \t [ 0.12877742 -1.95684838]. \t  -390.20241691311753 \t -0.004938559968799545\n",
            "86     \t [-2.00204856 -1.87930626]. \t  -3475.283431726853 \t -0.004938559968799545\n",
            "87     \t [ 0.88669579 -1.57734109]. \t  -558.659399419193 \t -0.004938559968799545\n",
            "88     \t [-0.44596638  1.84690402]. \t  -273.6871541784703 \t -0.004938559968799545\n",
            "89     \t [-1.11911962 -1.18457007]. \t  -598.3869779339484 \t -0.004938559968799545\n",
            "90     \t [0.90197033 0.79343018]. \t  -0.050092473861474356 \t -0.004938559968799545\n",
            "91     \t [0.98566747 0.99043632]. \t  -0.03591112795888479 \t -0.004938559968799545\n",
            "92     \t [-1.61856374  1.72133048]. \t  -87.57238376284555 \t -0.004938559968799545\n",
            "93     \t [ 1.3943274  -1.43945206]. \t  -1145.0310449248936 \t -0.004938559968799545\n",
            "94     \t [-1.98353862  1.55387886]. \t  -575.6017084221132 \t -0.004938559968799545\n",
            "95     \t [-1.14234013  0.82687416]. \t  -27.444408404006722 \t -0.004938559968799545\n",
            "96     \t [ 0.1251758  -1.38381926]. \t  -196.62205066614248 \t -0.004938559968799545\n",
            "97     \t [-0.29259507  0.5531349 ]. \t  -23.528579551977003 \t -0.004938559968799545\n",
            "98     \t [-1.35419313 -0.7584376 ]. \t  -677.5320326314333 \t -0.004938559968799545\n",
            "99     \t [-0.40607436 -1.07110048]. \t  -154.74586914162197 \t -0.004938559968799545\n",
            "100    \t [-0.50624145 -0.19383328]. \t  -22.528996627536284 \t -0.004938559968799545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq4J_FTytd4O",
        "outputId": "7e6c586c-337b-406d-c031-b8189be5f1a6"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 15\r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_loser_15 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_15 = dGPGO(surrogate_loser_15, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.31900409 -1.00146655]. \t  -122.17543449532224 \t -6.867717811955245\n",
            "init   \t [-1.01364994  0.85976824]. \t  -6.867717811955245 \t -6.867717811955245\n",
            "init   \t [-0.21482552 -1.11844164]. \t  -137.1031718367486 \t -6.867717811955245\n",
            "init   \t [-0.40259762  1.56572768]. \t  -198.98860193376754 \t -6.867717811955245\n",
            "init   \t [-0.25717364  1.55002954]. \t  -221.77381285664336 \t -6.867717811955245\n",
            "1      \t [-1.8012058   0.68545903]. \t  -662.6351334070664 \t -6.867717811955245\n",
            "2      \t [-0.43724669  0.32410651]. \t  \u001b[92m-3.8324996046651183\u001b[0m \t -3.8324996046651183\n",
            "3      \t [-0.74037338  0.72496404]. \t  -6.155122682350399 \t -3.8324996046651183\n",
            "4      \t [1.25594009 1.69598967]. \t  \u001b[92m-1.4722002702680579\u001b[0m \t -1.4722002702680579\n",
            "5      \t [0.99642726 0.16862642]. \t  -67.93731254820314 \t -1.4722002702680579\n",
            "6      \t [-0.32748232 -0.09152068]. \t  -5.712975739964262 \t -1.4722002702680579\n",
            "7      \t [-1.23315299  1.42836845]. \t  -5.838861619488394 \t -1.4722002702680579\n",
            "8      \t [1.11976051 1.97554298]. \t  -52.096456025954026 \t -1.4722002702680579\n",
            "9      \t [1.52955484 1.24989993]. \t  -119.01154052053846 \t -1.4722002702680579\n",
            "10     \t [0.29154696 0.11699657]. \t  \u001b[92m-0.6042860952742548\u001b[0m \t -0.6042860952742548\n",
            "11     \t [0.95020009 1.39892014]. \t  -24.608040773431387 \t -0.6042860952742548\n",
            "12     \t [-1.08444934  1.19282974]. \t  -4.373150917248846 \t -0.6042860952742548\n",
            "13     \t [-1.25985261  1.76845899]. \t  -8.391379564428181 \t -0.6042860952742548\n",
            "14     \t [-0.56419161  0.17175273]. \t  -4.594662333999015 \t -0.6042860952742548\n",
            "15     \t [ 0.17746211 -0.10349001]. \t  -2.4986043734294694 \t -0.6042860952742548\n",
            "16     \t [1.75978278 2.00396544]. \t  -120.01375610069695 \t -0.6042860952742548\n",
            "17     \t [ 1.74534324 -2.01574388]. \t  -2562.906427816463 \t -0.6042860952742548\n",
            "18     \t [0.1467519  1.24213598]. \t  -149.714434235153 \t -0.6042860952742548\n",
            "19     \t [0.87307096 1.35901039]. \t  -35.6280596638206 \t -0.6042860952742548\n",
            "20     \t [-0.10790502  0.01403524]. \t  -1.2280255716226158 \t -0.6042860952742548\n",
            "21     \t [0.18702334 0.02579743]. \t  -0.6693588430825558 \t -0.6042860952742548\n",
            "22     \t [0.25881451 0.0782059 ]. \t  \u001b[92m-0.5619469014730655\u001b[0m \t -0.5619469014730655\n",
            "23     \t [0.49790348 1.06233932]. \t  -66.5819590662643 \t -0.5619469014730655\n",
            "24     \t [0.1389005  0.06926979]. \t  -0.9912567810151823 \t -0.5619469014730655\n",
            "25     \t [0.26957856 0.10505275]. \t  -0.6383629076158982 \t -0.5619469014730655\n",
            "26     \t [-0.79377824 -1.52647203]. \t  -468.29098537736076 \t -0.5619469014730655\n",
            "27     \t [0.68320998 0.47494266]. \t  \u001b[92m-0.10702555738314529\u001b[0m \t -0.10702555738314529\n",
            "28     \t [0.45468556 0.37947763]. \t  -3.2812325866320244 \t -0.10702555738314529\n",
            "29     \t [-0.0198923  -0.92816516]. \t  -87.26270799406798 \t -0.10702555738314529\n",
            "30     \t [-0.29231659 -0.27329828]. \t  -14.54004238267034 \t -0.10702555738314529\n",
            "31     \t [ 0.82312635 -1.53627469]. \t  -490.1274956390867 \t -0.10702555738314529\n",
            "32     \t [ 0.18192633 -2.0209277 ]. \t  -422.5710679637814 \t -0.10702555738314529\n",
            "33     \t [0.48689264 0.29705517]. \t  -0.6231679491346888 \t -0.10702555738314529\n",
            "34     \t [-0.89207253  1.00619275]. \t  -8.006727594948323 \t -0.10702555738314529\n",
            "35     \t [0.6276886  0.41282151]. \t  -0.17406713455960235 \t -0.10702555738314529\n",
            "36     \t [-1.28838542  1.59686896]. \t  -5.634465412808513 \t -0.10702555738314529\n",
            "37     \t [1.32553206 0.05199988]. \t  -290.8205294941111 \t -0.10702555738314529\n",
            "38     \t [ 0.96702542 -1.29902543]. \t  -499.1497819678622 \t -0.10702555738314529\n",
            "39     \t [ 1.52335898 -1.74116497]. \t  -1650.0857154121345 \t -0.10702555738314529\n",
            "40     \t [0.48981704 0.28786431]. \t  -0.4901452940195897 \t -0.10702555738314529\n",
            "41     \t [ 0.60876009 -0.43284615]. \t  -64.70384880407995 \t -0.10702555738314529\n",
            "42     \t [-1.48294538  0.26723435]. \t  -379.38593650876345 \t -0.10702555738314529\n",
            "43     \t [0.58584159 0.33329162]. \t  -0.18136534652147004 \t -0.10702555738314529\n",
            "44     \t [0.54550469 0.29804652]. \t  -0.20658818071952822 \t -0.10702555738314529\n",
            "45     \t [ 1.94414784 -1.33544602]. \t  -2617.3743477510557 \t -0.10702555738314529\n",
            "46     \t [ 0.13817213 -0.44650176]. \t  -22.420459174192946 \t -0.10702555738314529\n",
            "47     \t [1.38415117 1.44425517]. \t  -22.39004806086102 \t -0.10702555738314529\n",
            "48     \t [1.17079819 1.38768064]. \t  \u001b[92m-0.05777435420106396\u001b[0m \t -0.05777435420106396\n",
            "49     \t [-0.00440638 -1.59305034]. \t  -254.79595807217444 \t -0.05777435420106396\n",
            "50     \t [-1.29320616 -1.48855631]. \t  -1004.4120042144065 \t -0.05777435420106396\n",
            "51     \t [0.48167906 1.58793747]. \t  -184.12130734420694 \t -0.05777435420106396\n",
            "52     \t [-1.9011107  -0.01322906]. \t  -1324.2564808383454 \t -0.05777435420106396\n",
            "53     \t [-1.15579233  0.22534611]. \t  -127.97064146641327 \t -0.05777435420106396\n",
            "54     \t [ 0.12801058 -1.83120073]. \t  -342.11829813971707 \t -0.05777435420106396\n",
            "55     \t [0.22798276 0.8511745 ]. \t  -64.46781299692562 \t -0.05777435420106396\n",
            "56     \t [-1.13629085 -1.93066034]. \t  -1042.5743678349131 \t -0.05777435420106396\n",
            "57     \t [-1.44601532 -0.39884523]. \t  -625.8961522131331 \t -0.05777435420106396\n",
            "58     \t [ 1.4866585  -0.20730164]. \t  -584.6457703960374 \t -0.05777435420106396\n",
            "59     \t [-1.68502746  0.51322119]. \t  -548.281791286128 \t -0.05777435420106396\n",
            "60     \t [0.54717588 0.30048743]. \t  -0.2051676183969559 \t -0.05777435420106396\n",
            "61     \t [1.25639194 1.45320796]. \t  -1.6360656210100721 \t -0.05777435420106396\n",
            "62     \t [1.28661493 1.62086164]. \t  -0.20128587323878006 \t -0.05777435420106396\n",
            "63     \t [1.2098789  1.46535097]. \t  \u001b[92m-0.04428755224829542\u001b[0m \t -0.04428755224829542\n",
            "64     \t [-0.57808571 -0.72258276]. \t  -114.16576044861316 \t -0.04428755224829542\n",
            "65     \t [ 1.95035338 -1.96655006]. \t  -3330.687513865893 \t -0.04428755224829542\n",
            "66     \t [1.76872942 0.34832836]. \t  -773.4728649615515 \t -0.04428755224829542\n",
            "67     \t [-0.05724343  1.9533385 ]. \t  -381.3918215074497 \t -0.04428755224829542\n",
            "68     \t [1.26647715 1.62377216]. \t  -0.11024491926290325 \t -0.04428755224829542\n",
            "69     \t [0.91692307 0.23882775]. \t  -36.23769107463086 \t -0.04428755224829542\n",
            "70     \t [1.0535951  0.55616467]. \t  -30.683166783002658 \t -0.04428755224829542\n",
            "71     \t [-0.01961047  1.89374533]. \t  -359.5211002445944 \t -0.04428755224829542\n",
            "72     \t [0.64286561 0.41001696]. \t  -0.1286072330775103 \t -0.04428755224829542\n",
            "73     \t [-0.96139332 -1.51235982]. \t  -597.5670219233991 \t -0.04428755224829542\n",
            "74     \t [-1.24483887  0.07042865]. \t  -223.8411389965985 \t -0.04428755224829542\n",
            "75     \t [-1.86428692  1.96795304]. \t  -235.49373628057674 \t -0.04428755224829542\n",
            "76     \t [0.74634834 0.52368188]. \t  -0.17558790752856746 \t -0.04428755224829542\n",
            "77     \t [-0.63261107  1.97065911]. \t  -249.30061465535087 \t -0.04428755224829542\n",
            "78     \t [1.90437555 0.06965906]. \t  -1266.0336628787975 \t -0.04428755224829542\n",
            "79     \t [-1.10184809  0.84917803]. \t  -17.732323399067432 \t -0.04428755224829542\n",
            "80     \t [-0.88245473  0.73950085]. \t  -3.69749981268287 \t -0.04428755224829542\n",
            "81     \t [0.64497093 0.41191547]. \t  -0.12770378357700504 \t -0.04428755224829542\n",
            "82     \t [-1.45764011  1.92065382]. \t  -10.204079062223498 \t -0.04428755224829542\n",
            "83     \t [1.24328263 1.53774161]. \t  -0.06560259500737861 \t -0.04428755224829542\n",
            "84     \t [-0.86777235 -1.25494479]. \t  -406.6843850848933 \t -0.04428755224829542\n",
            "85     \t [-0.85232843  1.35452091]. \t  -42.87669922124042 \t -0.04428755224829542\n",
            "86     \t [-0.16650858 -0.2351556 ]. \t  -8.271368864830501 \t -0.04428755224829542\n",
            "87     \t [-0.18601672 -0.25940638]. \t  -10.05074152806717 \t -0.04428755224829542\n",
            "88     \t [0.78873311 0.60254248]. \t  -0.08288301770115694 \t -0.04428755224829542\n",
            "89     \t [ 1.66125533 -1.00365242]. \t  -1416.7715446889222 \t -0.04428755224829542\n",
            "90     \t [ 0.60800516 -1.57700372]. \t  -379.107622928594 \t -0.04428755224829542\n",
            "91     \t [ 0.45080911 -1.64806577]. \t  -343.0307894880755 \t -0.04428755224829542\n",
            "92     \t [0.94661852 0.91241153]. \t  \u001b[92m-0.029499860745498308\u001b[0m \t -0.029499860745498308\n",
            "93     \t [-1.77464031  0.63760555]. \t  -638.5837589794946 \t -0.029499860745498308\n",
            "94     \t [-1.23939631 -0.96735506]. \t  -631.7452304759217 \t -0.029499860745498308\n",
            "95     \t [0.93798283 0.84226238]. \t  -0.1448420089182058 \t -0.029499860745498308\n",
            "96     \t [0.02856468 1.71387643]. \t  -294.40131162784513 \t -0.029499860745498308\n",
            "97     \t [0.94650351 0.91131675]. \t  \u001b[92m-0.0267254679106745\u001b[0m \t -0.0267254679106745\n",
            "98     \t [0.74135726 0.53852833]. \t  -0.07917769844144658 \t -0.0267254679106745\n",
            "99     \t [-2.03919919  1.0231488 ]. \t  -992.1749368582529 \t -0.0267254679106745\n",
            "100    \t [-1.07258036 -0.09198245]. \t  -158.65411831160364 \t -0.0267254679106745\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tJ-ta9Gtd7H",
        "outputId": "5e4c7420-5c17-42e3-9f3c-3c019e1edd4a"
      },
      "source": [
        "\r\n",
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 16\r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_loser_16 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_16 = dGPGO(surrogate_loser_16, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.49073237 -0.75216486]. \t  -884.9748202573794 \t -21.690996320546372\n",
            "init   \t [0.70512959 0.03240619]. \t  -21.690996320546372 \t -21.690996320546372\n",
            "init   \t [ 1.15368111 -0.88603985]. \t  -491.54136063574043 \t -21.690996320546372\n",
            "init   \t [-1.09072882  0.26132352]. \t  -90.5574614167091 \t -21.690996320546372\n",
            "init   \t [1.5360998 0.8967902]. \t  -214.26941031258198 \t -21.690996320546372\n",
            "1      \t [ 0.0221755  -1.04001206]. \t  -109.2209581414894 \t -21.690996320546372\n",
            "2      \t [0.22984789 0.62935581]. \t  -33.831328641897436 \t -21.690996320546372\n",
            "3      \t [0.27847476 0.05249583]. \t  \u001b[92m-0.5833607298068536\u001b[0m \t -0.5833607298068536\n",
            "4      \t [-1.89666678  0.44479792]. \t  -1002.2459176523861 \t -0.5833607298068536\n",
            "5      \t [-0.64908274  0.24042698]. \t  -5.9912828906732205 \t -0.5833607298068536\n",
            "6      \t [-0.90568542 -0.13633405]. \t  -95.14001721251631 \t -0.5833607298068536\n",
            "7      \t [-0.70879595  0.54496077]. \t  -3.10119619289842 \t -0.5833607298068536\n",
            "8      \t [ 0.40133467 -1.98033381]. \t  -458.9192215256066 \t -0.5833607298068536\n",
            "9      \t [ 0.39960744 -0.44253111]. \t  -36.627028547741325 \t -0.5833607298068536\n",
            "10     \t [0.50526239 0.34749532]. \t  -1.0949458446627538 \t -0.5833607298068536\n",
            "11     \t [0.35325267 0.22098225]. \t  -1.343626186243945 \t -0.5833607298068536\n",
            "12     \t [0.36058282 2.01233949]. \t  -354.72153426859774 \t -0.5833607298068536\n",
            "13     \t [ 0.45142949 -0.02723036]. \t  -5.637904947369633 \t -0.5833607298068536\n",
            "14     \t [0.44858559 0.14463035]. \t  -0.6243989373758893 \t -0.5833607298068536\n",
            "15     \t [ 0.23034354 -0.58857712]. \t  -41.761952530407115 \t -0.5833607298068536\n",
            "16     \t [-0.64879193  0.39829601]. \t  -2.7697489339403494 \t -0.5833607298068536\n",
            "17     \t [0.40356791 0.12336103]. \t  \u001b[92m-0.5118038424321075\u001b[0m \t -0.5118038424321075\n",
            "18     \t [0.36164892 0.10164547]. \t  \u001b[92m-0.49243209476833494\u001b[0m \t -0.49243209476833494\n",
            "19     \t [0.67686122 1.11927547]. \t  -43.81428231245156 \t -0.49243209476833494\n",
            "20     \t [0.49224278 0.28582549]. \t  \u001b[92m-0.44723847994298704\u001b[0m \t -0.44723847994298704\n",
            "21     \t [-1.64505925  0.84496156]. \t  -353.4246094702246 \t -0.44723847994298704\n",
            "22     \t [-1.45163433  1.73142691]. \t  -20.134225770346212 \t -0.44723847994298704\n",
            "23     \t [-1.09990965  0.96276837]. \t  -10.512144080158453 \t -0.44723847994298704\n",
            "24     \t [-1.65818853  1.93516594]. \t  -73.39448969348811 \t -0.44723847994298704\n",
            "25     \t [-0.78195468  0.53364097]. \t  -3.7808355450109574 \t -0.44723847994298704\n",
            "26     \t [-1.21768292  1.40012239]. \t  -5.600877967326659 \t -0.44723847994298704\n",
            "27     \t [0.76892328 0.51479858]. \t  -0.6377716476949535 \t -0.44723847994298704\n",
            "28     \t [0.38485341 0.76341975]. \t  -38.23874944180172 \t -0.44723847994298704\n",
            "29     \t [-0.01455274  1.61486514]. \t  -261.7398640546982 \t -0.44723847994298704\n",
            "30     \t [0.60631394 0.35566924]. \t  \u001b[92m-0.16926264601925312\u001b[0m \t -0.16926264601925312\n",
            "31     \t [0.49402913 0.22594906]. \t  -0.28882444719442163 \t -0.16926264601925312\n",
            "32     \t [0.87602124 1.10065017]. \t  -11.120058119862527 \t -0.16926264601925312\n",
            "33     \t [-1.97445608 -2.00286589]. \t  -3491.4319350150254 \t -0.16926264601925312\n",
            "34     \t [ 1.55749306 -1.20712058]. \t  -1320.1108231409005 \t -0.16926264601925312\n",
            "35     \t [0.23713683 1.18773581]. \t  -128.61162224204764 \t -0.16926264601925312\n",
            "36     \t [0.88022081 0.8572566 ]. \t  -0.6944429296422251 \t -0.16926264601925312\n",
            "37     \t [1.04560963 1.57958228]. \t  -23.64917436500178 \t -0.16926264601925312\n",
            "38     \t [1.92845623 2.03002895]. \t  -286.1052464867677 \t -0.16926264601925312\n",
            "39     \t [ 0.61478593 -0.26500832]. \t  -41.48944069562792 \t -0.16926264601925312\n",
            "40     \t [0.58037137 0.3187701 ]. \t  -0.20870751829868253 \t -0.16926264601925312\n",
            "41     \t [-0.86845363 -0.43874869]. \t  -145.80657003785683 \t -0.16926264601925312\n",
            "42     \t [0.65970292 0.3962484 ]. \t  -0.2675867340403693 \t -0.16926264601925312\n",
            "43     \t [0.68687634 0.47560627]. \t  \u001b[92m-0.09949588079885996\u001b[0m \t -0.09949588079885996\n",
            "44     \t [-1.94792302  0.21433422]. \t  -1290.3802851118294 \t -0.09949588079885996\n",
            "45     \t [-0.16806043  0.00923074]. \t  -1.4005167514366625 \t -0.09949588079885996\n",
            "46     \t [-1.24862228  1.74790749]. \t  -8.622730190813058 \t -0.09949588079885996\n",
            "47     \t [1.16704652 0.59980248]. \t  -58.122041039349625 \t -0.09949588079885996\n",
            "48     \t [-2.00507972  1.89669292]. \t  -460.0201820484579 \t -0.09949588079885996\n",
            "49     \t [-1.89921475 -1.4414918 ]. \t  -2557.1492100176924 \t -0.09949588079885996\n",
            "50     \t [-1.44755681  2.03501648]. \t  -6.355401732521033 \t -0.09949588079885996\n",
            "51     \t [ 0.35378044 -0.84419428]. \t  -94.38248797014575 \t -0.09949588079885996\n",
            "52     \t [ 0.9481272  -1.78426622]. \t  -719.9650394382073 \t -0.09949588079885996\n",
            "53     \t [ 1.65406445 -1.40107662]. \t  -1711.9095241778048 \t -0.09949588079885996\n",
            "54     \t [-1.11632408 -0.88810956]. \t  -459.9977901252435 \t -0.09949588079885996\n",
            "55     \t [0.6976035  0.45289812]. \t  -0.2053668778332181 \t -0.09949588079885996\n",
            "56     \t [0.73083496 0.50010988]. \t  -0.18811688846718624 \t -0.09949588079885996\n",
            "57     \t [-0.61019301 -1.3025744 ]. \t  -283.12504324506335 \t -0.09949588079885996\n",
            "58     \t [-1.96939784  0.54070658]. \t  -1122.9224080108465 \t -0.09949588079885996\n",
            "59     \t [ 0.75819958 -0.10407861]. \t  -46.15512888106376 \t -0.09949588079885996\n",
            "60     \t [-0.85982678 -0.39708684]. \t  -132.59693612405053 \t -0.09949588079885996\n",
            "61     \t [0.62939101 0.3950653 ]. \t  -0.13746503155297732 \t -0.09949588079885996\n",
            "62     \t [ 2.01318748 -1.93732052]. \t  -3589.329304979957 \t -0.09949588079885996\n",
            "63     \t [-0.05408798  0.06684726]. \t  -1.519700545075625 \t -0.09949588079885996\n",
            "64     \t [-1.50676836  0.61391828]. \t  -280.6607894481304 \t -0.09949588079885996\n",
            "65     \t [-0.83705712  0.88073911]. \t  -6.61746108713509 \t -0.09949588079885996\n",
            "66     \t [0.76691191 0.57108803]. \t  \u001b[92m-0.08345436494850517\u001b[0m \t -0.08345436494850517\n",
            "67     \t [ 0.57272774 -0.95069889]. \t  -163.69401005435313 \t -0.08345436494850517\n",
            "68     \t [-1.5895948   0.68742701]. \t  -345.0395757885496 \t -0.08345436494850517\n",
            "69     \t [0.26877118 0.47381962]. \t  -16.661479673194858 \t -0.08345436494850517\n",
            "70     \t [-1.52791129  2.01102875]. \t  -16.854535730329797 \t -0.08345436494850517\n",
            "71     \t [ 0.59946234 -0.79637604]. \t  -133.73187665435353 \t -0.08345436494850517\n",
            "72     \t [-0.43541378  0.47213973]. \t  -10.044121049607899 \t -0.08345436494850517\n",
            "73     \t [-1.7514003  -0.23603889]. \t  -1098.8430494818413 \t -0.08345436494850517\n",
            "74     \t [-0.41332787 -0.35282988]. \t  -29.420502585851374 \t -0.08345436494850517\n",
            "75     \t [-0.94227343 -0.16669875]. \t  -114.9858968322364 \t -0.08345436494850517\n",
            "76     \t [-1.04843339  0.61214391]. \t  -27.91966742391449 \t -0.08345436494850517\n",
            "77     \t [1.33962623 0.84995024]. \t  -89.35136915823698 \t -0.08345436494850517\n",
            "78     \t [0.8428471  0.71415851]. \t  \u001b[92m-0.026116265463732566\u001b[0m \t -0.026116265463732566\n",
            "79     \t [-1.82204876  0.4768997 ]. \t  -816.2072388153531 \t -0.026116265463732566\n",
            "80     \t [0.96502104 0.93573892]. \t  \u001b[92m-0.0032245849660961575\u001b[0m \t -0.0032245849660961575\n",
            "81     \t [ 0.53500734 -1.22816197]. \t  -229.55538676540743 \t -0.0032245849660961575\n",
            "82     \t [ 1.95780327 -0.0344688 ]. \t  -1496.6439525931778 \t -0.0032245849660961575\n",
            "83     \t [-0.74594318  1.73776012]. \t  -142.60211472973953 \t -0.0032245849660961575\n",
            "84     \t [0.40482454 0.38518204]. \t  -5.251564672638031 \t -0.0032245849660961575\n",
            "85     \t [0.80431251 0.633683  ]. \t  -0.05581173473667576 \t -0.0032245849660961575\n",
            "86     \t [1.02784239 1.1126412 ]. \t  -0.31640808521841157 \t -0.0032245849660961575\n",
            "87     \t [1.05697618 1.09381078]. \t  -0.057945483197729006 \t -0.0032245849660961575\n",
            "88     \t [0.96120873 0.93188314]. \t  -0.00784238103777529 \t -0.0032245849660961575\n",
            "89     \t [-0.49930821 -1.30813117]. \t  -244.80981656957928 \t -0.0032245849660961575\n",
            "90     \t [-0.63042838  1.71535492]. \t  -176.3482881164944 \t -0.0032245849660961575\n",
            "91     \t [ 0.49039704 -1.13699842]. \t  -190.0069262493083 \t -0.0032245849660961575\n",
            "92     \t [1.00045097 0.98805537]. \t  -0.016504144327074127 \t -0.0032245849660961575\n",
            "93     \t [ 0.21938768 -0.35648909]. \t  -16.981093934902173 \t -0.0032245849660961575\n",
            "94     \t [ 0.40091597 -0.29888962]. \t  -21.4842532107915 \t -0.0032245849660961575\n",
            "95     \t [-0.02629612 -0.737631  ]. \t  -55.56529365654726 \t -0.0032245849660961575\n",
            "96     \t [1.09132668 2.0162445 ]. \t  -68.11219215117217 \t -0.0032245849660961575\n",
            "97     \t [0.98188868 0.90391976]. \t  -0.36255880336726304 \t -0.0032245849660961575\n",
            "98     \t [1.03224282 1.06896304]. \t  \u001b[92m-0.002221450870597632\u001b[0m \t -0.002221450870597632\n",
            "99     \t [-1.03049915  1.23553349]. \t  -7.13679627376443 \t -0.002221450870597632\n",
            "100    \t [-1.01481    1.5943567]. \t  -35.92744461630901 \t -0.002221450870597632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqjDLo0otd9-",
        "outputId": "31fc4819-2af8-4eb6-b415-0923706bfd9b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 17\r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_loser_17 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_17 = dGPGO(surrogate_loser_17, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.62187122 0.94420838]. \t  -31.22188590191926 \t -31.22188590191926\n",
            "init   \t [-1.25596728  0.08059797]. \t  -229.14713198618372 \t -31.22188590191926\n",
            "init   \t [0.72246586 1.2470062 ]. \t  -52.64667111223406 \t -31.22188590191926\n",
            "init   \t [0.13726869 1.671308  ]. \t  -273.80846401311214 \t -31.22188590191926\n",
            "init   \t [-0.2315013  -0.99953783]. \t  -112.42501801714917 \t -31.22188590191926\n",
            "1      \t [1.59286487 0.71471414]. \t  -332.50370461464615 \t -31.22188590191926\n",
            "2      \t [ 1.90278748 -2.02756811]. \t  -3190.995555019365 \t -31.22188590191926\n",
            "3      \t [-1.34540721 -1.87107521]. \t  -1360.6211641082973 \t -31.22188590191926\n",
            "4      \t [-0.05612606 -0.17716441]. \t  \u001b[92m-4.366735792685078\u001b[0m \t -4.366735792685078\n",
            "5      \t [-0.14818959 -0.3550115 ]. \t  -15.52910245168684 \t -4.366735792685078\n",
            "6      \t [ 0.06392531 -0.23881863]. \t  -6.776523181613979 \t -4.366735792685078\n",
            "7      \t [0.07863961 0.06826898]. \t  \u001b[92m-1.2343570551701712\u001b[0m \t -1.2343570551701712\n",
            "8      \t [0.04751241 0.075279  ]. \t  -1.4404475611620164 \t -1.2343570551701712\n",
            "9      \t [-0.37850307 -0.75061539]. \t  -81.80241034086906 \t -1.2343570551701712\n",
            "10     \t [-0.83386556 -0.14471424]. \t  -73.9307927272724 \t -1.2343570551701712\n",
            "11     \t [0.13651715 0.03341295]. \t  \u001b[92m-0.767435687989711\u001b[0m \t -0.767435687989711\n",
            "12     \t [-0.15252992  1.50680842]. \t  -221.41832287515706 \t -0.767435687989711\n",
            "13     \t [ 0.09914949 -0.01195521]. \t  -0.8589938639648796 \t -0.767435687989711\n",
            "14     \t [-1.83933026  1.86959459]. \t  -237.1425038369888 \t -0.767435687989711\n",
            "15     \t [-0.63748997 -1.38065512]. \t  -322.03563613815305 \t -0.767435687989711\n",
            "16     \t [0.01783411 0.39098217]. \t  -16.226494790123382 \t -0.767435687989711\n",
            "17     \t [-0.53458742 -0.72927028]. \t  -105.38841833205716 \t -0.767435687989711\n",
            "18     \t [0.04802418 0.20630082]. \t  -5.067633597172671 \t -0.767435687989711\n",
            "19     \t [1.74151857 1.86840679]. \t  -136.15124722922013 \t -0.767435687989711\n",
            "20     \t [-0.35289534 -1.2149306 ]. \t  -181.247167573373 \t -0.767435687989711\n",
            "21     \t [1.73420509 1.46014498]. \t  -239.95969685251026 \t -0.767435687989711\n",
            "22     \t [0.17391461 1.21263513]. \t  -140.486753581382 \t -0.767435687989711\n",
            "23     \t [ 0.3935953  -1.31480281]. \t  -216.3754355014879 \t -0.767435687989711\n",
            "24     \t [-0.40145841  0.11863616]. \t  -2.1449887368294287 \t -0.767435687989711\n",
            "25     \t [1.7954402  0.87939316]. \t  -550.1658746391516 \t -0.767435687989711\n",
            "26     \t [0.78979885 0.20528743]. \t  -17.557972867201215 \t -0.767435687989711\n",
            "27     \t [1.12371334 1.22165208]. \t  \u001b[92m-0.1840581385236697\u001b[0m \t -0.1840581385236697\n",
            "28     \t [0.78560143 0.61215899]. \t  \u001b[92m-0.04847736861870728\u001b[0m \t -0.04847736861870728\n",
            "29     \t [0.25774702 1.44265543]. \t  -189.94961304576694 \t -0.04847736861870728\n",
            "30     \t [1.20429826 1.49952164]. \t  -0.2836771911602363 \t -0.04847736861870728\n",
            "31     \t [1.17272494 1.43730764]. \t  -0.41452969596738526 \t -0.04847736861870728\n",
            "32     \t [-1.49651819 -2.02411352]. \t  -1824.1294927044426 \t -0.04847736861870728\n",
            "33     \t [0.3186804  1.09882346]. \t  -99.9181963257552 \t -0.04847736861870728\n",
            "34     \t [-1.38459603  1.2141535 ]. \t  -55.100543620283794 \t -0.04847736861870728\n",
            "35     \t [-0.74083596 -1.65416337]. \t  -488.351976952918 \t -0.04847736861870728\n",
            "36     \t [-0.88311058  0.81469966]. \t  -3.6673163804431126 \t -0.04847736861870728\n",
            "37     \t [ 0.6677092  -1.23963205]. \t  -284.19052973424647 \t -0.04847736861870728\n",
            "38     \t [0.84508445 0.74732108]. \t  -0.1339132568038867 \t -0.04847736861870728\n",
            "39     \t [-1.91187252 -1.00609437]. \t  -2181.2982218772822 \t -0.04847736861870728\n",
            "40     \t [-0.35318094  1.10103328]. \t  -97.14658577025038 \t -0.04847736861870728\n",
            "41     \t [ 0.54088612 -1.76136025]. \t  -422.0687201794085 \t -0.04847736861870728\n",
            "42     \t [1.15930595 1.37799479]. \t  -0.14100900958555315 \t -0.04847736861870728\n",
            "43     \t [0.9389775  0.96886211]. \t  -0.7638175853516199 \t -0.04847736861870728\n",
            "44     \t [0.93362488 0.85191999]. \t  \u001b[92m-0.04335440223053822\u001b[0m \t -0.04335440223053822\n",
            "45     \t [-1.3093457   1.90911608]. \t  -9.12505201197023 \t -0.04335440223053822\n",
            "46     \t [0.42498651 1.46456472]. \t  -165.1837035566068 \t -0.04335440223053822\n",
            "47     \t [-0.41592949 -0.74377796]. \t  -86.05255252223562 \t -0.04335440223053822\n",
            "48     \t [-1.52637107  1.3341993 ]. \t  -105.50634993417196 \t -0.04335440223053822\n",
            "49     \t [-1.63676595  1.16647122]. \t  -235.72770352656477 \t -0.04335440223053822\n",
            "50     \t [-1.05600458  1.36478073]. \t  -10.458921203572837 \t -0.04335440223053822\n",
            "51     \t [0.94171099 0.85942387]. \t  -0.07845015341681784 \t -0.04335440223053822\n",
            "52     \t [ 1.16414869 -0.27786964]. \t  -266.7323671760979 \t -0.04335440223053822\n",
            "53     \t [ 0.67582072 -0.86045542]. \t  -173.60379613220516 \t -0.04335440223053822\n",
            "54     \t [ 1.96307728 -1.52272095]. \t  -2891.4880782547907 \t -0.04335440223053822\n",
            "55     \t [-1.63769437  1.23503965]. \t  -216.33925826148388 \t -0.04335440223053822\n",
            "56     \t [-2.03099333  0.76013649]. \t  -1141.3730869217545 \t -0.04335440223053822\n",
            "57     \t [0.92122461 0.5995163 ]. \t  -6.213204097598539 \t -0.04335440223053822\n",
            "58     \t [-0.79301495  0.56919613]. \t  -3.5710320327526817 \t -0.04335440223053822\n",
            "59     \t [1.13983853 1.35630082]. \t  -0.34524121457321233 \t -0.04335440223053822\n",
            "60     \t [-1.7722413  -1.23475196]. \t  -1922.2651483292495 \t -0.04335440223053822\n",
            "61     \t [ 0.53234742 -1.40459953]. \t  -285.15083875605234 \t -0.04335440223053822\n",
            "62     \t [0.52314246 1.00172271]. \t  -53.23229777535611 \t -0.04335440223053822\n",
            "63     \t [1.3560085  0.25684078]. \t  -250.37328181217796 \t -0.04335440223053822\n",
            "64     \t [0.95065482 0.90554263]. \t  \u001b[92m-0.002758242625587753\u001b[0m \t -0.002758242625587753\n",
            "65     \t [ 1.24926445 -1.44904194]. \t  -905.8937177577728 \t -0.002758242625587753\n",
            "66     \t [-0.25376903  0.75078484]. \t  -48.68452741058197 \t -0.002758242625587753\n",
            "67     \t [-0.31251138  0.16572607]. \t  -2.185939276476964 \t -0.002758242625587753\n",
            "68     \t [0.54072251 1.04809334]. \t  -57.321074583400176 \t -0.002758242625587753\n",
            "69     \t [ 0.8301388  -1.30815169]. \t  -398.9424420447321 \t -0.002758242625587753\n",
            "70     \t [-1.93510578  0.10608985]. \t  -1332.515481306013 \t -0.002758242625587753\n",
            "71     \t [0.40650213 0.08189062]. \t  -1.0470180309555928 \t -0.002758242625587753\n",
            "72     \t [-0.04385934  0.51270369]. \t  -27.179268024709717 \t -0.002758242625587753\n",
            "73     \t [-1.24530025  1.56947825]. \t  -5.076362904596787 \t -0.002758242625587753\n",
            "74     \t [1.13509276 1.30522457]. \t  -0.04643710583144919 \t -0.002758242625587753\n",
            "75     \t [-0.06305095 -0.65857327]. \t  -45.027153579346496 \t -0.002758242625587753\n",
            "76     \t [ 1.12189078 -1.62498668]. \t  -831.5445178179002 \t -0.002758242625587753\n",
            "77     \t [-1.37516928 -1.8899802 ]. \t  -1435.2910264935185 \t -0.002758242625587753\n",
            "78     \t [ 1.25378386 -1.87414395]. \t  -1187.6372739167462 \t -0.002758242625587753\n",
            "79     \t [1.01220441 1.03486883]. \t  -0.01078073154289555 \t -0.002758242625587753\n",
            "80     \t [-1.22027478 -1.5265442 ]. \t  -914.3228437697458 \t -0.002758242625587753\n",
            "81     \t [0.65221889 1.46184715]. \t  -107.54540135343447 \t -0.002758242625587753\n",
            "82     \t [ 1.55054754 -1.96262208]. \t  -1907.2145839346979 \t -0.002758242625587753\n",
            "83     \t [1.09561815 1.21014797]. \t  -0.01868586857163028 \t -0.002758242625587753\n",
            "84     \t [1.99429542 1.99971413]. \t  -392.0392903776973 \t -0.002758242625587753\n",
            "85     \t [1.21552989 1.40522623]. \t  -0.56898970885753 \t -0.002758242625587753\n",
            "86     \t [1.34299937 1.70234647]. \t  -1.1438344722033924 \t -0.002758242625587753\n",
            "87     \t [ 1.53227294 -2.04175182]. \t  -1927.1528159404543 \t -0.002758242625587753\n",
            "88     \t [1.2747173  1.60120578]. \t  -0.13163112839381907 \t -0.002758242625587753\n",
            "89     \t [-0.45368952 -0.42290734]. \t  -41.644803839985755 \t -0.002758242625587753\n",
            "90     \t [-0.06955067  0.76414131]. \t  -58.79819787744729 \t -0.002758242625587753\n",
            "91     \t [0.98077986 0.96992581]. \t  -0.006764101125278395 \t -0.002758242625587753\n",
            "92     \t [-0.88903176  0.80760815]. \t  -3.5981306052969417 \t -0.002758242625587753\n",
            "93     \t [-1.73497169  0.81764991]. \t  -488.17555231351713 \t -0.002758242625587753\n",
            "94     \t [-0.20004687  1.61504794]. \t  -249.51180708440614 \t -0.002758242625587753\n",
            "95     \t [1.00030486 0.94983827]. \t  -0.2577749979055035 \t -0.002758242625587753\n",
            "96     \t [-1.63142951  1.44204324]. \t  -155.64708326487352 \t -0.002758242625587753\n",
            "97     \t [1.06144614 1.11495861]. \t  -0.01748637430562179 \t -0.002758242625587753\n",
            "98     \t [1.54415053 0.93054645]. \t  -211.66536041100017 \t -0.002758242625587753\n",
            "99     \t [0.99939249 0.63883855]. \t  -12.956169399285843 \t -0.002758242625587753\n",
            "100    \t [1.8395479 1.5593048]. \t  -333.6329134872885 \t -0.002758242625587753\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOqNmgSIteAw",
        "outputId": "a4330966-e88e-4b60-c73b-8750ed433e9f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 18\r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_loser_18 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_18 = dGPGO(surrogate_loser_18, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.52956683 1.91914255]. \t  -17.95675982372887 \t -1.7663579664225912\n",
            "init   \t [1.51222084 0.12638491]. \t  -467.0068292530542 \t -1.7663579664225912\n",
            "init   \t [-1.09474477 -2.0013105 ]. \t  -1028.244988519411 \t -1.7663579664225912\n",
            "init   \t [-0.28479972 -0.39996883]. \t  -24.79447905426233 \t -1.7663579664225912\n",
            "init   \t [ 0.09287545 -0.0885072 ]. \t  -1.7663579664225912 \t -1.7663579664225912\n",
            "1      \t [-0.56813572  0.40153442]. \t  -3.0793038960278705 \t -1.7663579664225912\n",
            "2      \t [-0.15462805 -0.07652601]. \t  -2.341901869752791 \t -1.7663579664225912\n",
            "3      \t [-1.20242026  1.32756507]. \t  -6.248947237067588 \t -1.7663579664225912\n",
            "4      \t [ 0.34076202 -0.76279921]. \t  -77.68427478499966 \t -1.7663579664225912\n",
            "5      \t [-1.68770408  0.39178462]. \t  -610.6926759353652 \t -1.7663579664225912\n",
            "6      \t [-0.03540629  1.33846942]. \t  -179.88668010529636 \t -1.7663579664225912\n",
            "7      \t [-0.8823456   1.11145957]. \t  -14.627184329389557 \t -1.7663579664225912\n",
            "8      \t [-1.54842812  2.04257632]. \t  -19.100772316288136 \t -1.7663579664225912\n",
            "9      \t [-1.48102522  1.62501465]. \t  -38.4657364786583 \t -1.7663579664225912\n",
            "10     \t [-1.17658605  1.43183107]. \t  -4.9629269899146085 \t -1.7663579664225912\n",
            "11     \t [1.25007914 1.99328278]. \t  -18.60287771607783 \t -1.7663579664225912\n",
            "12     \t [0.18748892 0.28236639]. \t  -6.771665231097794 \t -1.7663579664225912\n",
            "13     \t [2.048 2.048]. \t  -461.7603900415999 \t -1.7663579664225912\n",
            "14     \t [1.13611964 1.32949764]. \t  \u001b[92m-0.16852826950798047\u001b[0m \t -0.16852826950798047\n",
            "15     \t [1.32792035 1.61792688]. \t  -2.2229733597680563 \t -0.16852826950798047\n",
            "16     \t [0.66848703 0.83939105]. \t  -15.516792861305607 \t -0.16852826950798047\n",
            "17     \t [-1.28543464  2.04464581]. \t  -20.613423791400496 \t -0.16852826950798047\n",
            "18     \t [-1.03226585 -1.05883981]. \t  -455.44299340967336 \t -0.16852826950798047\n",
            "19     \t [ 0.93458502 -0.79773117]. \t  -279.28864832088374 \t -0.16852826950798047\n",
            "20     \t [1.39236348 1.95887501]. \t  -0.19474890023561603 \t -0.16852826950798047\n",
            "21     \t [1.40458745 1.42902346]. \t  -29.740151978250694 \t -0.16852826950798047\n",
            "22     \t [0.05016724 0.06478462]. \t  -1.2899109775693323 \t -0.16852826950798047\n",
            "23     \t [-0.9724487  -0.53938542]. \t  -224.42549533519733 \t -0.16852826950798047\n",
            "24     \t [-1.62039324  1.92577765]. \t  -55.851985861387206 \t -0.16852826950798047\n",
            "25     \t [-0.27086497  0.23223916]. \t  -4.139107739749431 \t -0.16852826950798047\n",
            "26     \t [-1.92924409 -0.31165391]. \t  -1635.6029441475737 \t -0.16852826950798047\n",
            "27     \t [-0.45260348  0.19406349]. \t  -2.1216915859921293 \t -0.16852826950798047\n",
            "28     \t [ 0.11048423 -0.18739773]. \t  -4.775433930768888 \t -0.16852826950798047\n",
            "29     \t [-1.66391243 -0.01085516]. \t  -779.6360682164172 \t -0.16852826950798047\n",
            "30     \t [ 0.16093448 -1.44886951]. \t  -218.1985133803662 \t -0.16852826950798047\n",
            "31     \t [1.6702811  1.70236723]. \t  -118.70875420772408 \t -0.16852826950798047\n",
            "32     \t [ 0.0462226  -1.42077002]. \t  -203.3759959397796 \t -0.16852826950798047\n",
            "33     \t [-0.1208149   1.48729322]. \t  -218.1398643921702 \t -0.16852826950798047\n",
            "34     \t [1.14429545 1.27412903]. \t  \u001b[92m-0.14531061234750747\u001b[0m \t -0.14531061234750747\n",
            "35     \t [ 0.61123068 -0.5270834 ]. \t  -81.27473123256354 \t -0.14531061234750747\n",
            "36     \t [1.38648903 1.95227109]. \t  -0.23888990096381357 \t -0.14531061234750747\n",
            "37     \t [1.12024044 1.27597448]. \t  \u001b[92m-0.058708410729165106\u001b[0m \t -0.058708410729165106\n",
            "38     \t [ 1.88724723 -0.1642529 ]. \t  -1389.0612815590353 \t -0.058708410729165106\n",
            "39     \t [0.89588206 0.44978386]. \t  -12.459092029628481 \t -0.058708410729165106\n",
            "40     \t [ 0.01331491 -0.54313407]. \t  -30.49227055750872 \t -0.058708410729165106\n",
            "41     \t [1.01842587 1.09550373]. \t  -0.3403739417468834 \t -0.058708410729165106\n",
            "42     \t [-1.57117056 -1.41562841]. \t  -1515.3160328313832 \t -0.058708410729165106\n",
            "43     \t [-1.14378271  1.53790107]. \t  -9.870275462570547 \t -0.058708410729165106\n",
            "44     \t [ 0.30717576 -1.52429166]. \t  -262.48233492832753 \t -0.058708410729165106\n",
            "45     \t [-1.56696708  2.0126539 ]. \t  -26.19047522490724 \t -0.058708410729165106\n",
            "46     \t [1.88525821 1.26502271]. \t  -524.8162642938845 \t -0.058708410729165106\n",
            "47     \t [ 0.62318168 -1.80127686]. \t  -479.5909388679256 \t -0.058708410729165106\n",
            "48     \t [1.04833034 1.13063514]. \t  -0.1024361471659078 \t -0.058708410729165106\n",
            "49     \t [-1.84079572 -0.74649412]. \t  -1717.9116308337475 \t -0.058708410729165106\n",
            "50     \t [-0.88092941  0.62727091]. \t  -5.751019127351177 \t -0.058708410729165106\n",
            "51     \t [1.83553193 0.33688002]. \t  -920.1808958748647 \t -0.058708410729165106\n",
            "52     \t [-1.52331428 -1.84815522]. \t  -1744.1244075997931 \t -0.058708410729165106\n",
            "53     \t [-0.73564415 -0.06838934]. \t  -40.169002790667555 \t -0.058708410729165106\n",
            "54     \t [0.66462833 0.28171671]. \t  -2.6729253866224685 \t -0.058708410729165106\n",
            "55     \t [1.39556159 0.18113757]. \t  -312.1926517882368 \t -0.058708410729165106\n",
            "56     \t [0.72385481 0.5282031 ]. \t  -0.0780516430958971 \t -0.058708410729165106\n",
            "57     \t [-2.04673134  0.73703322]. \t  -1200.965413094416 \t -0.058708410729165106\n",
            "58     \t [-0.36103456  1.36325542]. \t  -153.85898978774878 \t -0.058708410729165106\n",
            "59     \t [-0.90866029 -2.02634107]. \t  -817.0360048393575 \t -0.058708410729165106\n",
            "60     \t [1.22290955 0.92966209]. \t  -32.06782246041859 \t -0.058708410729165106\n",
            "61     \t [-0.79383284  0.19903415]. \t  -21.80569745199871 \t -0.058708410729165106\n",
            "62     \t [ 1.92822791 -0.76753784]. \t  -2012.9229642283742 \t -0.058708410729165106\n",
            "63     \t [ 1.57226794 -1.26678748]. \t  -1398.200459272032 \t -0.058708410729165106\n",
            "64     \t [0.9881643  0.92755245]. \t  -0.23941983709691012 \t -0.058708410729165106\n",
            "65     \t [0.80554936 0.70715048]. \t  -0.3770091531700728 \t -0.058708410729165106\n",
            "66     \t [0.83257272 0.65495279]. \t  -0.17414348972741428 \t -0.058708410729165106\n",
            "67     \t [0.8505218  0.72786466]. \t  \u001b[92m-0.0243483749920794\u001b[0m \t -0.0243483749920794\n",
            "68     \t [1.0212155  0.97096363]. \t  -0.5176622726142415 \t -0.0243483749920794\n",
            "69     \t [0.91253656 0.79462043]. \t  -0.15283022315571587 \t -0.0243483749920794\n",
            "70     \t [0.92354311 0.81902767]. \t  -0.12079525787715831 \t -0.0243483749920794\n",
            "71     \t [-1.45974772 -1.46417609]. \t  -1298.4812654067393 \t -0.0243483749920794\n",
            "72     \t [-1.00611908 -1.01242955]. \t  -413.96760905821293 \t -0.0243483749920794\n",
            "73     \t [0.91719488 0.81693209]. \t  -0.06597551841845316 \t -0.0243483749920794\n",
            "74     \t [-1.88198369  0.9001927 ]. \t  -706.147826850092 \t -0.0243483749920794\n",
            "75     \t [1.34733018 1.80186027]. \t  -0.13869718257733363 \t -0.0243483749920794\n",
            "76     \t [0.94375639 0.86248619]. \t  -0.08263062507113914 \t -0.0243483749920794\n",
            "77     \t [-1.38585433  1.52392818]. \t  -21.42653644467692 \t -0.0243483749920794\n",
            "78     \t [ 0.21168654 -2.04525519]. \t  -437.45918748948486 \t -0.0243483749920794\n",
            "79     \t [-0.26562798 -0.10382778]. \t  -4.642861829112339 \t -0.0243483749920794\n",
            "80     \t [ 1.07400472 -0.48560451]. \t  -268.6672967922276 \t -0.0243483749920794\n",
            "81     \t [0.00470101 1.36244757]. \t  -186.6109362309 \t -0.0243483749920794\n",
            "82     \t [0.94328388 0.85009212]. \t  -0.1607649780410142 \t -0.0243483749920794\n",
            "83     \t [ 0.15233627 -0.3486703 ]. \t  -14.547756874936322 \t -0.0243483749920794\n",
            "84     \t [0.58467639 0.35330111]. \t  -0.18561456194862833 \t -0.0243483749920794\n",
            "85     \t [ 1.07502315 -1.54739593]. \t  -730.6647565439483 \t -0.0243483749920794\n",
            "86     \t [-0.66106048 -1.2168329 ]. \t  -276.275766282547 \t -0.0243483749920794\n",
            "87     \t [0.81351869 0.65559561]. \t  -0.038640448719723115 \t -0.0243483749920794\n",
            "88     \t [-0.4204121   0.06342461]. \t  -3.3017519346355586 \t -0.0243483749920794\n",
            "89     \t [0.83657766 0.72191524]. \t  -0.07534062005519546 \t -0.0243483749920794\n",
            "90     \t [-1.42651633 -0.83545454]. \t  -829.8095373075021 \t -0.0243483749920794\n",
            "91     \t [0.56601284 0.31318939]. \t  -0.19350174366378786 \t -0.0243483749920794\n",
            "92     \t [1.01822452 1.0009088 ]. \t  -0.12901492330007186 \t -0.0243483749920794\n",
            "93     \t [ 0.73320313 -2.01624691]. \t  -652.2778548155615 \t -0.0243483749920794\n",
            "94     \t [1.3049085 0.6343088]. \t  -114.25736576089821 \t -0.0243483749920794\n",
            "95     \t [1.08881537 1.18329627]. \t  \u001b[92m-0.00838218409886892\u001b[0m \t -0.00838218409886892\n",
            "96     \t [-0.84241255  1.53069914]. \t  -70.80519133967313 \t -0.00838218409886892\n",
            "97     \t [0.96485522 0.89783782]. \t  -0.11084757031898443 \t -0.00838218409886892\n",
            "98     \t [-0.84467376 -1.08464724]. \t  -326.7267386814881 \t -0.00838218409886892\n",
            "99     \t [-0.169081   1.8286972]. \t  -325.40592360796836 \t -0.00838218409886892\n",
            "100    \t [1.0736673  1.15605205]. \t  \u001b[92m-0.0065096651679119495\u001b[0m \t -0.0065096651679119495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4HnKuoqteDo",
        "outputId": "d3191d80-4e74-4c79-8a00-a627af40363f"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 19\r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_loser_19 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_19 = dGPGO(surrogate_loser_19, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.24348908 1.07107119]. \t  -102.94304782477323 \t -4.219752052396591\n",
            "init   \t [-1.04639875  1.11284007]. \t  -4.219752052396591 \t -4.219752052396591\n",
            "init   \t [-0.59608235  1.21527255]. \t  -76.50031976976825 \t -4.219752052396591\n",
            "init   \t [-0.63078391 -0.22989031]. \t  -42.07005958957044 \t -4.219752052396591\n",
            "init   \t [-0.8056685  -0.23710138]. \t  -81.79603430667063 \t -4.219752052396591\n",
            "1      \t [ 0.45286922 -0.60514617]. \t  -65.94770306515855 \t -4.219752052396591\n",
            "2      \t [-1.9115265   1.53808142]. \t  -456.16001072637476 \t -4.219752052396591\n",
            "3      \t [-0.76878948  0.5483007 ]. \t  \u001b[92m-3.311257684438914\u001b[0m \t -3.311257684438914\n",
            "4      \t [1.50989435 1.30714346]. \t  -94.86236329029082 \t -3.311257684438914\n",
            "5      \t [-0.06855973 -0.20779091]. \t  -5.6570771189820555 \t -3.311257684438914\n",
            "6      \t [-0.21718482 -1.25152481]. \t  -170.14216241918132 \t -3.311257684438914\n",
            "7      \t [-1.12098267  0.71437221]. \t  -33.89989778483975 \t -3.311257684438914\n",
            "8      \t [0.86718777 0.38448612]. \t  -13.525359581074524 \t -3.311257684438914\n",
            "9      \t [-0.3698933   0.13024091]. \t  \u001b[92m-1.8809374880110328\u001b[0m \t -1.8809374880110328\n",
            "10     \t [-0.85352564  0.82928547]. \t  -4.451207116317159 \t -1.8809374880110328\n",
            "11     \t [0.83889826 0.78763991]. \t  \u001b[92m-0.7297005765149895\u001b[0m \t -0.7297005765149895\n",
            "12     \t [0.8226043  0.86534855]. \t  -3.5911331260262944 \t -0.7297005765149895\n",
            "13     \t [-1.53636148  0.48205596]. \t  -359.2532426805832 \t -0.7297005765149895\n",
            "14     \t [ 2.02523343 -2.0352233 ]. \t  -3767.074858043112 \t -0.7297005765149895\n",
            "15     \t [0.88284465 0.72015273]. \t  \u001b[92m-0.36492308373772725\u001b[0m \t -0.36492308373772725\n",
            "16     \t [0.759251   0.60408869]. \t  \u001b[92m-0.13428305632045917\u001b[0m \t -0.13428305632045917\n",
            "17     \t [-0.59840815  0.22619528]. \t  -4.294591524836828 \t -0.13428305632045917\n",
            "18     \t [-0.7745717   1.93339717]. \t  -180.95422369574854 \t -0.13428305632045917\n",
            "19     \t [0.21469522 0.04673812]. \t  -0.616745081528505 \t -0.13428305632045917\n",
            "20     \t [-1.75212771  1.1013791 ]. \t  -395.1019391827967 \t -0.13428305632045917\n",
            "21     \t [1.4252979  0.46022507]. \t  -247.06323074062726 \t -0.13428305632045917\n",
            "22     \t [0.37273513 0.03011682]. \t  -1.5775241694602857 \t -0.13428305632045917\n",
            "23     \t [1.06437523 1.70505825]. \t  -32.74126330082493 \t -0.13428305632045917\n",
            "24     \t [0.76101456 0.58943797]. \t  \u001b[92m-0.06771235104415185\u001b[0m \t -0.06771235104415185\n",
            "25     \t [ 0.60120811 -0.72900613]. \t  -119.0687515408353 \t -0.06771235104415185\n",
            "26     \t [0.6981431  0.44858296]. \t  -0.2418232592397735 \t -0.06771235104415185\n",
            "27     \t [-0.02586991 -1.69586234]. \t  -288.8743529540324 \t -0.06771235104415185\n",
            "28     \t [0.7284284  0.49569926]. \t  -0.19561273230144782 \t -0.06771235104415185\n",
            "29     \t [0.11330097 1.90764284]. \t  -359.81511194786157 \t -0.06771235104415185\n",
            "30     \t [1.41028662 1.82126185]. \t  -2.9788701110792903 \t -0.06771235104415185\n",
            "31     \t [1.22026773 1.09116792]. \t  -15.879797912924316 \t -0.06771235104415185\n",
            "32     \t [0.92173749 0.9008191 ]. \t  -0.26846459008552587 \t -0.06771235104415185\n",
            "33     \t [1.17559437 1.46732948]. \t  -0.7585676841325021 \t -0.06771235104415185\n",
            "34     \t [1.36646387 1.84186363]. \t  -0.19860803646555897 \t -0.06771235104415185\n",
            "35     \t [0.97145593 0.99725933]. \t  -0.2873898375386886 \t -0.06771235104415185\n",
            "36     \t [ 0.54668634 -0.75384016]. \t  -111.02450826383281 \t -0.06771235104415185\n",
            "37     \t [0.72896395 0.54696825]. \t  -0.09773360590918789 \t -0.06771235104415185\n",
            "38     \t [1.14191476 1.27311495]. \t  -0.11533894445110274 \t -0.06771235104415185\n",
            "39     \t [0.99785923 1.02203016]. \t  -0.06921107633250193 \t -0.06771235104415185\n",
            "40     \t [0.3122556  1.58620122]. \t  -222.09506467666787 \t -0.06771235104415185\n",
            "41     \t [-1.64096815  1.06322663]. \t  -272.51797697246604 \t -0.06771235104415185\n",
            "42     \t [-1.37063836  1.96020876]. \t  -6.285117237218309 \t -0.06771235104415185\n",
            "43     \t [-1.48643837  1.6254754 ]. \t  -40.29073407872109 \t -0.06771235104415185\n",
            "44     \t [0.95687951 0.91392703]. \t  \u001b[92m-0.00214544917233134\u001b[0m \t -0.00214544917233134\n",
            "45     \t [0.96673899 0.86586977]. \t  -0.4732744870171971 \t -0.00214544917233134\n",
            "46     \t [ 0.04979269 -1.63019338]. \t  -267.46490688217597 \t -0.00214544917233134\n",
            "47     \t [0.92442281 0.82308284]. \t  -0.10477754316675009 \t -0.00214544917233134\n",
            "48     \t [1.09287213 1.18906238]. \t  -0.01144177199045211 \t -0.00214544917233134\n",
            "49     \t [-1.55336905 -0.94182605]. \t  -1131.975543495534 \t -0.00214544917233134\n",
            "50     \t [0.55456785 0.27760079]. \t  -0.28807838200332336 \t -0.00214544917233134\n",
            "51     \t [-0.12291179  1.65516704]. \t  -270.24052394820245 \t -0.00214544917233134\n",
            "52     \t [-0.8913757  -1.21622906]. \t  -407.9008014958416 \t -0.00214544917233134\n",
            "53     \t [1.40039471 2.01317744]. \t  -0.4314663470058887 \t -0.00214544917233134\n",
            "54     \t [1.78058467 0.35588287]. \t  -792.8060006187319 \t -0.00214544917233134\n",
            "55     \t [0.22960627 0.10039259]. \t  -0.8207832359447311 \t -0.00214544917233134\n",
            "56     \t [ 0.77667859 -1.45008068]. \t  -421.65819600229224 \t -0.00214544917233134\n",
            "57     \t [1.16552328 0.63863447]. \t  -51.840050038487895 \t -0.00214544917233134\n",
            "58     \t [-1.88003489  1.2894185 ]. \t  -512.3476918814229 \t -0.00214544917233134\n",
            "59     \t [0.07486711 0.19051247]. \t  -4.274945064997554 \t -0.00214544917233134\n",
            "60     \t [0.88993571 0.79143583]. \t  -0.01214436931057295 \t -0.00214544917233134\n",
            "61     \t [-0.73526457  1.37349327]. \t  -72.37993316314449 \t -0.00214544917233134\n",
            "62     \t [1.02234657 1.04675329]. \t  \u001b[92m-0.0007429711644439714\u001b[0m \t -0.0007429711644439714\n",
            "63     \t [-1.26788648  1.07391939]. \t  -33.61799191629573 \t -0.0007429711644439714\n",
            "64     \t [1.08412302 1.15192111]. \t  -0.061840240111214015 \t -0.0007429711644439714\n",
            "65     \t [0.00666815 0.1064489 ]. \t  -2.1188986542501347 \t -0.0007429711644439714\n",
            "66     \t [-0.99005096 -1.41917455]. \t  -579.6605564382465 \t -0.0007429711644439714\n",
            "67     \t [-0.12782104 -0.34805222]. \t  -14.550019663292455 \t -0.0007429711644439714\n",
            "68     \t [-0.21021943 -1.84038436]. \t  -356.6275139476734 \t -0.0007429711644439714\n",
            "69     \t [ 1.25787611 -1.03913335]. \t  -687.2327766306505 \t -0.0007429711644439714\n",
            "70     \t [1.45679267 2.03870159]. \t  -0.9066076663296834 \t -0.0007429711644439714\n",
            "71     \t [-1.88464203  0.89496835]. \t  -714.236756096389 \t -0.0007429711644439714\n",
            "72     \t [-1.48129145  1.4231625 ]. \t  -65.61044659048798 \t -0.0007429711644439714\n",
            "73     \t [0.70714689 0.47395272]. \t  -0.1539048247424365 \t -0.0007429711644439714\n",
            "74     \t [0.91878218 1.19058995]. \t  -12.00791942197261 \t -0.0007429711644439714\n",
            "75     \t [1.34073137 1.92806476]. \t  -1.8192313711411956 \t -0.0007429711644439714\n",
            "76     \t [-0.44059091  0.50493421]. \t  -11.73582806019964 \t -0.0007429711644439714\n",
            "77     \t [-1.63805227  1.99291998]. \t  -54.61007531448305 \t -0.0007429711644439714\n",
            "78     \t [-1.96120296 -0.54796173]. \t  -1939.737311709064 \t -0.0007429711644439714\n",
            "79     \t [-0.25470364  0.21967616]. \t  -3.9706539656228124 \t -0.0007429711644439714\n",
            "80     \t [1.51229804 0.73918524]. \t  -239.8495473713162 \t -0.0007429711644439714\n",
            "81     \t [-0.80986668 -0.00261677]. \t  -46.637947996074075 \t -0.0007429711644439714\n",
            "82     \t [ 0.48940055 -1.91505832]. \t  -464.4784249439519 \t -0.0007429711644439714\n",
            "83     \t [1.00462471 0.49002277]. \t  -26.961873973475413 \t -0.0007429711644439714\n",
            "84     \t [-0.22239426 -1.4187918 ]. \t  -217.07035042605008 \t -0.0007429711644439714\n",
            "85     \t [ 0.23122171 -1.58196944]. \t  -268.05510264287017 \t -0.0007429711644439714\n",
            "86     \t [0.26118112 1.94675998]. \t  -353.43875976637065 \t -0.0007429711644439714\n",
            "87     \t [-1.6533929  -1.47474582]. \t  -1778.148931412991 \t -0.0007429711644439714\n",
            "88     \t [-1.73150366 -1.92966205]. \t  -2435.7498493694384 \t -0.0007429711644439714\n",
            "89     \t [0.46346717 0.22266945]. \t  -0.29405745378780723 \t -0.0007429711644439714\n",
            "90     \t [ 1.09161653 -1.74010343]. \t  -859.5125247473624 \t -0.0007429711644439714\n",
            "91     \t [0.57770317 0.37451944]. \t  -0.34462310723257716 \t -0.0007429711644439714\n",
            "92     \t [ 1.86123761 -1.17692962]. \t  -2154.755215436421 \t -0.0007429711644439714\n",
            "93     \t [-0.48850096 -0.58947631]. \t  -70.79216904289397 \t -0.0007429711644439714\n",
            "94     \t [0.48560572 0.26495299]. \t  -0.34951582377183243 \t -0.0007429711644439714\n",
            "95     \t [ 0.07153458 -0.88529216]. \t  -80.14492959171838 \t -0.0007429711644439714\n",
            "96     \t [0.26290806 1.8942863 ]. \t  -333.66626838296577 \t -0.0007429711644439714\n",
            "98     \t [ 0.17833914 -1.71005162]. \t  -304.0815231773848 \t -0.0007429711644439714\n",
            "99     \t [ 1.29958474 -1.45677609]. \t  -989.630449668447 \t -0.0007429711644439714\n",
            "100    \t [-0.01542798  2.0425412 ]. \t  -418.1313208144742 \t -0.0007429711644439714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfiDmH_bteGg",
        "outputId": "ef758466-84ea-4aa8-a68e-c24aa392fafc"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Loser' Acquisition Function run number = 20\r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_loser_20 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "loser_20 = dGPGO(surrogate_loser_20, Acquisition_new(util_loser), f_syn_polarity, param, n_jobs = -1) # define BayesOpt\r\n",
        "loser_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.2888388  0.26430978]. \t  -3.777577453542735 \t -3.777577453542735\n",
            "init   \t [-0.04734225 -0.66978712]. \t  -46.25914429040042 \t -3.777577453542735\n",
            "init   \t [-0.50844394  0.13121892]. \t  -3.895838569726608 \t -3.777577453542735\n",
            "init   \t [-1.76903664  0.34623103]. \t  -782.3209705533411 \t -3.777577453542735\n",
            "init   \t [-1.07357076 -1.38954104]. \t  -650.524506936468 \t -3.777577453542735\n",
            "1      \t [-0.12445779  0.09875914]. \t  \u001b[92m-1.9577846182863219\u001b[0m \t -1.9577846182863219\n",
            "2      \t [1.53218166 0.73906821]. \t  -259.01444417510237 \t -1.9577846182863219\n",
            "3      \t [ 0.92617356 -1.26728579]. \t  -451.6033360820788 \t -1.9577846182863219\n",
            "4      \t [0.04098166 1.4092975 ]. \t  -199.05856175553572 \t -1.9577846182863219\n",
            "5      \t [-0.35605494  0.10030413]. \t  \u001b[92m-1.908956308825034\u001b[0m \t -1.908956308825034\n",
            "6      \t [ 0.44408175 -0.04477678]. \t  -6.1647374965964925 \t -1.908956308825034\n",
            "7      \t [-0.50597028  0.52152132]. \t  -9.31778908557592 \t -1.908956308825034\n",
            "8      \t [ 0.07418699 -0.06313874]. \t  \u001b[92m-1.3283082955160073\u001b[0m \t -1.3283082955160073\n",
            "9      \t [-0.48316149  0.04533448]. \t  -5.738325805710129 \t -1.3283082955160073\n",
            "10     \t [0.19938642 0.09381441]. \t  \u001b[92m-0.9332246150755861\u001b[0m \t -0.9332246150755861\n",
            "11     \t [-0.4132618   0.24390081]. \t  -2.531896475294343 \t -0.9332246150755861\n",
            "12     \t [ 1.61316771 -0.64057739]. \t  -1052.0078787746854 \t -0.9332246150755861\n",
            "13     \t [ 0.1922373  -0.04692773]. \t  -1.356114779508586 \t -0.9332246150755861\n",
            "14     \t [-0.09641529 -0.55990481]. \t  -33.60107280746357 \t -0.9332246150755861\n",
            "15     \t [0.30449669 0.04409213]. \t  \u001b[92m-0.720174622641462\u001b[0m \t -0.720174622641462\n",
            "16     \t [-0.53802008  1.68551668]. \t  -197.26136577179662 \t -0.720174622641462\n",
            "17     \t [ 0.53483739 -1.11396349]. \t  -196.22044177322152 \t -0.720174622641462\n",
            "18     \t [ 1.08585101 -0.14887929]. \t  -176.35294415771122 \t -0.720174622641462\n",
            "19     \t [0.28720071 0.03015814]. \t  -0.7818849215335612 \t -0.720174622641462\n",
            "20     \t [0.38644135 0.13973336]. \t  \u001b[92m-0.3856770475051456\u001b[0m \t -0.3856770475051456\n",
            "21     \t [0.59758858 1.12328572]. \t  -58.864134396647785 \t -0.3856770475051456\n",
            "22     \t [-1.20433052  1.77068799]. \t  -15.11674392603221 \t -0.3856770475051456\n",
            "23     \t [0.39304684 0.1292966 ]. \t  -0.43184182513176 \t -0.3856770475051456\n",
            "24     \t [1.98335063 1.77854771]. \t  -465.4263786808205 \t -0.3856770475051456\n",
            "25     \t [ 1.9065061  -0.75279122]. \t  -1925.8871723897323 \t -0.3856770475051456\n",
            "26     \t [-1.82681554  1.9426022 ]. \t  -202.49653787335362 \t -0.3856770475051456\n",
            "27     \t [-1.28963972  1.53715749]. \t  -6.830380602629708 \t -0.3856770475051456\n",
            "28     \t [-0.97310767  1.07730258]. \t  -5.59263230344469 \t -0.3856770475051456\n",
            "29     \t [-1.12981645  1.43216933]. \t  -6.959872717729103 \t -0.3856770475051456\n",
            "30     \t [-1.82429495  2.02042425]. \t  -178.9656971094364 \t -0.3856770475051456\n",
            "31     \t [0.32647522 0.09581771]. \t  -0.4652313823830797 \t -0.3856770475051456\n",
            "32     \t [0.71743311 1.86409402]. \t  -182.16349544847446 \t -0.3856770475051456\n",
            "33     \t [-1.75119222 -1.53266927]. \t  -2122.965094391892 \t -0.3856770475051456\n",
            "34     \t [-0.34315729 -1.82150672]. \t  -377.8784220837419 \t -0.3856770475051456\n",
            "35     \t [0.71344874 0.57377747]. \t  -0.5016056477204732 \t -0.3856770475051456\n",
            "36     \t [ 0.23209137 -0.67313579]. \t  -53.44290326077746 \t -0.3856770475051456\n",
            "37     \t [0.71680633 0.63813981]. \t  -1.6259561502064939 \t -0.3856770475051456\n",
            "38     \t [ 0.23765545 -1.4905528 ]. \t  -239.91225349165356 \t -0.3856770475051456\n",
            "39     \t [1.5768618  1.87960322]. \t  -37.1643059079143 \t -0.3856770475051456\n",
            "40     \t [1.2227872 1.9454104]. \t  -20.317806259435226 \t -0.3856770475051456\n",
            "41     \t [1.23911497 1.5713019 ]. \t  \u001b[92m-0.18602820046103113\u001b[0m \t -0.18602820046103113\n",
            "42     \t [1.33976262 1.80300852]. \t  \u001b[92m-0.1219102711382461\u001b[0m \t -0.1219102711382461\n",
            "43     \t [1.15796021 1.42087308]. \t  -0.664971072607035 \t -0.1219102711382461\n",
            "44     \t [1.37576884 1.90701543]. \t  -0.16158125926322597 \t -0.1219102711382461\n",
            "45     \t [1.26091935 1.37815125]. \t  -4.552578064003302 \t -0.1219102711382461\n",
            "46     \t [1.42376987 1.95616896]. \t  -0.6829951413682424 \t -0.1219102711382461\n",
            "47     \t [-1.61358248 -0.27306354]. \t  -834.3779756965681 \t -0.1219102711382461\n",
            "48     \t [-0.73357865 -0.93107915]. \t  -218.86508952773173 \t -0.1219102711382461\n",
            "49     \t [-1.88685634  0.14283044]. \t  -1176.193766234887 \t -0.1219102711382461\n",
            "50     \t [1.18393232 1.36293762]. \t  -0.18405026548519626 \t -0.1219102711382461\n",
            "51     \t [-0.27884781 -1.65221693]. \t  -300.91612072205027 \t -0.1219102711382461\n",
            "52     \t [0.95958742 0.4540251 ]. \t  -21.79026178952775 \t -0.1219102711382461\n",
            "53     \t [-1.23526845  1.31858593]. \t  -9.293846013936001 \t -0.1219102711382461\n",
            "54     \t [1.16727424 1.37071159]. \t  \u001b[92m-0.03467588777220765\u001b[0m \t -0.03467588777220765\n",
            "55     \t [0.84650621 0.67813071]. \t  -0.1713395092054355 \t -0.03467588777220765\n",
            "56     \t [0.68192035 0.41370143]. \t  -0.3644867267060248 \t -0.03467588777220765\n",
            "57     \t [1.38024534 1.90179781]. \t  -0.1456619528361899 \t -0.03467588777220765\n",
            "58     \t [0.86138726 0.78131544]. \t  -0.17387812398445762 \t -0.03467588777220765\n",
            "59     \t [1.01939187 1.03552353]. \t  \u001b[92m-0.001698275401478381\u001b[0m \t -0.001698275401478381\n",
            "60     \t [-0.79141436  0.56135413]. \t  -3.631438818286104 \t -0.001698275401478381\n",
            "61     \t [ 0.05867034 -1.4849026 ]. \t  -222.40312704335412 \t -0.001698275401478381\n",
            "62     \t [1.17938961 0.52339748]. \t  -75.29862545450827 \t -0.001698275401478381\n",
            "63     \t [ 0.9797235  -1.05616155]. \t  -406.4339491431488 \t -0.001698275401478381\n",
            "64     \t [0.89953123 0.79230413]. \t  -0.038494015891723674 \t -0.001698275401478381\n",
            "65     \t [0.75258149 0.56861868]. \t  -0.06171757795640556 \t -0.001698275401478381\n",
            "66     \t [-0.31349505 -0.18238048]. \t  -9.602251893713131 \t -0.001698275401478381\n",
            "67     \t [1.29570111 1.68430532]. \t  -0.09042461646734903 \t -0.001698275401478381\n",
            "68     \t [1.22010126 1.48256734]. \t  -0.052140900376403884 \t -0.001698275401478381\n",
            "69     \t [1.26030215 0.3072364 ]. \t  -164.19590973551527 \t -0.001698275401478381\n",
            "70     \t [1.76491521 1.48479772]. \t  -266.3168113776189 \t -0.001698275401478381\n",
            "71     \t [0.9378632  0.83764791]. \t  -0.17975293198643705 \t -0.001698275401478381\n",
            "72     \t [-1.66686773 -1.97826056]. \t  -2269.73983224685 \t -0.001698275401478381\n",
            "73     \t [0.91273974 0.83358512]. \t  -0.007638489299499125 \t -0.001698275401478381\n",
            "74     \t [1.22371457 1.48714945]. \t  -0.06071476267657547 \t -0.001698275401478381\n",
            "75     \t [0.58494512 0.5863745 ]. \t  -6.1363039550597644 \t -0.001698275401478381\n",
            "76     \t [1.27947381 1.62033334]. \t  -0.10606103609796559 \t -0.001698275401478381\n",
            "77     \t [0.79135292 1.58866863]. \t  -92.67052567735936 \t -0.001698275401478381\n",
            "78     \t [-0.45584474 -1.99917753]. \t  -489.1920031520368 \t -0.001698275401478381\n",
            "79     \t [0.74078949 0.54122632]. \t  -0.07287939025297656 \t -0.001698275401478381\n",
            "80     \t [1.34227767 1.77403493]. \t  -0.1937414006596084 \t -0.001698275401478381\n",
            "81     \t [ 1.21183187 -0.47090083]. \t  -376.1865774429796 \t -0.001698275401478381\n",
            "82     \t [ 0.53417678 -0.92816523]. \t  -147.47765806935433 \t -0.001698275401478381\n",
            "83     \t [0.66456458 0.44058435]. \t  -0.11262964677660582 \t -0.001698275401478381\n",
            "84     \t [-0.16315989 -0.73093746]. \t  -58.74244509940196 \t -0.001698275401478381\n",
            "85     \t [ 1.30373351 -0.58779085]. \t  -523.3633306799418 \t -0.001698275401478381\n",
            "86     \t [0.57389326 0.34698358]. \t  -0.21264901190978835 \t -0.001698275401478381\n",
            "87     \t [0.62451653 0.39346195]. \t  -0.14217192306761597 \t -0.001698275401478381\n",
            "88     \t [0.62742699 0.38542423]. \t  -0.14560107322342758 \t -0.001698275401478381\n",
            "89     \t [0.06839996 1.52341975]. \t  -231.52535975324432 \t -0.001698275401478381\n",
            "90     \t [0.62018364 0.37872747]. \t  -0.1477417902693745 \t -0.001698275401478381\n",
            "91     \t [0.970399   0.92872497]. \t  -0.01764455556492652 \t -0.001698275401478381\n",
            "92     \t [1.04503976 1.08390155]. \t  -0.00876330905533472 \t -0.001698275401478381\n",
            "93     \t [-0.18733043  1.95873802]. \t  -371.45089028383757 \t -0.001698275401478381\n",
            "94     \t [ 1.18442453 -0.54371545]. \t  -378.9501784625779 \t -0.001698275401478381\n",
            "95     \t [-0.8874626 -0.222455 ]. \t  -105.58157972927293 \t -0.001698275401478381\n",
            "96     \t [0.99870085 1.02741027]. \t  -0.0900429631432507 \t -0.001698275401478381\n",
            "97     \t [0.87780046 0.76482178]. \t  -0.018195278625614486 \t -0.001698275401478381\n",
            "98     \t [0.04630707 1.35580958]. \t  -184.1504868396169 \t -0.001698275401478381\n",
            "99     \t [ 1.46831754 -0.18825834]. \t  -549.7536009552517 \t -0.001698275401478381\n",
            "100    \t [1.04147996 1.09470167]. \t  -0.011762966578556024 \t -0.001698275401478381\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kM4bcwSteJO",
        "outputId": "b388a379-1434-4059-9d75-ac991b7d1643"
      },
      "source": [
        "end_lose = time.time()\r\n",
        "end_lose\r\n",
        "\r\n",
        "time_lose = end_lose - start_lose\r\n",
        "time_lose\r\n",
        "\r\n",
        "start_win = time.time()\r\n",
        "start_win"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1615133511.9080303"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NMzLM7nteQH",
        "outputId": "112d7dbd-1fb6-4ced-bd72-c467414c6737"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 1 \r\n",
        "\r\n",
        "np.random.seed(run_num_1)\r\n",
        "surrogate_winner_1 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_1 = dGPGO(surrogate_winner_1, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_1.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.45944904 -1.35549029]. \t  -245.71064611316496 \t -108.5713485785257\n",
            "init   \t [-0.26190226  1.10289909]. \t  -108.5713485785257 \t -108.5713485785257\n",
            "init   \t [-0.83834755 -1.43702853]. \t  -461.2775269355244 \t -108.5713485785257\n",
            "init   \t [-1.95592878 -0.32676048]. \t  -1732.9949421003257 \t -108.5713485785257\n",
            "init   \t [-1.07035795 -0.66496024]. \t  -332.12317010404104 \t -108.5713485785257\n",
            "1      \t [-0.24340498 -0.21806097]. \t  \u001b[92m-9.235970652898509\u001b[0m \t -9.235970652898509\n",
            "2      \t [-0.46322802 -0.47484553]. \t  -49.67181969335121 \t -9.235970652898509\n",
            "3      \t [ 1.65474772 -1.44930184]. \t  -1753.9374899654747 \t -9.235970652898509\n",
            "4      \t [0.36510189 0.52897566]. \t  -16.0590668902325 \t -9.235970652898509\n",
            "5      \t [-0.04252081  0.12322193]. \t  \u001b[92m-2.560983415256553\u001b[0m \t -2.560983415256553\n",
            "6      \t [-0.04915232 -0.05501156]. \t  \u001b[92m-1.430512457575215\u001b[0m \t -1.430512457575215\n",
            "7      \t [-0.01821572 -0.04137437]. \t  \u001b[92m-1.2107038123696634\u001b[0m \t -1.2107038123696634\n",
            "8      \t [-0.0114277   0.01226394]. \t  \u001b[92m-1.0377078075703994\u001b[0m \t -1.0377078075703994\n",
            "9      \t [-0.02841438  0.12251698]. \t  -2.538958791429727 \t -1.0377078075703994\n",
            "10     \t [2.048 2.048]. \t  -461.76039004145935 \t -1.0377078075703994\n",
            "11     \t [-0.05077951  0.05760172]. \t  -1.4068924533934384 \t -1.0377078075703994\n",
            "12     \t [-0.04446007  0.04039531]. \t  -1.2384958435446398 \t -1.0377078075703994\n",
            "13     \t [-0.01023269  0.03677599]. \t  -1.155048346445624 \t -1.0377078075703994\n",
            "14     \t [-0.12915453 -0.10013774]. \t  -2.6396492814507035 \t -1.0377078075703994\n",
            "15     \t [-0.03899093  0.01952358]. \t  -1.1119139975970473 \t -1.0377078075703994\n",
            "16     \t [-0.04260543  0.01151395]. \t  -1.0964326088510612 \t -1.0377078075703994\n",
            "17     \t [-0.07217971 -0.06190638]. \t  -1.600028993230608 \t -1.0377078075703994\n",
            "18     \t [ 0.00625794 -0.00155856]. \t  \u001b[92m-0.9877785528445128\u001b[0m \t -0.9877785528445128\n",
            "19     \t [-0.01753365  0.01025846]. \t  -1.0452770326965617 \t -0.9877785528445128\n",
            "20     \t [0.03477236 0.00535921]. \t  \u001b[92m-0.9333867277559209\u001b[0m \t -0.9333867277559209\n",
            "21     \t [ 0.04983163 -0.00055601]. \t  \u001b[92m-0.9037436077337533\u001b[0m \t -0.9037436077337533\n",
            "22     \t [ 0.01984191 -0.00659136]. \t  -0.9655889802902117 \t -0.9037436077337533\n",
            "23     \t [0.02553644 0.0286881 ]. \t  -1.028180912682159 \t -0.9037436077337533\n",
            "24     \t [0.02941939 0.04276897]. \t  -1.117616820675472 \t -0.9037436077337533\n",
            "25     \t [0.00439697 0.05519942]. \t  -1.2957096172359646 \t -0.9037436077337533\n",
            "26     \t [-0.05352009  0.02282866]. \t  -1.1497617403086644 \t -0.9037436077337533\n",
            "27     \t [0.0299905  0.04286174]. \t  -1.117001976046654 \t -0.9037436077337533\n",
            "28     \t [ 0.0244465  -0.04845401]. \t  -1.192310972031851 \t -0.9037436077337533\n",
            "29     \t [0.17360833 0.01406421]. \t  \u001b[92m-0.7087658322764571\u001b[0m \t -0.7087658322764571\n",
            "30     \t [ 0.08790572 -0.00157696]. \t  -0.8405731142667451 \t -0.7087658322764571\n",
            "31     \t [0.38680959 0.19059251]. \t  \u001b[92m-0.543863512452272\u001b[0m \t -0.543863512452272\n",
            "32     \t [0.22312723 0.06408368]. \t  -0.6239743554978994 \t -0.543863512452272\n",
            "33     \t [0.49276452 0.23345206]. \t  \u001b[92m-0.26605780063349754\u001b[0m \t -0.26605780063349754\n",
            "34     \t [0.60791091 0.30695169]. \t  -0.5456598119952061 \t -0.26605780063349754\n",
            "35     \t [0.38568711 0.13610595]. \t  -0.3933790191564531 \t -0.26605780063349754\n",
            "36     \t [0.49998341 0.24537955]. \t  \u001b[92m-0.2521361457624336\u001b[0m \t -0.2521361457624336\n",
            "37     \t [0.52879081 0.25726292]. \t  -0.2720207811358813 \t -0.2521361457624336\n",
            "38     \t [0.48856785 0.23214251]. \t  -0.2658609971185279 \t -0.2521361457624336\n",
            "39     \t [0.52054068 0.25301585]. \t  -0.26208982691975025 \t -0.2521361457624336\n",
            "40     \t [0.48330255 0.23983824]. \t  -0.27089111450919506 \t -0.2521361457624336\n",
            "41     \t [0.48995881 0.24802626]. \t  -0.2664887162056237 \t -0.2521361457624336\n",
            "42     \t [0.51149405 0.24141691]. \t  -0.27947944987108786 \t -0.2521361457624336\n",
            "43     \t [0.50620499 0.2337764 ]. \t  -0.29431052391362766 \t -0.2521361457624336\n",
            "44     \t [0.48286228 0.23240355]. \t  -0.2674880334556849 \t -0.2521361457624336\n",
            "45     \t [0.50111845 0.25240068]. \t  \u001b[92m-0.24904689160711327\u001b[0m \t -0.24904689160711327\n",
            "46     \t [0.52170305 0.23851858]. \t  -0.342037189851298 \t -0.24904689160711327\n",
            "47     \t [0.510533   0.25080709]. \t  -0.24925430520384462 \t -0.24904689160711327\n",
            "48     \t [0.48306703 0.24628824]. \t  -0.2839497641697027 \t -0.24904689160711327\n",
            "49     \t [0.39450029 0.18087913]. \t  -0.4303793277736464 \t -0.24904689160711327\n",
            "50     \t [0.47556846 0.2191282 ]. \t  -0.27998059786509394 \t -0.24904689160711327\n",
            "51     \t [0.4563697  0.18067785]. \t  -0.3716847684822282 \t -0.24904689160711327\n",
            "52     \t [0.51142895 0.20751165]. \t  -0.5308194484007095 \t -0.24904689160711327\n",
            "53     \t [0.4658223  0.21025331]. \t  -0.2898846707895975 \t -0.24904689160711327\n",
            "54     \t [0.51756967 0.25865519]. \t  \u001b[92m-0.241245723218997\u001b[0m \t -0.241245723218997\n",
            "55     \t [0.47572756 0.23208972]. \t  -0.278194346706143 \t -0.241245723218997\n",
            "56     \t [0.49844161 0.2269214 ]. \t  -0.2978832312708213 \t -0.241245723218997\n",
            "57     \t [0.48116172 0.23260775]. \t  -0.2693122243173048 \t -0.241245723218997\n",
            "58     \t [0.50980889 0.24733175]. \t  -0.2560962565041652 \t -0.241245723218997\n",
            "59     \t [0.47835284 0.24833856]. \t  -0.3102075617011836 \t -0.241245723218997\n",
            "60     \t [0.48893266 0.22273118]. \t  -0.2878370114221772 \t -0.241245723218997\n",
            "61     \t [0.56530184 0.27996264]. \t  -0.34580642462613975 \t -0.241245723218997\n",
            "62     \t [0.52215821 0.27343213]. \t  \u001b[92m-0.22839407243625823\u001b[0m \t -0.22839407243625823\n",
            "63     \t [0.48440458 0.24304827]. \t  -0.2728954336967147 \t -0.22839407243625823\n",
            "64     \t [0.49100489 0.23265392]. \t  -0.26618568614031335 \t -0.22839407243625823\n",
            "65     \t [0.50900628 0.2496439 ]. \t  -0.24999278636350655 \t -0.22839407243625823\n",
            "66     \t [0.52315639 0.25566482]. \t  -0.25987993424294603 \t -0.22839407243625823\n",
            "67     \t [0.49458901 0.23497792]. \t  -0.26473393867243744 \t -0.22839407243625823\n",
            "68     \t [0.49904446 0.22564178]. \t  -0.3057292893194183 \t -0.22839407243625823\n",
            "69     \t [0.52225125 0.25779358]. \t  -0.25060246211185006 \t -0.22839407243625823\n",
            "70     \t [0.49433528 0.23880807]. \t  -0.25878739443954213 \t -0.22839407243625823\n",
            "71     \t [0.542968  0.2668851]. \t  -0.28688202447825706 \t -0.22839407243625823\n",
            "72     \t [0.4037376  0.18282593]. \t  -0.3948195429303121 \t -0.22839407243625823\n",
            "73     \t [0.49085356 0.23007231]. \t  -0.27103471588457084 \t -0.22839407243625823\n",
            "74     \t [0.48065225 0.2408819 ]. \t  -0.27943481769716344 \t -0.22839407243625823\n",
            "75     \t [0.48250072 0.24962179]. \t  -0.2960793968218568 \t -0.22839407243625823\n",
            "76     \t [0.46615724 0.25553611]. \t  -0.4311684452111014 \t -0.22839407243625823\n",
            "77     \t [0.49946274 0.2350334 ]. \t  -0.2713589763850545 \t -0.22839407243625823\n",
            "78     \t [0.49307402 0.23108583]. \t  -0.2714608555083023 \t -0.22839407243625823\n",
            "79     \t [0.27105041 0.10615831]. \t  -0.6382310341579652 \t -0.22839407243625823\n",
            "80     \t [0.50396567 0.24854651]. \t  -0.24900385150566098 \t -0.22839407243625823\n",
            "81     \t [0.53856213 0.26277112]. \t  -0.2873341250100274 \t -0.22839407243625823\n",
            "82     \t [0.53894788 0.27748301]. \t  -0.2294217915244514 \t -0.22839407243625823\n",
            "83     \t [0.49171486 0.23920321]. \t  -0.25901957527773956 \t -0.22839407243625823\n",
            "84     \t [0.47266657 0.25026176]. \t  -0.3501624600156519 \t -0.22839407243625823\n",
            "85     \t [0.50227064 0.24666531]. \t  -0.250882269365293 \t -0.22839407243625823\n",
            "86     \t [0.53757911 0.27874387]. \t  \u001b[92m-0.22433405187354893\u001b[0m \t -0.22433405187354893\n",
            "87     \t [0.54503338 0.27331342]. \t  -0.2633911946952769 \t -0.22433405187354893\n",
            "88     \t [0.51715992 0.26684708]. \t  -0.23317142797916174 \t -0.22433405187354893\n",
            "89     \t [0.49293814 0.2502563 ]. \t  -0.26239454364305126 \t -0.22433405187354893\n",
            "90     \t [0.50731573 0.25586771]. \t  -0.24296325288506784 \t -0.22433405187354893\n",
            "91     \t [0.49426996 0.25639174]. \t  -0.2703771398133231 \t -0.22433405187354893\n",
            "92     \t [0.51779298 0.25318274]. \t  -0.2548046424725009 \t -0.22433405187354893\n",
            "93     \t [0.48532973 0.24623843]. \t  -0.2763205347068003 \t -0.22433405187354893\n",
            "94     \t [0.48708226 0.23163322]. \t  -0.2662384509138874 \t -0.22433405187354893\n",
            "95     \t [0.52834397 0.25228473]. \t  -0.29461946851543797 \t -0.22433405187354893\n",
            "96     \t [0.40836656 0.16078733]. \t  -0.35360128622997594 \t -0.22433405187354893\n",
            "97     \t [0.5188104 0.2620037]. \t  -0.2366707508103308 \t -0.22433405187354893\n",
            "98     \t [0.48431704 0.24776626]. \t  -0.28336153431367833 \t -0.22433405187354893\n",
            "99     \t [0.54722622 0.29467902]. \t  \u001b[92m-0.207286557986465\u001b[0m \t -0.207286557986465\n",
            "100    \t [0.50708228 0.24776401]. \t  -0.2517446195583657 \t -0.207286557986465\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBLcW6tlteS2",
        "outputId": "2e476053-bf59-495d-f20f-eecec682832b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 2 \r\n",
        "\r\n",
        "np.random.seed(run_num_2)\r\n",
        "surrogate_winner_2 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_2 = dGPGO(surrogate_winner_2, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_2.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.44173258 -1.74529086]. \t  -1462.4037722675753 \t -1.3013277264983028\n",
            "init   \t [ 1.6176405  -0.26012243]. \t  -828.0271838688934 \t -1.3013277264983028\n",
            "init   \t [-1.525032    0.31071385]. \t  -412.40181251788414 \t -1.3013277264983028\n",
            "init   \t [ 1.39456889 -0.26574622]. \t  -488.8170397507174 \t -1.3013277264983028\n",
            "init   \t [0.80244966 0.75627765]. \t  -1.3013277264983028 \t -1.3013277264983028\n",
            "1      \t [ 0.3416094  -0.26301868]. \t  -14.851876424078617 \t -1.3013277264983028\n",
            "2      \t [0.6855781  0.03183019]. \t  -19.299657736173522 \t -1.3013277264983028\n",
            "3      \t [0.97082692 1.75663077]. \t  -66.28094057138775 \t -1.3013277264983028\n",
            "4      \t [ 0.4012189  -0.61484873]. \t  -60.5490332477914 \t -1.3013277264983028\n",
            "5      \t [0.74643173 1.18923762]. \t  -40.01646799175069 \t -1.3013277264983028\n",
            "6      \t [1.26326281 1.25582424]. \t  -11.629898844843463 \t -1.3013277264983028\n",
            "7      \t [-0.60872981 -0.60160043]. \t  -97.09604225497269 \t -1.3013277264983028\n",
            "8      \t [1.01127776 0.91898856]. \t  \u001b[92m-1.0753749780709922\u001b[0m \t -1.0753749780709922\n",
            "9      \t [1.75257387 2.02042805]. \t  -111.04478253507864 \t -1.0753749780709922\n",
            "10     \t [1.03307194 1.16688105]. \t  \u001b[92m-0.9939747958753087\u001b[0m \t -0.9939747958753087\n",
            "11     \t [0.94592403 0.90128628]. \t  \u001b[92m-0.007167423215701069\u001b[0m \t -0.007167423215701069\n",
            "12     \t [-0.28265879  0.40422392]. \t  -12.164074131875529 \t -0.007167423215701069\n",
            "13     \t [-0.04582126  0.21740297]. \t  -5.72929653894694 \t -0.007167423215701069\n",
            "14     \t [0.99081535 1.04466817]. \t  -0.396393892714814 \t -0.007167423215701069\n",
            "15     \t [0.93614181 0.83114822]. \t  -0.2085018340860972 \t -0.007167423215701069\n",
            "16     \t [0.97377284 0.94207696]. \t  \u001b[92m-0.0044782152963684565\u001b[0m \t -0.0044782152963684565\n",
            "17     \t [0.90591466 0.84510522]. \t  -0.06850446654765592 \t -0.0044782152963684565\n",
            "18     \t [0.97029755 0.92917234]. \t  -0.016023504023272062 \t -0.0044782152963684565\n",
            "19     \t [0.98756316 1.00519882]. \t  -0.0896622955846962 \t -0.0044782152963684565\n",
            "20     \t [0.92863401 0.80912705]. \t  -0.2884798114089675 \t -0.0044782152963684565\n",
            "21     \t [0.91622811 0.84710224]. \t  -0.012836801198571043 \t -0.0044782152963684565\n",
            "22     \t [0.97713263 0.9737006 ]. \t  -0.036290865593265595 \t -0.0044782152963684565\n",
            "23     \t [1.05273501 1.07139335]. \t  -0.13862964752958798 \t -0.0044782152963684565\n",
            "24     \t [1.02004222 1.08214649]. \t  -0.17396022128027783 \t -0.0044782152963684565\n",
            "25     \t [1.00671786 0.99574653]. \t  -0.03149576108407639 \t -0.0044782152963684565\n",
            "26     \t [0.99876624 0.99501226]. \t  \u001b[92m-0.0006374398825675263\u001b[0m \t -0.0006374398825675263\n",
            "27     \t [0.99058262 1.01095785]. \t  -0.08832096084938067 \t -0.0006374398825675263\n",
            "28     \t [0.95453816 0.9314189 ]. \t  -0.043177569645034455 \t -0.0006374398825675263\n",
            "29     \t [1.02379645 1.06778506]. \t  -0.039083770342532145 \t -0.0006374398825675263\n",
            "30     \t [0.93463555 0.87974744]. \t  -0.008121252851976166 \t -0.0006374398825675263\n",
            "31     \t [0.98500902 1.0039614 ]. \t  -0.11391929186536505 \t -0.0006374398825675263\n",
            "32     \t [1.041409   1.06447721]. \t  -0.0419370341222494 \t -0.0006374398825675263\n",
            "33     \t [0.8989027  0.79424622]. \t  -0.029209072141887515 \t -0.0006374398825675263\n",
            "34     \t [0.77159835 0.66613696]. \t  -0.5530483629766313 \t -0.0006374398825675263\n",
            "35     \t [1.03423423 1.07232491]. \t  -0.0018926180277721205 \t -0.0006374398825675263\n",
            "36     \t [0.87053935 0.76388366]. \t  -0.020414150695507532 \t -0.0006374398825675263\n",
            "37     \t [0.88389212 0.70913383]. \t  -0.5337755389309186 \t -0.0006374398825675263\n",
            "38     \t [1.02035057 1.04774938]. \t  -0.004815264900050885 \t -0.0006374398825675263\n",
            "39     \t [0.77810113 0.62794707]. \t  -0.09988977667128089 \t -0.0006374398825675263\n",
            "40     \t [-0.04994327 -0.47790787]. \t  -24.18100809678868 \t -0.0006374398825675263\n",
            "41     \t [0.87695894 0.7605198 ]. \t  -0.022427453996169733 \t -0.0006374398825675263\n",
            "42     \t [0.97167129 0.97770743]. \t  -0.11344557234968804 \t -0.0006374398825675263\n",
            "43     \t [1.02294921 1.03689159]. \t  -0.009615418184507665 \t -0.0006374398825675263\n",
            "44     \t [1.05196285 1.1100522 ]. \t  -0.0038741315827555273 \t -0.0006374398825675263\n",
            "45     \t [1.01655728 1.01999397]. \t  -0.018215996478832985 \t -0.0006374398825675263\n",
            "46     \t [1.00480633 1.03933218]. \t  -0.08821082187753819 \t -0.0006374398825675263\n",
            "47     \t [1.03416965 1.09681   ]. \t  -0.07571372365339352 \t -0.0006374398825675263\n",
            "48     \t [0.9062454  0.83659881]. \t  -0.03225428217551571 \t -0.0006374398825675263\n",
            "49     \t [1.05860626 1.11653454]. \t  -0.005126098679714258 \t -0.0006374398825675263\n",
            "50     \t [1.03079365 1.06040917]. \t  -0.0014004028668312095 \t -0.0006374398825675263\n",
            "51     \t [-0.14839151  0.57626651]. \t  -32.03771747488113 \t -0.0006374398825675263\n",
            "52     \t [0.81860185 0.70839011]. \t  -0.17944979583301768 \t -0.0006374398825675263\n",
            "53     \t [0.89010825 0.81716447]. \t  -0.0739367161473296 \t -0.0006374398825675263\n",
            "54     \t [1.04891739 1.10685779]. \t  -0.006788739129767102 \t -0.0006374398825675263\n",
            "55     \t [0.89933639 0.81314669]. \t  -0.012017366533826119 \t -0.0006374398825675263\n",
            "56     \t [0.85937517 0.74245641]. \t  -0.021320404916820065 \t -0.0006374398825675263\n",
            "57     \t [0.92610907 0.86047976]. \t  -0.006244856960350488 \t -0.0006374398825675263\n",
            "58     \t [0.90164588 0.81469902]. \t  -0.00997411662844118 \t -0.0006374398825675263\n",
            "59     \t [0.87219721 0.77238761]. \t  -0.029928273118275 \t -0.0006374398825675263\n",
            "60     \t [0.91697945 0.83167702]. \t  -0.015309174083675693 \t -0.0006374398825675263\n",
            "61     \t [0.90299989 0.81472135]. \t  -0.00945627988943194 \t -0.0006374398825675263\n",
            "62     \t [0.90973624 0.84605733]. \t  -0.04214094964079723 \t -0.0006374398825675263\n",
            "63     \t [0.99509052 1.01263454]. \t  -0.05033190231884972 \t -0.0006374398825675263\n",
            "64     \t [0.94979191 0.9119047 ]. \t  -0.012124902206142835 \t -0.0006374398825675263\n",
            "65     \t [1.10038648 1.17323894]. \t  -0.15153963744063803 \t -0.0006374398825675263\n",
            "66     \t [0.97851695 0.97406152]. \t  -0.02790506341504377 \t -0.0006374398825675263\n",
            "67     \t [0.85956466 0.75075728]. \t  -0.03389706030444811 \t -0.0006374398825675263\n",
            "68     \t [0.90849076 0.82389217]. \t  -0.008588061811057531 \t -0.0006374398825675263\n",
            "69     \t [0.89753372 0.82698924]. \t  -0.05639151732285118 \t -0.0006374398825675263\n",
            "70     \t [0.87080223 0.75015754]. \t  -0.023316364942661198 \t -0.0006374398825675263\n",
            "71     \t [0.9073244  0.82182983]. \t  -0.008786939969523073 \t -0.0006374398825675263\n",
            "72     \t [0.89952984 0.81986718]. \t  -0.021571614226863246 \t -0.0006374398825675263\n",
            "73     \t [0.94750065 0.92745223]. \t  -0.09093402461081884 \t -0.0006374398825675263\n",
            "74     \t [0.90992433 0.82935315]. \t  -0.008307074570331955 \t -0.0006374398825675263\n",
            "75     \t [1.02814738 1.04511018]. \t  -0.015136767994032706 \t -0.0006374398825675263\n",
            "76     \t [1.02313061 1.06741945]. \t  -0.04306666535614064 \t -0.0006374398825675263\n",
            "77     \t [0.82232255 0.68741538]. \t  -0.044115528452358097 \t -0.0006374398825675263\n",
            "78     \t [1.01907046 1.03731169]. \t  \u001b[92m-0.000505987196082239\u001b[0m \t -0.000505987196082239\n",
            "79     \t [1.0549527  1.17937295]. \t  -0.4445501691766277 \t -0.000505987196082239\n",
            "80     \t [0.96263175 0.92771602]. \t  -0.0015079285582448206 \t -0.000505987196082239\n",
            "81     \t [1.06903702 1.12575246]. \t  -0.03396502036596871 \t -0.000505987196082239\n",
            "82     \t [0.81354266 0.68977157]. \t  -0.11271845774340614 \t -0.000505987196082239\n",
            "83     \t [0.87216338 0.77993531]. \t  -0.05346143032341362 \t -0.000505987196082239\n",
            "84     \t [0.8549139  0.73773159]. \t  -0.025747447448513153 \t -0.000505987196082239\n",
            "85     \t [0.8596579  0.74172549]. \t  -0.020432364961408756 \t -0.000505987196082239\n",
            "86     \t [0.87303723 0.77843029]. \t  -0.042481262532562 \t -0.000505987196082239\n",
            "87     \t [0.87391946 0.77599491]. \t  -0.0309262717466713 \t -0.000505987196082239\n",
            "88     \t [0.92251495 0.88220145]. \t  -0.10314598110628942 \t -0.000505987196082239\n",
            "89     \t [1.02518564 1.06844962]. \t  -0.03106370717837973 \t -0.000505987196082239\n",
            "90     \t [0.89764355 0.82205895]. \t  -0.03702955379111199 \t -0.000505987196082239\n",
            "91     \t [1.02471901 1.15355845]. \t  -1.0720305071093335 \t -0.000505987196082239\n",
            "92     \t [1.01793585 1.05021295]. \t  -0.019976467955052795 \t -0.000505987196082239\n",
            "93     \t [0.84389453 0.70029914]. \t  -0.03843211393946 \t -0.000505987196082239\n",
            "94     \t [0.88965749 0.79149336]. \t  -0.012175470344681189 \t -0.000505987196082239\n",
            "95     \t [0.90937513 0.83709848]. \t  -0.018485411891522342 \t -0.000505987196082239\n",
            "96     \t [0.88379499 0.7810532 ]. \t  -0.013503767052655501 \t -0.000505987196082239\n",
            "97     \t [1.02383459 1.05779792]. \t  -0.009708701337220542 \t -0.000505987196082239\n",
            "98     \t [0.86024114 0.72220128]. \t  -0.051264784675742664 \t -0.000505987196082239\n",
            "99     \t [0.87970148 0.77828186]. \t  -0.016414041601147157 \t -0.000505987196082239\n",
            "100    \t [1.03298504 1.09621814]. \t  -0.08611888121527393 \t -0.000505987196082239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emgjXwfeuvRY",
        "outputId": "55a43f68-b76a-4bc9-9ff6-0d225666a517"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 3 \r\n",
        "\r\n",
        "np.random.seed(run_num_3)\r\n",
        "surrogate_winner_3 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_3 = dGPGO(surrogate_winner_3, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_3.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.03045369 -1.60050678]. \t  -708.8071948417274 \t -1.118465165857483\n",
            "init   \t [-0.05684724 -0.0006915 ]. \t  -1.118465165857483 \t -1.118465165857483\n",
            "init   \t [ 0.64352256 -1.08181622]. \t  -223.90997740226942 \t -1.118465165857483\n",
            "init   \t [ 0.46200787 -1.55790376]. \t  -314.05929962166397 \t -1.118465165857483\n",
            "init   \t [ 0.86918061 -0.52199202]. \t  -163.20929387450548 \t -1.118465165857483\n",
            "1      \t [-0.43437161 -0.93825591]. \t  -129.0555838309792 \t -1.118465165857483\n",
            "2      \t [ 0.19495409 -0.37706849]. \t  -17.876873045749676 \t -1.118465165857483\n",
            "3      \t [-0.49728358  1.2249034 ]. \t  -97.8144664264383 \t -1.118465165857483\n",
            "4      \t [-0.12009866  0.33438996]. \t  -11.492462396732765 \t -1.118465165857483\n",
            "5      \t [-0.79827797  0.28794331]. \t  -15.435160479535469 \t -1.118465165857483\n",
            "6      \t [-0.32150589  0.12670989]. \t  -1.8008713741167075 \t -1.118465165857483\n",
            "7      \t [-2.03148734  2.03316047]. \t  -447.5815327911581 \t -1.118465165857483\n",
            "8      \t [-0.13577844  0.04495817]. \t  -1.3603363570068554 \t -1.118465165857483\n",
            "9      \t [-0.04538614  0.04821176]. \t  -1.3058315430303264 \t -1.118465165857483\n",
            "10     \t [2.048 2.048]. \t  -461.76039004157315 \t -1.118465165857483\n",
            "11     \t [-0.0793641   0.06344341]. \t  -1.4915790823549346 \t -1.118465165857483\n",
            "12     \t [-0.02782895 -0.01556283]. \t  \u001b[92m-1.0831230168030845\u001b[0m \t -1.0831230168030845\n",
            "13     \t [-0.64715488  0.05587571]. \t  -15.885208777901983 \t -1.0831230168030845\n",
            "14     \t [-0.08196983 -0.01617556]. \t  -1.2230750218904767 \t -1.0831230168030845\n",
            "15     \t [ 0.06633418 -0.07340495]. \t  -1.4770964269563291 \t -1.0831230168030845\n",
            "16     \t [ 0.07548558 -0.03349161]. \t  \u001b[92m-1.008310002009149\u001b[0m \t -1.008310002009149\n",
            "17     \t [0.01952758 0.05622715]. \t  -1.2732017815854952 \t -1.008310002009149\n",
            "18     \t [ 0.04930046 -0.01196222]. \t  \u001b[92m-0.9245447750035782\u001b[0m \t -0.9245447750035782\n",
            "19     \t [0.02203972 0.01033259]. \t  -0.9661023279439747 \t -0.9245447750035782\n",
            "20     \t [ 0.03638884 -0.00042971]. \t  -0.9288540634747674 \t -0.9245447750035782\n",
            "21     \t [ 0.02388386 -0.06687965]. \t  -1.407754185678367 \t -0.9245447750035782\n",
            "22     \t [ 0.0540078  -0.02042229]. \t  -0.9493727438249533 \t -0.9245447750035782\n",
            "23     \t [-0.09298219  0.0390485 ]. \t  -1.287043172888457 \t -0.9245447750035782\n",
            "24     \t [ 0.06878222 -0.00848231]. \t  \u001b[92m-0.8846256855701233\u001b[0m \t -0.8846256855701233\n",
            "25     \t [ 0.0947144  -0.03389232]. \t  -1.003266846275922 \t -0.8846256855701233\n",
            "26     \t [0.11869798 0.01203762]. \t  \u001b[92m-0.7771141481748791\u001b[0m \t -0.7771141481748791\n",
            "27     \t [0.74840538 0.40049757]. \t  -2.610932461790987 \t -0.7771141481748791\n",
            "28     \t [ 0.14718934 -0.00097465]. \t  -0.7785400306897763 \t -0.7771141481748791\n",
            "29     \t [0.55562201 0.27046137]. \t  \u001b[92m-0.343812107142925\u001b[0m \t -0.343812107142925\n",
            "30     \t [0.59778751 0.28114019]. \t  -0.7425670631886769 \t -0.343812107142925\n",
            "31     \t [0.47330177 0.21744567]. \t  \u001b[92m-0.2817260611444393\u001b[0m \t -0.2817260611444393\n",
            "32     \t [0.58925352 0.31719625]. \t  \u001b[92m-0.25885346908103735\u001b[0m \t -0.25885346908103735\n",
            "33     \t [0.50527116 0.25516119]. \t  \u001b[92m-0.24475852382951466\u001b[0m \t -0.24475852382951466\n",
            "34     \t [0.43771586 0.16200225]. \t  -0.4037375642223691 \t -0.24475852382951466\n",
            "35     \t [0.18314199 0.05923717]. \t  -0.7332863810766189 \t -0.24475852382951466\n",
            "36     \t [0.57020322 0.26957941]. \t  -0.49333106062118787 \t -0.24475852382951466\n",
            "37     \t [0.51069402 0.31521316]. \t  -0.5354083256716863 \t -0.24475852382951466\n",
            "38     \t [0.42063265 0.14928329]. \t  -0.412110701609558 \t -0.24475852382951466\n",
            "39     \t [0.45333948 0.25444718]. \t  -0.5382570245992073 \t -0.24475852382951466\n",
            "40     \t [0.24970101 0.09565069]. \t  -0.6738382312242064 \t -0.24475852382951466\n",
            "41     \t [0.46532718 0.19979577]. \t  -0.313876406702765 \t -0.24475852382951466\n",
            "42     \t [0.44562547 0.20761297]. \t  -0.31548686503730533 \t -0.24475852382951466\n",
            "43     \t [0.59962448 0.3232074 ]. \t  -0.2923754828418502 \t -0.24475852382951466\n",
            "44     \t [0.49819001 0.25521999]. \t  -0.2567507133649035 \t -0.24475852382951466\n",
            "45     \t [0.49367673 0.26510531]. \t  -0.30211044363102574 \t -0.24475852382951466\n",
            "46     \t [0.28459752 0.07749784]. \t  -0.5130242451048878 \t -0.24475852382951466\n",
            "47     \t [0.59486321 0.3522337 ]. \t  \u001b[92m-0.16440103457299302\u001b[0m \t -0.16440103457299302\n",
            "48     \t [0.19110626 0.04007027]. \t  -0.6555683748307305 \t -0.16440103457299302\n",
            "49     \t [0.16989887 0.04372498]. \t  -0.7111479119631363 \t -0.16440103457299302\n",
            "50     \t [0.50503993 0.23068934]. \t  -0.3044043562571936 \t -0.16440103457299302\n",
            "51     \t [0.59747003 0.2884291 ]. \t  -0.6318218772114295 \t -0.16440103457299302\n",
            "52     \t [0.5589818  0.34425513]. \t  -0.2955859175118482 \t -0.16440103457299302\n",
            "53     \t [0.57333166 0.26674581]. \t  -0.5659919011534992 \t -0.16440103457299302\n",
            "54     \t [0.58113472 0.37294661]. \t  -0.2995567237269079 \t -0.16440103457299302\n",
            "55     \t [0.39482735 0.14450246]. \t  -0.37919843263702246 \t -0.16440103457299302\n",
            "56     \t [0.67948731 0.37379006]. \t  -0.8755969893484845 \t -0.16440103457299302\n",
            "57     \t [0.50342214 0.20335079]. \t  -0.4974208127261569 \t -0.16440103457299302\n",
            "58     \t [0.65409005 0.44048041]. \t  \u001b[92m-0.1356473974031823\u001b[0m \t -0.1356473974031823\n",
            "59     \t [0.58558214 0.34925119]. \t  -0.17576775048136442 \t -0.1356473974031823\n",
            "60     \t [0.64002329 0.36349434]. \t  -0.3424314188206703 \t -0.1356473974031823\n",
            "61     \t [0.51705507 0.26175901]. \t  -0.23635719056603624 \t -0.1356473974031823\n",
            "62     \t [0.48368608 0.22439606]. \t  -0.27571209110645223 \t -0.1356473974031823\n",
            "63     \t [0.51667157 0.21496047]. \t  -0.5038923959029675 \t -0.1356473974031823\n",
            "64     \t [0.67384007 0.3818738 ]. \t  -0.6274714956260921 \t -0.1356473974031823\n",
            "65     \t [0.5826105  0.32116765]. \t  -0.20758359689788686 \t -0.1356473974031823\n",
            "66     \t [0.63036022 0.40342946]. \t  -0.14032468197340112 \t -0.1356473974031823\n",
            "67     \t [0.60957144 0.38993911]. \t  -0.18614990823734112 \t -0.1356473974031823\n",
            "68     \t [0.45019792 0.18254182]. \t  -0.3428295776369602 \t -0.1356473974031823\n",
            "69     \t [0.56426419 0.33970553]. \t  -0.2352835107227363 \t -0.1356473974031823\n",
            "70     \t [0.58430581 0.37635071]. \t  -0.2948640493932142 \t -0.1356473974031823\n",
            "71     \t [0.46376843 0.21912401]. \t  -0.28917876983316004 \t -0.1356473974031823\n",
            "72     \t [0.66974218 0.45957582]. \t  \u001b[92m-0.12121698002851679\u001b[0m \t -0.12121698002851679\n",
            "73     \t [0.61709422 0.33781198]. \t  -0.3314591746101646 \t -0.12121698002851679\n",
            "74     \t [0.64956115 0.47811396]. \t  -0.43847458956550267 \t -0.12121698002851679\n",
            "75     \t [0.6779176  0.42669142]. \t  -0.21185207502130116 \t -0.12121698002851679\n",
            "76     \t [0.52319782 0.23565169]. \t  -0.3723814969973347 \t -0.12121698002851679\n",
            "77     \t [0.61242305 0.35565298]. \t  -0.18788689645475626 \t -0.12121698002851679\n",
            "78     \t [0.61344547 0.37300014]. \t  -0.1505234645924037 \t -0.12121698002851679\n",
            "79     \t [0.62980756 0.39649255]. \t  -0.13704516278448511 \t -0.12121698002851679\n",
            "80     \t [0.37884915 0.14805258]. \t  -0.3878767608058582 \t -0.12121698002851679\n",
            "81     \t [0.42819889 0.18287101]. \t  -0.32697986720869066 \t -0.12121698002851679\n",
            "82     \t [0.64739634 0.44347936]. \t  -0.1836573418400133 \t -0.12121698002851679\n",
            "83     \t [0.33189226 0.10968788]. \t  -0.4463895390689518 \t -0.12121698002851679\n",
            "84     \t [0.63302968 0.42076929]. \t  -0.17483828542744462 \t -0.12121698002851679\n",
            "85     \t [0.66607124 0.40728463]. \t  -0.24375893853407796 \t -0.12121698002851679\n",
            "86     \t [0.4614915  0.22789274]. \t  -0.31224707922689454 \t -0.12121698002851679\n",
            "87     \t [0.61235701 0.37927863]. \t  -0.15211396910658342 \t -0.12121698002851679\n",
            "88     \t [0.69814503 0.51050225]. \t  -0.1444579077833862 \t -0.12121698002851679\n",
            "89     \t [0.42201032 0.1244722 ]. \t  -0.6215879787640066 \t -0.12121698002851679\n",
            "90     \t [0.69455047 0.47766748]. \t  \u001b[92m-0.09553942247983162\u001b[0m \t -0.09553942247983162\n",
            "91     \t [0.4603834  0.18568678]. \t  -0.36017683577406956 \t -0.09553942247983162\n",
            "92     \t [0.68056908 0.49998912]. \t  -0.23756940311669864 \t -0.09553942247983162\n",
            "93     \t [0.63337043 0.43644668]. \t  -0.25894563892125305 \t -0.09553942247983162\n",
            "94     \t [0.70301415 0.52775326]. \t  -0.20058890030764914 \t -0.09553942247983162\n",
            "95     \t [0.42000586 0.1823243 ]. \t  -0.33989710794123873 \t -0.09553942247983162\n",
            "96     \t [0.71450283 0.55856099]. \t  -0.31235709828360564 \t -0.09553942247983162\n",
            "97     \t [0.56101088 0.30347405]. \t  -0.2053883103534291 \t -0.09553942247983162\n",
            "98     \t [0.51661597 0.24096288]. \t  -0.300892366124329 \t -0.09553942247983162\n",
            "99     \t [0.68463265 0.48381743]. \t  -0.12224419225683178 \t -0.09553942247983162\n",
            "100    \t [0.48337668 0.24542357]. \t  -0.2807542412869918 \t -0.09553942247983162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8riJpBBKuvT4",
        "outputId": "4cb05258-e912-4415-e8cd-bb53bf3dcd95"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 4 \r\n",
        "\r\n",
        "np.random.seed(run_num_4)\r\n",
        "surrogate_winner_4 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_4 = dGPGO(surrogate_winner_4, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_4.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.39002285 1.31602418]. \t  -38.11488737154776 \t -12.122423820878506\n",
            "init   \t [0.58248055 0.68494384]. \t  -12.122423820878506 \t -12.122423820878506\n",
            "init   \t [-1.88768295 -1.7790059 ]. \t  -2862.4120613667637 \t -12.122423820878506\n",
            "init   \t [-0.91545338 -0.75218738]. \t  -256.5560079635026 \t -12.122423820878506\n",
            "init   \t [-1.25504548 -1.24070756]. \t  -797.98450090518 \t -12.122423820878506\n",
            "1      \t [-0.01561008 -1.81748949]. \t  -331.44684829955645 \t -12.122423820878506\n",
            "2      \t [0.50841602 1.31881741]. \t  -112.67174440814816 \t -12.122423820878506\n",
            "3      \t [1.36834208 0.38159453]. \t  -222.37385625305683 \t -12.122423820878506\n",
            "4      \t [2.048 2.048]. \t  -461.7603900415955 \t -12.122423820878506\n",
            "5      \t [0.91287537 0.99245835]. \t  \u001b[92m-2.539409929926347\u001b[0m \t -2.539409929926347\n",
            "6      \t [0.81238336 0.83487338]. \t  -3.094433691599613 \t -2.539409929926347\n",
            "7      \t [0.67528643 0.80420936]. \t  -12.229595244259485 \t -2.539409929926347\n",
            "8      \t [0.64159567 0.46258973]. \t  \u001b[92m-0.38799026318947605\u001b[0m \t -0.38799026318947605\n",
            "9      \t [0.63889803 0.48837006]. \t  -0.7732677054234907 \t -0.38799026318947605\n",
            "10     \t [1.02447466 1.05787174]. \t  \u001b[92m-0.007526933606340728\u001b[0m \t -0.007526933606340728\n",
            "11     \t [1.04087162 1.05699897]. \t  -0.07144437636081619 \t -0.007526933606340728\n",
            "12     \t [0.95795486 1.00778042]. \t  -0.8136211695581069 \t -0.007526933606340728\n",
            "13     \t [1.09946958 1.10496089]. \t  -1.08884317534586 \t -0.007526933606340728\n",
            "14     \t [0.99263215 0.98485162]. \t  \u001b[92m-7.609122487114929e-05\u001b[0m \t -7.609122487114929e-05\n",
            "15     \t [0.64248587 0.48656821]. \t  -0.6721670066363943 \t -7.609122487114929e-05\n",
            "16     \t [0.99315598 1.02252984]. \t  -0.1308812964941194 \t -7.609122487114929e-05\n",
            "17     \t [1.0411107  1.13274933]. \t  -0.24020361252712316 \t -7.609122487114929e-05\n",
            "18     \t [1.03370032 1.06483527]. \t  -0.0025055071534173033 \t -7.609122487114929e-05\n",
            "19     \t [0.71207608 0.62727302]. \t  -1.5282015040551202 \t -7.609122487114929e-05\n",
            "20     \t [1.05890617 1.16691093]. \t  -0.21166742528090302 \t -7.609122487114929e-05\n",
            "21     \t [1.05511253 1.10124015]. \t  -0.017490968202670595 \t -7.609122487114929e-05\n",
            "22     \t [1.05109681 1.15510888]. \t  -0.2556639974835669 \t -7.609122487114929e-05\n",
            "23     \t [1.06852067 1.10885127]. \t  -0.11283837677433724 \t -7.609122487114929e-05\n",
            "24     \t [1.05267071 1.07505575]. \t  -0.11206971232179758 \t -7.609122487114929e-05\n",
            "25     \t [0.98337932 1.02385468]. \t  -0.32312509034549197 \t -7.609122487114929e-05\n",
            "26     \t [1.04002882 1.08669991]. \t  -0.004142438290603089 \t -7.609122487114929e-05\n",
            "27     \t [1.06029109 1.12920554]. \t  -0.006123363728800146 \t -7.609122487114929e-05\n",
            "28     \t [0.62096396 0.46643639]. \t  -0.7971812303771456 \t -7.609122487114929e-05\n",
            "29     \t [1.06638777 1.18212218]. \t  -0.20636152122113835 \t -7.609122487114929e-05\n",
            "30     \t [1.06156926 1.20670775]. \t  -0.6402512000068741 \t -7.609122487114929e-05\n",
            "31     \t [1.10270724 1.11050181]. \t  -1.1227606915964512 \t -7.609122487114929e-05\n",
            "32     \t [0.65469883 0.51244082]. \t  -0.8216488115609103 \t -7.609122487114929e-05\n",
            "33     \t [1.06996077 1.14423188]. \t  -0.00492863352866977 \t -7.609122487114929e-05\n",
            "34     \t [1.01586111 1.01434984]. \t  -0.03131192845511335 \t -7.609122487114929e-05\n",
            "35     \t [1.04190646 1.05860775]. \t  -0.07444744495122008 \t -7.609122487114929e-05\n",
            "36     \t [1.10168093 1.16334397]. \t  -0.26392076257957164 \t -7.609122487114929e-05\n",
            "37     \t [1.11703424 1.1815487 ]. \t  -0.4521633044595778 \t -7.609122487114929e-05\n",
            "38     \t [1.07102965 1.08707263]. \t  -0.36542776103151753 \t -7.609122487114929e-05\n",
            "39     \t [1.05270911 1.10431465]. \t  -0.004285101092749413 \t -7.609122487114929e-05\n",
            "40     \t [1.12156396 1.24172466]. \t  -0.040960405581813915 \t -7.609122487114929e-05\n",
            "41     \t [-1.45105429  2.01681009]. \t  -6.795296299002971 \t -7.609122487114929e-05\n",
            "42     \t [1.09851409 1.93635811]. \t  -53.24495413886921 \t -7.609122487114929e-05\n",
            "43     \t [1.05089455 1.12629888]. \t  -0.050636794386600036 \t -7.609122487114929e-05\n",
            "44     \t [1.11119498 1.23793131]. \t  -0.01337367314688253 \t -7.609122487114929e-05\n",
            "45     \t [1.07375631 1.18080432]. \t  -0.08301171938945892 \t -7.609122487114929e-05\n",
            "46     \t [0.97845838 1.01379752]. \t  -0.31874873675487125 \t -7.609122487114929e-05\n",
            "47     \t [1.07714914 1.10116537]. \t  -0.3550543593537903 \t -7.609122487114929e-05\n",
            "48     \t [1.09824836 1.25141829]. \t  -0.21457939899382356 \t -7.609122487114929e-05\n",
            "49     \t [1.11608974 1.23559086]. \t  -0.02360815481642009 \t -7.609122487114929e-05\n",
            "50     \t [0.75229367 0.63473203]. \t  -0.5345134695061833 \t -7.609122487114929e-05\n",
            "51     \t [1.12134619 1.23322074]. \t  -0.0732721508975812 \t -7.609122487114929e-05\n",
            "52     \t [-1.26970931  1.74104809]. \t  -6.812749474321932 \t -7.609122487114929e-05\n",
            "53     \t [0.9681027  0.97207736]. \t  -0.12250118770800637 \t -7.609122487114929e-05\n",
            "54     \t [1.07182486 1.1481299 ]. \t  -0.0052048644043587045 \t -7.609122487114929e-05\n",
            "55     \t [1.10315233 1.20010997]. \t  -0.038982475760691436 \t -7.609122487114929e-05\n",
            "56     \t [1.08688139 1.18961493]. \t  -0.014443662423734177 \t -7.609122487114929e-05\n",
            "57     \t [1.08045299 1.18439579]. \t  -0.03543095559491287 \t -7.609122487114929e-05\n",
            "58     \t [1.06315251 1.1508209 ]. \t  -0.046126600114161476 \t -7.609122487114929e-05\n",
            "59     \t [1.07100417 1.1422701 ]. \t  -0.007326262010653961 \t -7.609122487114929e-05\n",
            "60     \t [1.10789304 1.24050968]. \t  -0.028756587438520158 \t -7.609122487114929e-05\n",
            "61     \t [1.07581378 1.16521861]. \t  -0.011899504053982192 \t -7.609122487114929e-05\n",
            "62     \t [1.00235511 1.00819279]. \t  -0.0012145175324413291 \t -7.609122487114929e-05\n",
            "63     \t [1.07316022 1.19826131]. \t  -0.22240082594183727 \t -7.609122487114929e-05\n",
            "64     \t [1.05596694 1.09509791]. \t  -0.04300546185935616 \t -7.609122487114929e-05\n",
            "65     \t [1.05247254 1.11456624]. \t  -0.007470037925941408 \t -7.609122487114929e-05\n",
            "66     \t [1.09807588 1.2128767 ]. \t  -0.014668477225899486 \t -7.609122487114929e-05\n",
            "67     \t [1.10922624 1.23753904]. \t  -0.01705146159482875 \t -7.609122487114929e-05\n",
            "68     \t [1.01625479 1.0542106 ]. \t  -0.04621790751510625 \t -7.609122487114929e-05\n",
            "69     \t [1.05590079 1.11535538]. \t  -0.003143293609482454 \t -7.609122487114929e-05\n",
            "70     \t [1.08638351 1.15943123]. \t  -0.05071741626973286 \t -7.609122487114929e-05\n",
            "71     \t [1.10635868 1.26403414]. \t  -0.1713489806766362 \t -7.609122487114929e-05\n",
            "72     \t [1.08773501 1.20399335]. \t  -0.05106921663118912 \t -7.609122487114929e-05\n",
            "73     \t [0.99423362 1.06599218]. \t  -0.60052929774006 \t -7.609122487114929e-05\n",
            "74     \t [1.10876649 1.23914091]. \t  -0.02139063672822682 \t -7.609122487114929e-05\n",
            "75     \t [1.08832432 1.16388804]. \t  -0.050079899391706394 \t -7.609122487114929e-05\n",
            "76     \t [1.10547024 1.21048195]. \t  -0.024539381867656043 \t -7.609122487114929e-05\n",
            "77     \t [1.07946458 1.15642783]. \t  -0.014086695969303124 \t -7.609122487114929e-05\n",
            "78     \t [1.07709783 1.16943602]. \t  -0.014586150907004564 \t -7.609122487114929e-05\n",
            "79     \t [1.09630571 1.21183661]. \t  -0.019175836530616375 \t -7.609122487114929e-05\n",
            "80     \t [1.09232173 1.2099931 ]. \t  -0.03683588156091136 \t -7.609122487114929e-05\n",
            "81     \t [1.05926842 1.13732318]. \t  -0.026840986204838182 \t -7.609122487114929e-05\n",
            "82     \t [1.06090218 1.13511165]. \t  -0.012921632642932737 \t -7.609122487114929e-05\n",
            "83     \t [0.98175084 1.02168614]. \t  -0.33501171956273984 \t -7.609122487114929e-05\n",
            "84     \t [1.06064013 1.15609169]. \t  -0.1006111458830693 \t -7.609122487114929e-05\n",
            "85     \t [1.07015664 1.13878899]. \t  -0.009077358659126148 \t -7.609122487114929e-05\n",
            "86     \t [1.01625088 1.05095804]. \t  -0.033359665530254955 \t -7.609122487114929e-05\n",
            "87     \t [1.04087196 1.08041778]. \t  -0.002568513824782761 \t -7.609122487114929e-05\n",
            "88     \t [0.96526392 0.9152645 ]. \t  -0.028332463399477637 \t -7.609122487114929e-05\n",
            "89     \t [0.86143634 0.77349168]. \t  -0.11791593753678435 \t -7.609122487114929e-05\n",
            "90     \t [0.95882891 0.91249418]. \t  -0.006399236220649434 \t -7.609122487114929e-05\n",
            "91     \t [1.10691612 1.25881434]. \t  -0.12399840227691632 \t -7.609122487114929e-05\n",
            "92     \t [0.85993225 0.76658598]. \t  -0.09307353213237372 \t -7.609122487114929e-05\n",
            "93     \t [1.11616933 1.27277509]. \t  -0.08607772875164396 \t -7.609122487114929e-05\n",
            "94     \t [1.10783085 1.25385352]. \t  -0.08219377488083228 \t -7.609122487114929e-05\n",
            "95     \t [1.00515502 1.03437865]. \t  -0.05782849156390372 \t -7.609122487114929e-05\n",
            "96     \t [0.88988013 0.81891993]. \t  -0.08520622149002038 \t -7.609122487114929e-05\n",
            "97     \t [0.98823202 0.97179003]. \t  -0.0024545030838182443 \t -7.609122487114929e-05\n",
            "98     \t [1.11242231 1.24219398]. \t  -0.014857728504865948 \t -7.609122487114929e-05\n",
            "99     \t [1.08868323 1.23442592]. \t  -0.24987711273430172 \t -7.609122487114929e-05\n",
            "100    \t [1.07607026 1.14882672]. \t  -0.014068552599091965 \t -7.609122487114929e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrvkDXP0uvWU",
        "outputId": "d3366395-a2e4-4719-f151-73807ab864d7"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 5 \r\n",
        "\r\n",
        "np.random.seed(run_num_5)\r\n",
        "surrogate_winner_5 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_5 = dGPGO(surrogate_winner_5, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_5.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.23491901 0.17105153]. \t  -1.9278091788796494 \t -1.9278091788796494\n",
            "init   \t [ 0.24749582 -0.48313101]. \t  -30.201785940713883 \t -1.9278091788796494\n",
            "init   \t [-1.56019024  1.41721238]. \t  -109.97965348801837 \t -1.9278091788796494\n",
            "init   \t [0.98198164 1.77093658]. \t  -65.06852657321838 \t -1.9278091788796494\n",
            "init   \t [1.47180752 0.85239655]. \t  -172.83511538815276 \t -1.9278091788796494\n",
            "1      \t [-0.22001069  0.22846888]. \t  -4.730736857945067 \t -1.9278091788796494\n",
            "2      \t [-0.07416303  0.46379185]. \t  -22.156954227355797 \t -1.9278091788796494\n",
            "3      \t [-0.29597547 -0.02291115]. \t  -2.900856674650684 \t -1.9278091788796494\n",
            "4      \t [0.05565497 0.02726996]. \t  \u001b[92m-0.9502184397025056\u001b[0m \t -0.9502184397025056\n",
            "5      \t [-1.50626859 -0.1212174 ]. \t  -577.5212366419105 \t -0.9502184397025056\n",
            "6      \t [ 1.48550141 -0.82940074]. \t  -922.0352504296516 \t -0.9502184397025056\n",
            "7      \t [-0.22025069 -1.71197199]. \t  -311.41882342699097 \t -0.9502184397025056\n",
            "8      \t [0.62023358 0.46981674]. \t  \u001b[92m-0.8688838471827798\u001b[0m \t -0.8688838471827798\n",
            "9      \t [0.50083269 0.35333969]. \t  -1.299922346333929 \t -0.8688838471827798\n",
            "10     \t [0.60820496 0.60893807]. \t  -5.866788534274495 \t -0.8688838471827798\n",
            "11     \t [-1.4107308   2.02308415]. \t  -5.920013812757701 \t -0.8688838471827798\n",
            "12     \t [-0.09570075 -0.05869583]. \t  -1.660983007703209 \t -0.8688838471827798\n",
            "13     \t [ 0.15370686 -0.01216942]. \t  \u001b[92m-0.844341815650457\u001b[0m \t -0.844341815650457\n",
            "14     \t [0.56016789 0.38138274]. \t  \u001b[92m-0.6503561651750591\u001b[0m \t -0.6503561651750591\n",
            "15     \t [0.56127444 0.36470527]. \t  \u001b[92m-0.43925337723689406\u001b[0m \t -0.43925337723689406\n",
            "16     \t [-1.81813025  2.01651068]. \t  -174.11637156784465 \t -0.43925337723689406\n",
            "17     \t [-0.65646873  1.80183425]. \t  -190.6759218434711 \t -0.43925337723689406\n",
            "18     \t [-1.29762706  1.92167207]. \t  -10.93569011375251 \t -0.43925337723689406\n",
            "19     \t [0.47897968 0.23845816]. \t  \u001b[92m-0.27962824284233395\u001b[0m \t -0.27962824284233395\n",
            "20     \t [0.49470264 0.26641495]. \t  -0.3023461028942937 \t -0.27962824284233395\n",
            "21     \t [0.46936177 0.21190166]. \t  -0.2886309290501605 \t -0.27962824284233395\n",
            "22     \t [0.47917186 0.22710248]. \t  \u001b[92m-0.2718885439291488\u001b[0m \t -0.2718885439291488\n",
            "23     \t [0.4780115  0.25464372]. \t  -0.3408475576252339 \t -0.2718885439291488\n",
            "24     \t [0.35419852 0.1362039 ]. \t  -0.42861002044094054 \t -0.2718885439291488\n",
            "25     \t [0.36114662 0.09962187]. \t  -0.5030285104341864 \t -0.2718885439291488\n",
            "26     \t [0.35250833 0.15446849]. \t  -0.5104879313908647 \t -0.2718885439291488\n",
            "27     \t [0.35255914 0.09657011]. \t  -0.4960629799934465 \t -0.2718885439291488\n",
            "28     \t [-1.23805454  2.03458898]. \t  -30.190209175073587 \t -0.2718885439291488\n",
            "29     \t [0.33916896 0.13951617]. \t  -0.4966275866845784 \t -0.2718885439291488\n",
            "30     \t [0.34379329 0.13156173]. \t  -0.44847733913275734 \t -0.2718885439291488\n",
            "31     \t [0.21929055 0.03164357]. \t  -0.636550304394025 \t -0.2718885439291488\n",
            "32     \t [0.55118127 0.3238116 ]. \t  \u001b[92m-0.2414814892612943\u001b[0m \t -0.2414814892612943\n",
            "33     \t [0.44681272 0.22400513]. \t  -0.3653743079780498 \t -0.2414814892612943\n",
            "34     \t [0.51384815 0.30790414]. \t  -0.42875057122369326 \t -0.2414814892612943\n",
            "35     \t [0.41460168 0.19006125]. \t  -0.37569406265632826 \t -0.2414814892612943\n",
            "36     \t [0.52065112 0.27682139]. \t  \u001b[92m-0.23307448151255522\u001b[0m \t -0.23307448151255522\n",
            "37     \t [0.49394095 0.23996578]. \t  -0.257705279634833 \t -0.23307448151255522\n",
            "38     \t [0.20978718 0.00795567]. \t  -0.7544325625847027 \t -0.23307448151255522\n",
            "39     \t [0.35051216 0.09085773]. \t  -0.5242411624252258 \t -0.23307448151255522\n",
            "40     \t [0.52180431 0.29839744]. \t  -0.2968846025794403 \t -0.23307448151255522\n",
            "41     \t [0.52885795 0.30186899]. \t  -0.27116235328413135 \t -0.23307448151255522\n",
            "42     \t [0.51578165 0.32698716]. \t  -0.6060362940459971 \t -0.23307448151255522\n",
            "43     \t [0.56789889 0.30825071]. \t  \u001b[92m-0.2070416824355861\u001b[0m \t -0.2070416824355861\n",
            "44     \t [0.48111253 0.27896806]. \t  -0.49485779546928355 \t -0.2070416824355861\n",
            "45     \t [0.58020824 0.36502456]. \t  -0.2567843930852464 \t -0.2070416824355861\n",
            "46     \t [0.51017881 0.24393991]. \t  -0.2666325380451608 \t -0.2070416824355861\n",
            "47     \t [0.61769818 0.41156855]. \t  -0.2362597792338174 \t -0.2070416824355861\n",
            "48     \t [0.54288934 0.30323047]. \t  -0.2161779260765419 \t -0.2070416824355861\n",
            "49     \t [0.50062623 0.28570758]. \t  -0.37244151715233476 \t -0.2070416824355861\n",
            "50     \t [0.58063915 0.33687617]. \t  \u001b[92m-0.1758705762477758\u001b[0m \t -0.1758705762477758\n",
            "51     \t [0.61611042 0.39581897]. \t  \u001b[92m-0.17370249964115364\u001b[0m \t -0.17370249964115364\n",
            "52     \t [0.60118454 0.34517043]. \t  -0.18546789834002134 \t -0.17370249964115364\n",
            "53     \t [0.29946474 0.0851017 ]. \t  -0.49284493557776726 \t -0.17370249964115364\n",
            "54     \t [0.6417044 0.406388 ]. \t  \u001b[92m-0.13128800190561268\u001b[0m \t -0.13128800190561268\n",
            "55     \t [0.43476914 0.16124376]. \t  -0.3966612415937474 \t -0.13128800190561268\n",
            "56     \t [0.61469895 0.35343143]. \t  -0.20810701022720063 \t -0.13128800190561268\n",
            "57     \t [0.66049542 0.39556826]. \t  -0.2807979766184698 \t -0.13128800190561268\n",
            "58     \t [0.65296364 0.40359962]. \t  -0.17224460505428568 \t -0.13128800190561268\n",
            "59     \t [0.59024028 0.35205739]. \t  -0.169252711758634 \t -0.13128800190561268\n",
            "60     \t [0.57844028 0.33379453]. \t  -0.17777637446095557 \t -0.13128800190561268\n",
            "61     \t [0.58565963 0.36185334]. \t  -0.20723332198739713 \t -0.13128800190561268\n",
            "62     \t [0.60342077 0.38106263]. \t  -0.18599179456352402 \t -0.13128800190561268\n",
            "63     \t [0.60478848 0.37319226]. \t  -0.16170246891773205 \t -0.13128800190561268\n",
            "64     \t [0.37149082 0.11493527]. \t  -0.44824698715638184 \t -0.13128800190561268\n",
            "65     \t [0.5845678  0.29348692]. \t  -0.4052222262277342 \t -0.13128800190561268\n",
            "66     \t [0.60291384 0.3910467 ]. \t  -0.2335314087064989 \t -0.13128800190561268\n",
            "67     \t [0.37544938 0.13070821]. \t  -0.40057798452096716 \t -0.13128800190561268\n",
            "68     \t [0.57614117 0.36987496]. \t  -0.32357269914169506 \t -0.13128800190561268\n",
            "69     \t [0.62319241 0.34804952]. \t  -0.30454818974210274 \t -0.13128800190561268\n",
            "70     \t [0.58041662 0.36960275]. \t  -0.2831054838405783 \t -0.13128800190561268\n",
            "71     \t [0.64785329 0.42737772]. \t  \u001b[92m-0.12988075448968567\u001b[0m \t -0.12988075448968567\n",
            "72     \t [-0.12084202  0.03962251]. \t  -1.3188854391011409 \t -0.12988075448968567\n",
            "73     \t [0.65798709 0.41539304]. \t  -0.14778700006465303 \t -0.12988075448968567\n",
            "74     \t [0.65454641 0.42274385]. \t  \u001b[92m-0.12257255485774107\u001b[0m \t -0.12257255485774107\n",
            "75     \t [0.64467232 0.41471986]. \t  -0.12633564867249344 \t -0.12257255485774107\n",
            "76     \t [0.42211916 0.17891751]. \t  -0.33399998740615805 \t -0.12257255485774107\n",
            "77     \t [0.62296927 0.41581062]. \t  -0.21899154094380457 \t -0.12257255485774107\n",
            "78     \t [0.61631136 0.40789479]. \t  -0.22592584000339422 \t -0.12257255485774107\n",
            "79     \t [0.58696071 0.34678937]. \t  -0.17111515097286717 \t -0.12257255485774107\n",
            "80     \t [0.48756362 0.23159658]. \t  -0.26633856793622246 \t -0.12257255485774107\n",
            "81     \t [0.59605955 0.40193596]. \t  -0.3807805606056779 \t -0.12257255485774107\n",
            "82     \t [0.64778972 0.45281323]. \t  -0.23415462603725262 \t -0.12257255485774107\n",
            "83     \t [0.64431502 0.44182304]. \t  -0.19770044981483198 \t -0.12257255485774107\n",
            "84     \t [0.6215546  0.38597864]. \t  -0.14323327722071294 \t -0.12257255485774107\n",
            "85     \t [0.68741626 0.47801339]. \t  \u001b[92m-0.10070317772970017\u001b[0m \t -0.10070317772970017\n",
            "86     \t [0.58233138 0.3204791 ]. \t  -0.20915753238737672 \t -0.10070317772970017\n",
            "87     \t [0.61312966 0.37907794]. \t  -0.15066088231816285 \t -0.10070317772970017\n",
            "88     \t [0.22408311 0.05586106]. \t  -0.6052368026524136 \t -0.10070317772970017\n",
            "89     \t [0.18487441 0.02371836]. \t  -0.6753712853584742 \t -0.10070317772970017\n",
            "90     \t [0.59125542 0.39276955]. \t  -0.35358020977675203 \t -0.10070317772970017\n",
            "91     \t [0.57569025 0.33448146]. \t  -0.1809764657918715 \t -0.10070317772970017\n",
            "92     \t [0.64744592 0.43544396]. \t  -0.15072575645460531 \t -0.10070317772970017\n",
            "93     \t [0.6518551  0.44553663]. \t  -0.16372974525044734 \t -0.10070317772970017\n",
            "94     \t [0.66956674 0.44385132]. \t  -0.11118271105955067 \t -0.10070317772970017\n",
            "95     \t [0.2697211  0.02744031]. \t  -0.738599269222399 \t -0.10070317772970017\n",
            "96     \t [0.69498355 0.4422967 ]. \t  -0.25872826166592167 \t -0.10070317772970017\n",
            "97     \t [0.64887166 0.38744775]. \t  -0.23609758813993914 \t -0.10070317772970017\n",
            "98     \t [0.63764791 0.38184024]. \t  -0.1925781452969893 \t -0.10070317772970017\n",
            "99     \t [0.66670872 0.41395984]. \t  -0.20435637191870468 \t -0.10070317772970017\n",
            "100    \t [0.59511527 0.3141356 ]. \t  -0.32414436360611953 \t -0.10070317772970017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO3I_9cbuvY2",
        "outputId": "4959b01c-b3ab-4065-d5da-d8fe5be1c4e0"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 6 \r\n",
        "\r\n",
        "np.random.seed(run_num_6)\r\n",
        "surrogate_winner_6 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_6 = dGPGO(surrogate_winner_6, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_6.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.60915518 -0.68821072]. \t  -1074.6314195925434 \t -3.0269049669752817\n",
            "init   \t [ 1.31575449 -1.87721062]. \t  -1302.169546932896 \t -3.0269049669752817\n",
            "init   \t [-1.60703824  0.38933325]. \t  -487.8262244570432 \t -3.0269049669752817\n",
            "init   \t [ 0.12213192 -0.33256477]. \t  -12.844955340992902 \t -3.0269049669752817\n",
            "init   \t [-0.67416945  0.50183959]. \t  -3.0269049669752817 \t -3.0269049669752817\n",
            "1      \t [-0.06756934  0.5044222 ]. \t  -26.125364719936726 \t -3.0269049669752817\n",
            "2      \t [-0.46160649 -0.5077204 ]. \t  -54.091694322312925 \t -3.0269049669752817\n",
            "3      \t [-0.27427447  0.04468444]. \t  \u001b[92m-1.7170570344409537\u001b[0m \t -1.7170570344409537\n",
            "4      \t [-0.71442187  1.46427117]. \t  -93.92652952435489 \t -1.7170570344409537\n",
            "5      \t [-0.48240915  0.62004717]. \t  -17.199880127411973 \t -1.7170570344409537\n",
            "6      \t [1.15424041 1.1124837 ]. \t  -4.854432826251765 \t -1.7170570344409537\n",
            "7      \t [0.48519717 0.3180747 ]. \t  \u001b[92m-0.9482632093142556\u001b[0m \t -0.9482632093142556\n",
            "8      \t [0.14387441 0.07741571]. \t  -1.0546200127162013 \t -0.9482632093142556\n",
            "9      \t [0.81192409 0.84115075]. \t  -3.3452259125089627 \t -0.9482632093142556\n",
            "10     \t [2.048 2.048]. \t  -461.7603900415846 \t -0.9482632093142556\n",
            "11     \t [1.15689675 0.89844943]. \t  -19.381155339446778 \t -0.9482632093142556\n",
            "12     \t [1.00227452 1.12930212]. \t  -1.5562090803912634 \t -0.9482632093142556\n",
            "13     \t [-0.5040535  0.2521811]. \t  -2.262533699212602 \t -0.9482632093142556\n",
            "14     \t [-1.64886576  2.02848959]. \t  -54.663578807527045 \t -0.9482632093142556\n",
            "15     \t [0.49807982 0.41454714]. \t  -3.022938008731005 \t -0.9482632093142556\n",
            "16     \t [0.33936924 0.13892049]. \t  \u001b[92m-0.49283457729895436\u001b[0m \t -0.49283457729895436\n",
            "17     \t [0.35213959 0.15966314]. \t  -0.5468927539887649 \t -0.49283457729895436\n",
            "18     \t [0.32699723 0.12454535]. \t  \u001b[92m-0.48397269024054607\u001b[0m \t -0.48397269024054607\n",
            "19     \t [0.36967579 0.13263797]. \t  \u001b[92m-0.39892643660242794\u001b[0m \t -0.39892643660242794\n",
            "20     \t [ 0.00030378 -0.03349028]. \t  -1.1115530204098512 \t -0.39892643660242794\n",
            "21     \t [-0.3673256   0.20934144]. \t  -2.4233137748769504 \t -0.39892643660242794\n",
            "22     \t [0.38011721 0.1859075 ]. \t  -0.5558030996031271 \t -0.39892643660242794\n",
            "23     \t [0.25970236 0.08595338]. \t  -0.5822954458833632 \t -0.39892643660242794\n",
            "24     \t [0.3641366  0.14410076]. \t  -0.41755943956747227 \t -0.39892643660242794\n",
            "25     \t [0.34287125 0.14658059]. \t  -0.5160336843114723 \t -0.39892643660242794\n",
            "26     \t [0.35352324 0.16442245]. \t  -0.5735132830426445 \t -0.39892643660242794\n",
            "27     \t [0.35235417 0.11866485]. \t  -0.422457610549093 \t -0.39892643660242794\n",
            "28     \t [0.35319668 0.10561561]. \t  -0.4549589821715636 \t -0.39892643660242794\n",
            "29     \t [0.33286825 0.12419993]. \t  -0.46301718795729896 \t -0.39892643660242794\n",
            "30     \t [0.18253323 0.00938006]. \t  -0.7255562406686946 \t -0.39892643660242794\n",
            "31     \t [0.33713196 0.08841596]. \t  -0.5031098960436735 \t -0.39892643660242794\n",
            "32     \t [0.34602471 0.11579311]. \t  -0.42923602712126396 \t -0.39892643660242794\n",
            "33     \t [0.35306815 0.09549545]. \t  -0.503561117886802 \t -0.39892643660242794\n",
            "34     \t [0.29489173 0.10260216]. \t  -0.5216418373734665 \t -0.39892643660242794\n",
            "35     \t [0.2561318  0.04459111]. \t  -0.5974919575587364 \t -0.39892643660242794\n",
            "36     \t [0.31279999 0.14189926]. \t  -0.6663319247001203 \t -0.39892643660242794\n",
            "37     \t [0.19748198 0.05662184]. \t  -0.6750911450567384 \t -0.39892643660242794\n",
            "38     \t [0.34875857 0.11324938]. \t  -0.43114314001312226 \t -0.39892643660242794\n",
            "39     \t [0.35525584 0.1351601 ]. \t  -0.4237113447659652 \t -0.39892643660242794\n",
            "40     \t [0.3247494  0.07629851]. \t  -0.5410153270076755 \t -0.39892643660242794\n",
            "41     \t [0.38293062 0.20904073]. \t  -0.7702114324351725 \t -0.39892643660242794\n",
            "42     \t [0.30947595 0.14919503]. \t  -0.762189555049063 \t -0.39892643660242794\n",
            "43     \t [1.00669997 1.05237084]. \t  \u001b[92m-0.15156836641873228\u001b[0m \t -0.15156836641873228\n",
            "44     \t [0.22096415 0.02240028]. \t  -0.6767242325595801 \t -0.15156836641873228\n",
            "45     \t [0.3683152  0.11894201]. \t  -0.42696173224586365 \t -0.15156836641873228\n",
            "46     \t [1.01379986 1.0747987 ]. \t  -0.2211707882598672 \t -0.15156836641873228\n",
            "47     \t [0.36090093 0.14176091]. \t  -0.42169891011799404 \t -0.15156836641873228\n",
            "48     \t [0.36966843 0.14176559]. \t  -0.3999299599912608 \t -0.15156836641873228\n",
            "49     \t [1.00351378 1.06017085]. \t  -0.2823019797521305 \t -0.15156836641873228\n",
            "50     \t [1.00376048 1.06821843]. \t  -0.3682608022952167 \t -0.15156836641873228\n",
            "51     \t [0.14031239 0.02943435]. \t  -0.7485627617948236 \t -0.15156836641873228\n",
            "52     \t [0.13401277 0.00763048]. \t  -0.7606025919126845 \t -0.15156836641873228\n",
            "53     \t [0.1165378  0.00273835]. \t  -0.7922618827706503 \t -0.15156836641873228\n",
            "54     \t [1.00189531 1.04853972]. \t  -0.20021961561736903 \t -0.15156836641873228\n",
            "55     \t [0.98274226 1.02544852]. \t  -0.3563030614639138 \t -0.15156836641873228\n",
            "56     \t [0.41406658 0.14321976]. \t  -0.42301904064519863 \t -0.15156836641873228\n",
            "57     \t [0.28310613 0.10049677]. \t  -0.5553396699120449 \t -0.15156836641873228\n",
            "58     \t [0.44061248 0.21305681]. \t  -0.3487014186718563 \t -0.15156836641873228\n",
            "59     \t [0.26090729 0.11073137]. \t  -0.7282349131641535 \t -0.15156836641873228\n",
            "60     \t [0.37697432 0.15587036]. \t  -0.40709675299483844 \t -0.15156836641873228\n",
            "61     \t [0.41667352 0.18614302]. \t  -0.3559603569278629 \t -0.15156836641873228\n",
            "62     \t [0.39829326 0.15726937]. \t  -0.3622381851737281 \t -0.15156836641873228\n",
            "63     \t [0.9663492  0.98281155]. \t  -0.2410439516959646 \t -0.15156836641873228\n",
            "64     \t [0.9976303  1.08603634]. \t  -0.8239271101194313 \t -0.15156836641873228\n",
            "65     \t [0.34442504 0.13241049]. \t  -0.44877256217750994 \t -0.15156836641873228\n",
            "66     \t [0.96294547 1.02244589]. \t  -0.9073325448556405 \t -0.15156836641873228\n",
            "67     \t [0.35761089 0.14057111]. \t  -0.4287561316981041 \t -0.15156836641873228\n",
            "68     \t [0.3876033  0.14961929]. \t  -0.37506779232442616 \t -0.15156836641873228\n",
            "69     \t [0.27360542 0.09248704]. \t  -0.5587205972186458 \t -0.15156836641873228\n",
            "70     \t [1.02836208 1.12010301]. \t  -0.39236065794854863 \t -0.15156836641873228\n",
            "71     \t [1.04275756 1.05641919]. \t  \u001b[92m-0.09745839662049569\u001b[0m \t -0.09745839662049569\n",
            "72     \t [0.99968375 1.03293038]. \t  -0.112646087767333 \t -0.09745839662049569\n",
            "73     \t [0.31268851 0.13468679]. \t  -0.6086517683798859 \t -0.09745839662049569\n",
            "74     \t [1.01639251 1.03783931]. \t  \u001b[92m-0.0025588772655143932\u001b[0m \t -0.0025588772655143932\n",
            "75     \t [0.392002   0.15294566]. \t  -0.36971339480818394 \t -0.0025588772655143932\n",
            "76     \t [ 0.06535568 -0.03136615]. \t  -1.0005632415594168 \t -0.0025588772655143932\n",
            "77     \t [1.04728686 1.09847122]. \t  \u001b[92m-0.0025120877928910476\u001b[0m \t -0.0025120877928910476\n",
            "78     \t [1.04737912 1.06979608]. \t  -0.07626657198068858 \t -0.0025120877928910476\n",
            "79     \t [0.34263699 0.13000644]. \t  -0.448018096826991 \t -0.0025120877928910476\n",
            "80     \t [0.37404018 0.10329289]. \t  -0.5258781053497257 \t -0.0025120877928910476\n",
            "81     \t [1.03708145 1.08391916]. \t  -0.008399537323934813 \t -0.0025120877928910476\n",
            "82     \t [0.3131992  0.11577053]. \t  -0.5029422300342977 \t -0.0025120877928910476\n",
            "83     \t [1.02325398 1.04952514]. \t  \u001b[92m-0.001154019711397316\u001b[0m \t -0.001154019711397316\n",
            "84     \t [0.99964766 1.04776383]. \t  -0.2349185199019206 \t -0.001154019711397316\n",
            "85     \t [1.02862826 1.06758484]. \t  -0.009861201498883723 \t -0.001154019711397316\n",
            "86     \t [1.03291449 1.03742469]. \t  -0.08803559045509912 \t -0.001154019711397316\n",
            "87     \t [1.02700917 1.07335359]. \t  -0.035346886069966034 \t -0.001154019711397316\n",
            "88     \t [0.38571345 0.18598162]. \t  -0.5157822220572235 \t -0.001154019711397316\n",
            "89     \t [1.02545822 1.08589163]. \t  -0.11848294604512319 \t -0.001154019711397316\n",
            "90     \t [0.34622136 0.13415482]. \t  -0.44783429492418686 \t -0.001154019711397316\n",
            "91     \t [1.00972925 1.03910237]. \t  -0.03831186842561234 \t -0.001154019711397316\n",
            "92     \t [1.01219509 1.07467523]. \t  -0.25151389020746573 \t -0.001154019711397316\n",
            "93     \t [1.02410742 1.08801581]. \t  -0.15440048318213473 \t -0.001154019711397316\n",
            "94     \t [1.03373152 1.02379313]. \t  -0.20191100018747934 \t -0.001154019711397316\n",
            "95     \t [1.0299841  1.04296253]. \t  -0.03295692094967122 \t -0.001154019711397316\n",
            "96     \t [1.02100255 1.05627868]. \t  -0.019574837620978926 \t -0.001154019711397316\n",
            "97     \t [0.34962256 0.13586982]. \t  -0.4415790892055322 \t -0.001154019711397316\n",
            "98     \t [1.0531055  1.06323149]. \t  -0.21258156894110014 \t -0.001154019711397316\n",
            "99     \t [0.3446072  0.12558956]. \t  -0.43421204700317945 \t -0.001154019711397316\n",
            "100    \t [1.0505979  1.09359573]. \t  -0.012883166177472483 \t -0.001154019711397316\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rQbLZD8uvbI",
        "outputId": "f9d90357-5eee-4f9f-aa05-7c8b9fc0f1b1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 7 \r\n",
        "\r\n",
        "np.random.seed(run_num_7)\r\n",
        "surrogate_winner_7 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_7 = dGPGO(surrogate_winner_7, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_7.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.42268934 -0.80954733]. \t  -808.7939502616841 \t -2.0077595729598063\n",
            "init   \t [-1.79389885 -0.16441204]. \t  -1151.9264213711547 \t -2.0077595729598063\n",
            "init   \t [1.37319786 1.74897991]. \t  -2.0077595729598063 \t -2.0077595729598063\n",
            "init   \t [0.92974688 1.09976052]. \t  -5.543015908052858 \t -2.0077595729598063\n",
            "init   \t [-0.94533605  0.58994398]. \t  -13.008689168416776 \t -2.0077595729598063\n",
            "1      \t [-0.37355351  1.06768858]. \t  -88.03221521709018 \t -2.0077595729598063\n",
            "2      \t [ 0.03732114 -0.06513629]. \t  \u001b[92m-1.369363486338532\u001b[0m \t -1.369363486338532\n",
            "3      \t [1.6909501  0.61210427]. \t  -505.4717867923329 \t -1.369363486338532\n",
            "4      \t [-0.28519494  0.27836718]. \t  -5.533848669121014 \t -1.369363486338532\n",
            "5      \t [-1.42814168  1.39317793]. \t  -47.68055708950086 \t -1.369363486338532\n",
            "6      \t [ 0.21013104 -1.42152215]. \t  -215.44485994952188 \t -1.369363486338532\n",
            "7      \t [0.95357792 1.84391201]. \t  -87.3500875780384 \t -1.369363486338532\n",
            "8      \t [-0.96030465  0.93243349]. \t  -3.8532974266292483 \t -1.369363486338532\n",
            "9      \t [2.048 2.048]. \t  -461.7603900415999 \t -1.369363486338532\n",
            "10     \t [1.02615805 1.31654299]. \t  -6.946156691987973 \t -1.369363486338532\n",
            "11     \t [-1.90584876  1.93438431]. \t  -296.7219712569592 \t -1.369363486338532\n",
            "12     \t [0.46213398 0.26553063]. \t  \u001b[92m-0.5593132411126935\u001b[0m \t -0.5593132411126935\n",
            "13     \t [0.56487911 0.60482749]. \t  -8.354012547911596 \t -0.5593132411126935\n",
            "14     \t [ 0.48664236 -0.23328774]. \t  -22.36373833523198 \t -0.5593132411126935\n",
            "15     \t [-0.44814536  0.13644553]. \t  -2.511715863814974 \t -0.5593132411126935\n",
            "16     \t [1.25046    1.58017626]. \t  \u001b[92m-0.09004127972613968\u001b[0m \t -0.09004127972613968\n",
            "17     \t [1.30579574 1.87860309]. \t  -3.103756311192025 \t -0.09004127972613968\n",
            "18     \t [0.3253884  0.10373203]. \t  -0.4555611611686538 \t -0.09004127972613968\n",
            "19     \t [1.29294277 1.70738216]. \t  -0.2131298983154008 \t -0.09004127972613968\n",
            "20     \t [-0.81659556  0.66275578]. \t  -3.3016779649595347 \t -0.09004127972613968\n",
            "21     \t [-0.19877663  0.01707947]. \t  -1.487387935733628 \t -0.09004127972613968\n",
            "22     \t [-0.41069447  0.20412318]. \t  -2.115752076829976 \t -0.09004127972613968\n",
            "23     \t [0.36672049 0.16798547]. \t  -0.5132783988625711 \t -0.09004127972613968\n",
            "24     \t [0.38068824 0.1465462 ]. \t  -0.3838103581685603 \t -0.09004127972613968\n",
            "25     \t [1.33396384 1.73684567]. \t  -0.29312585567653665 \t -0.09004127972613968\n",
            "26     \t [0.33421746 0.15911675]. \t  -0.6680887788446221 \t -0.09004127972613968\n",
            "27     \t [1.29767259 1.67867758]. \t  -0.09139319176124136 \t -0.09004127972613968\n",
            "28     \t [1.26598536 1.56174932]. \t  -0.23859919480853495 \t -0.09004127972613968\n",
            "29     \t [1.32697266 1.75445673]. \t  -0.11100673204726075 \t -0.09004127972613968\n",
            "30     \t [1.32393505 1.75408922]. \t  -0.105099091733362 \t -0.09004127972613968\n",
            "31     \t [1.33365532 1.73928361]. \t  -0.2661910147291956 \t -0.09004127972613968\n",
            "32     \t [1.27182004 1.57762427]. \t  -0.2331026552967497 \t -0.09004127972613968\n",
            "33     \t [0.39171428 0.13659381]. \t  -0.39839120440840275 \t -0.09004127972613968\n",
            "34     \t [1.25698179 1.59456888]. \t  \u001b[92m-0.08725546691512442\u001b[0m \t -0.08725546691512442\n",
            "35     \t [1.32942458 1.77407377]. \t  -0.11301498620043413 \t -0.08725546691512442\n",
            "36     \t [1.27476812 1.62866836]. \t  \u001b[92m-0.0768185504021595\u001b[0m \t -0.0768185504021595\n",
            "37     \t [1.34772332 1.79093841]. \t  -0.18552775741190763 \t -0.0768185504021595\n",
            "38     \t [0.33881261 0.1235201 ]. \t  -0.44478326721388745 \t -0.0768185504021595\n",
            "39     \t [1.27060259 1.59862759]. \t  -0.09820038367733123 \t -0.0768185504021595\n",
            "40     \t [1.28847137 1.65315619]. \t  -0.0881189398791939 \t -0.0768185504021595\n",
            "41     \t [1.35350624 1.77972429]. \t  -0.3980236712639157 \t -0.0768185504021595\n",
            "42     \t [1.3217597  1.76279835]. \t  -0.12833441695079956 \t -0.0768185504021595\n",
            "43     \t [1.31274827 1.77260042]. \t  -0.3407855953501275 \t -0.0768185504021595\n",
            "44     \t [1.3367105  1.75045825]. \t  -0.24540968219533638 \t -0.0768185504021595\n",
            "45     \t [1.31169037 1.71205742]. \t  -0.10433210379994093 \t -0.0768185504021595\n",
            "46     \t [1.29691748 1.68390773]. \t  -0.08852586110019804 \t -0.0768185504021595\n",
            "47     \t [1.24725058 1.56798329]. \t  \u001b[92m-0.07638331121843762\u001b[0m \t -0.07638331121843762\n",
            "48     \t [1.28887432 1.63658274]. \t  -0.14403456766505288 \t -0.07638331121843762\n",
            "49     \t [0.37713679 0.11462119]. \t  -0.4641951033489359 \t -0.07638331121843762\n",
            "50     \t [0.39956565 0.18833283]. \t  -0.4427763541988444 \t -0.07638331121843762\n",
            "51     \t [1.24558232 1.59362825]. \t  -0.23799774150302205 \t -0.07638331121843762\n",
            "52     \t [1.26400656 1.59954233]. \t  \u001b[92m-0.07003426173126782\u001b[0m \t -0.07003426173126782\n",
            "53     \t [1.22602631 1.5203536 ]. \t  -0.08071693693262533 \t -0.07003426173126782\n",
            "54     \t [0.37286943 0.14615346]. \t  -0.3983648207838524 \t -0.07003426173126782\n",
            "55     \t [1.27986929 1.61110254]. \t  -0.15102642279796077 \t -0.07003426173126782\n",
            "56     \t [1.33223074 1.7577327 ]. \t  -0.1396389401933671 \t -0.07003426173126782\n",
            "57     \t [1.32359206 1.75281746]. \t  -0.10479673947920518 \t -0.07003426173126782\n",
            "58     \t [1.27385881 1.61934257]. \t  -0.07613682166648879 \t -0.07003426173126782\n",
            "59     \t [1.26920987 1.64225928]. \t  -0.17085392230467644 \t -0.07003426173126782\n",
            "60     \t [1.26662319 1.61163542]. \t  -0.07641853550938701 \t -0.07003426173126782\n",
            "61     \t [1.26800365 1.59896134]. \t  -0.07969705252997498 \t -0.07003426173126782\n",
            "62     \t [1.31300535 1.75979871]. \t  -0.22624854467405775 \t -0.07003426173126782\n",
            "63     \t [1.33389427 1.74409107]. \t  -0.2352686778294532 \t -0.07003426173126782\n",
            "64     \t [0.36639983 0.14192395]. \t  -0.40733989771941137 \t -0.07003426173126782\n",
            "65     \t [1.34726377 1.84267145]. \t  -0.19650220550938508 \t -0.07003426173126782\n",
            "66     \t [1.33041756 1.76880647]. \t  -0.1093208215317134 \t -0.07003426173126782\n",
            "67     \t [1.3249195 1.7687872]. \t  -0.12346311693417553 \t -0.07003426173126782\n",
            "68     \t [0.35873312 0.13030131]. \t  -0.4114830115057504 \t -0.07003426173126782\n",
            "69     \t [0.34721843 0.11530361]. \t  -0.42888741301870686 \t -0.07003426173126782\n",
            "70     \t [1.21953821 1.48434388]. \t  \u001b[92m-0.0490552709680343\u001b[0m \t -0.0490552709680343\n",
            "71     \t [1.33743616 1.72819155]. \t  -0.4804200168068571 \t -0.0490552709680343\n",
            "72     \t [1.29496138 1.63095987]. \t  -0.2982814119432187 \t -0.0490552709680343\n",
            "73     \t [1.34864529 1.73303812]. \t  -0.8578206644236346 \t -0.0490552709680343\n",
            "74     \t [1.30993247 1.75935978]. \t  -0.28473298069595676 \t -0.0490552709680343\n",
            "75     \t [1.35502511 1.8868619 ]. \t  -0.38379055745864094 \t -0.0490552709680343\n",
            "76     \t [1.29600495 1.62443288]. \t  -0.39227822550843344 \t -0.0490552709680343\n",
            "77     \t [1.32457931 1.79726282]. \t  -0.2881290768965223 \t -0.0490552709680343\n",
            "78     \t [1.25908996 1.57634025]. \t  -0.07516883247303882 \t -0.0490552709680343\n",
            "79     \t [1.30200699 1.69045699]. \t  -0.09347894850477301 \t -0.0490552709680343\n",
            "80     \t [1.32658194 1.74572451]. \t  -0.12652305362895092 \t -0.0490552709680343\n",
            "81     \t [1.23870894 1.5515619 ]. \t  -0.08643558881598505 \t -0.0490552709680343\n",
            "82     \t [1.31119196 1.75906094]. \t  -0.2555357348186981 \t -0.0490552709680343\n",
            "83     \t [1.24689635 1.59924655]. \t  -0.2589474748614695 \t -0.0490552709680343\n",
            "84     \t [1.38363685 1.95261225]. \t  -0.2928057683817886 \t -0.0490552709680343\n",
            "85     \t [1.36407377 1.88512876]. \t  -0.19223955837486045 \t -0.0490552709680343\n",
            "86     \t [1.35120437 1.91491507]. \t  -0.9183274260008599 \t -0.0490552709680343\n",
            "87     \t [1.30214822 1.72311787]. \t  -0.16707205344247522 \t -0.0490552709680343\n",
            "88     \t [1.32703997 1.74409086]. \t  -0.1356658584091965 \t -0.0490552709680343\n",
            "89     \t [1.33071847 1.7737575 ]. \t  -0.1102425139235239 \t -0.0490552709680343\n",
            "90     \t [1.26616991 1.54460228]. \t  -0.41405452221911193 \t -0.0490552709680343\n",
            "91     \t [1.32135896 1.7470228 ]. \t  -0.10337835239181162 \t -0.0490552709680343\n",
            "92     \t [1.38123335 1.95257597]. \t  -0.3457777670756157 \t -0.0490552709680343\n",
            "93     \t [1.30623684 1.71433512]. \t  -0.1003103558526618 \t -0.0490552709680343\n",
            "94     \t [1.39721535 2.03003732]. \t  -0.7634776093778264 \t -0.0490552709680343\n",
            "95     \t [1.23477338 1.5652451 ]. \t  -0.21979055889492916 \t -0.0490552709680343\n",
            "96     \t [1.33136936 1.77679292]. \t  -0.11161066157661588 \t -0.0490552709680343\n",
            "97     \t [1.30201292 1.69989086]. \t  -0.09337704015984706 \t -0.0490552709680343\n",
            "98     \t [1.34442247 1.87959665]. \t  -0.6388265814132037 \t -0.0490552709680343\n",
            "99     \t [0.36645256 0.11546992]. \t  -0.436792399129995 \t -0.0490552709680343\n",
            "100    \t [1.25474028 1.59626495]. \t  -0.1128175803839066 \t -0.0490552709680343\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YRio_skuvd2",
        "outputId": "9500dfdf-7fde-41a6-9976-64fb8f1379fe"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 8 \r\n",
        "\r\n",
        "np.random.seed(run_num_8)\r\n",
        "surrogate_winner_8 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_8 = dGPGO(surrogate_winner_8, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_8.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.59948851 -1.8750873 ]. \t  -1972.3059423071145 \t -7.238275380208551\n",
            "init   \t [ 0.40834083 -0.81972959]. \t  -97.66272647293047 \t -7.238275380208551\n",
            "init   \t [ 1.21492186 -0.54806067]. \t  -409.7425700197761 \t -7.238275380208551\n",
            "init   \t [ 1.55897178 -0.82804067]. \t  -1062.051446884513 \t -7.238275380208551\n",
            "init   \t [-0.69999303  0.28146449]. \t  -7.238275380208551 \t -7.238275380208551\n",
            "1      \t [0.61427538 0.32831856]. \t  \u001b[92m-0.3890371497654512\u001b[0m \t -0.3890371497654512\n",
            "2      \t [ 0.48608361 -0.18688867]. \t  -18.171051935372514 \t -0.3890371497654512\n",
            "3      \t [-1.36023668  0.93255582]. \t  -89.78584733200618 \t -0.3890371497654512\n",
            "4      \t [1.39308184 0.79714469]. \t  -130.92113103053694 \t -0.3890371497654512\n",
            "5      \t [-0.64049808  0.74548271]. \t  -13.930149211427867 \t -0.3890371497654512\n",
            "6      \t [0.81361171 0.14216547]. \t  -27.053793653740094 \t -0.3890371497654512\n",
            "7      \t [0.51600044 1.44335958]. \t  -138.79143097841416 \t -0.3890371497654512\n",
            "8      \t [0.03936935 0.57311173]. \t  -33.59109867025242 \t -0.3890371497654512\n",
            "9      \t [-1.04827351  0.459537  ]. \t  -45.07103437698407 \t -0.3890371497654512\n",
            "10     \t [0.70260273 0.69282086]. \t  -4.055324764880087 \t -0.3890371497654512\n",
            "11     \t [-1.45841171  2.02404169]. \t  -7.103102879159884 \t -0.3890371497654512\n",
            "12     \t [-0.69582168  0.49546107]. \t  -2.888564935749705 \t -0.3890371497654512\n",
            "13     \t [-1.26702613  1.88590601]. \t  -13.010281715505432 \t -0.3890371497654512\n",
            "14     \t [0.60156656 0.46731794]. \t  -1.2704158960856264 \t -0.3890371497654512\n",
            "15     \t [1.37490891 1.57259441]. \t  -10.23897661963018 \t -0.3890371497654512\n",
            "16     \t [0.60616572 0.35460059]. \t  \u001b[92m-0.1715824940770919\u001b[0m \t -0.1715824940770919\n",
            "17     \t [0.58219271 0.36429288]. \t  -0.23879744579988543 \t -0.1715824940770919\n",
            "18     \t [0.59713041 0.30251123]. \t  -0.4544819637050215 \t -0.1715824940770919\n",
            "19     \t [0.5843719  0.28109416]. \t  -0.5375187732770126 \t -0.1715824940770919\n",
            "20     \t [0.5972727  0.37215738]. \t  -0.18597524614960226 \t -0.1715824940770919\n",
            "21     \t [0.58306244 0.2995476 ]. \t  -0.33716772863423605 \t -0.1715824940770919\n",
            "22     \t [0.61831099 0.44766181]. \t  -0.5727922425806182 \t -0.1715824940770919\n",
            "23     \t [0.56878358 0.26885569]. \t  -0.4847090284075555 \t -0.1715824940770919\n",
            "24     \t [0.58558941 0.31719846]. \t  -0.2378699359385182 \t -0.1715824940770919\n",
            "25     \t [0.59871758 0.28444671]. \t  -0.7088647688926061 \t -0.1715824940770919\n",
            "26     \t [0.59800765 0.41172006]. \t  -0.4543536296694427 \t -0.1715824940770919\n",
            "27     \t [0.57894996 0.32362368]. \t  -0.19064505864584733 \t -0.1715824940770919\n",
            "28     \t [0.58616195 0.26323502]. \t  -0.8168872051009916 \t -0.1715824940770919\n",
            "29     \t [0.56509176 0.2584247 ]. \t  -0.5600748359513341 \t -0.1715824940770919\n",
            "30     \t [0.61520578 0.39081614]. \t  \u001b[92m-0.1632891926938679\u001b[0m \t -0.1632891926938679\n",
            "31     \t [0.59671211 0.36583522]. \t  -0.17218617216856974 \t -0.1632891926938679\n",
            "32     \t [0.64515053 0.44871786]. \t  -0.231534369821775 \t -0.1632891926938679\n",
            "33     \t [0.59284985 0.34882979]. \t  -0.1664688171605358 \t -0.1632891926938679\n",
            "34     \t [0.60223218 0.33891159]. \t  -0.21473006210579365 \t -0.1632891926938679\n",
            "35     \t [-0.60859441  0.42410286]. \t  -2.8761136906193894 \t -0.1632891926938679\n",
            "36     \t [-1.50307926  1.84109023]. \t  -23.750936554234382 \t -0.1632891926938679\n",
            "37     \t [0.60540764 0.40831475]. \t  -0.3303965046427626 \t -0.1632891926938679\n",
            "38     \t [0.55087377 0.29917355]. \t  -0.2035533766898566 \t -0.1632891926938679\n",
            "39     \t [0.56423406 0.32502293]. \t  -0.19433131666478604 \t -0.1632891926938679\n",
            "40     \t [0.59114592 0.36062742]. \t  -0.1796473154256033 \t -0.1632891926938679\n",
            "41     \t [0.53933278 0.30380653]. \t  -0.22892418730372363 \t -0.1632891926938679\n",
            "42     \t [0.57979506 0.28759069]. \t  -0.4124924721167584 \t -0.1632891926938679\n",
            "43     \t [0.5631328  0.32758325]. \t  -0.20180393829817128 \t -0.1632891926938679\n",
            "44     \t [0.61296076 0.30779971]. \t  -0.6111281465906971 \t -0.1632891926938679\n",
            "45     \t [0.53270337 0.28957486]. \t  -0.22173243181297383 \t -0.1632891926938679\n",
            "46     \t [0.57915056 0.42233846]. \t  -0.932676539088966 \t -0.1632891926938679\n",
            "47     \t [0.56813119 0.28788023]. \t  -0.3082615523838378 \t -0.1632891926938679\n",
            "48     \t [0.64898864 0.46154446]. \t  -0.2860874672527634 \t -0.1632891926938679\n",
            "49     \t [0.58760309 0.35127712]. \t  -0.17367089403883443 \t -0.1632891926938679\n",
            "50     \t [0.57010623 0.35924152]. \t  -0.3019122420300122 \t -0.1632891926938679\n",
            "51     \t [0.5761787 0.3389133]. \t  -0.18442892914004566 \t -0.1632891926938679\n",
            "52     \t [0.57001621 0.37051763]. \t  -0.3928142648587124 \t -0.1632891926938679\n",
            "53     \t [0.59249728 0.29984709]. \t  -0.42826327377009976 \t -0.1632891926938679\n",
            "54     \t [0.55947775 0.27412831]. \t  -0.34528010536897524 \t -0.1632891926938679\n",
            "55     \t [0.61991022 0.42473446]. \t  -0.308054411098864 \t -0.1632891926938679\n",
            "56     \t [0.59267855 0.38216446]. \t  -0.2613707491160069 \t -0.1632891926938679\n",
            "57     \t [0.59340807 0.33860947]. \t  -0.18360597343410365 \t -0.1632891926938679\n",
            "58     \t [0.62283493 0.40902679]. \t  -0.1867889816317044 \t -0.1632891926938679\n",
            "59     \t [0.59038854 0.30005541]. \t  -0.40303776560537863 \t -0.1632891926938679\n",
            "60     \t [0.5802427  0.35659729]. \t  -0.21585969983193243 \t -0.1632891926938679\n",
            "61     \t [0.53876941 0.28443361]. \t  -0.21614289057553204 \t -0.1632891926938679\n",
            "62     \t [0.58215466 0.29026773]. \t  -0.41114389594308776 \t -0.1632891926938679\n",
            "63     \t [0.59804854 0.41373966]. \t  -0.47603478147224976 \t -0.1632891926938679\n",
            "64     \t [0.58297749 0.36807539]. \t  -0.2535030663014206 \t -0.1632891926938679\n",
            "65     \t [0.61465118 0.38543685]. \t  \u001b[92m-0.15433184697143174\u001b[0m \t -0.15433184697143174\n",
            "66     \t [0.55945794 0.26324277]. \t  -0.441587666794512 \t -0.15433184697143174\n",
            "67     \t [0.64043142 0.35681329]. \t  -0.41379565638305266 \t -0.15433184697143174\n",
            "68     \t [0.57878133 0.34407507]. \t  -0.18568295828925896 \t -0.15433184697143174\n",
            "69     \t [0.59727432 0.41534922]. \t  -0.5057317009607385 \t -0.15433184697143174\n",
            "70     \t [0.62203269 0.44104986]. \t  -0.4358129146697225 \t -0.15433184697143174\n",
            "71     \t [0.5809919  0.32933058]. \t  -0.18232628720540775 \t -0.15433184697143174\n",
            "72     \t [0.64164592 0.38357428]. \t  -0.2075766413877763 \t -0.15433184697143174\n",
            "73     \t [0.5910059  0.35506839]. \t  -0.17061748314603709 \t -0.15433184697143174\n",
            "74     \t [0.59630083 0.35758787]. \t  -0.16337830855626717 \t -0.15433184697143174\n",
            "75     \t [0.64312717 0.40107895]. \t  \u001b[92m-0.14306736202443632\u001b[0m \t -0.14306736202443632\n",
            "76     \t [0.62492492 0.4042641 ]. \t  -0.1595407035554987 \t -0.14306736202443632\n",
            "77     \t [0.62872146 0.40520017]. \t  -0.14766757016287577 \t -0.14306736202443632\n",
            "78     \t [0.59186314 0.35932437]. \t  -0.1747160632524465 \t -0.14306736202443632\n",
            "79     \t [0.57036643 0.30871597]. \t  -0.21214727094098695 \t -0.14306736202443632\n",
            "80     \t [1.49272094 2.03685682]. \t  -3.9046001322604873 \t -0.14306736202443632\n",
            "81     \t [0.60961833 0.3872611 ]. \t  -0.1768168804457008 \t -0.14306736202443632\n",
            "82     \t [0.62596565 0.39564907]. \t  \u001b[92m-0.14135792754455878\u001b[0m \t -0.14135792754455878\n",
            "83     \t [0.58263413 0.36368248]. \t  -0.23285487206116642 \t -0.14135792754455878\n",
            "84     \t [0.62650373 0.45708506]. \t  -0.5565330343864582 \t -0.14135792754455878\n",
            "85     \t [0.5911997  0.36203721]. \t  -0.18279305302282178 \t -0.14135792754455878\n",
            "86     \t [0.61006183 0.41315374]. \t  -0.319973941898481 \t -0.14135792754455878\n",
            "87     \t [0.53707386 0.3133608 ]. \t  -0.27636368977912346 \t -0.14135792754455878\n",
            "88     \t [0.5738987  0.32311623]. \t  -0.18546043554185865 \t -0.14135792754455878\n",
            "89     \t [0.60151539 0.39705888]. \t  -0.28296240199460676 \t -0.14135792754455878\n",
            "90     \t [0.60321458 0.33989337]. \t  -0.2149161824406874 \t -0.14135792754455878\n",
            "91     \t [0.60537975 0.39299404]. \t  -0.2259999722335559 \t -0.14135792754455878\n",
            "92     \t [0.57319607 0.31520889]. \t  -0.19997007786578272 \t -0.14135792754455878\n",
            "93     \t [0.61785983 0.30654725]. \t  -0.7115881007395285 \t -0.14135792754455878\n",
            "94     \t [0.56636861 0.33736775]. \t  -0.21557342824886613 \t -0.14135792754455878\n",
            "95     \t [0.55804831 0.31358113]. \t  -0.19578924979529194 \t -0.14135792754455878\n",
            "96     \t [0.61604062 0.39519023]. \t  -0.17202420135680543 \t -0.14135792754455878\n",
            "97     \t [0.57861969 0.36918768]. \t  -0.2958074923475944 \t -0.14135792754455878\n",
            "98     \t [0.65411075 0.53097372]. \t  -1.182865281747275 \t -0.14135792754455878\n",
            "99     \t [0.60764773 0.41675177]. \t  -0.3797173903873009 \t -0.14135792754455878\n",
            "100    \t [0.61355829 0.41228948]. \t  -0.27775702692359705 \t -0.14135792754455878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejw6v-Ihuvf_",
        "outputId": "96697ee0-ba03-4c90-d6ec-2ef0c3ba4496"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 9 \r\n",
        "\r\n",
        "np.random.seed(run_num_9)\r\n",
        "surrogate_winner_9 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_9 = dGPGO(surrogate_winner_9, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_9.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.24284125 0.11273132]. \t  -205.0993354093405 \t -3.486729021084656\n",
            "init   \t [-1.56011944  0.5721352 ]. \t  -353.19808767458017 \t -3.486729021084656\n",
            "init   \t [-1.67557012 -0.68720361]. \t  -1228.4786390381805 \t -3.486729021084656\n",
            "init   \t [-0.29744764  0.22276429]. \t  -3.486729021084656 \t -3.486729021084656\n",
            "init   \t [0.52480624 0.8085215 ]. \t  -28.645360944397154 \t -3.486729021084656\n",
            "1      \t [-0.42225505  1.10130175]. \t  -87.2161566781657 \t -3.486729021084656\n",
            "2      \t [0.10042724 0.2605786 ]. \t  -7.083903974371054 \t -3.486729021084656\n",
            "3      \t [1.34027586 1.76760892]. \t  \u001b[92m-0.19833152756144756\u001b[0m \t -0.19833152756144756\n",
            "4      \t [-0.16376849  0.45743937]. \t  -19.89765101096383 \t -0.19833152756144756\n",
            "5      \t [0.8831722  1.50538748]. \t  -52.63334324473526 \t -0.19833152756144756\n",
            "6      \t [2.048 2.048]. \t  -461.7603900415993 \t -0.19833152756144756\n",
            "7      \t [-1.40882103  1.92255786]. \t  -6.189536990740514 \t -0.19833152756144756\n",
            "8      \t [1.16175347 1.04462758]. \t  -9.331320157547005 \t -0.19833152756144756\n",
            "9      \t [1.18697317 2.03894195]. \t  -39.729577607419245 \t -0.19833152756144756\n",
            "10     \t [-0.14628688 -0.26394143]. \t  -9.455938362287094 \t -0.19833152756144756\n",
            "11     \t [1.22770016 1.44928814]. \t  -0.38777829539983516 \t -0.19833152756144756\n",
            "12     \t [-0.12124281  0.00022401]. \t  -1.2781403498924486 \t -0.19833152756144756\n",
            "13     \t [ 0.0878735  -2.02227909]. \t  -412.9223162098738 \t -0.19833152756144756\n",
            "14     \t [0.79863822 0.61817166]. \t  \u001b[92m-0.07916410691926373\u001b[0m \t -0.07916410691926373\n",
            "15     \t [1.27040302 1.54520919]. \t  -0.5452881802574249 \t -0.07916410691926373\n",
            "16     \t [0.89260044 0.87114516]. \t  -0.5652135901647416 \t -0.07916410691926373\n",
            "17     \t [1.27011413 1.72607676]. \t  -1.3473061162237787 \t -0.07916410691926373\n",
            "18     \t [ 0.20375397 -0.21293833]. \t  -7.1086921582618245 \t -0.07916410691926373\n",
            "19     \t [0.82179662 0.6839055 ]. \t  \u001b[92m-0.039076650256262196\u001b[0m \t -0.039076650256262196\n",
            "20     \t [-1.74840599  2.0282906 ]. \t  -113.36230050494683 \t -0.039076650256262196\n",
            "21     \t [-0.91639214  1.867156  ]. \t  -109.22382249002695 \t -0.039076650256262196\n",
            "22     \t [-1.22581903  1.30097773]. \t  -9.020727272166402 \t -0.039076650256262196\n",
            "23     \t [-0.78557891  0.72499431]. \t  -4.351671911862469 \t -0.039076650256262196\n",
            "24     \t [-1.32608912  1.70087858]. \t  -5.742855780528302 \t -0.039076650256262196\n",
            "25     \t [0.55445804 0.31745805]. \t  -0.20857641342136113 \t -0.039076650256262196\n",
            "26     \t [0.70643645 0.54222941]. \t  -0.27260449736656456 \t -0.039076650256262196\n",
            "27     \t [0.17335805 0.01348511]. \t  -0.7107864715639177 \t -0.039076650256262196\n",
            "28     \t [0.8350255 0.7023627]. \t  \u001b[92m-0.029812611538933036\u001b[0m \t -0.029812611538933036\n",
            "29     \t [0.77599541 0.62233456]. \t  -0.09084349101575989 \t -0.029812611538933036\n",
            "30     \t [0.83463103 0.73041561]. \t  -0.14163581469626216 \t -0.029812611538933036\n",
            "31     \t [0.71864708 0.54711259]. \t  -0.17315669428649705 \t -0.029812611538933036\n",
            "32     \t [0.82579583 0.70635207]. \t  -0.08994809223518399 \t -0.029812611538933036\n",
            "33     \t [1.28363445 1.65284835]. \t  -0.08308116494154591 \t -0.029812611538933036\n",
            "34     \t [1.27205378 1.61861308]. \t  -0.07403749037800182 \t -0.029812611538933036\n",
            "35     \t [0.6408322  0.45457397]. \t  -0.3217932711085142 \t -0.029812611538933036\n",
            "36     \t [1.2683565  1.59068907]. \t  -0.10455624541758708 \t -0.029812611538933036\n",
            "37     \t [0.8006381  0.65506034]. \t  -0.05945446082410312 \t -0.029812611538933036\n",
            "38     \t [1.31992912 1.74969558]. \t  -0.10795371048952698 \t -0.029812611538933036\n",
            "39     \t [1.26642229 1.5848537 ]. \t  -0.10697346879458314 \t -0.029812611538933036\n",
            "40     \t [1.3072666  1.71183839]. \t  -0.09524937631238296 \t -0.029812611538933036\n",
            "41     \t [1.26354807 1.59274078]. \t  -0.07091144896219523 \t -0.029812611538933036\n",
            "42     \t [1.32859811 1.73788447]. \t  -0.18244283163808275 \t -0.029812611538933036\n",
            "43     \t [1.24306111 1.54398337]. \t  -0.05922695254703714 \t -0.029812611538933036\n",
            "44     \t [1.31445674 1.75013367]. \t  -0.14877780937473087 \t -0.029812611538933036\n",
            "45     \t [0.80186834 0.66991416]. \t  -0.11173191969560997 \t -0.029812611538933036\n",
            "46     \t [0.83432269 0.70029004]. \t  \u001b[92m-0.029209353426092856\u001b[0m \t -0.029209353426092856\n",
            "47     \t [0.9893174 0.9791484]. \t  \u001b[92m-0.00013007648945196428\u001b[0m \t -0.00013007648945196428\n",
            "48     \t [0.89447804 0.82761449]. \t  -0.08688927067214554 \t -0.00013007648945196428\n",
            "49     \t [1.29116197 1.68269824]. \t  -0.10910816618480956 \t -0.00013007648945196428\n",
            "50     \t [0.70691577 0.52564867]. \t  -0.1530766179598852 \t -0.00013007648945196428\n",
            "51     \t [0.74011542 0.58033134]. \t  -0.17355860163308282 \t -0.00013007648945196428\n",
            "52     \t [1.27938119 1.65821708]. \t  -0.12385350990901911 \t -0.00013007648945196428\n",
            "53     \t [0.68448552 0.52756716]. \t  -0.4482009541983395 \t -0.00013007648945196428\n",
            "54     \t [1.29111231 1.66065193]. \t  -0.08873943568246659 \t -0.00013007648945196428\n",
            "55     \t [1.03131835 1.08424417]. \t  -0.04352661113057776 \t -0.00013007648945196428\n",
            "56     \t [0.95578475 0.94600941]. \t  -0.10748201007920469 \t -0.00013007648945196428\n",
            "57     \t [1.29010345 1.66064594]. \t  -0.08554457464369178 \t -0.00013007648945196428\n",
            "58     \t [1.30445696 1.7736664 ]. \t  -0.611935773060879 \t -0.00013007648945196428\n",
            "59     \t [1.00231572 1.01679636]. \t  -0.014790859610477337 \t -0.00013007648945196428\n",
            "60     \t [1.11052663 1.22428929]. \t  -0.020280356071885546 \t -0.00013007648945196428\n",
            "61     \t [1.01061457 1.03810091]. \t  -0.02819945896863307 \t -0.00013007648945196428\n",
            "62     \t [1.01787519 1.05790888]. \t  -0.048013584923256636 \t -0.00013007648945196428\n",
            "63     \t [1.26345341 1.60854907]. \t  -0.08437613786685842 \t -0.00013007648945196428\n",
            "64     \t [1.09874576 1.23153558]. \t  -0.06876738272378215 \t -0.00013007648945196428\n",
            "65     \t [1.01536463 1.07227567]. \t  -0.17089046215436782 \t -0.00013007648945196428\n",
            "66     \t [0.99054517 1.00945596]. \t  -0.08004387041276372 \t -0.00013007648945196428\n",
            "67     \t [0.96754755 0.95463507]. \t  -0.03522941178368676 \t -0.00013007648945196428\n",
            "68     \t [0.79151375 0.65007463]. \t  -0.09907099571899394 \t -0.00013007648945196428\n",
            "69     \t [0.98686568 1.00639444]. \t  -0.10573630645673177 \t -0.00013007648945196428\n",
            "70     \t [1.25117247 1.55795791]. \t  -0.06867464607512438 \t -0.00013007648945196428\n",
            "71     \t [1.28082171 1.63700312]. \t  -0.08008663068279206 \t -0.00013007648945196428\n",
            "72     \t [1.02434572 1.08639325]. \t  -0.13830128482189913 \t -0.00013007648945196428\n",
            "73     \t [1.04343776 1.09730088]. \t  -0.009177476344461151 \t -0.00013007648945196428\n",
            "74     \t [1.25113368 1.56320705]. \t  -0.06352114005497682 \t -0.00013007648945196428\n",
            "75     \t [0.94808783 0.88381763]. \t  -0.025353876616298517 \t -0.00013007648945196428\n",
            "76     \t [0.97346366 0.96755557]. \t  -0.04040103991929831 \t -0.00013007648945196428\n",
            "77     \t [0.86887913 0.79359782]. \t  -0.16655074093276773 \t -0.00013007648945196428\n",
            "78     \t [1.08952619 1.19933504]. \t  -0.023064629144867788 \t -0.00013007648945196428\n",
            "79     \t [0.96865643 0.9610416 ]. \t  -0.05272190635532916 \t -0.00013007648945196428\n",
            "80     \t [1.24678383 1.55791388]. \t  -0.06208835311609067 \t -0.00013007648945196428\n",
            "81     \t [0.82526781 0.72325748]. \t  -0.2085354371102078 \t -0.00013007648945196428\n",
            "82     \t [1.01088398 1.03736822]. \t  -0.024087077133204605 \t -0.00013007648945196428\n",
            "83     \t [1.28624518 1.70902751]. \t  -0.3800616821769872 \t -0.00013007648945196428\n",
            "84     \t [0.74803632 0.61062863]. \t  -0.32430317329107716 \t -0.00013007648945196428\n",
            "85     \t [1.05047046 1.12678074]. \t  -0.056801588662937476 \t -0.00013007648945196428\n",
            "86     \t [1.26764553 1.60154805]. \t  -0.07452549152070878 \t -0.00013007648945196428\n",
            "87     \t [1.06260063 1.13759807]. \t  -0.01110642749342224 \t -0.00013007648945196428\n",
            "88     \t [1.02384445 1.0592876 ]. \t  -0.012734973221194348 \t -0.00013007648945196428\n",
            "89     \t [1.10742934 1.2404705 ]. \t  -0.0313396736726982 \t -0.00013007648945196428\n",
            "90     \t [1.03988981 1.12907372]. \t  -0.22914788488625362 \t -0.00013007648945196428\n",
            "91     \t [1.23910634 1.53657967]. \t  -0.05731467952020228 \t -0.00013007648945196428\n",
            "92     \t [1.08856563 1.19790017]. \t  -0.02454952405740518 \t -0.00013007648945196428\n",
            "93     \t [1.06381787 1.13468398]. \t  -0.004958098726808429 \t -0.00013007648945196428\n",
            "94     \t [1.27461963 1.62685365]. \t  -0.07589925546036563 \t -0.00013007648945196428\n",
            "95     \t [1.31308771 1.75164243]. \t  -0.17333621057035675 \t -0.00013007648945196428\n",
            "96     \t [1.07397328 1.14831134]. \t  -0.008080463335309537 \t -0.00013007648945196428\n",
            "97     \t [1.01074626 1.02959685]. \t  -0.006497660736050515 \t -0.00013007648945196428\n",
            "98     \t [1.18304132 1.41963669]. \t  -0.07370409784016638 \t -0.00013007648945196428\n",
            "99     \t [1.25297747 1.56995528]. \t  -0.06399760193736197 \t -0.00013007648945196428\n",
            "100    \t [1.09393139 1.22644745]. \t  -0.09739813785826246 \t -0.00013007648945196428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4xG5dbuuvig",
        "outputId": "8ca0e2d0-2550-4702-add6-e0e63a5ffce3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 10 \r\n",
        "\r\n",
        "np.random.seed(run_num_10)\r\n",
        "surrogate_winner_10 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_10 = dGPGO(surrogate_winner_10, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_10.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.62910294 -1.57693156]. \t  -389.29291138113445 \t -8.580376531587937\n",
            "init   \t [ 1.84435861 -0.07294402]. \t  -1207.999341247548 \t -8.580376531587937\n",
            "init   \t [ 1.5256557  -1.17828534]. \t  -1229.4172568797705 \t -8.580376531587937\n",
            "init   \t [-1.88125338 -0.42109149]. \t  -1576.6245828972787 \t -8.580376531587937\n",
            "init   \t [-1.09309052  1.39977001]. \t  -8.580376531587937 \t -8.580376531587937\n",
            "1      \t [-0.86019812  2.03750805]. \t  -171.82841453589126 \t -8.580376531587937\n",
            "2      \t [-0.84764361  1.31952701]. \t  -39.53717004170931 \t -8.580376531587937\n",
            "3      \t [-2.02005588  1.7049428 ]. \t  -573.5076811336743 \t -8.580376531587937\n",
            "4      \t [-0.07028794 -2.03698889]. \t  -418.0930364826361 \t -8.580376531587937\n",
            "5      \t [-1.15353637  1.42250668]. \t  \u001b[92m-5.481554342848031\u001b[0m \t -5.481554342848031\n",
            "6      \t [0.29269679 1.06821323]. \t  -97.03912079359641 \t -5.481554342848031\n",
            "7      \t [-0.99815483  1.54778519]. \t  -34.404774157120485 \t -5.481554342848031\n",
            "8      \t [-1.24167567  1.42485703]. \t  -6.391704532491689 \t -5.481554342848031\n",
            "9      \t [-1.20116058  1.38542307]. \t  \u001b[92m-5.174166969356492\u001b[0m \t -5.174166969356492\n",
            "10     \t [-1.24945816  1.42907483]. \t  -6.8043329981292 \t -5.174166969356492\n",
            "11     \t [-1.12416167  1.48676969]. \t  -9.486310540169754 \t -5.174166969356492\n",
            "12     \t [-0.19007848  0.75534134]. \t  -53.14280650078184 \t -5.174166969356492\n",
            "13     \t [-1.28030865  1.39268104]. \t  -11.2764859323235 \t -5.174166969356492\n",
            "14     \t [-1.28093008  1.47784237]. \t  -7.857570054445271 \t -5.174166969356492\n",
            "15     \t [-1.11706762  1.35613089]. \t  -5.654665495589338 \t -5.174166969356492\n",
            "16     \t [-1.1708629   1.51232162]. \t  -6.7120894452678534 \t -5.174166969356492\n",
            "17     \t [-1.10744314  1.29764674]. \t  \u001b[92m-4.948494628972292\u001b[0m \t -4.948494628972292\n",
            "18     \t [1.12122293 1.8851272 ]. \t  -39.45138038287415 \t -4.948494628972292\n",
            "19     \t [-0.81639652  0.82661655]. \t  -5.862922147210541 \t -4.948494628972292\n",
            "20     \t [-1.19879632  1.45559073]. \t  \u001b[92m-4.868849314633235\u001b[0m \t -4.868849314633235\n",
            "21     \t [-1.10397118  1.26461739]. \t  \u001b[92m-4.6370547099193145\u001b[0m \t -4.6370547099193145\n",
            "22     \t [-0.94444828  0.98627625]. \t  -4.670009272547558 \t -4.6370547099193145\n",
            "23     \t [-1.27445159  1.53515777]. \t  -5.966460507388275 \t -4.6370547099193145\n",
            "24     \t [-1.15310687  1.34621741]. \t  -4.663299020167492 \t -4.6370547099193145\n",
            "25     \t [-1.02322616  1.08553322]. \t  \u001b[92m-4.241988381024795\u001b[0m \t -4.241988381024795\n",
            "26     \t [-1.11012364  1.23407558]. \t  -4.452911159515432 \t -4.241988381024795\n",
            "27     \t [-1.06256928  1.15906475]. \t  -4.34425967689282 \t -4.241988381024795\n",
            "28     \t [-0.9968035   1.02134445]. \t  \u001b[92m-4.06410418664041\u001b[0m \t -4.06410418664041\n",
            "29     \t [-1.1348441   1.32467881]. \t  -4.693039804701808 \t -4.06410418664041\n",
            "30     \t [-1.14190072  1.30147078]. \t  -4.588347056267705 \t -4.06410418664041\n",
            "31     \t [-1.06550505  1.16352378]. \t  -4.34596358366319 \t -4.06410418664041\n",
            "32     \t [-1.00228852  1.03976571]. \t  -4.132946708532046 \t -4.06410418664041\n",
            "33     \t [-1.10498867  1.18184558]. \t  -4.584283952659391 \t -4.06410418664041\n",
            "34     \t [-1.02348903  1.08679727]. \t  -4.248701312141734 \t -4.06410418664041\n",
            "35     \t [-0.92847403  0.94162632]. \t  -4.352028056249298 \t -4.06410418664041\n",
            "36     \t [-1.02922152  1.07216387]. \t  -4.134295788351243 \t -4.06410418664041\n",
            "37     \t [-1.09477119  1.24055099]. \t  -4.564693466961353 \t -4.06410418664041\n",
            "38     \t [-1.04865877  1.11334034]. \t  -4.215649003343949 \t -4.06410418664041\n",
            "39     \t [-1.20022928  1.44251098]. \t  -4.84139328672772 \t -4.06410418664041\n",
            "40     \t [-1.00477909  1.04183889]. \t  -4.123196177705579 \t -4.06410418664041\n",
            "41     \t [-1.0321279   1.07104597]. \t  -4.132859213056132 \t -4.06410418664041\n",
            "42     \t [-1.02144067  1.06655348]. \t  -4.140104109981373 \t -4.06410418664041\n",
            "43     \t [-1.07047845  1.14130294]. \t  -4.289016536958283 \t -4.06410418664041\n",
            "44     \t [-0.97715014  1.01038297]. \t  -4.217820459553135 \t -4.06410418664041\n",
            "45     \t [-1.01133505  1.04566641]. \t  -4.097762430449245 \t -4.06410418664041\n",
            "46     \t [-1.09926096  1.21901494]. \t  -4.418218131688616 \t -4.06410418664041\n",
            "47     \t [-1.05592138  1.12100485]. \t  -4.230454716913187 \t -4.06410418664041\n",
            "48     \t [-1.00619624  1.03553273]. \t  -4.07819291572307 \t -4.06410418664041\n",
            "49     \t [-1.00518168  1.04599044]. \t  -4.1474911881575585 \t -4.06410418664041\n",
            "50     \t [-1.03442106  1.07684112]. \t  -4.143512368059949 \t -4.06410418664041\n",
            "51     \t [-1.02790048  1.07108971]. \t  -4.133435276343188 \t -4.06410418664041\n",
            "52     \t [-1.00330778  1.0385764 ]. \t  -4.11532172155321 \t -4.06410418664041\n",
            "53     \t [-1.05016652  1.13267069]. \t  -4.292111730809283 \t -4.06410418664041\n",
            "54     \t [-1.04819638  1.10919917]. \t  -4.206098828038302 \t -4.06410418664041\n",
            "55     \t [-0.97024251  0.99322414]. \t  -4.150735217527494 \t -4.06410418664041\n",
            "56     \t [-1.08827635  1.18029515]. \t  -4.3625385634106655 \t -4.06410418664041\n",
            "57     \t [-0.97651252  0.99936328]. \t  -4.116242870901619 \t -4.06410418664041\n",
            "58     \t [-0.97547148  0.96747614]. \t  \u001b[92m-3.927868938036411\u001b[0m \t -3.927868938036411\n",
            "59     \t [-0.99358327  1.01142191]. \t  -4.033007010795279 \t -3.927868938036411\n",
            "60     \t [-1.02943121  1.07233674]. \t  -4.1344875060883055 \t -3.927868938036411\n",
            "61     \t [-1.04734918  1.10067631]. \t  -4.193034451357154 \t -3.927868938036411\n",
            "62     \t [-1.09501683  1.19808859]. \t  -4.389190248771924 \t -3.927868938036411\n",
            "63     \t [-1.06737499  1.11940279]. \t  -4.313586951460168 \t -3.927868938036411\n",
            "64     \t [-1.07467155  1.15322415]. \t  -4.304549266420671 \t -3.927868938036411\n",
            "65     \t [-1.02893733  1.0677326 ]. \t  -4.124723767064737 \t -3.927868938036411\n",
            "66     \t [-1.01742361  1.04790537]. \t  -4.0862659236614105 \t -3.927868938036411\n",
            "67     \t [-1.03660714  1.09957259]. \t  -4.2103598048157425 \t -3.927868938036411\n",
            "68     \t [-0.98751519  0.99790462]. \t  -4.00182910732964 \t -3.927868938036411\n",
            "69     \t [-0.92293491  0.89861036]. \t  \u001b[92m-3.9167168456044106\u001b[0m \t -3.9167168456044106\n",
            "70     \t [-0.98813864  0.99258478]. \t  -3.9788318035244354 \t -3.9167168456044106\n",
            "71     \t [-1.01323464  1.01254241]. \t  -4.073000387793313 \t -3.9167168456044106\n",
            "72     \t [-0.97061932  0.94993225]. \t  \u001b[92m-3.8894719933092574\u001b[0m \t -3.8894719933092574\n",
            "73     \t [-1.03983374  1.08060853]. \t  -4.160963391498716 \t -3.8894719933092574\n",
            "74     \t [-1.0146645   1.03627954]. \t  -4.063409729285456 \t -3.8894719933092574\n",
            "75     \t [-1.04436529  1.1144371 ]. \t  -4.235779861968078 \t -3.8894719933092574\n",
            "76     \t [-0.90501211  0.88003126]. \t  -4.000980187438509 \t -3.8894719933092574\n",
            "77     \t [-0.95943599  0.94605823]. \t  -3.9046226938518434 \t -3.8894719933092574\n",
            "78     \t [-0.92298348  0.86784173]. \t  \u001b[92m-3.723284104144583\u001b[0m \t -3.723284104144583\n",
            "79     \t [-0.96770445  0.94760867]. \t  -3.88430814326191 \t -3.723284104144583\n",
            "80     \t [-0.84330094  0.79535772]. \t  -4.106743409406419 \t -3.723284104144583\n",
            "81     \t [-0.33092163  0.22439835]. \t  \u001b[92m-3.0913057194185223\u001b[0m \t -3.0913057194185223\n",
            "82     \t [-0.2932727  0.1798048]. \t  \u001b[92m-2.552321720338453\u001b[0m \t -2.552321720338453\n",
            "83     \t [-0.23225617  0.0819878 ]. \t  \u001b[92m-1.5971067704340807\u001b[0m \t -1.5971067704340807\n",
            "84     \t [2.048 2.048]. \t  -461.76039004152665 \t -1.5971067704340807\n",
            "85     \t [0.69397055 2.02017491]. \t  -236.81643187175303 \t -1.5971067704340807\n",
            "86     \t [1.21937494 1.46912262]. \t  \u001b[92m-0.07964089562700595\u001b[0m \t -0.07964089562700595\n",
            "87     \t [1.05268988 1.52115441]. \t  -17.059546623448156 \t -0.07964089562700595\n",
            "88     \t [1.26311062 1.60868965]. \t  -0.08676016158028851 \t -0.07964089562700595\n",
            "89     \t [1.23880464 1.55268686]. \t  -0.08960764224270026 \t -0.07964089562700595\n",
            "90     \t [1.24235036 1.62115796]. \t  -0.6628286703828982 \t -0.07964089562700595\n",
            "91     \t [1.25055053 1.54088889]. \t  -0.11561914881681803 \t -0.07964089562700595\n",
            "92     \t [1.24595717 1.63651623]. \t  -0.7678929470199486 \t -0.07964089562700595\n",
            "93     \t [1.25045895 1.5821935 ]. \t  -0.09712474879472308 \t -0.07964089562700595\n",
            "94     \t [1.27210457 1.58664612]. \t  -0.1739217429225 \t -0.07964089562700595\n",
            "95     \t [1.22698892 1.63579036]. \t  -1.7490345271589534 \t -0.07964089562700595\n",
            "96     \t [1.18441455 1.4374353 ]. \t  -0.15370723930503633 \t -0.07964089562700595\n",
            "97     \t [-0.20280692  0.08701088]. \t  -1.6572441110816645 \t -0.07964089562700595\n",
            "98     \t [1.23854091 1.62046535]. \t  -0.8048112853621611 \t -0.07964089562700595\n",
            "99     \t [1.23858154 1.63525498]. \t  -1.0804733034471674 \t -0.07964089562700595\n",
            "100    \t [1.24202048 1.61937513]. \t  -0.6477877440659604 \t -0.07964089562700595\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSj_CQIAuvk3",
        "outputId": "8092c9b4-510a-4283-e2c5-d9921f55a7f5"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 11 \r\n",
        "\r\n",
        "np.random.seed(run_num_11)\r\n",
        "surrogate_winner_11 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_11 = dGPGO(surrogate_winner_11, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_11.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.97032207 1.31583119]. \t  -659.5505379977991 \t -45.00854175284071\n",
            "init   \t [ 0.59758024 -0.3125739 ]. \t  -45.00854175284071 \t -45.00854175284071\n",
            "init   \t [-1.21933423 -0.03314987]. \t  -235.94289844157058 \t -45.00854175284071\n",
            "init   \t [-1.48046507 -0.19447384]. \t  -575.5719351651762 \t -45.00854175284071\n",
            "init   \t [-1.58214297 -2.0360213 ]. \t  -2067.0989998073865 \t -45.00854175284071\n",
            "1      \t [-0.59782787  0.66030788]. \t  \u001b[92m-11.728483422473923\u001b[0m \t -11.728483422473923\n",
            "2      \t [-0.43876341  0.04538055]. \t  \u001b[92m-4.234845502147651\u001b[0m \t -4.234845502147651\n",
            "3      \t [-1.11754926  0.6578201 ]. \t  -39.42349061733262 \t -4.234845502147651\n",
            "4      \t [ 1.30792565 -1.05782521]. \t  -766.5511117868322 \t -4.234845502147651\n",
            "5      \t [0.40998735 0.06537876]. \t  \u001b[92m-1.4030670242634493\u001b[0m \t -1.4030670242634493\n",
            "6      \t [-0.79921704  0.35413041]. \t  -11.337891814894654 \t -1.4030670242634493\n",
            "7      \t [-0.36159767  1.94598938]. \t  -331.36230478725497 \t -1.4030670242634493\n",
            "8      \t [ 0.06291129 -0.31421417]. \t  -11.001477506391325 \t -1.4030670242634493\n",
            "9      \t [0.47410544 0.01831169]. \t  -4.539314958153981 \t -1.4030670242634493\n",
            "10     \t [ 0.28669101 -0.07844097]. \t  -3.0890963878351774 \t -1.4030670242634493\n",
            "11     \t [0.15972128 0.77057178]. \t  -56.21764147944858 \t -1.4030670242634493\n",
            "12     \t [0.29376625 0.11477954]. \t  \u001b[92m-0.5798824739975276\u001b[0m \t -0.5798824739975276\n",
            "13     \t [0.29718375 0.09301436]. \t  \u001b[92m-0.4961561000196185\u001b[0m \t -0.4961561000196185\n",
            "14     \t [0.30503915 0.05039106]. \t  -0.6649395956477923 \t -0.4961561000196185\n",
            "15     \t [0.36483916 0.0319114 ]. \t  -1.4274966306776735 \t -0.4961561000196185\n",
            "16     \t [0.31828075 0.09476678]. \t  \u001b[92m-0.46901287820011445\u001b[0m \t -0.46901287820011445\n",
            "17     \t [0.3263786  0.09077239]. \t  -0.47857394668090647 \t -0.46901287820011445\n",
            "18     \t [0.27916582 0.07238379]. \t  -0.5226819055260457 \t -0.46901287820011445\n",
            "19     \t [0.30408944 0.12278155]. \t  -0.5761681438713501 \t -0.46901287820011445\n",
            "20     \t [0.36685127 0.08070307]. \t  -0.6911481026459887 \t -0.46901287820011445\n",
            "21     \t [0.35764318 0.07941426]. \t  -0.6477928028699376 \t -0.46901287820011445\n",
            "22     \t [0.32533896 0.03981437]. \t  -0.8911777374268237 \t -0.46901287820011445\n",
            "23     \t [0.33153006 0.09025934]. \t  -0.48547546337117203 \t -0.46901287820011445\n",
            "24     \t [0.33371994 0.08528502]. \t  -0.5119665339931246 \t -0.46901287820011445\n",
            "25     \t [0.33863919 0.08817792]. \t  -0.5076156216549975 \t -0.46901287820011445\n",
            "26     \t [0.2939025  0.13586599]. \t  -0.7434730956729864 \t -0.46901287820011445\n",
            "27     \t [0.317358   0.11149953]. \t  -0.47762834192446396 \t -0.46901287820011445\n",
            "28     \t [0.32779563 0.04861604]. \t  -0.7980019042770724 \t -0.46901287820011445\n",
            "29     \t [0.33150003 0.07506675]. \t  -0.5681739114259473 \t -0.46901287820011445\n",
            "30     \t [0.33079823 0.07630759]. \t  -0.5575236298870233 \t -0.46901287820011445\n",
            "31     \t [0.30835354 0.10706273]. \t  -0.49272882883062585 \t -0.46901287820011445\n",
            "32     \t [0.34186295 0.07873023]. \t  -0.5786107068767647 \t -0.46901287820011445\n",
            "33     \t [0.34672232 0.0490411 ]. \t  -0.9333635936499164 \t -0.46901287820011445\n",
            "34     \t [0.32400172 0.10334582]. \t  \u001b[92m-0.45723978906306634\u001b[0m \t -0.45723978906306634\n",
            "35     \t [0.33843189 0.09065184]. \t  -0.49471834989108887 \t -0.45723978906306634\n",
            "36     \t [0.28011081 0.14251277]. \t  -0.9284897318347474 \t -0.45723978906306634\n",
            "37     \t [0.30709766 0.08818397]. \t  -0.4838652175733073 \t -0.45723978906306634\n",
            "38     \t [0.35303841 0.07931571]. \t  -0.6239532535061348 \t -0.45723978906306634\n",
            "39     \t [0.28740364 0.07480761]. \t  -0.5138670376696178 \t -0.45723978906306634\n",
            "40     \t [0.29222595 0.08938743]. \t  -0.502537256647282 \t -0.45723978906306634\n",
            "41     \t [0.28430262 0.14027623]. \t  -0.8656321671208502 \t -0.45723978906306634\n",
            "42     \t [0.28585057 0.08491518]. \t  -0.5110363680345879 \t -0.45723978906306634\n",
            "43     \t [0.3440776  0.06652454]. \t  -0.6992304913290742 \t -0.45723978906306634\n",
            "44     \t [0.30171653 0.11157052]. \t  -0.5297793272474428 \t -0.45723978906306634\n",
            "45     \t [0.2756891  0.10668765]. \t  -0.6187719342631293 \t -0.45723978906306634\n",
            "46     \t [0.3375207  0.07016796]. \t  -0.6303048582423767 \t -0.45723978906306634\n",
            "47     \t [0.30194244 0.10276909]. \t  -0.5007400052941586 \t -0.45723978906306634\n",
            "48     \t [0.30972746 0.09728419]. \t  -0.47665926288344834 \t -0.45723978906306634\n",
            "49     \t [0.27925902 0.07415081]. \t  -0.5209381187143672 \t -0.45723978906306634\n",
            "50     \t [0.26511492 0.09127138]. \t  -0.5840950445892822 \t -0.45723978906306634\n",
            "51     \t [0.29794724 0.10956626]. \t  -0.5361158876852811 \t -0.45723978906306634\n",
            "52     \t [0.28943826 0.11711989]. \t  -0.6160894043392318 \t -0.45723978906306634\n",
            "53     \t [0.32121205 0.10823601]. \t  -0.4633122495216807 \t -0.45723978906306634\n",
            "54     \t [0.34259593 0.05983022]. \t  -0.7632854065629451 \t -0.45723978906306634\n",
            "55     \t [0.31956966 0.06941793]. \t  -0.5699592014120356 \t -0.45723978906306634\n",
            "56     \t [0.3137895  0.06675137]. \t  -0.5714529682841761 \t -0.45723978906306634\n",
            "57     \t [0.31333941 0.10212532]. \t  -0.4730580661603614 \t -0.45723978906306634\n",
            "58     \t [0.34768526 0.06745645]. \t  -0.7109759172054546 \t -0.45723978906306634\n",
            "59     \t [0.32468453 0.07221453]. \t  -0.5663115857243096 \t -0.45723978906306634\n",
            "60     \t [0.33662995 0.06824496]. \t  -0.6432332657573261 \t -0.45723978906306634\n",
            "61     \t [0.34412549 0.09202941]. \t  -0.49983015220023896 \t -0.45723978906306634\n",
            "62     \t [0.26768106 0.07031548]. \t  -0.5364699696465215 \t -0.45723978906306634\n",
            "63     \t [0.31416916 0.09762228]. \t  -0.4704805803809295 \t -0.45723978906306634\n",
            "64     \t [0.28251687 0.13473336]. \t  -0.8163760996049008 \t -0.45723978906306634\n",
            "65     \t [0.34915028 0.05758896]. \t  -0.8372724270943601 \t -0.45723978906306634\n",
            "66     \t [0.30329546 0.08034406]. \t  -0.4989556769970878 \t -0.45723978906306634\n",
            "67     \t [0.3065883  0.10027316]. \t  -0.4847595787715418 \t -0.45723978906306634\n",
            "68     \t [0.31948206 0.087874  ]. \t  -0.4832538775570532 \t -0.45723978906306634\n",
            "69     \t [0.29883915 0.13418298]. \t  -0.6930313069620269 \t -0.45723978906306634\n",
            "70     \t [0.31688991 0.10192678]. \t  -0.4668666773755801 \t -0.45723978906306634\n",
            "71     \t [0.3145811  0.07642227]. \t  -0.520599732205647 \t -0.45723978906306634\n",
            "72     \t [0.33697068 0.06304571]. \t  -0.6946685511330273 \t -0.45723978906306634\n",
            "73     \t [0.3417587  0.06799308]. \t  -0.6714834532944024 \t -0.45723978906306634\n",
            "74     \t [0.3022265  0.09394535]. \t  -0.4875661905699364 \t -0.45723978906306634\n",
            "75     \t [0.38827248 0.04338019]. \t  -1.527156737992315 \t -0.45723978906306634\n",
            "76     \t [0.31090707 0.1046664 ]. \t  -0.48125418087906713 \t -0.45723978906306634\n",
            "77     \t [0.23707156 0.06373427]. \t  -0.5877319178315542 \t -0.45723978906306634\n",
            "78     \t [0.30656182 0.07159064]. \t  -0.5309855277215515 \t -0.45723978906306634\n",
            "79     \t [0.29826514 0.07823281]. \t  -0.5039435670909583 \t -0.45723978906306634\n",
            "80     \t [0.26127188 0.09217334]. \t  -0.6028896979073514 \t -0.45723978906306634\n",
            "81     \t [0.32300984 0.09442044]. \t  -0.4681462372126295 \t -0.45723978906306634\n",
            "82     \t [0.27019859 0.07246649]. \t  -0.5326393418322427 \t -0.45723978906306634\n",
            "83     \t [0.31607444 0.10961725]. \t  -0.47719074216465396 \t -0.45723978906306634\n",
            "84     \t [0.24477745 0.09752784]. \t  -0.7118261555548665 \t -0.45723978906306634\n",
            "85     \t [0.33066161 0.04542571]. \t  -0.8564805593192565 \t -0.45723978906306634\n",
            "86     \t [0.29723255 0.06289473]. \t  -0.558664828395567 \t -0.45723978906306634\n",
            "87     \t [0.29304008 0.07956399]. \t  -0.5037720435810468 \t -0.45723978906306634\n",
            "88     \t [0.28653085 0.06277654]. \t  -0.546377572442709 \t -0.45723978906306634\n",
            "89     \t [0.34091468 0.05884506]. \t  -0.7636142421265643 \t -0.45723978906306634\n",
            "90     \t [0.35185259 0.05800224]. \t  -0.8530329046104683 \t -0.45723978906306634\n",
            "91     \t [0.2846039  0.03628799]. \t  -0.7117024394010923 \t -0.45723978906306634\n",
            "92     \t [0.34988139 0.07364123]. \t  -0.6605616157222469 \t -0.45723978906306634\n",
            "93     \t [0.31441007 0.08657232]. \t  -0.4851167626600377 \t -0.45723978906306634\n",
            "94     \t [0.30187749 0.07316788]. \t  -0.5196388831543384 \t -0.45723978906306634\n",
            "95     \t [0.27718123 0.04166198]. \t  -0.646141968123645 \t -0.45723978906306634\n",
            "96     \t [0.33135621 0.09602754]. \t  -0.46604415436255 \t -0.45723978906306634\n",
            "97     \t [0.37853998 0.0805291 ]. \t  -0.780137139439298 \t -0.45723978906306634\n",
            "98     \t [0.30897465 0.05570416]. \t  -0.6356111440805678 \t -0.45723978906306634\n",
            "99     \t [0.3046018  0.08666305]. \t  -0.48732312662714933 \t -0.45723978906306634\n",
            "100    \t [0.31454032 0.1535412 ]. \t  -0.768031970063967 \t -0.45723978906306634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l98Nt7Tguvna",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53eb250-626c-4bf5-e275-114a42052b32"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 12\r\n",
        "\r\n",
        "np.random.seed(run_num_12)\r\n",
        "surrogate_winner_12 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_12 = dGPGO(surrogate_winner_12, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_12.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [-1.26353633  0.50015753]. \t  -125.32555445527122 \t -19.52113145175031\n",
            "init   \t [-0.25506718  1.16882876]. \t  -123.40590278793995 \t -19.52113145175031\n",
            "init   \t [ 1.14678091 -0.93146069]. \t  -504.72793807789463 \t -19.52113145175031\n",
            "init   \t [-0.91560241  1.23646844]. \t  -19.52113145175031 \t -19.52113145175031\n",
            "init   \t [1.87653879 1.53982007]. \t  -393.43336554061835 \t -19.52113145175031\n",
            "1      \t [-1.50623477  1.94347507]. \t  \u001b[92m-16.861146488467096\u001b[0m \t -16.861146488467096\n",
            "2      \t [-1.14990345  1.69337798]. \t  -18.393608334399698 \t -16.861146488467096\n",
            "3      \t [-1.88339956  1.77564137]. \t  -322.15382782460546 \t -16.861146488467096\n",
            "4      \t [-0.86966369  0.87027651]. \t  \u001b[92m-4.7943664446544645\u001b[0m \t -4.7943664446544645\n",
            "5      \t [-1.01552628  2.01715566]. \t  -101.25474126604377 \t -4.7943664446544645\n",
            "6      \t [-0.66808211  0.52087702]. \t  \u001b[92m-3.3381685877394665\u001b[0m \t -3.3381685877394665\n",
            "7      \t [-0.65475451  0.65572418]. \t  -7.892052597058424 \t -3.3381685877394665\n",
            "8      \t [-0.76569733 -0.68937339]. \t  -165.85000843284482 \t -3.3381685877394665\n",
            "9      \t [-0.68429422  0.36013874]. \t  -4.005836955451489 \t -3.3381685877394665\n",
            "10     \t [-0.80755996  0.76660189]. \t  -4.5771256756922085 \t -3.3381685877394665\n",
            "11     \t [-0.65422553  0.36512059]. \t  \u001b[92m-3.1319829404767123\u001b[0m \t -3.1319829404767123\n",
            "12     \t [-1.36270021  2.01155618]. \t  -7.972602046428044 \t -3.1319829404767123\n",
            "13     \t [-0.66651812  0.50743464]. \t  -3.1765579962600317 \t -3.1319829404767123\n",
            "14     \t [-0.7288202   0.58993981]. \t  -3.3341039588225785 \t -3.1319829404767123\n",
            "15     \t [-0.64735661  0.4320146 ]. \t  \u001b[92m-2.730538575470688\u001b[0m \t -2.730538575470688\n",
            "16     \t [-0.53035835  0.25652756]. \t  \u001b[92m-2.403264867985325\u001b[0m \t -2.403264867985325\n",
            "17     \t [-0.5849903   0.36844549]. \t  -2.5810051694876193 \t -2.403264867985325\n",
            "18     \t [-0.46281919  0.19031798]. \t  \u001b[92m-2.196882756436439\u001b[0m \t -2.196882756436439\n",
            "19     \t [-0.61108047  0.3735515 ]. \t  -2.5955820227816644 \t -2.196882756436439\n",
            "20     \t [-0.4708643   0.24300769]. \t  -2.2087873687528963 \t -2.196882756436439\n",
            "21     \t [-0.54447811  0.3013876 ]. \t  -2.387844296470349 \t -2.196882756436439\n",
            "22     \t [-0.37921453  0.12652536]. \t  \u001b[92m-1.9320866772901695\u001b[0m \t -1.9320866772901695\n",
            "23     \t [-0.40296166  0.16327751]. \t  -1.9683823150041542 \t -1.9320866772901695\n",
            "24     \t [-0.42519494  0.18755524]. \t  -2.0357564752541375 \t -1.9320866772901695\n",
            "25     \t [-0.43377785  0.17715628]. \t  -2.0678342217157857 \t -1.9320866772901695\n",
            "26     \t [-0.34126427  0.16206612]. \t  -2.006969816639793 \t -1.9320866772901695\n",
            "27     \t [-0.43541451  0.2059307 ]. \t  -2.0871303987469565 \t -1.9320866772901695\n",
            "28     \t [-0.42950815  0.21601803]. \t  -2.1429756328320657 \t -1.9320866772901695\n",
            "29     \t [-0.2991534   0.09485371]. \t  \u001b[92m-1.6906735475865815\u001b[0m \t -1.6906735475865815\n",
            "30     \t [-0.30083714  0.05624311]. \t  -1.8095511559086717 \t -1.6906735475865815\n",
            "31     \t [-0.44641065  0.20789523]. \t  -2.0995217379223408 \t -1.6906735475865815\n",
            "32     \t [-0.23628937  0.0812408 ]. \t  \u001b[92m-1.5929687354397932\u001b[0m \t -1.5929687354397932\n",
            "33     \t [-0.26211288  0.0847704 ]. \t  -1.6187445247123102 \t -1.5929687354397932\n",
            "34     \t [-0.29603966  0.15452194]. \t  -2.127045141199744 \t -1.5929687354397932\n",
            "35     \t [-0.235474   0.0591571]. \t  \u001b[92m-1.5277717504173038\u001b[0m \t -1.5277717504173038\n",
            "36     \t [-0.21791538  0.04466768]. \t  \u001b[92m-1.4841128001899129\u001b[0m \t -1.4841128001899129\n",
            "37     \t [-0.44503843  0.18394022]. \t  -2.1080706541148557 \t -1.4841128001899129\n",
            "38     \t [-0.44490049  0.19055966]. \t  -2.093179104754921 \t -1.4841128001899129\n",
            "39     \t [-0.54433665  0.28640904]. \t  -2.394763541169112 \t -1.4841128001899129\n",
            "40     \t [-0.21922401  0.05243174]. \t  -1.488419122834641 \t -1.4841128001899129\n",
            "41     \t [-0.30217857  0.09500382]. \t  -1.69703206268767 \t -1.4841128001899129\n",
            "42     \t [-0.2202692   0.04626467]. \t  -1.489564907327275 \t -1.4841128001899129\n",
            "43     \t [-0.46256621  0.22216113]. \t  -2.145813474495893 \t -1.4841128001899129\n",
            "44     \t [-0.44249747  0.20453846]. \t  -2.0884280082652875 \t -1.4841128001899129\n",
            "45     \t [-0.27793908  0.08541541]. \t  -1.639795479320503 \t -1.4841128001899129\n",
            "46     \t [-0.20589151  0.01332026]. \t  -1.5386869009639943 \t -1.4841128001899129\n",
            "47     \t [-0.27003603  0.10885148]. \t  -1.7421025706454039 \t -1.4841128001899129\n",
            "48     \t [-0.12230551  0.02656591]. \t  \u001b[92m-1.2730425420781628\u001b[0m \t -1.2730425420781628\n",
            "49     \t [-0.14317623  0.03603782]. \t  -1.3309960348660252 \t -1.2730425420781628\n",
            "50     \t [-0.05130295  0.05408245]. \t  -1.3699528732698683 \t -1.2730425420781628\n",
            "51     \t [-0.15876402  0.06465727]. \t  -1.4983742071561057 \t -1.2730425420781628\n",
            "52     \t [-0.1848343  0.0263193]. \t  -1.4099858129832974 \t -1.2730425420781628\n",
            "53     \t [-0.36161201  0.15360008]. \t  -1.9061393615587463 \t -1.2730425420781628\n",
            "54     \t [-0.17945016  0.06853757]. \t  -1.523127396877815 \t -1.2730425420781628\n",
            "55     \t [-0.08638931  0.0318314 ]. \t  \u001b[92m-1.2396230792969218\u001b[0m \t -1.2396230792969218\n",
            "56     \t [-0.15069758  0.0409963 ]. \t  -1.3575446640713789 \t -1.2396230792969218\n",
            "57     \t [-0.08240699  0.03698859]. \t  -1.2627948601957653 \t -1.2396230792969218\n",
            "58     \t [-0.38758954  0.13779077]. \t  -1.9408673705688428 \t -1.2396230792969218\n",
            "59     \t [-0.15189808  0.04112975]. \t  -1.3594737065314775 \t -1.2396230792969218\n",
            "60     \t [-0.11154649  0.03584158]. \t  -1.290286718169293 \t -1.2396230792969218\n",
            "61     \t [0.10488363 0.01591772]. \t  \u001b[92m-0.8036511477197786\u001b[0m \t -0.8036511477197786\n",
            "62     \t [0.44647123 0.16072358]. \t  \u001b[92m-0.4554902747603816\u001b[0m \t -0.4554902747603816\n",
            "63     \t [0.43700336 0.10872673]. \t  -0.9933925850411585 \t -0.4554902747603816\n",
            "64     \t [0.29152647 0.08445539]. \t  -0.5019630745359162 \t -0.4554902747603816\n",
            "65     \t [0.4095291  0.14889531]. \t  \u001b[92m-0.38407051112296825\u001b[0m \t -0.38407051112296825\n",
            "66     \t [0.32470127 0.12813324]. \t  -0.5075679139284003 \t -0.38407051112296825\n",
            "67     \t [0.37372946 0.13139855]. \t  -0.39906262327645176 \t -0.38407051112296825\n",
            "68     \t [0.4044599  0.14568365]. \t  -0.3867239065946425 \t -0.38407051112296825\n",
            "69     \t [0.43290316 0.14856899]. \t  -0.47242346727747453 \t -0.38407051112296825\n",
            "70     \t [0.41434062 0.13595278]. \t  -0.47062705724394904 \t -0.38407051112296825\n",
            "71     \t [0.40344307 0.16054116]. \t  \u001b[92m-0.35637530309087373\u001b[0m \t -0.35637530309087373\n",
            "72     \t [0.40958909 0.13024552]. \t  -0.4893427872892319 \t -0.35637530309087373\n",
            "73     \t [0.27427833 0.09374965]. \t  -0.5609748465680515 \t -0.35637530309087373\n",
            "74     \t [0.39680222 0.15612013]. \t  -0.3640249522032243 \t -0.35637530309087373\n",
            "75     \t [0.37428032 0.14462632]. \t  -0.39358678211331666 \t -0.35637530309087373\n",
            "76     \t [0.34456529 0.11264434]. \t  -0.43329240064140917 \t -0.35637530309087373\n",
            "77     \t [0.42366665 0.13970524]. \t  -0.4904701588900805 \t -0.35637530309087373\n",
            "78     \t [0.38461107 0.15778365]. \t  -0.388421497545561 \t -0.35637530309087373\n",
            "79     \t [0.39355104 0.14818329]. \t  -0.37226817363486286 \t -0.35637530309087373\n",
            "80     \t [0.40764528 0.15443056]. \t  -0.3646765455623968 \t -0.35637530309087373\n",
            "81     \t [0.37469023 0.13769268]. \t  -0.3917413527477133 \t -0.35637530309087373\n",
            "82     \t [0.16712125 0.06087344]. \t  -0.8022172791282137 \t -0.35637530309087373\n",
            "83     \t [0.44163896 0.16320044]. \t  -0.4131744379562468 \t -0.35637530309087373\n",
            "84     \t [0.28810401 0.07990979]. \t  -0.507753264822105 \t -0.35637530309087373\n",
            "85     \t [0.38338456 0.16428995]. \t  -0.4101651451826364 \t -0.35637530309087373\n",
            "86     \t [0.4660673  0.15553637]. \t  -0.6655553984381732 \t -0.35637530309087373\n",
            "87     \t [0.32530625 0.11923282]. \t  -0.4731908935535787 \t -0.35637530309087373\n",
            "88     \t [0.46861039 0.17895593]. \t  -0.44753399897793655 \t -0.35637530309087373\n",
            "89     \t [0.25863344 0.07758931]. \t  -0.5610692175251883 \t -0.35637530309087373\n",
            "90     \t [0.40784447 0.16038385]. \t  \u001b[92m-0.3541922994852176\u001b[0m \t -0.3541922994852176\n",
            "91     \t [0.46697591 0.17953296]. \t  -0.4325980486209191 \t -0.3541922994852176\n",
            "92     \t [0.43075269 0.18210332]. \t  \u001b[92m-0.3252289981329783\u001b[0m \t -0.3252289981329783\n",
            "93     \t [0.12185044 0.03251253]. \t  -0.8023518859088725 \t -0.3252289981329783\n",
            "94     \t [0.38260506 0.13125898]. \t  -0.4040610936255426 \t -0.3252289981329783\n",
            "95     \t [0.38889648 0.13109827]. \t  -0.41401831749072815 \t -0.3252289981329783\n",
            "96     \t [0.51104627 0.1990036 ]. \t  -0.6255205915730981 \t -0.3252289981329783\n",
            "97     \t [0.46109708 0.18115174]. \t  -0.3893818328186217 \t -0.3252289981329783\n",
            "98     \t [0.31113927 0.0928612 ]. \t  -0.47608654636525294 \t -0.3252289981329783\n",
            "99     \t [0.41533944 0.15741075]. \t  -0.36461718058103076 \t -0.3252289981329783\n",
            "100    \t [0.3155666  0.12412993]. \t  -0.5287077987904951 \t -0.3252289981329783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bpn-kmNuvqC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725212e7-7b1e-4171-c318-58237c255da4"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 13 \r\n",
        "\r\n",
        "np.random.seed(run_num_13)\r\n",
        "surrogate_winner_13 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_13 = dGPGO(surrogate_winner_13, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_13.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.65799909 -0.35389173]. \t  -62.0309701572776 \t -62.0309701572776\n",
            "init   \t [ 0.68854808 -1.10615174]. \t  -249.81607110978882 \t -62.0309701572776\n",
            "init   \t [1.26025046 0.56040843]. \t  -105.7097019264073 \t -62.0309701572776\n",
            "init   \t [-1.34269398 -0.98145948]. \t  -780.713409392333 \t -62.0309701572776\n",
            "init   \t [ 1.70115068 -0.15230767]. \t  -928.4380405275942 \t -62.0309701572776\n",
            "1      \t [0.41252595 0.76998136]. \t  \u001b[92m-36.32157338742175\u001b[0m \t -36.32157338742175\n",
            "2      \t [1.54860385 1.60259675]. \t  -63.59526476967183 \t -36.32157338742175\n",
            "3      \t [0.45556931 0.16768069]. \t  \u001b[92m-0.45530831505501257\u001b[0m \t -0.45530831505501257\n",
            "4      \t [0.91417469 0.99028044]. \t  -2.3964024031176527 \t -0.45530831505501257\n",
            "5      \t [-1.27753898  2.04311694]. \t  -22.08019532964846 \t -0.45530831505501257\n",
            "6      \t [0.7240154  0.49387999]. \t  \u001b[92m-0.16808753734866744\u001b[0m \t -0.16808753734866744\n",
            "7      \t [0.9232793  0.83785357]. \t  \u001b[92m-0.027176090942447852\u001b[0m \t -0.027176090942447852\n",
            "8      \t [-0.02205795  2.01721403]. \t  -407.7635751607349 \t -0.027176090942447852\n",
            "9      \t [-1.89458414  1.79555632]. \t  -330.18372768117916 \t -0.027176090942447852\n",
            "10     \t [0.55528864 0.1818009 ]. \t  -1.7991208645006422 \t -0.027176090942447852\n",
            "11     \t [ 0.29516525 -0.45108545]. \t  -29.46357480792951 \t -0.027176090942447852\n",
            "12     \t [0.715594   0.58307452]. \t  -0.5849832899704364 \t -0.027176090942447852\n",
            "13     \t [-0.54127169  0.62328154]. \t  -13.285756442297105 \t -0.027176090942447852\n",
            "14     \t [0.90891339 0.84169476]. \t  -0.03254304278692681 \t -0.027176090942447852\n",
            "15     \t [0.96651657 1.01631444]. \t  -0.6761504531087236 \t -0.027176090942447852\n",
            "16     \t [0.58370241 0.34906214]. \t  -0.18028202022974416 \t -0.027176090942447852\n",
            "17     \t [0.9613911  0.90765984]. \t  -0.02908981523803406 \t -0.027176090942447852\n",
            "18     \t [0.88775224 0.77909571]. \t  \u001b[92m-0.02071456270902733\u001b[0m \t -0.02071456270902733\n",
            "19     \t [0.55791282 0.30014988]. \t  -0.20779948617983374 \t -0.02071456270902733\n",
            "20     \t [1.00486412 1.00478782]. \t  \u001b[92m-0.002487861808627833\u001b[0m \t -0.002487861808627833\n",
            "21     \t [1.06509099 1.14804164]. \t  -0.022794954441637434 \t -0.002487861808627833\n",
            "22     \t [1.03992921 1.10862244]. \t  -0.07541347649503878 \t -0.002487861808627833\n",
            "23     \t [0.99033067 0.98997906]. \t  -0.008602128385834724 \t -0.002487861808627833\n",
            "24     \t [1.02667399 1.0636183 ]. \t  -0.009848584950261956 \t -0.002487861808627833\n",
            "25     \t [0.8996095  0.81467662]. \t  -0.012972019494106939 \t -0.002487861808627833\n",
            "26     \t [1.00209058 0.99086883]. \t  -0.01773780841269722 \t -0.002487861808627833\n",
            "27     \t [0.78191432 0.60590908]. \t  -0.05056542333416948 \t -0.002487861808627833\n",
            "28     \t [1.02604658 1.06125241]. \t  -0.007870887551794516 \t -0.002487861808627833\n",
            "29     \t [1.05637923 1.13475622]. \t  -0.038594646965806965 \t -0.002487861808627833\n",
            "30     \t [1.06934168 1.18016436]. \t  -0.13929722573436695 \t -0.002487861808627833\n",
            "31     \t [0.95945875 0.92152027]. \t  \u001b[92m-0.0017355964225492944\u001b[0m \t -0.0017355964225492944\n",
            "32     \t [0.93424698 0.86594736]. \t  -0.009043239479377638 \t -0.0017355964225492944\n",
            "33     \t [0.95137967 0.89142888]. \t  -0.021117601861897418 \t -0.0017355964225492944\n",
            "34     \t [1.02078487 1.04567085]. \t  -0.0017782437426422412 \t -0.0017355964225492944\n",
            "35     \t [0.95040202 0.86158563]. \t  -0.1761686243273721 \t -0.0017355964225492944\n",
            "36     \t [0.96225711 0.91288908]. \t  -0.018453883030438968 \t -0.0017355964225492944\n",
            "37     \t [1.0129962  1.03627104]. \t  -0.010389588587179132 \t -0.0017355964225492944\n",
            "38     \t [0.86956707 0.76602532]. \t  -0.02677111160833409 \t -0.0017355964225492944\n",
            "39     \t [1.0124572  1.03254623]. \t  -0.005745213001546387 \t -0.0017355964225492944\n",
            "40     \t [1.0275188  1.04328329]. \t  -0.016411293445117516 \t -0.0017355964225492944\n",
            "41     \t [1.01783759 1.04455199]. \t  -0.007643197477645209 \t -0.0017355964225492944\n",
            "42     \t [1.00391727 1.01330047]. \t  -0.0029862368783662367 \t -0.0017355964225492944\n",
            "43     \t [1.03178983 1.05001454]. \t  -0.022255724355914358 \t -0.0017355964225492944\n",
            "44     \t [1.02361667 1.02391579]. \t  -0.05756069953558509 \t -0.0017355964225492944\n",
            "45     \t [1.05569522 1.10316328]. \t  -0.015936848586212627 \t -0.0017355964225492944\n",
            "46     \t [1.03294593 1.0882649 ]. \t  -0.04640167516863241 \t -0.0017355964225492944\n",
            "47     \t [1.02962678 1.11592012]. \t  -0.3121167904891954 \t -0.0017355964225492944\n",
            "48     \t [1.04929121 1.10519304]. \t  -0.0041776933492625274 \t -0.0017355964225492944\n",
            "49     \t [0.94529115 0.87995263]. \t  -0.021550912268796102 \t -0.0017355964225492944\n",
            "50     \t [1.03642601 1.09083896]. \t  -0.029082731333489664 \t -0.0017355964225492944\n",
            "51     \t [0.83743151 0.68253509]. \t  -0.061608931186862545 \t -0.0017355964225492944\n",
            "52     \t [1.02731891 1.0209986 ]. \t  -0.11898294627291264 \t -0.0017355964225492944\n",
            "53     \t [0.80565597 0.67108591]. \t  -0.08618883798788461 \t -0.0017355964225492944\n",
            "54     \t [1.02280402 1.04059394]. \t  -0.0035826821429571862 \t -0.0017355964225492944\n",
            "55     \t [1.03025639 1.06011772]. \t  \u001b[92m-0.0010871932315206971\u001b[0m \t -0.0010871932315206971\n",
            "56     \t [0.91548737 0.83023545]. \t  -0.013354455109837553 \t -0.0010871932315206971\n",
            "57     \t [1.08392872 1.20637119]. \t  -0.10607834119462149 \t -0.0010871932315206971\n",
            "58     \t [0.98889748 0.97856586]. \t  \u001b[92m-0.00016520705189970985\u001b[0m \t -0.00016520705189970985\n",
            "59     \t [1.05013788 1.11990885]. \t  -0.03182077969314625 \t -0.00016520705189970985\n",
            "60     \t [0.8221016  0.66652715]. \t  -0.04034132185288897 \t -0.00016520705189970985\n",
            "61     \t [1.02717576 1.05045077]. \t  -0.002890812569298093 \t -0.00016520705189970985\n",
            "62     \t [0.86514858 0.74837069]. \t  -0.01818614590869873 \t -0.00016520705189970985\n",
            "63     \t [0.7468701  0.55525692]. \t  -0.0647290942946857 \t -0.00016520705189970985\n",
            "64     \t [1.01979787 1.01850713]. \t  -0.046533395335351405 \t -0.00016520705189970985\n",
            "65     \t [1.01452593 1.02177294]. \t  -0.005820909035604972 \t -0.00016520705189970985\n",
            "66     \t [0.97654994 0.94598787]. \t  -0.006420387205935527 \t -0.00016520705189970985\n",
            "67     \t [1.00174746 1.00454692]. \t  \u001b[92m-0.00011308280418017014\u001b[0m \t -0.00011308280418017014\n",
            "68     \t [0.83278569 0.69348729]. \t  -0.02796082631567785 \t -0.00011308280418017014\n",
            "69     \t [0.90254411 0.81380526]. \t  -0.009558585810556512 \t -0.00011308280418017014\n",
            "70     \t [1.01836327 1.03083918]. \t  -0.004211748363841408 \t -0.00011308280418017014\n",
            "71     \t [0.95094488 0.88084384]. \t  -0.05740753289404289 \t -0.00011308280418017014\n",
            "72     \t [0.9892504  1.03792607]. \t  -0.35187987979331087 \t -0.00011308280418017014\n",
            "73     \t [1.07380962 1.18657243]. \t  -0.11770852503465379 \t -0.00011308280418017014\n",
            "74     \t [0.99947356 0.98032551]. \t  -0.034677768923178684 \t -0.00011308280418017014\n",
            "75     \t [0.98779293 0.96203564]. \t  -0.018915891360111098 \t -0.00011308280418017014\n",
            "76     \t [0.9300638  0.85940705]. \t  -0.008040108185481669 \t -0.00011308280418017014\n",
            "77     \t [1.07017766 1.17692087]. \t  -0.10503793751328144 \t -0.00011308280418017014\n",
            "78     \t [0.99734222 0.9836063 ]. \t  -0.012295219966970283 \t -0.00011308280418017014\n",
            "79     \t [0.98343774 0.98498796]. \t  -0.03209432365798245 \t -0.00011308280418017014\n",
            "80     \t [1.0030339  1.01037743]. \t  -0.001858575933725149 \t -0.00011308280418017014\n",
            "81     \t [1.00205876 0.98537448]. \t  -0.03515026588510608 \t -0.00011308280418017014\n",
            "82     \t [0.86317059 0.74530915]. \t  -0.018728323387900515 \t -0.00011308280418017014\n",
            "83     \t [1.03316034 1.08081274]. \t  -0.0190353681929665 \t -0.00011308280418017014\n",
            "84     \t [1.00404706 1.00377486]. \t  -0.0018961481914600638 \t -0.00011308280418017014\n",
            "85     \t [0.97552344 0.9380648 ]. \t  -0.019043964394079054 \t -0.00011308280418017014\n",
            "86     \t [0.68829972 0.49371831]. \t  -0.13700443391318373 \t -0.00011308280418017014\n",
            "87     \t [1.01684792 1.01528648]. \t  -0.03522743693131413 \t -0.00011308280418017014\n",
            "88     \t [1.05367715 1.11849767]. \t  -0.009707521367533514 \t -0.00011308280418017014\n",
            "89     \t [0.95632312 0.90339167]. \t  -0.014367244874169278 \t -0.00011308280418017014\n",
            "90     \t [1.01022732 0.98451542]. \t  -0.13002024335624968 \t -0.00011308280418017014\n",
            "91     \t [1.0261638  1.06568251]. \t  -0.0167383363178485 \t -0.00011308280418017014\n",
            "92     \t [0.80306896 0.66212722]. \t  -0.06839151667692246 \t -0.00011308280418017014\n",
            "93     \t [1.0058912  1.01431955]. \t  -0.0006609292951706178 \t -0.00011308280418017014\n",
            "94     \t [1.05420823 1.13347822]. \t  -0.05188226153547816 \t -0.00011308280418017014\n",
            "95     \t [1.02582736 1.03831908]. \t  -0.020274570662792504 \t -0.00011308280418017014\n",
            "96     \t [1.0543526  1.12737882]. \t  -0.027664201050402813 \t -0.00011308280418017014\n",
            "97     \t [0.99576418 1.00656296]. \t  -0.02256793318728872 \t -0.00011308280418017014\n",
            "98     \t [1.0083649  1.00929217]. \t  -0.005706370975094011 \t -0.00011308280418017014\n",
            "99     \t [1.03759956 1.02149911]. \t  -0.305166167192988 \t -0.00011308280418017014\n",
            "100    \t [1.07347658 1.19192403]. \t  -0.16199366918159797 \t -0.00011308280418017014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NdFRXtPuvsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862b9ee4-3e66-4f02-be76-ae20230c54f3"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 14 \r\n",
        "\r\n",
        "np.random.seed(run_num_14)\r\n",
        "surrogate_winner_14 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_14 = dGPGO(surrogate_winner_14, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_14.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.6043691  0.02928512]. \t  -11.444570460525577 \t -4.306489127802793\n",
            "init   \t [0.11608629 1.6231842 ]. \t  -259.89734211112767 \t -4.306489127802793\n",
            "init   \t [0.81916392 0.87776093]. \t  -4.306489127802793 \t -4.306489127802793\n",
            "init   \t [ 0.89021801 -1.13533148]. \t  -371.66089065122344 \t -4.306489127802793\n",
            "init   \t [-1.33056707 -0.17677726]. \t  -384.58487399689074 \t -4.306489127802793\n",
            "1      \t [1.25104361 0.50990532]. \t  -111.40873577912899 \t -4.306489127802793\n",
            "2      \t [0.43239451 0.4019702 ]. \t  -4.9448988288537805 \t -4.306489127802793\n",
            "3      \t [0.57115872 0.34014499]. \t  \u001b[92m-0.20328900694839472\u001b[0m \t -0.20328900694839472\n",
            "4      \t [1.48080501 1.85403221]. \t  -11.706414919185276 \t -0.20328900694839472\n",
            "5      \t [1.10938183 1.45882137]. \t  -5.214620634629662 \t -0.20328900694839472\n",
            "6      \t [0.29720091 0.01926622]. \t  -0.9708847651486041 \t -0.20328900694839472\n",
            "7      \t [0.32439607 0.00678355]. \t  -1.4256664504724315 \t -0.20328900694839472\n",
            "8      \t [0.53294582 0.36715779]. \t  -0.9091418491797468 \t -0.20328900694839472\n",
            "9      \t [0.42769578 0.2484376 ]. \t  -0.7567394714528257 \t -0.20328900694839472\n",
            "10     \t [0.56201051 0.27722877]. \t  -0.3410396534190825 \t -0.20328900694839472\n",
            "11     \t [0.54645422 0.24777936]. \t  -0.4641016852037067 \t -0.20328900694839472\n",
            "12     \t [0.51715357 0.2913541 ]. \t  -0.29029172316835955 \t -0.20328900694839472\n",
            "13     \t [1.35988486 1.98454069]. \t  -1.9588773842778444 \t -0.20328900694839472\n",
            "14     \t [0.5269281  0.36869632]. \t  -1.0526815022283942 \t -0.20328900694839472\n",
            "15     \t [0.56444473 0.23946179]. \t  -0.8159600451855866 \t -0.20328900694839472\n",
            "16     \t [0.55828404 0.28577227]. \t  -0.2622395622419632 \t -0.20328900694839472\n",
            "17     \t [0.26221476 0.04930829]. \t  -0.5821506697237814 \t -0.20328900694839472\n",
            "18     \t [0.60293015 0.51183714]. \t  -2.357320749944526 \t -0.20328900694839472\n",
            "19     \t [1.38641395 1.98945653]. \t  -0.6024181045095048 \t -0.20328900694839472\n",
            "20     \t [1.51740048 2.04767331]. \t  -6.761582145990568 \t -0.20328900694839472\n",
            "21     \t [0.48123945 0.18818944]. \t  -0.45748558386665106 \t -0.20328900694839472\n",
            "22     \t [0.48585339 0.22452938]. \t  -0.2776272982525059 \t -0.20328900694839472\n",
            "23     \t [0.58250368 0.30259678]. \t  -0.3090931506733943 \t -0.20328900694839472\n",
            "24     \t [0.50407034 0.26263052]. \t  -0.25324555430314166 \t -0.20328900694839472\n",
            "25     \t [0.50452257 0.22088028]. \t  -0.35881591436259663 \t -0.20328900694839472\n",
            "26     \t [0.54270336 0.33809429]. \t  -0.39893164939677206 \t -0.20328900694839472\n",
            "27     \t [0.51617725 0.19620521]. \t  -0.7273622600792106 \t -0.20328900694839472\n",
            "28     \t [0.5073661  0.26451719]. \t  -0.24772465584380893 \t -0.20328900694839472\n",
            "29     \t [0.49508798 0.24331185]. \t  -0.2552602411379301 \t -0.20328900694839472\n",
            "30     \t [0.51110113 0.27373382]. \t  -0.2546707252083132 \t -0.20328900694839472\n",
            "31     \t [0.51732538 0.26756677]. \t  -0.23297513897647043 \t -0.20328900694839472\n",
            "32     \t [0.38235176 0.0774018 ]. \t  -0.8547104331141755 \t -0.20328900694839472\n",
            "33     \t [0.55414276 0.33529337]. \t  -0.2784208627597361 \t -0.20328900694839472\n",
            "34     \t [1.408128   2.01488019]. \t  -0.26932550830279745 \t -0.20328900694839472\n",
            "35     \t [1.40865169 2.03050709]. \t  -0.380509456224294 \t -0.20328900694839472\n",
            "36     \t [0.52720647 0.26094163]. \t  -0.25245084863556455 \t -0.20328900694839472\n",
            "37     \t [0.48860899 0.26581447]. \t  -0.3348302753671364 \t -0.20328900694839472\n",
            "38     \t [1.41189123 2.04540756]. \t  -0.43974979607482456 \t -0.20328900694839472\n",
            "39     \t [0.51402075 0.25076684]. \t  -0.25426741134758746 \t -0.20328900694839472\n",
            "40     \t [0.5496951 0.2654041]. \t  -0.3379086415560596 \t -0.20328900694839472\n",
            "41     \t [0.55804911 0.26201133]. \t  -0.4394304574936758 \t -0.20328900694839472\n",
            "42     \t [0.55096787 0.27814829]. \t  -0.26623375506005365 \t -0.20328900694839472\n",
            "43     \t [0.4925673  0.20475024]. \t  -0.4009190985131538 \t -0.20328900694839472\n",
            "44     \t [0.41047082 0.13203015]. \t  -0.48044967382134307 \t -0.20328900694839472\n",
            "45     \t [0.22097095 0.06866789]. \t  -0.6462477456338495 \t -0.20328900694839472\n",
            "46     \t [0.53583388 0.29934952]. \t  -0.2304113422213497 \t -0.20328900694839472\n",
            "47     \t [0.56546417 0.35798276]. \t  -0.3349979332609618 \t -0.20328900694839472\n",
            "48     \t [0.47079852 0.21945611]. \t  -0.28053607208673287 \t -0.20328900694839472\n",
            "49     \t [0.52908551 0.2528952 ]. \t  -0.2948564824140569 \t -0.20328900694839472\n",
            "50     \t [0.53072115 0.28493775]. \t  -0.22129377102102035 \t -0.20328900694839472\n",
            "51     \t [0.51604774 0.25921872]. \t  -0.23923171155737138 \t -0.20328900694839472\n",
            "52     \t [0.53173331 0.29655704]. \t  -0.23836390420653555 \t -0.20328900694839472\n",
            "53     \t [0.55696483 0.33695228]. \t  -0.2677960992394211 \t -0.20328900694839472\n",
            "54     \t [0.25147084 0.05162865]. \t  -0.5737726425739018 \t -0.20328900694839472\n",
            "55     \t [0.54245296 0.30067585]. \t  -0.2134717624120805 \t -0.20328900694839472\n",
            "56     \t [0.54152798 0.28134014]. \t  -0.22438714777471072 \t -0.20328900694839472\n",
            "57     \t [0.55947149 0.3039631 ]. \t  \u001b[92m-0.202247005986964\u001b[0m \t -0.202247005986964\n",
            "58     \t [0.51416841 0.22154982]. \t  -0.41938186856152876 \t -0.202247005986964\n",
            "59     \t [0.55234833 0.26198169]. \t  -0.38621321595321034 \t -0.202247005986964\n",
            "60     \t [0.5387002  0.29867763]. \t  -0.21998806902004003 \t -0.202247005986964\n",
            "61     \t [0.56060273 0.34507061]. \t  -0.28790435561774896 \t -0.202247005986964\n",
            "62     \t [0.55052151 0.30582469]. \t  -0.20278758079938503 \t -0.202247005986964\n",
            "63     \t [0.55189286 0.27605392]. \t  -0.2822064167461346 \t -0.202247005986964\n",
            "64     \t [0.45794371 0.13751362]. \t  -0.8150919390570295 \t -0.202247005986964\n",
            "65     \t [0.55024337 0.31434133]. \t  -0.21567576878702086 \t -0.202247005986964\n",
            "66     \t [0.37786466 0.17046489]. \t  -0.46368829871864603 \t -0.202247005986964\n",
            "67     \t [0.31007399 0.06829333]. \t  -0.5535743174390906 \t -0.202247005986964\n",
            "68     \t [0.49567379 0.24863184]. \t  -0.25520889959652104 \t -0.202247005986964\n",
            "69     \t [0.54506286 0.33764092]. \t  -0.3713769732694193 \t -0.202247005986964\n",
            "70     \t [0.54008084 0.22884887]. \t  -0.6063926714283074 \t -0.202247005986964\n",
            "71     \t [0.48920646 0.21271805]. \t  -0.33169217988557553 \t -0.202247005986964\n",
            "72     \t [0.54982117 0.36663929]. \t  -0.6165727637254652 \t -0.202247005986964\n",
            "73     \t [0.51096598 0.27951486]. \t  -0.2731157223773117 \t -0.202247005986964\n",
            "74     \t [0.52265807 0.25166377]. \t  -0.27411338786557793 \t -0.202247005986964\n",
            "75     \t [0.54144174 0.28801654]. \t  -0.2129203337558734 \t -0.202247005986964\n",
            "76     \t [0.38570827 0.13651862]. \t  -0.3923660816508386 \t -0.202247005986964\n",
            "77     \t [1.40156387 2.04110397]. \t  -0.7498906831136226 \t -0.202247005986964\n",
            "78     \t [-1.55657316  2.03368051]. \t  -21.686804873361325 \t -0.202247005986964\n",
            "79     \t [0.5130557  0.22063498]. \t  -0.41851558185254323 \t -0.202247005986964\n",
            "80     \t [0.531137   0.28369132]. \t  -0.22008367626735165 \t -0.202247005986964\n",
            "81     \t [0.54356182 0.29367599]. \t  -0.20865388330121443 \t -0.202247005986964\n",
            "82     \t [0.46955481 0.25712401]. \t  -0.4156378206102125 \t -0.202247005986964\n",
            "83     \t [0.56998147 0.35963139]. \t  -0.30568968345176406 \t -0.202247005986964\n",
            "84     \t [0.54152361 0.27513852]. \t  -0.2429952810276805 \t -0.202247005986964\n",
            "85     \t [0.57033757 0.27575871]. \t  -0.42989465418864514 \t -0.202247005986964\n",
            "86     \t [0.54821208 0.32735524]. \t  -0.27603691917257633 \t -0.202247005986964\n",
            "87     \t [0.45829836 0.19935346]. \t  -0.3048552886985304 \t -0.202247005986964\n",
            "88     \t [0.5427468  0.29030202]. \t  -0.21090554321779398 \t -0.202247005986964\n",
            "89     \t [0.51194875 0.25125018]. \t  -0.24994748501140193 \t -0.202247005986964\n",
            "90     \t [0.42804961 0.22066806]. \t  -0.4673144797598864 \t -0.202247005986964\n",
            "91     \t [0.56388409 0.3290824 ]. \t  -0.20255615818389724 \t -0.202247005986964\n",
            "92     \t [0.61395357 0.41416269]. \t  -0.2875922621762603 \t -0.202247005986964\n",
            "93     \t [0.58101324 0.34503946]. \t  \u001b[92m-0.1811196498764788\u001b[0m \t -0.1811196498764788\n",
            "94     \t [0.52273264 0.31626365]. \t  -0.4128065602707759 \t -0.1811196498764788\n",
            "95     \t [0.35344831 0.11789753]. \t  -0.4229686222766103 \t -0.1811196498764788\n",
            "96     \t [0.52575949 0.23585908]. \t  -0.38944755165823386 \t -0.1811196498764788\n",
            "97     \t [0.4870752  0.21233033]. \t  -0.3251522390838983 \t -0.1811196498764788\n",
            "98     \t [1.38397797 1.93323633]. \t  \u001b[92m-0.1792702946456919\u001b[0m \t -0.1792702946456919\n",
            "99     \t [0.55085214 0.32032329]. \t  -0.23024484729836223 \t -0.1792702946456919\n",
            "100    \t [0.59381972 0.37377189]. \t  -0.20971477662054855 \t -0.1792702946456919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d86panpOuvum",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f504d35d-dbf7-4a98-aefa-2cb3f97b5628"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 15 \r\n",
        "\r\n",
        "np.random.seed(run_num_15)\r\n",
        "surrogate_winner_15 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_15 = dGPGO(surrogate_winner_15, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_15.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 0.31900409 -1.00146655]. \t  -122.17543449532224 \t -6.867717811955245\n",
            "init   \t [-1.01364994  0.85976824]. \t  -6.867717811955245 \t -6.867717811955245\n",
            "init   \t [-0.21482552 -1.11844164]. \t  -137.1031718367486 \t -6.867717811955245\n",
            "init   \t [-0.40259762  1.56572768]. \t  -198.98860193376754 \t -6.867717811955245\n",
            "init   \t [-0.25717364  1.55002954]. \t  -221.77381285664336 \t -6.867717811955245\n",
            "1      \t [-1.84861506  0.69959197]. \t  -746.7505016058286 \t -6.867717811955245\n",
            "2      \t [-0.42796005  0.33778267]. \t  \u001b[92m-4.430202280076179\u001b[0m \t -4.430202280076179\n",
            "3      \t [-0.7074198   0.79484568]. \t  -11.582589742750914 \t -4.430202280076179\n",
            "4      \t [1.6089636  1.82993015]. \t  -57.95369667628582 \t -4.430202280076179\n",
            "5      \t [-0.59090979 -0.0996047 ]. \t  -22.671259942052473 \t -4.430202280076179\n",
            "6      \t [1.14388383 0.19643536]. \t  -123.68285737421314 \t -4.430202280076179\n",
            "7      \t [-0.58838014  0.27993893]. \t  \u001b[92m-2.961887727459571\u001b[0m \t -2.961887727459571\n",
            "8      \t [-0.03373258 -0.2255786 ]. \t  -6.20863940029643 \t -2.961887727459571\n",
            "9      \t [-1.09342966  1.28792933]. \t  -5.235132174329438 \t -2.961887727459571\n",
            "10     \t [-0.29564907  0.03010664]. \t  \u001b[92m-2.007055318204653\u001b[0m \t -2.007055318204653\n",
            "11     \t [0.27693721 0.02148514]. \t  \u001b[92m-0.8276240389920496\u001b[0m \t -0.8276240389920496\n",
            "12     \t [0.02676757 0.03742437]. \t  -1.0819280922738057 \t -0.8276240389920496\n",
            "13     \t [ 0.27026875 -0.07069128]. \t  -2.5985253123907355 \t -0.8276240389920496\n",
            "14     \t [-0.92385741  0.86961192]. \t  -3.727146427876876 \t -0.8276240389920496\n",
            "15     \t [-1.73194208  2.04378046]. \t  -98.82707620220617 \t -0.8276240389920496\n",
            "16     \t [2.048 2.048]. \t  -461.7603900415949 \t -0.8276240389920496\n",
            "17     \t [1.10417112 1.37838435]. \t  -2.545013025164969 \t -0.8276240389920496\n",
            "18     \t [1.11923601 1.97068182]. \t  -51.56555246059928 \t -0.8276240389920496\n",
            "19     \t [1.36959282 1.50182773]. \t  -14.120965319462519 \t -0.8276240389920496\n",
            "20     \t [1.28505361 1.61782158]. \t  \u001b[92m-0.1937567905819192\u001b[0m \t -0.1937567905819192\n",
            "21     \t [0.58240887 0.81096776]. \t  -22.430855336620425 \t -0.1937567905819192\n",
            "22     \t [1.34878274 2.02163051]. \t  -4.218858403930429 \t -0.1937567905819192\n",
            "23     \t [-1.34547679  1.73983637]. \t  -5.997883352509011 \t -0.1937567905819192\n",
            "24     \t [0.13070749 0.03541112]. \t  -0.789256163552619 \t -0.1937567905819192\n",
            "25     \t [1.38491576 1.89293816]. \t  -0.21092793569604018 \t -0.1937567905819192\n",
            "26     \t [1.34958946 1.78616327]. \t  -0.24631711933946476 \t -0.1937567905819192\n",
            "27     \t [1.35669853 1.8243726 ]. \t  \u001b[92m-0.15366709599093162\u001b[0m \t -0.15366709599093162\n",
            "28     \t [1.36549672 1.88864302]. \t  -0.19148449297044617 \t -0.15366709599093162\n",
            "29     \t [1.39143292 1.8719191 ]. \t  -0.5649533961441024 \t -0.15366709599093162\n",
            "30     \t [0.14961149 0.04710515]. \t  -0.784276119202382 \t -0.15366709599093162\n",
            "31     \t [ 0.17680959 -0.01313057]. \t  -0.874709211124052 \t -0.15366709599093162\n",
            "32     \t [ 0.02942311 -0.00915161]. \t  -0.9520541864574973 \t -0.15366709599093162\n",
            "33     \t [0.18568479 0.0184047 ]. \t  -0.6889470558625896 \t -0.15366709599093162\n",
            "34     \t [1.36557347 1.8563839 ]. \t  \u001b[92m-0.1407117295712158\u001b[0m \t -0.1407117295712158\n",
            "35     \t [1.38371753 1.86856904]. \t  -0.3598077247499273 \t -0.1407117295712158\n",
            "36     \t [0.16830179 0.01059125]. \t  -0.7231722454074846 \t -0.1407117295712158\n",
            "37     \t [1.29029173 1.65692483]. \t  \u001b[92m-0.09055447023634869\u001b[0m \t -0.09055447023634869\n",
            "38     \t [1.32808233 1.77173696]. \t  -0.11393328722602215 \t -0.09055447023634869\n",
            "39     \t [1.36795013 1.88548863]. \t  -0.1555543714367947 \t -0.09055447023634869\n",
            "40     \t [1.30909053 1.68979673]. \t  -0.15275976735658942 \t -0.09055447023634869\n",
            "41     \t [1.26059458 1.58895471]. \t  \u001b[92m-0.0679116081482804\u001b[0m \t -0.0679116081482804\n",
            "42     \t [1.38561803 1.94397027]. \t  -0.20645958099698067 \t -0.0679116081482804\n",
            "43     \t [1.34172747 1.76065282]. \t  -0.27343369866313194 \t -0.0679116081482804\n",
            "44     \t [1.37645047 1.83225359]. \t  -0.5306208033700909 \t -0.0679116081482804\n",
            "45     \t [1.36131477 1.82569996]. \t  -0.20605213834654704 \t -0.0679116081482804\n",
            "46     \t [1.299949   1.65005727]. \t  -0.24845402223963287 \t -0.0679116081482804\n",
            "47     \t [0.18550013 0.0292486 ]. \t  -0.666074342918091 \t -0.0679116081482804\n",
            "48     \t [1.36100033 1.81264613]. \t  -0.2877377611294973 \t -0.0679116081482804\n",
            "49     \t [1.30264434 1.71325571]. \t  -0.11840256219724846 \t -0.0679116081482804\n",
            "50     \t [0.12486941 0.0437576 ]. \t  -0.8451815733870475 \t -0.0679116081482804\n",
            "51     \t [1.38661619 1.926327  ]. \t  -0.1507843653806524 \t -0.0679116081482804\n",
            "52     \t [0.20192553 0.01118629]. \t  -0.7244656424172831 \t -0.0679116081482804\n",
            "53     \t [1.39122241 1.90477179]. \t  -0.2474759540765564 \t -0.0679116081482804\n",
            "54     \t [1.38775297 1.88283847]. \t  -0.33542290237997685 \t -0.0679116081482804\n",
            "55     \t [0.20152071 0.00280397]. \t  -0.7805032607749872 \t -0.0679116081482804\n",
            "56     \t [1.34737696 1.85989028]. \t  -0.3183897517624397 \t -0.0679116081482804\n",
            "57     \t [1.36628168 1.85086291]. \t  -0.1593248512266358 \t -0.0679116081482804\n",
            "58     \t [1.34101097 1.79527815]. \t  -0.11720794489639212 \t -0.0679116081482804\n",
            "59     \t [1.31754511 1.68648202]. \t  -0.34529682017749064 \t -0.0679116081482804\n",
            "60     \t [0.18774564 0.03106929]. \t  -0.6615036624786592 \t -0.0679116081482804\n",
            "61     \t [1.39206579 1.87708144]. \t  -0.5229630492605348 \t -0.0679116081482804\n",
            "62     \t [1.29056874 1.62521176]. \t  -0.24729013718523896 \t -0.0679116081482804\n",
            "63     \t [1.30873326 1.68495816]. \t  -0.1727370492713402 \t -0.0679116081482804\n",
            "64     \t [1.27170017 1.60756261]. \t  -0.08315006205488185 \t -0.0679116081482804\n",
            "65     \t [1.35882654 1.79544475]. \t  -0.3884978814045257 \t -0.0679116081482804\n",
            "66     \t [1.35272694 1.74699197]. \t  -0.811296037637192 \t -0.0679116081482804\n",
            "67     \t [1.35362079 1.75753671]. \t  -0.6838417975737991 \t -0.0679116081482804\n",
            "68     \t [1.3588386  1.87342018]. \t  -0.20154553685710697 \t -0.0679116081482804\n",
            "69     \t [1.33696844 1.77155068]. \t  -0.13893669305577772 \t -0.0679116081482804\n",
            "70     \t [1.23475731 1.5334601 ]. \t  \u001b[92m-0.06291578589212876\u001b[0m \t -0.06291578589212876\n",
            "71     \t [1.18144087 1.45660959]. \t  -0.4026706676427339 \t -0.06291578589212876\n",
            "72     \t [1.31230308 1.70413745]. \t  -0.12994019349863162 \t -0.06291578589212876\n",
            "73     \t [1.35522645 1.79422794]. \t  -0.3060532720329865 \t -0.06291578589212876\n",
            "74     \t [1.30700585 1.75904395]. \t  -0.35210999715138014 \t -0.06291578589212876\n",
            "75     \t [1.30110817 1.65213155]. \t  -0.2567298304194101 \t -0.06291578589212876\n",
            "76     \t [0.23078564 0.00029468]. \t  -0.8722445833700333 \t -0.06291578589212876\n",
            "77     \t [1.27568754 1.63835669]. \t  -0.08805522509348834 \t -0.06291578589212876\n",
            "78     \t [1.36052387 1.89560203]. \t  -0.32868668869805245 \t -0.06291578589212876\n",
            "79     \t [1.39068005 1.94248031]. \t  -0.1598377236173964 \t -0.06291578589212876\n",
            "80     \t [1.36003604 1.82983592]. \t  -0.1690762518700435 \t -0.06291578589212876\n",
            "81     \t [1.40200213 1.91401913]. \t  -0.4277673286159597 \t -0.06291578589212876\n",
            "82     \t [1.3288598  1.78184963]. \t  -0.13368887029234805 \t -0.06291578589212876\n",
            "83     \t [1.24328263 1.53774161]. \t  -0.06560259500737861 \t -0.06291578589212876\n",
            "84     \t [1.3734322  1.91296417]. \t  -0.21046401394939365 \t -0.06291578589212876\n",
            "85     \t [0.12446875 0.01812667]. \t  -0.767248873499014 \t -0.06291578589212876\n",
            "86     \t [1.35842192 1.89764976]. \t  -0.4024099873756027 \t -0.06291578589212876\n",
            "87     \t [1.36231166 1.77606798]. \t  -0.7684742246636204 \t -0.06291578589212876\n",
            "88     \t [1.33062373 1.74835948]. \t  -0.15859624031444425 \t -0.06291578589212876\n",
            "89     \t [0.19282039 0.00136027]. \t  -0.7798420680568618 \t -0.06291578589212876\n",
            "90     \t [1.30445989 1.6943309 ]. \t  -0.09800251790418373 \t -0.06291578589212876\n",
            "91     \t [1.22273292 1.4951645 ]. \t  \u001b[92m-0.04961074129662009\u001b[0m \t -0.04961074129662009\n",
            "92     \t [1.27898659 1.59753352]. \t  -0.2243171484515704 \t -0.04961074129662009\n",
            "93     \t [1.40585985 2.00416639]. \t  -0.24158689763294866 \t -0.04961074129662009\n",
            "94     \t [1.30961142 1.74449062]. \t  -0.18234549223363022 \t -0.04961074129662009\n",
            "95     \t [1.2896182  1.61227211]. \t  -0.3423796691483003 \t -0.04961074129662009\n",
            "96     \t [1.36231493 1.84096703]. \t  -0.15357733087337394 \t -0.04961074129662009\n",
            "97     \t [0.16035119 0.00564886]. \t  -0.7452651085455306 \t -0.04961074129662009\n",
            "98     \t [1.3669428  1.88888345]. \t  -0.1760626649422016 \t -0.04961074129662009\n",
            "99     \t [1.35941165 1.77872102]. \t  -0.6091349368826666 \t -0.04961074129662009\n",
            "100    \t [1.37323715 1.9299951 ]. \t  -0.3348011080662286 \t -0.04961074129662009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "any0xrgYuvxA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91c96591-4962-4114-a5e8-f2dd1b8e3f1e"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 16 \r\n",
        "\r\n",
        "np.random.seed(run_num_16)\r\n",
        "surrogate_winner_16 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_16 = dGPGO(surrogate_winner_16, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_16.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [ 1.49073237 -0.75216486]. \t  -884.9748202573794 \t -21.690996320546372\n",
            "init   \t [0.70512959 0.03240619]. \t  -21.690996320546372 \t -21.690996320546372\n",
            "init   \t [ 1.15368111 -0.88603985]. \t  -491.54136063574043 \t -21.690996320546372\n",
            "init   \t [-1.09072882  0.26132352]. \t  -90.5574614167091 \t -21.690996320546372\n",
            "init   \t [1.5360998 0.8967902]. \t  -214.26941031258198 \t -21.690996320546372\n",
            "1      \t [ 0.01996577 -1.04171686]. \t  -109.56093787644141 \t -21.690996320546372\n",
            "2      \t [0.23120961 0.63095635]. \t  -33.94148650510817 \t -21.690996320546372\n",
            "3      \t [ 0.26507858 -0.00151668]. \t  \u001b[92m-1.0553941344317241\u001b[0m \t -1.0553941344317241\n",
            "4      \t [-1.89666678  0.44479792]. \t  -1002.2459176523861 \t -1.0553941344317241\n",
            "5      \t [-0.63504896  0.26338423]. \t  -4.630668962094292 \t -1.0553941344317241\n",
            "6      \t [-0.95331428 -0.21604105]. \t  -130.34400379341838 \t -1.0553941344317241\n",
            "7      \t [-0.70879595  0.54496077]. \t  -3.10119619289842 \t -1.0553941344317241\n",
            "8      \t [ 0.40133467 -1.98033381]. \t  -458.9192215256066 \t -1.0553941344317241\n",
            "9      \t [ 0.35020156 -0.4749381 ]. \t  -36.13233236239288 \t -1.0553941344317241\n",
            "10     \t [0.50526239 0.34749532]. \t  -1.0949458446627538 \t -1.0553941344317241\n",
            "11     \t [2.048 2.048]. \t  -461.76039004160214 \t -1.0553941344317241\n",
            "12     \t [0.40809836 0.16132094]. \t  \u001b[92m-0.353075871180534\u001b[0m \t -0.353075871180534\n",
            "13     \t [0.22745743 2.01623824]. \t  -386.5233800592825 \t -0.353075871180534\n",
            "14     \t [0.48010423 0.15187927]. \t  -0.8884146417894396 \t -0.353075871180534\n",
            "15     \t [0.31335755 0.05056185]. \t  -0.6983500179268851 \t -0.353075871180534\n",
            "16     \t [-0.6553      0.39431484]. \t  -2.86324193725272 \t -0.353075871180534\n",
            "17     \t [0.40356791 0.12336103]. \t  -0.5118038424321075 \t -0.353075871180534\n",
            "18     \t [0.5372851  0.34924668]. \t  -0.5809944516120413 \t -0.353075871180534\n",
            "19     \t [0.64261158 0.33573674]. \t  -0.7239098316198683 \t -0.353075871180534\n",
            "20     \t [0.53183555 0.22772029]. \t  -0.5230959965183897 \t -0.353075871180534\n",
            "21     \t [0.2999978  0.04599288]. \t  -0.683654082984601 \t -0.353075871180534\n",
            "22     \t [0.45786096 0.22951745]. \t  \u001b[92m-0.3334392944159116\u001b[0m \t -0.3334392944159116\n",
            "23     \t [0.26019131 0.06090733]. \t  -0.5519302809219571 \t -0.3334392944159116\n",
            "24     \t [0.58164768 0.3107194 ]. \t  \u001b[92m-0.25116501012007575\u001b[0m \t -0.25116501012007575\n",
            "25     \t [0.53393136 0.23676139]. \t  -0.4507148124891821 \t -0.25116501012007575\n",
            "26     \t [0.62643242 0.34308649]. \t  -0.382908273770838 \t -0.25116501012007575\n",
            "27     \t [0.52248363 0.2877346 ]. \t  \u001b[92m-0.24976471362434333\u001b[0m \t -0.24976471362434333\n",
            "28     \t [0.58682498 0.40049915]. \t  -0.48583408959664576 \t -0.24976471362434333\n",
            "29     \t [0.58045446 0.21072265]. \t  -1.7687818452976747 \t -0.24976471362434333\n",
            "30     \t [0.56211802 0.33353227]. \t  \u001b[92m-0.22256053250377006\u001b[0m \t -0.22256053250377006\n",
            "31     \t [0.49402913 0.22594906]. \t  -0.28882444719442163 \t -0.22256053250377006\n",
            "32     \t [0.57716626 0.36447815]. \t  -0.2771160892930471 \t -0.22256053250377006\n",
            "33     \t [0.54689277 0.20970335]. \t  -1.0043339942626746 \t -0.22256053250377006\n",
            "34     \t [0.52484787 0.24067014]. \t  -0.34683974009443386 \t -0.22256053250377006\n",
            "35     \t [0.49652896 0.24105326]. \t  -0.256494624176258 \t -0.22256053250377006\n",
            "36     \t [0.40623364 0.05153721]. \t  -1.6405238031555172 \t -0.22256053250377006\n",
            "37     \t [0.33180997 0.05390183]. \t  -0.7622772343580044 \t -0.22256053250377006\n",
            "38     \t [0.56491234 0.30039731]. \t  -0.22437748697550058 \t -0.22256053250377006\n",
            "39     \t [0.5871201  0.29645204]. \t  -0.4033529462816193 \t -0.22256053250377006\n",
            "40     \t [0.58037137 0.3187701 ]. \t  \u001b[92m-0.20870751829868253\u001b[0m \t -0.20870751829868253\n",
            "41     \t [0.39006408 0.19104537]. \t  -0.5233069329683829 \t -0.20870751829868253\n",
            "42     \t [0.65970292 0.3962484 ]. \t  -0.2675867340403693 \t -0.20870751829868253\n",
            "43     \t [0.46256986 0.2347887 ]. \t  -0.3321693247116677 \t -0.20870751829868253\n",
            "44     \t [0.56627194 0.30854507]. \t  \u001b[92m-0.20280664867157058\u001b[0m \t -0.20280664867157058\n",
            "45     \t [0.45018465 0.12468724]. \t  -0.9103690128716642 \t -0.20280664867157058\n",
            "46     \t [0.68293357 0.41922513]. \t  -0.32306152380459824 \t -0.20280664867157058\n",
            "47     \t [0.09396112 0.06750432]. \t  -1.1651893453174487 \t -0.20280664867157058\n",
            "48     \t [0.64459461 0.41095608]. \t  \u001b[92m-0.12837972701197717\u001b[0m \t -0.12837972701197717\n",
            "49     \t [0.24684382 0.10168448]. \t  -0.7333217575003703 \t -0.12837972701197717\n",
            "50     \t [0.59638783 0.35084303]. \t  -0.16524090717287504 \t -0.12837972701197717\n",
            "51     \t [0.12641613 0.01076564]. \t  -0.765868821837579 \t -0.12837972701197717\n",
            "52     \t [0.37545361 0.10975691]. \t  -0.4874552362549551 \t -0.12837972701197717\n",
            "53     \t [0.27245845 0.0820183 ]. \t  -0.5353768480486781 \t -0.12837972701197717\n",
            "54     \t [0.60820826 0.35028474]. \t  -0.19204445602900327 \t -0.12837972701197717\n",
            "55     \t [0.4759627  0.24942532]. \t  -0.32698664125050125 \t -0.12837972701197717\n",
            "56     \t [0.60971008 0.38904193]. \t  -0.18223981309784268 \t -0.12837972701197717\n",
            "57     \t [0.63532852 0.40218155]. \t  -0.13319867606817193 \t -0.12837972701197717\n",
            "58     \t [0.61477802 0.32806157]. \t  -0.3973015923301577 \t -0.12837972701197717\n",
            "59     \t [0.1919607  0.05405545]. \t  -0.6825340263368421 \t -0.12837972701197717\n",
            "60     \t [0.33859671 0.10327156]. \t  -0.45039603248506305 \t -0.12837972701197717\n",
            "61     \t [0.62939101 0.3950653 ]. \t  -0.13746503155297732 \t -0.12837972701197717\n",
            "62     \t [0.44298748 0.16333505]. \t  -0.41852277272479027 \t -0.12837972701197717\n",
            "63     \t [0.49177725 0.23731995]. \t  -0.2603378443639042 \t -0.12837972701197717\n",
            "64     \t [ 0.07249159 -0.03505065]. \t  -1.0227266546243792 \t -0.12837972701197717\n",
            "65     \t [0.57933747 0.25699186]. \t  -0.7953826800082135 \t -0.12837972701197717\n",
            "66     \t [0.34871487 0.09570369]. \t  -0.49124490605477217 \t -0.12837972701197717\n",
            "67     \t [0.50396438 0.22718051]. \t  -0.31787310381124717 \t -0.12837972701197717\n",
            "68     \t [0.52953463 0.22609564]. \t  -0.5163092587788832 \t -0.12837972701197717\n",
            "69     \t [0.63298718 0.36271898]. \t  -0.27874740419758426 \t -0.12837972701197717\n",
            "70     \t [0.34340952 0.12015953]. \t  -0.4316080934605817 \t -0.12837972701197717\n",
            "71     \t [0.52645865 0.32232294]. \t  -0.42822217137792085 \t -0.12837972701197717\n",
            "72     \t [0.64906985 0.37768739]. \t  -0.31328523955761167 \t -0.12837972701197717\n",
            "73     \t [0.69443436 0.44337457]. \t  -0.24441535637224712 \t -0.12837972701197717\n",
            "74     \t [0.58742855 0.29850314]. \t  -0.3870838965126534 \t -0.12837972701197717\n",
            "75     \t [0.60237597 0.41801706]. \t  -0.46237020389538197 \t -0.12837972701197717\n",
            "76     \t [0.62500318 0.38187463]. \t  -0.1482864573488614 \t -0.12837972701197717\n",
            "77     \t [0.52793206 0.23685278]. \t  -0.39806974054191324 \t -0.12837972701197717\n",
            "78     \t [0.46635569 0.17442617]. \t  -0.47020519029039104 \t -0.12837972701197717\n",
            "79     \t [0.69205398 0.45482312]. \t  -0.1529868851306731 \t -0.12837972701197717\n",
            "80     \t [0.5549721 0.2371681]. \t  -0.6996810513989299 \t -0.12837972701197717\n",
            "81     \t [0.62180732 0.41236128]. \t  -0.20916577595121738 \t -0.12837972701197717\n",
            "82     \t [0.3074974  0.10025154]. \t  -0.48280530745301076 \t -0.12837972701197717\n",
            "83     \t [0.69682117 0.47721397]. \t  \u001b[92m-0.09888258718426764\u001b[0m \t -0.09888258718426764\n",
            "84     \t [0.59503173 0.32784345]. \t  -0.23274449461078706 \t -0.09888258718426764\n",
            "85     \t [0.41875808 0.19069111]. \t  -0.361351567690872 \t -0.09888258718426764\n",
            "86     \t [0.26792259 0.01302017]. \t  -0.8812386507773886 \t -0.09888258718426764\n",
            "87     \t [0.64004301 0.40737106]. \t  -0.13009069696179948 \t -0.09888258718426764\n",
            "88     \t [0.62346001 0.37429386]. \t  -0.1625429221726278 \t -0.09888258718426764\n",
            "89     \t [0.49223547 0.24270275]. \t  -0.25784137917219363 \t -0.09888258718426764\n",
            "90     \t [0.51339493 0.24699193]. \t  -0.26428219496884153 \t -0.09888258718426764\n",
            "91     \t [0.70456239 0.51267795]. \t  -0.11375399098627118 \t -0.09888258718426764\n",
            "92     \t [0.3736714  0.11638022]. \t  -0.4463442126365754 \t -0.09888258718426764\n",
            "93     \t [0.44717181 0.21947514]. \t  -0.34369281370762217 \t -0.09888258718426764\n",
            "94     \t [0.65934227 0.44898912]. \t  -0.1363735944917624 \t -0.09888258718426764\n",
            "95     \t [0.70641726 0.51913255]. \t  -0.12662081456126495 \t -0.09888258718426764\n",
            "96     \t [0.4689075  0.18913653]. \t  -0.3765399203658575 \t -0.09888258718426764\n",
            "97     \t [0.32079207 0.11469541]. \t  -0.4752187804701284 \t -0.09888258718426764\n",
            "98     \t [0.55974808 0.28714801]. \t  -0.2623081414943556 \t -0.09888258718426764\n",
            "99     \t [0.61907584 0.36017728]. \t  -0.19836084855010655 \t -0.09888258718426764\n",
            "100    \t [0.6539618 0.4292783]. \t  -0.12000237501618326 \t -0.09888258718426764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reLyKt6Quvzx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd49d633-aeae-42d3-82e1-ad7d508c93a1"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 17 \r\n",
        "\r\n",
        "np.random.seed(run_num_17)\r\n",
        "surrogate_winner_17 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_17 = dGPGO(surrogate_winner_17, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_17.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.62187122 0.94420838]. \t  -31.22188590191926 \t -31.22188590191926\n",
            "init   \t [-1.25596728  0.08059797]. \t  -229.14713198618372 \t -31.22188590191926\n",
            "init   \t [0.72246586 1.2470062 ]. \t  -52.64667111223406 \t -31.22188590191926\n",
            "init   \t [0.13726869 1.671308  ]. \t  -273.80846401311214 \t -31.22188590191926\n",
            "init   \t [-0.2315013  -0.99953783]. \t  -112.42501801714917 \t -31.22188590191926\n",
            "1      \t [1.59154318 0.71488025]. \t  -330.90939372608307 \t -31.22188590191926\n",
            "2      \t [ 1.90278748 -2.02756811]. \t  -3190.995555019365 \t -31.22188590191926\n",
            "3      \t [-1.34517553 -1.87101687]. \t  -1360.1182233917418 \t -31.22188590191926\n",
            "4      \t [-0.02657765 -0.265442  ]. \t  \u001b[92m-8.1373574173772\u001b[0m \t -8.1373574173772\n",
            "5      \t [-0.11800724 -0.48280333]. \t  -25.9239139486735 \t -8.1373574173772\n",
            "6      \t [0.17410434 0.2275663 ]. \t  \u001b[92m-4.57301698225734\u001b[0m \t -4.57301698225734\n",
            "7      \t [ 0.16880014 -0.08991209]. \t  \u001b[92m-2.0928813779559055\u001b[0m \t -2.0928813779559055\n",
            "8      \t [ 0.11000127 -0.08835003]. \t  \u001b[92m-1.8011242711769184\u001b[0m \t -1.8011242711769184\n",
            "9      \t [ 0.09690795 -0.36454039]. \t  -14.798054602184033 \t -1.8011242711769184\n",
            "10     \t [0.27088125 0.11803988]. \t  \u001b[92m-0.7310945524116613\u001b[0m \t -0.7310945524116613\n",
            "11     \t [0.13651715 0.03341295]. \t  -0.767435687989711 \t -0.7310945524116613\n",
            "12     \t [0.24095925 0.10250317]. \t  -0.7736503010545083 \t -0.7310945524116613\n",
            "13     \t [ 0.09914949 -0.01195521]. \t  -0.8589938639648796 \t -0.7310945524116613\n",
            "14     \t [ 0.15655755 -0.00149097]. \t  -0.7790016189274264 \t -0.7310945524116613\n",
            "15     \t [0.17899295 0.08865385]. \t  -0.9945826133821974 \t -0.7310945524116613\n",
            "16     \t [0.21282535 0.00783273]. \t  -0.7599833377091938 \t -0.7310945524116613\n",
            "17     \t [0.45764483 0.39507609]. \t  -3.7402698587196275 \t -0.7310945524116613\n",
            "18     \t [0.22747769 0.07048066]. \t  \u001b[92m-0.6318891127289603\u001b[0m \t -0.6318891127289603\n",
            "19     \t [0.24945707 0.08598632]. \t  \u001b[92m-0.619756535212478\u001b[0m \t -0.619756535212478\n",
            "20     \t [0.179539   0.02316406]. \t  -0.6813830883349713 \t -0.619756535212478\n",
            "21     \t [0.23588715 0.11504764]. \t  -0.9367624973999101 \t -0.619756535212478\n",
            "22     \t [0.28433571 0.07980235]. \t  \u001b[92m-0.5122844629710339\u001b[0m \t -0.5122844629710339\n",
            "23     \t [0.26606972 0.09214186]. \t  -0.5842306229031192 \t -0.5122844629710339\n",
            "24     \t [0.36144274 0.15219381]. \t  \u001b[92m-0.45420839184651896\u001b[0m \t -0.45420839184651896\n",
            "25     \t [0.37037352 0.15696146]. \t  \u001b[92m-0.4355737890124034\u001b[0m \t -0.4355737890124034\n",
            "26     \t [0.32442401 0.12779732]. \t  -0.507236850230389 \t -0.4355737890124034\n",
            "27     \t [0.34947548 0.15456016]. \t  -0.5283334842515973 \t -0.4355737890124034\n",
            "28     \t [0.39047379 0.16892992]. \t  \u001b[92m-0.3986158347133346\u001b[0m \t -0.3986158347133346\n",
            "29     \t [0.36611682 0.17413139]. \t  -0.5625275651254099 \t -0.3986158347133346\n",
            "30     \t [0.41371027 0.18659639]. \t  \u001b[92m-0.3675756293240812\u001b[0m \t -0.3675756293240812\n",
            "31     \t [0.40912829 0.22325425]. \t  -0.6612560128101932 \t -0.3675756293240812\n",
            "32     \t [0.4146129 0.1918167]. \t  -0.3823301984610061 \t -0.3675756293240812\n",
            "33     \t [0.41023012 0.18757076]. \t  -0.3850080983606259 \t -0.3675756293240812\n",
            "34     \t [0.42995116 0.2044932 ]. \t  \u001b[92m-0.36350977669721896\u001b[0m \t -0.36350977669721896\n",
            "35     \t [0.4073414 0.1758249]. \t  \u001b[92m-0.3610410453769874\u001b[0m \t -0.3610410453769874\n",
            "36     \t [0.37917859 0.17084518]. \t  -0.4586910963554728 \t -0.3610410453769874\n",
            "37     \t [0.24816966 0.07723654]. \t  -0.589736001301476 \t -0.3610410453769874\n",
            "38     \t [0.40310761 0.17996737]. \t  -0.3868062878841255 \t -0.3610410453769874\n",
            "39     \t [0.27583663 0.10965623]. \t  -0.6371096602358873 \t -0.3610410453769874\n",
            "40     \t [0.41071993 0.17620929]. \t  \u001b[92m-0.3529036763389397\u001b[0m \t -0.3529036763389397\n",
            "41     \t [0.31755267 0.18801183]. \t  -1.2256323978926982 \t -0.3529036763389397\n",
            "42     \t [0.43542473 0.19882803]. \t  \u001b[92m-0.32727069227545474\u001b[0m \t -0.32727069227545474\n",
            "43     \t [0.42786163 0.17106242]. \t  -0.34174988757900226 \t -0.32727069227545474\n",
            "44     \t [0.35763026 0.16447013]. \t  -0.5463806763230817 \t -0.32727069227545474\n",
            "45     \t [0.37544861 0.16551534]. \t  -0.4503527907666706 \t -0.32727069227545474\n",
            "46     \t [0.40496817 0.20814114]. \t  -0.5489138018926949 \t -0.32727069227545474\n",
            "47     \t [0.43578536 0.21212114]. \t  -0.3676766277594705 \t -0.32727069227545474\n",
            "48     \t [0.2981444  0.14383629]. \t  -0.7945098739007379 \t -0.32727069227545474\n",
            "49     \t [0.28470155 0.06694585]. \t  -0.5315585991207111 \t -0.32727069227545474\n",
            "50     \t [0.40586804 0.20829765]. \t  -0.5428167229706892 \t -0.32727069227545474\n",
            "51     \t [0.36886198 0.16706967]. \t  -0.4945003397413912 \t -0.32727069227545474\n",
            "52     \t [0.45644117 0.17034271]. \t  -0.43982454298314694 \t -0.32727069227545474\n",
            "53     \t [0.30748668 0.10341706]. \t  -0.4874406168997555 \t -0.32727069227545474\n",
            "54     \t [0.34945189 0.13062799]. \t  -0.4304571786065492 \t -0.32727069227545474\n",
            "55     \t [0.18972087 0.00451736]. \t  -0.7556302139388469 \t -0.32727069227545474\n",
            "56     \t [0.32377468 0.13542946]. \t  -0.5509131310730404 \t -0.32727069227545474\n",
            "57     \t [0.44802988 0.18283456]. \t  -0.33669846275594695 \t -0.32727069227545474\n",
            "58     \t [0.30003486 0.08481241]. \t  -0.49266405517222484 \t -0.32727069227545474\n",
            "59     \t [0.42596383 0.22124687]. \t  -0.48793499283145414 \t -0.32727069227545474\n",
            "60     \t [0.44623598 0.20921708]. \t  \u001b[92m-0.3168364773787901\u001b[0m \t -0.3168364773787901\n",
            "61     \t [0.33976792 0.14745602]. \t  -0.5383945925790431 \t -0.3168364773787901\n",
            "62     \t [0.39654423 0.15008407]. \t  -0.3692900842119118 \t -0.3168364773787901\n",
            "63     \t [0.41044709 0.20759578]. \t  -0.5006802677324185 \t -0.3168364773787901\n",
            "64     \t [0.49268931 0.21094188]. \t  -0.35849373858232275 \t -0.3168364773787901\n",
            "65     \t [0.40790354 0.18974601]. \t  -0.40515051716028094 \t -0.3168364773787901\n",
            "66     \t [0.30079274 0.10157607]. \t  -0.5012113357020618 \t -0.3168364773787901\n",
            "67     \t [0.43207443 0.26451198]. \t  -0.9281916729344923 \t -0.3168364773787901\n",
            "68     \t [0.41921817 0.24924535]. \t  -0.8775542130238063 \t -0.3168364773787901\n",
            "69     \t [0.46328104 0.18232392]. \t  -0.39243114049328537 \t -0.3168364773787901\n",
            "70     \t [0.449961   0.21052078]. \t  \u001b[92m-0.3090326111102614\u001b[0m \t -0.3090326111102614\n",
            "71     \t [0.43280246 0.2189299 ]. \t  -0.42164443416213904 \t -0.3090326111102614\n",
            "72     \t [0.41767201 0.22045179]. \t  -0.5507231751437572 \t -0.3090326111102614\n",
            "73     \t [0.45842223 0.23653092]. \t  -0.3628967954587079 \t -0.3090326111102614\n",
            "74     \t [0.38321104 0.15568287]. \t  -0.3882293522430977 \t -0.3090326111102614\n",
            "75     \t [0.47910399 0.24126506]. \t  \u001b[92m-0.2850788614851459\u001b[0m \t -0.2850788614851459\n",
            "76     \t [0.46754734 0.22250982]. \t  \u001b[92m-0.2850341024527629\u001b[0m \t -0.2850341024527629\n",
            "77     \t [0.32094088 0.12347461]. \t  -0.5030297513635734 \t -0.2850341024527629\n",
            "78     \t [0.43799629 0.20224666]. \t  -0.326676462062533 \t -0.2850341024527629\n",
            "79     \t [0.18785041 0.033805  ]. \t  -0.6598068141698 \t -0.2850341024527629\n",
            "80     \t [0.38936476 0.18658808]. \t  -0.495257577315959 \t -0.2850341024527629\n",
            "81     \t [0.43577074 0.20572372]. \t  -0.3434059032710773 \t -0.2850341024527629\n",
            "82     \t [0.31745017 0.11798768]. \t  -0.4955032649605998 \t -0.2850341024527629\n",
            "83     \t [0.44172709 0.22472792]. \t  -0.39931482478025604 \t -0.2850341024527629\n",
            "84     \t [0.46143183 0.19012338]. \t  -0.34202122381316674 \t -0.2850341024527629\n",
            "85     \t [0.46689342 0.22877041]. \t  -0.2958254988783405 \t -0.2850341024527629\n",
            "86     \t [0.37039132 0.17400015]. \t  -0.5319077854805717 \t -0.2850341024527629\n",
            "87     \t [0.4421754  0.18865099]. \t  -0.31588536039052073 \t -0.2850341024527629\n",
            "88     \t [0.4417473  0.21871125]. \t  -0.36720327515278506 \t -0.2850341024527629\n",
            "89     \t [0.44970722 0.21528452]. \t  -0.3198470065459449 \t -0.2850341024527629\n",
            "90     \t [0.27363586 0.0826453 ]. \t  -0.5336401479891333 \t -0.2850341024527629\n",
            "91     \t [0.4264486  0.19322563]. \t  -0.3418825834055267 \t -0.2850341024527629\n",
            "92     \t [0.4575589  0.19650809]. \t  -0.3107598775344779 \t -0.2850341024527629\n",
            "93     \t [0.43953368 0.21154356]. \t  -0.3478083630027336 \t -0.2850341024527629\n",
            "94     \t [0.43484689 0.20675923]. \t  -0.35061176327400456 \t -0.2850341024527629\n",
            "95     \t [0.49271417 0.26798883]. \t  -0.3209517110500488 \t -0.2850341024527629\n",
            "96     \t [0.48718328 0.24134386]. \t  \u001b[92m-0.26457803964320364\u001b[0m \t -0.26457803964320364\n",
            "97     \t [0.41513298 0.1727552 ]. \t  -0.3420870521980936 \t -0.26457803964320364\n",
            "98     \t [0.45168066 0.19912762]. \t  -0.30304315478138166 \t -0.26457803964320364\n",
            "99     \t [0.50413451 0.23691162]. \t  -0.27560430745448466 \t -0.26457803964320364\n",
            "100    \t [0.43930949 0.22347551]. \t  -0.4072932466514494 \t -0.26457803964320364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cI3FtfYSuv2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "734a6261-22cf-42fd-84b8-36fd74d29780"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 18 \r\n",
        "\r\n",
        "np.random.seed(run_num_18)\r\n",
        "surrogate_winner_18 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_18 = dGPGO(surrogate_winner_18, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_18.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [1.52956683 1.91914255]. \t  -17.95675982372887 \t -1.7663579664225912\n",
            "init   \t [1.51222084 0.12638491]. \t  -467.0068292530542 \t -1.7663579664225912\n",
            "init   \t [-1.09474477 -2.0013105 ]. \t  -1028.244988519411 \t -1.7663579664225912\n",
            "init   \t [-0.28479972 -0.39996883]. \t  -24.79447905426233 \t -1.7663579664225912\n",
            "init   \t [ 0.09287545 -0.0885072 ]. \t  -1.7663579664225912 \t -1.7663579664225912\n",
            "1      \t [-0.5665104   0.39819259]. \t  -3.050843192852575 \t -1.7663579664225912\n",
            "2      \t [-0.23873731  0.00668631]. \t  -1.7875716837331337 \t -1.7663579664225912\n",
            "3      \t [-1.27952978  1.39374527]. \t  -11.123103441610695 \t -1.7663579664225912\n",
            "4      \t [ 0.22862326 -0.56730241]. \t  -38.98184498360407 \t -1.7663579664225912\n",
            "5      \t [0.19543167 1.7825688 ]. \t  -304.93183543505734 \t -1.7663579664225912\n",
            "6      \t [-1.66560028  0.56641366]. \t  -494.54820388452134 \t -1.7663579664225912\n",
            "7      \t [2.048 2.048]. \t  -461.7603900415999 \t -1.7663579664225912\n",
            "8      \t [1.14639951 1.91721756]. \t  -36.38061180770056 \t -1.7663579664225912\n",
            "9      \t [-0.69181663  1.19386436]. \t  -54.021087891019675 \t -1.7663579664225912\n",
            "10     \t [-1.49523616  1.88448063]. \t  -18.563898858843064 \t -1.7663579664225912\n",
            "11     \t [1.35451222 1.31148536]. \t  -27.501386974959384 \t -1.7663579664225912\n",
            "12     \t [1.41970989 1.64187439]. \t  -14.141458664392017 \t -1.7663579664225912\n",
            "13     \t [1.28092475 2.048     ]. \t  -16.662691811618892 \t -1.7663579664225912\n",
            "14     \t [0.65360834 0.72442792]. \t  -8.954201731632901 \t -1.7663579664225912\n",
            "15     \t [1.01817205 1.10686814]. \t  \u001b[92m-0.49304759898622136\u001b[0m \t -0.49304759898622136\n",
            "16     \t [1.36891057 1.83171667]. \t  \u001b[92m-0.31417461684167003\u001b[0m \t -0.31417461684167003\n",
            "17     \t [-0.76317292  0.74143757]. \t  -5.637027092544606 \t -0.31417461684167003\n",
            "18     \t [-0.03793185 -0.17564052]. \t  -4.2130118059337 \t -0.31417461684167003\n",
            "19     \t [0.8718773  0.83493823]. \t  -0.5754438043303597 \t -0.31417461684167003\n",
            "20     \t [1.38163606 1.94116186]. \t  \u001b[92m-0.2496114630509464\u001b[0m \t -0.2496114630509464\n",
            "21     \t [0.88721102 0.85811015]. \t  -0.516349351839153 \t -0.2496114630509464\n",
            "22     \t [0.91953278 0.93217614]. \t  -0.7570479617129472 \t -0.2496114630509464\n",
            "23     \t [1.36999925 1.86171839]. \t  \u001b[92m-0.15994134760531115\u001b[0m \t -0.15994134760531115\n",
            "24     \t [0.97934729 1.04276801]. \t  -0.7001068518100352 \t -0.15994134760531115\n",
            "25     \t [0.221227   0.05703809]. \t  -0.6130430475671615 \t -0.15994134760531115\n",
            "26     \t [1.01703387 1.01151268]. \t  \u001b[92m-0.052480522980403066\u001b[0m \t -0.052480522980403066\n",
            "27     \t [0.06975513 0.05563374]. \t  -1.1230941487960542 \t -0.052480522980403066\n",
            "28     \t [1.41179522 1.9461429 ]. \t  -0.39068997552164403 \t -0.052480522980403066\n",
            "29     \t [1.34776901 1.74962666]. \t  -0.5678974167541917 \t -0.052480522980403066\n",
            "30     \t [1.3518823 1.8251171]. \t  -0.12443057444973657 \t -0.052480522980403066\n",
            "31     \t [1.36588773 1.83882381]. \t  -0.20583446166745967 \t -0.052480522980403066\n",
            "32     \t [1.04979312 1.13951449]. \t  -0.14272136690162013 \t -0.052480522980403066\n",
            "33     \t [1.05681225 1.10630803]. \t  \u001b[92m-0.014345462707764456\u001b[0m \t -0.014345462707764456\n",
            "34     \t [1.35289791 1.82019308]. \t  -0.1348182270861123 \t -0.014345462707764456\n",
            "35     \t [1.07673455 1.15188797]. \t  \u001b[92m-0.011467255387135954\u001b[0m \t -0.011467255387135954\n",
            "36     \t [1.04118109 1.06817336]. \t  -0.026928288773511465 \t -0.011467255387135954\n",
            "37     \t [0.24887026 0.07350005]. \t  -0.5775676722806272 \t -0.011467255387135954\n",
            "38     \t [1.35780233 1.83074895]. \t  -0.144607372103097 \t -0.011467255387135954\n",
            "39     \t [1.37414802 1.87220684]. \t  -0.16583037588454538 \t -0.011467255387135954\n",
            "40     \t [1.36881113 1.84858665]. \t  -0.19880830544607495 \t -0.011467255387135954\n",
            "41     \t [1.02880276 1.00455466]. \t  -0.29114008739825453 \t -0.011467255387135954\n",
            "42     \t [1.01466144 1.07384858]. \t  -0.19655905251190667 \t -0.011467255387135954\n",
            "43     \t [0.94974871 0.96676086]. \t  -0.4216292668332252 \t -0.011467255387135954\n",
            "44     \t [0.97036778 0.97033995]. \t  -0.08339823869310183 \t -0.011467255387135954\n",
            "45     \t [0.99291052 0.99428759]. \t  \u001b[92m-0.007133643108056444\u001b[0m \t -0.007133643108056444\n",
            "46     \t [1.01737628 1.09373932]. \t  -0.34469266412947236 \t -0.007133643108056444\n",
            "47     \t [1.01817433 1.05447756]. \t  -0.032009273185164895 \t -0.007133643108056444\n",
            "48     \t [1.38332352 1.87099175]. \t  -0.3283464637613579 \t -0.007133643108056444\n",
            "49     \t [0.08569777 0.02927015]. \t  -0.8840237101743782 \t -0.007133643108056444\n",
            "50     \t [1.07462369 1.21114751]. \t  -0.3228917590598745 \t -0.007133643108056444\n",
            "51     \t [1.32258367 1.74894731]. \t  -0.10406808135811729 \t -0.007133643108056444\n",
            "52     \t [0.9595137  0.96352746]. \t  -0.18534497801872685 \t -0.007133643108056444\n",
            "53     \t [1.38820152 1.90850322]. \t  -0.18529736475046907 \t -0.007133643108056444\n",
            "54     \t [0.85190899 0.75170219]. \t  -0.0892880925184944 \t -0.007133643108056444\n",
            "55     \t [0.92282625 0.87445254]. \t  -0.058141723666202245 \t -0.007133643108056444\n",
            "56     \t [0.45546528 0.2660668 ]. \t  -0.6401271334376297 \t -0.007133643108056444\n",
            "57     \t [1.00352233 1.06287251]. \t  -0.31154868948288844 \t -0.007133643108056444\n",
            "58     \t [0.99131066 1.00543815]. \t  -0.051792329950353905 \t -0.007133643108056444\n",
            "59     \t [1.31673595 1.76084976]. \t  -0.17352550306562214 \t -0.007133643108056444\n",
            "60     \t [1.02304262 1.08131063]. \t  -0.12090124108225275 \t -0.007133643108056444\n",
            "61     \t [1.08491482 1.21727224]. \t  -0.16907242694894944 \t -0.007133643108056444\n",
            "62     \t [1.38739869 1.92747996]. \t  -0.1507562583990603 \t -0.007133643108056444\n",
            "63     \t [1.3378291  1.78266919]. \t  -0.11919438094274389 \t -0.007133643108056444\n",
            "64     \t [1.06474821 1.12582497]. \t  -0.010376221392191815 \t -0.007133643108056444\n",
            "65     \t [0.99965417 0.9948398 ]. \t  \u001b[92m-0.0019970068225773304\u001b[0m \t -0.0019970068225773304\n",
            "66     \t [1.03291481 1.10746399]. \t  -0.16552169993364532 \t -0.0019970068225773304\n",
            "67     \t [1.42657811 2.02402725]. \t  -0.19428511490146064 \t -0.0019970068225773304\n",
            "68     \t [0.99973321 1.01851906]. \t  -0.03630015528832474 \t -0.0019970068225773304\n",
            "69     \t [1.36802334 1.87237032]. \t  -0.13551905229265873 \t -0.0019970068225773304\n",
            "70     \t [1.03431601 1.0921998 ]. \t  -0.05130960871993776 \t -0.0019970068225773304\n",
            "71     \t [1.02830924 1.04806837]. \t  -0.009546539587558207 \t -0.0019970068225773304\n",
            "72     \t [1.27161564 1.59400707]. \t  -0.12667167612151714 \t -0.0019970068225773304\n",
            "73     \t [1.08982846 1.16704715]. \t  -0.05083098208416397 \t -0.0019970068225773304\n",
            "74     \t [1.03657237 1.04083141]. \t  -0.11457562490921001 \t -0.0019970068225773304\n",
            "75     \t [1.1471592  1.31816771]. \t  -0.022136963929955275 \t -0.0019970068225773304\n",
            "76     \t [1.12006417 1.27889219]. \t  -0.07370005894138378 \t -0.0019970068225773304\n",
            "77     \t [0.9602403  0.91756236]. \t  -0.0036049995218690855 \t -0.0019970068225773304\n",
            "78     \t [1.05675889 1.10717882]. \t  -0.012361937438166503 \t -0.0019970068225773304\n",
            "79     \t [1.33366641 1.76579868]. \t  -0.1278902845389587 \t -0.0019970068225773304\n",
            "80     \t [1.24431132 1.55177789]. \t  -0.06089018204651312 \t -0.0019970068225773304\n",
            "81     \t [1.32642925 1.7721559 ]. \t  -0.12279023997360565 \t -0.0019970068225773304\n",
            "82     \t [1.02047202 1.05158   ]. \t  -0.010857537610377782 \t -0.0019970068225773304\n",
            "83     \t [1.05188387 1.11459078]. \t  -0.009303412992477144 \t -0.0019970068225773304\n",
            "84     \t [1.19816914 1.46541992]. \t  -0.1281383464177524 \t -0.0019970068225773304\n",
            "85     \t [1.09021258 1.16403149]. \t  -0.06832011250115175 \t -0.0019970068225773304\n",
            "86     \t [1.06689562 1.13259435]. \t  -0.007692076318457641 \t -0.0019970068225773304\n",
            "87     \t [1.08355145 1.12891769]. \t  -0.21097814949066165 \t -0.0019970068225773304\n",
            "88     \t [1.08582729 1.21148342]. \t  -0.11274777752492081 \t -0.0019970068225773304\n",
            "89     \t [1.38804113 1.92462036]. \t  -0.15099118661078093 \t -0.0019970068225773304\n",
            "90     \t [1.01940603 1.06416795]. \t  -0.0627730880602807 \t -0.0019970068225773304\n",
            "91     \t [1.07433971 1.18639357]. \t  -0.10913155562531944 \t -0.0019970068225773304\n",
            "92     \t [0.96996583 0.94273169]. \t  \u001b[92m-0.0012622823171280775\u001b[0m \t -0.0012622823171280775\n",
            "93     \t [1.37199415 1.87125184]. \t  -0.15073640070926242 \t -0.0012622823171280775\n",
            "94     \t [1.20640457 1.45800608]. \t  -0.0432757797485219 \t -0.0012622823171280775\n",
            "95     \t [1.08881537 1.18329627]. \t  -0.00838218409886892 \t -0.0012622823171280775\n",
            "96     \t [1.09367702 1.18536583]. \t  -0.020360847440088085 \t -0.0012622823171280775\n",
            "97     \t [1.00973899 1.03239282]. \t  -0.01653005535923015 \t -0.0012622823171280775\n",
            "98     \t [1.10669138 1.21303768]. \t  -0.02513797177392528 \t -0.0012622823171280775\n",
            "99     \t [0.80551578 0.68359382]. \t  -0.15849802186657375 \t -0.0012622823171280775\n",
            "100    \t [1.0736673  1.15605205]. \t  -0.0065096651679119495 \t -0.0012622823171280775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YT-CgKvuv4q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b53f31f-6eff-4019-e480-2328d2ca9136"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 19 \r\n",
        "\r\n",
        "np.random.seed(run_num_19)\r\n",
        "surrogate_winner_19 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_19 = dGPGO(surrogate_winner_19, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_19.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.24348908 1.07107119]. \t  -102.94304782477323 \t -4.219752052396591\n",
            "init   \t [-1.04639875  1.11284007]. \t  -4.219752052396591 \t -4.219752052396591\n",
            "init   \t [-0.59608235  1.21527255]. \t  -76.50031976976825 \t -4.219752052396591\n",
            "init   \t [-0.63078391 -0.22989031]. \t  -42.07005958957044 \t -4.219752052396591\n",
            "init   \t [-0.8056685  -0.23710138]. \t  -81.79603430667063 \t -4.219752052396591\n",
            "1      \t [ 0.45732394 -0.61167271]. \t  -67.6686991495879 \t -4.219752052396591\n",
            "2      \t [-1.91402271  1.47248897]. \t  -488.5369871668717 \t -4.219752052396591\n",
            "3      \t [-0.770978    0.54385886]. \t  \u001b[92m-3.3918752419961646\u001b[0m \t -3.3918752419961646\n",
            "4      \t [1.46517021 1.26496797]. \t  -77.96570754557084 \t -3.3918752419961646\n",
            "5      \t [-0.04155768 -0.18268815]. \t  -4.4857385553352955 \t -3.3918752419961646\n",
            "6      \t [-0.22871362 -0.95001582]. \t  -101.9754251271357 \t -3.3918752419961646\n",
            "7      \t [-1.03157196  0.69596287]. \t  -17.68277711813692 \t -3.3918752419961646\n",
            "8      \t [0.85707475 0.47647651]. \t  -6.682020603047874 \t -3.3918752419961646\n",
            "9      \t [ 1.908228   -1.76639366]. \t  -2925.176808602748 \t -3.3918752419961646\n",
            "10     \t [-0.85919451  0.81563011]. \t  -4.05591079174479 \t -3.3918752419961646\n",
            "11     \t [0.91476286 0.81029031]. \t  \u001b[92m-0.07749449873171547\u001b[0m \t -0.07749449873171547\n",
            "12     \t [0.91116855 1.04373998]. \t  -4.5666220361741665 \t -0.07749449873171547\n",
            "13     \t [0.89052936 0.76899733]. \t  \u001b[92m-0.06980104637585811\u001b[0m \t -0.06980104637585811\n",
            "14     \t [0.89282801 0.78859495]. \t  \u001b[92m-0.01879079021982339\u001b[0m \t -0.01879079021982339\n",
            "15     \t [0.90200284 0.78467663]. \t  -0.0933123863438736 \t -0.01879079021982339\n",
            "16     \t [ 0.18017432 -0.29357342]. \t  -11.302074926875267 \t -0.01879079021982339\n",
            "17     \t [-0.43072548  0.1093718 ]. \t  -2.6268977967442764 \t -0.01879079021982339\n",
            "18     \t [0.93583843 0.78788381]. \t  -0.7769292644472852 \t -0.01879079021982339\n",
            "19     \t [0.882544   0.79921934]. \t  -0.05514884568007164 \t -0.01879079021982339\n",
            "20     \t [0.90058961 0.74078249]. \t  -0.5037984122775261 \t -0.01879079021982339\n",
            "21     \t [-1.05211431  1.64802798]. \t  -33.48830477221415 \t -0.01879079021982339\n",
            "22     \t [0.89770256 0.77160556]. \t  -0.12786917035097714 \t -0.01879079021982339\n",
            "23     \t [0.89497482 0.7446561 ]. \t  -0.32826773086653194 \t -0.01879079021982339\n",
            "24     \t [0.8894176  0.80322083]. \t  -0.027008115487001556 \t -0.01879079021982339\n",
            "25     \t [0.91563501 0.78076956]. \t  -0.3390997589990717 \t -0.01879079021982339\n",
            "26     \t [0.88816508 0.77076917]. \t  -0.045152456613058535 \t -0.01879079021982339\n",
            "27     \t [0.87183477 0.84697349]. \t  -0.7711984834532738 \t -0.01879079021982339\n",
            "28     \t [0.90231875 0.78045211]. \t  -0.1232927598641485 \t -0.01879079021982339\n",
            "29     \t [0.88865391 0.75229284]. \t  -0.15237074248945387 \t -0.01879079021982339\n",
            "30     \t [0.89400265 0.76030384]. \t  -0.16284368540623095 \t -0.01879079021982339\n",
            "31     \t [0.89835093 0.79957502]. \t  \u001b[92m-0.015896748045800414\u001b[0m \t -0.015896748045800414\n",
            "32     \t [0.87360663 0.79886292]. \t  -0.14324135285780595 \t -0.015896748045800414\n",
            "33     \t [0.87682571 0.74321243]. \t  -0.08076368668214338 \t -0.015896748045800414\n",
            "34     \t [0.91286149 0.68806131]. \t  -2.1174886041787397 \t -0.015896748045800414\n",
            "35     \t [0.87157537 0.76029481]. \t  -0.016535288999081414 \t -0.015896748045800414\n",
            "36     \t [0.87598055 0.71081471]. \t  -0.3349133778711821 \t -0.015896748045800414\n",
            "37     \t [0.9072693  0.72272855]. \t  -1.016796292899233 \t -0.015896748045800414\n",
            "38     \t [0.88346857 0.83122512]. \t  -0.27071380551163976 \t -0.015896748045800414\n",
            "39     \t [0.8781081  0.83791227]. \t  -0.4615952285669671 \t -0.015896748045800414\n",
            "40     \t [0.81877195 0.79019677]. \t  -1.4682696860342999 \t -0.015896748045800414\n",
            "41     \t [0.8688247  0.80731511]. \t  -0.2923990006339204 \t -0.015896748045800414\n",
            "42     \t [0.91063022 0.74685207]. \t  -0.6868857532603456 \t -0.015896748045800414\n",
            "43     \t [-0.37763354  0.03761708]. \t  -3.0001643731707306 \t -0.015896748045800414\n",
            "44     \t [0.86731783 0.80192578]. \t  -0.2644700833880386 \t -0.015896748045800414\n",
            "45     \t [0.88041549 0.85725147]. \t  -0.6886706303604816 \t -0.015896748045800414\n",
            "46     \t [0.88550148 0.85457828]. \t  -0.5096472977881151 \t -0.015896748045800414\n",
            "47     \t [0.88677218 0.757011  ]. \t  -0.09898568049552457 \t -0.015896748045800414\n",
            "48     \t [0.90385319 0.86304276]. \t  -0.22169300888001817 \t -0.015896748045800414\n",
            "49     \t [0.9008139  0.68730936]. \t  -1.5513172186975768 \t -0.015896748045800414\n",
            "50     \t [0.8915519  0.90859419]. \t  -1.3051985666807002 \t -0.015896748045800414\n",
            "51     \t [0.88497351 0.81289205]. \t  -0.1015228906505841 \t -0.015896748045800414\n",
            "52     \t [0.91804029 0.91804029]. \t  -0.5728580342425337 \t -0.015896748045800414\n",
            "53     \t [0.90151734 0.88946251]. \t  -0.598432872734771 \t -0.015896748045800414\n",
            "54     \t [0.90507038 0.85487261]. \t  -0.13660503278426359 \t -0.015896748045800414\n",
            "55     \t [0.91650899 0.88541617]. \t  -0.21333586663699844 \t -0.015896748045800414\n",
            "56     \t [0.85396135 0.73547559]. \t  -0.02520309222894406 \t -0.015896748045800414\n",
            "57     \t [0.91970034 0.87954855]. \t  -0.12001586086595432 \t -0.015896748045800414\n",
            "58     \t [0.91253227 0.93228044]. \t  -0.9989754648602635 \t -0.015896748045800414\n",
            "59     \t [0.89204695 0.77800117]. \t  -0.043147997397361366 \t -0.015896748045800414\n",
            "60     \t [0.88993571 0.79143583]. \t  \u001b[92m-0.01214436931057295\u001b[0m \t -0.01214436931057295\n",
            "61     \t [0.8974185  0.80040436]. \t  -0.012978759495425853 \t -0.01214436931057295\n",
            "62     \t [0.89685127 0.9202604 ]. \t  -1.3543426848324895 \t -0.01214436931057295\n",
            "63     \t [0.79558897 0.70030657]. \t  -0.49531569020400845 \t -0.01214436931057295\n",
            "64     \t [0.93266536 0.89349318]. \t  -0.06036458374888046 \t -0.01214436931057295\n",
            "65     \t [0.9119962  0.76779412]. \t  -0.4166147383127691 \t -0.01214436931057295\n",
            "66     \t [0.91440467 0.87182682]. \t  -0.13471079467618266 \t -0.01214436931057295\n",
            "67     \t [0.9148164  0.83575508]. \t  \u001b[92m-0.007384835490022059\u001b[0m \t -0.007384835490022059\n",
            "68     \t [0.91970871 0.8304249 ]. \t  -0.03028359849395009 \t -0.007384835490022059\n",
            "69     \t [0.93691875 0.92261171]. \t  -0.2046381691161187 \t -0.007384835490022059\n",
            "70     \t [0.91178147 0.81251034]. \t  -0.04325867333760928 \t -0.007384835490022059\n",
            "71     \t [0.91659442 0.76551476]. \t  -0.5639286848063777 \t -0.007384835490022059\n",
            "72     \t [0.93808754 0.85522274]. \t  -0.06526523394492538 \t -0.007384835490022059\n",
            "73     \t [0.72271755 0.49209201]. \t  -0.16826272359707226 \t -0.007384835490022059\n",
            "74     \t [0.9000857  0.88558891]. \t  -0.57902135740435 \t -0.007384835490022059\n",
            "75     \t [0.66676766 0.48051498]. \t  -0.24018246867968057 \t -0.007384835490022059\n",
            "76     \t [0.760346   0.56883274]. \t  -0.06607058613229129 \t -0.007384835490022059\n",
            "77     \t [0.67293838 0.48715788]. \t  -0.2246993945923063 \t -0.007384835490022059\n",
            "78     \t [0.9182014  0.74099377]. \t  -1.0491328887509277 \t -0.007384835490022059\n",
            "79     \t [0.76149786 0.62370136]. \t  -0.2489232807605527 \t -0.007384835490022059\n",
            "80     \t [0.90472908 0.82133861]. \t  -0.009862739374449505 \t -0.007384835490022059\n",
            "81     \t [0.70098854 0.49473457]. \t  -0.09052985588791553 \t -0.007384835490022059\n",
            "82     \t [0.91651644 0.8008078 ]. \t  -0.16059102684336052 \t -0.007384835490022059\n",
            "83     \t [0.88393212 0.80936215]. \t  -0.0920183568943424 \t -0.007384835490022059\n",
            "84     \t [0.68873651 0.47019911]. \t  -0.09861457708765904 \t -0.007384835490022059\n",
            "85     \t [0.70516657 0.47135763]. \t  -0.15401945587539972 \t -0.007384835490022059\n",
            "86     \t [0.90733754 0.84945463]. \t  -0.07719476004946972 \t -0.007384835490022059\n",
            "87     \t [0.94104512 0.9009788 ]. \t  -0.02723134443715174 \t -0.007384835490022059\n",
            "88     \t [0.87194306 0.7698143 ]. \t  -0.02547989879175158 \t -0.007384835490022059\n",
            "89     \t [0.87691015 0.74105218]. \t  -0.09309939497769706 \t -0.007384835490022059\n",
            "90     \t [0.8917909  0.81542098]. \t  -0.0522307941737475 \t -0.007384835490022059\n",
            "91     \t [0.57770317 0.37451944]. \t  -0.34462310723257716 \t -0.007384835490022059\n",
            "92     \t [0.90360391 0.87107203]. \t  -0.3071026568406295 \t -0.007384835490022059\n",
            "93     \t [0.74953429 0.61930492]. \t  -0.39339576452045844 \t -0.007384835490022059\n",
            "94     \t [0.8994638  0.86470598]. \t  -0.32003199850290764 \t -0.007384835490022059\n",
            "95     \t [0.69983114 0.38427399]. \t  -1.2029077003711366 \t -0.007384835490022059\n",
            "96     \t [0.64712875 0.37963904]. \t  -0.2776853169025776 \t -0.007384835490022059\n",
            "97     \t [0.63852374 0.38116041]. \t  -0.20116681674169995 \t -0.007384835490022059\n",
            "98     \t [0.63671878 0.49754149]. \t  -0.9807796632993792 \t -0.007384835490022059\n",
            "99     \t [0.90767162 0.79628338]. \t  -0.08461443969241722 \t -0.007384835490022059\n",
            "100    \t [0.89000299 0.80469692]. \t  -0.027954209794588673 \t -0.007384835490022059\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHz_Jg2_uv7E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7f37026-d083-4372-b8af-971f745a737b"
      },
      "source": [
        "### Bayesian optimization runs (x20): 'Winner' Acquisition Function run number = 20 \r\n",
        "\r\n",
        "np.random.seed(run_num_20)\r\n",
        "surrogate_winner_20 = dGaussianProcess(d_cov_func)\r\n",
        "\r\n",
        "winner_20 = dGPGO(surrogate_winner_20, Acquisition_new(util_winner), f_syn_polarity, param, n_jobs = -1) # Define BayesOpt\r\n",
        "winner_20.run(max_iter = max_iter, init_evals = n_init) # run"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation \t Proposed point \t  Current eval. \t Best eval.\n",
            "init   \t [0.2888388  0.26430978]. \t  -3.777577453542735 \t -3.777577453542735\n",
            "init   \t [-0.04734225 -0.66978712]. \t  -46.25914429040042 \t -3.777577453542735\n",
            "init   \t [-0.50844394  0.13121892]. \t  -3.895838569726608 \t -3.777577453542735\n",
            "init   \t [-1.76903664  0.34623103]. \t  -782.3209705533411 \t -3.777577453542735\n",
            "init   \t [-1.07357076 -1.38954104]. \t  -650.524506936468 \t -3.777577453542735\n",
            "1      \t [-0.14322119  0.14917468]. \t  \u001b[92m-2.962355281229574\u001b[0m \t -2.962355281229574\n",
            "2      \t [1.52145555 0.45818482]. \t  -344.9839327128462 \t -2.962355281229574\n",
            "3      \t [ 0.91665306 -1.2697849 ]. \t  -445.2328736090343 \t -2.962355281229574\n",
            "4      \t [0.45675925 1.47513464]. \t  -160.69875945676344 \t -2.962355281229574\n",
            "5      \t [-0.33507699 -0.19432169]. \t  -11.182681117657154 \t -2.962355281229574\n",
            "6      \t [-0.34574027  0.31186662]. \t  -5.510110640704662 \t -2.962355281229574\n",
            "7      \t [0.29019975 0.00490018]. \t  \u001b[92m-1.1329147013860652\u001b[0m \t -1.1329147013860652\n",
            "8      \t [0.30595124 0.0961033 ]. \t  \u001b[92m-0.4823272546355962\u001b[0m \t -0.4823272546355962\n",
            "9      \t [0.33664448 0.10459482]. \t  \u001b[92m-0.44767002356263197\u001b[0m \t -0.44767002356263197\n",
            "10     \t [0.32192471 0.09570348]. \t  -0.4660778220546405 \t -0.44767002356263197\n",
            "11     \t [-0.26839983  0.05094474]. \t  -1.6533326838433873 \t -0.44767002356263197\n",
            "12     \t [0.31131688 0.08001813]. \t  -0.5028456682708745 \t -0.44767002356263197\n",
            "13     \t [0.31653543 0.09338956]. \t  -0.4717547813530824 \t -0.44767002356263197\n",
            "14     \t [0.29883098 0.09416061]. \t  -0.4940005803722769 \t -0.44767002356263197\n",
            "15     \t [0.31920716 0.09934978]. \t  -0.46412578893430895 \t -0.44767002356263197\n",
            "16     \t [0.30403367 0.06650336]. \t  -0.551621762469545 \t -0.44767002356263197\n",
            "17     \t [0.30500675 0.09645037]. \t  -0.4841861063902875 \t -0.44767002356263197\n",
            "18     \t [0.30367015 0.08710733]. \t  -0.48748466399638135 \t -0.44767002356263197\n",
            "19     \t [0.31909535 0.10092646]. \t  -0.46371131109545444 \t -0.44767002356263197\n",
            "20     \t [0.31982523 0.08069735]. \t  -0.5092541139934812 \t -0.44767002356263197\n",
            "21     \t [0.34085198 0.07681343]. \t  -0.5894493322518854 \t -0.44767002356263197\n",
            "22     \t [0.31135862 0.11563181]. \t  -0.5091496640369201 \t -0.44767002356263197\n",
            "23     \t [0.31798486 0.10104798]. \t  -0.46514509592330744 \t -0.44767002356263197\n",
            "24     \t [0.29413536 0.08365453]. \t  -0.49906346949538527 \t -0.44767002356263197\n",
            "25     \t [0.299319   0.09897944]. \t  -0.49976651073177886 \t -0.44767002356263197\n",
            "26     \t [0.31171006 0.10915276]. \t  -0.4881180799430617 \t -0.44767002356263197\n",
            "27     \t [0.31024374 0.10380455]. \t  -0.481469037987544 \t -0.44767002356263197\n",
            "28     \t [0.3247431  0.09344081]. \t  -0.4704133492095369 \t -0.44767002356263197\n",
            "29     \t [0.30939637 0.10241347]. \t  -0.48140544238243804 \t -0.44767002356263197\n",
            "30     \t [0.31155467 0.10792736]. \t  -0.4857532029775587 \t -0.44767002356263197\n",
            "31     \t [0.31470329 0.0881226 ]. \t  -0.4815465380867779 \t -0.44767002356263197\n",
            "32     \t [0.2948831  0.09229581]. \t  -0.500041159511031 \t -0.44767002356263197\n",
            "33     \t [0.32551456 0.09281459]. \t  -0.47221006874984667 \t -0.44767002356263197\n",
            "34     \t [0.31350634 0.09740696]. \t  -0.4713508622318662 \t -0.44767002356263197\n",
            "35     \t [0.3213755  0.09473346]. \t  -0.46783932804106365 \t -0.44767002356263197\n",
            "36     \t [0.31139179 0.10683874]. \t  -0.4839306327107647 \t -0.44767002356263197\n",
            "37     \t [0.30972889 0.09038359]. \t  -0.47955267921445643 \t -0.44767002356263197\n",
            "38     \t [0.30273462 0.09051979]. \t  -0.4863063480149027 \t -0.44767002356263197\n",
            "39     \t [0.30368981 0.09990266]. \t  -0.490738696691998 \t -0.44767002356263197\n",
            "40     \t [0.31102749 0.10440683]. \t  -0.4805640645752329 \t -0.44767002356263197\n",
            "41     \t [0.31528607 0.10174434]. \t  -0.4693802753079266 \t -0.44767002356263197\n",
            "42     \t [0.30235491 0.09459702]. \t  -0.48771898105448963 \t -0.44767002356263197\n",
            "43     \t [0.31636396 0.0851813 ]. \t  -0.48957371119383736 \t -0.44767002356263197\n",
            "44     \t [0.31568727 0.0986315 ]. \t  -0.46838938035365096 \t -0.44767002356263197\n",
            "45     \t [0.31536141 0.09010896]. \t  -0.4774607742579181 \t -0.44767002356263197\n",
            "46     \t [0.31732679 0.09739558]. \t  -0.46713217975985194 \t -0.44767002356263197\n",
            "47     \t [0.30376303 0.08413444]. \t  -0.49136787348211525 \t -0.44767002356263197\n",
            "48     \t [0.33263911 0.08833413]. \t  -0.4951648950482022 \t -0.44767002356263197\n",
            "49     \t [0.33089703 0.08090358]. \t  -0.5294334020691259 \t -0.44767002356263197\n",
            "50     \t [0.32204077 0.09433697]. \t  -0.46841456590765806 \t -0.44767002356263197\n",
            "51     \t [0.32170182 0.09426652]. \t  -0.4685994851134361 \t -0.44767002356263197\n",
            "52     \t [0.31669996 0.09965607]. \t  -0.4669402628706641 \t -0.44767002356263197\n",
            "53     \t [0.31853297 0.08808015]. \t  -0.48230806062551057 \t -0.44767002356263197\n",
            "54     \t [0.31678023 0.08651202]. \t  -0.48593743304706777 \t -0.44767002356263197\n",
            "55     \t [0.34811047 0.07917511]. \t  -0.6014086134975964 \t -0.44767002356263197\n",
            "56     \t [0.31863738 0.08611157]. \t  -0.4880271319995015 \t -0.44767002356263197\n",
            "57     \t [0.33347469 0.09051907]. \t  -0.487048275943436 \t -0.44767002356263197\n",
            "58     \t [0.30853888 0.08911324]. \t  -0.481818764609963 \t -0.44767002356263197\n",
            "59     \t [0.32848789 0.08846335]. \t  -0.4887235611899965 \t -0.44767002356263197\n",
            "60     \t [0.30628365 0.10764393]. \t  -0.5003810537195211 \t -0.44767002356263197\n",
            "61     \t [0.33389164 0.08783228]. \t  -0.4996389782093217 \t -0.44767002356263197\n",
            "62     \t [0.34996393 0.07947712]. \t  -0.6074264964591561 \t -0.44767002356263197\n",
            "63     \t [0.30486702 0.0912295 ]. \t  -0.4835037712140826 \t -0.44767002356263197\n",
            "64     \t [0.3018784  0.11755674]. \t  -0.5572079913783102 \t -0.44767002356263197\n",
            "65     \t [0.30088797 0.0784905 ]. \t  -0.5032611805709466 \t -0.44767002356263197\n",
            "66     \t [0.3013054  0.08761205]. \t  -0.4891808702519391 \t -0.44767002356263197\n",
            "67     \t [0.29705455 0.09031954]. \t  -0.49456416966027106 \t -0.44767002356263197\n",
            "68     \t [0.31971724 0.09129724]. \t  -0.4747133599611185 \t -0.44767002356263197\n",
            "69     \t [0.32030737 0.0986733 ]. \t  -0.4635214654119954 \t -0.44767002356263197\n",
            "70     \t [0.31958314 0.09023293]. \t  -0.4771291808021703 \t -0.44767002356263197\n",
            "71     \t [0.31631151 0.10152419]. \t  -0.4676463989937266 \t -0.44767002356263197\n",
            "72     \t [0.31491307 0.10271643]. \t  -0.47060165250972014 \t -0.44767002356263197\n",
            "73     \t [0.33541577 0.0749523 ]. \t  -0.5826832141449203 \t -0.44767002356263197\n",
            "74     \t [0.3085087  0.09760573]. \t  -0.47874979061226836 \t -0.44767002356263197\n",
            "75     \t [0.316129   0.07207348]. \t  -0.5453201327028357 \t -0.44767002356263197\n",
            "76     \t [0.31462842 0.09220525]. \t  -0.47433889960186687 \t -0.44767002356263197\n",
            "77     \t [0.32733538 0.08777165]. \t  -0.49002372076505524 \t -0.44767002356263197\n",
            "78     \t [0.31644211 0.06667863]. \t  -0.5791883157840194 \t -0.44767002356263197\n",
            "79     \t [0.31123633 0.08288118]. \t  -0.49395866666993277 \t -0.44767002356263197\n",
            "80     \t [0.28920859 0.07270433]. \t  -0.5171868421174315 \t -0.44767002356263197\n",
            "81     \t [0.31733464 0.09229061]. \t  -0.4731059214795227 \t -0.44767002356263197\n",
            "82     \t [0.3052591  0.09723484]. \t  -0.48430657430331775 \t -0.44767002356263197\n",
            "83     \t [0.31522431 0.10090743]. \t  -0.46915523713575996 \t -0.44767002356263197\n",
            "84     \t [0.31510441 0.09582549]. \t  -0.47028279681525875 \t -0.44767002356263197\n",
            "85     \t [0.31409004 0.10286277]. \t  -0.4722450623024053 \t -0.44767002356263197\n",
            "86     \t [0.30336065 0.099864  ]. \t  -0.49144717289324685 \t -0.44767002356263197\n",
            "87     \t [0.30920517 0.10541109]. \t  -0.48680787953203203 \t -0.44767002356263197\n",
            "88     \t [0.3028034 0.0901641]. \t  -0.48631590005103165 \t -0.44767002356263197\n",
            "89     \t [0.31063486 0.09112703]. \t  -0.4781047482262733 \t -0.44767002356263197\n",
            "90     \t [0.32009884 0.09378326]. \t  -0.4697998417449274 \t -0.44767002356263197\n",
            "91     \t [0.30110849 0.08684279]. \t  -0.4899112886083694 \t -0.44767002356263197\n",
            "92     \t [0.32139379 0.09775782]. \t  -0.46357128826941957 \t -0.44767002356263197\n",
            "93     \t [0.3065098  0.08971305]. \t  -0.4827223612412366 \t -0.44767002356263197\n",
            "94     \t [0.30142288 0.0875117 ]. \t  -0.4891282554261116 \t -0.44767002356263197\n",
            "95     \t [0.30707211 0.10982138]. \t  -0.5042612366919343 \t -0.44767002356263197\n",
            "96     \t [0.30713834 0.10469855]. \t  -0.49079974992034725 \t -0.44767002356263197\n",
            "97     \t [0.30925989 0.09533984]. \t  -0.47713100707801054 \t -0.44767002356263197\n",
            "98     \t [0.30646992 0.08407646]. \t  -0.4906810099783001 \t -0.44767002356263197\n",
            "99     \t [0.31059885 0.06196051]. \t  -0.5943757973188611 \t -0.44767002356263197\n",
            "100    \t [0.3051528  0.11086416]. \t  -0.5143044331963325 \t -0.44767002356263197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUnhsKpCuv9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6453e240-da32-4401-806e-e282753cb554"
      },
      "source": [
        "end_win = time.time()\r\n",
        "end_win\r\n",
        "\r\n",
        "time_win = end_win - start_win\r\n",
        "time_win"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19491.90955233574"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJG0SLpwuwAL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "208a6fa7-c92e-4eb2-f6d2-a0ba458ce5e1"
      },
      "source": [
        "### Training regret minimization: run number = 1\r\n",
        "\r\n",
        "loser_output_1 = np.append(np.max(loser_1.GP.y[0:n_init]),loser_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_1 = np.append(np.max(winner_1.GP.y[0:n_init]),winner_1.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_1 = np.log(y_global_orig - loser_output_1)\r\n",
        "regret_winner_1 = np.log(y_global_orig - winner_output_1)\r\n",
        "\r\n",
        "train_regret_loser_1 = min_max_array(regret_loser_1)\r\n",
        "train_regret_winner_1 = min_max_array(regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1 = min(train_regret_loser_1)\r\n",
        "min_train_regret_winner_1 = min(train_regret_winner_1)\r\n",
        "\r\n",
        "min_train_regret_loser_1, min_train_regret_winner_1"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.458439884041693, -1.573653104930495)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lA9eZf0uwCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "962a3acd-c5f9-4b4e-a5d4-02cbf7af0033"
      },
      "source": [
        "### Training regret minimization: run number = 2\r\n",
        "\r\n",
        "loser_output_2 = np.append(np.max(loser_2.GP.y[0:n_init]),loser_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_2 = np.append(np.max(winner_2.GP.y[0:n_init]),winner_2.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_2 = np.log(y_global_orig - loser_output_2)\r\n",
        "regret_winner_2 = np.log(y_global_orig - winner_output_2)\r\n",
        "\r\n",
        "train_regret_loser_2 = min_max_array(regret_loser_2)\r\n",
        "train_regret_winner_2 = min_max_array(regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2 = min(train_regret_loser_2)\r\n",
        "min_train_regret_winner_2 = min(train_regret_winner_2)\r\n",
        "\r\n",
        "min_train_regret_loser_2, min_train_regret_winner_2"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.63379973658395, -7.588999193182263)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glrTGcpAuwFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "370bf570-5618-49d9-aea1-064859bdf13c"
      },
      "source": [
        "### Training regret minimization: run number = 3\r\n",
        "\r\n",
        "loser_output_3 = np.append(np.max(loser_3.GP.y[0:n_init]),loser_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_3 = np.append(np.max(winner_3.GP.y[0:n_init]),winner_3.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_3 = np.log(y_global_orig - loser_output_3)\r\n",
        "regret_winner_3 = np.log(y_global_orig - winner_output_3)\r\n",
        "\r\n",
        "train_regret_loser_3 = min_max_array(regret_loser_3)\r\n",
        "train_regret_winner_3 = min_max_array(regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3 = min(train_regret_loser_3)\r\n",
        "min_train_regret_winner_3 = min(train_regret_winner_3)\r\n",
        "\r\n",
        "min_train_regret_loser_3, min_train_regret_winner_3"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.219348312441808, -2.348216315838344)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaRwfbxeuwIR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57fc4187-b289-481f-f1a3-ca6cf9b0af7c"
      },
      "source": [
        "### Training regret minimization: run number = 4\r\n",
        "\r\n",
        "loser_output_4 = np.append(np.max(loser_4.GP.y[0:n_init]),loser_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_4 = np.append(np.max(winner_4.GP.y[0:n_init]),winner_4.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_4 = np.log(y_global_orig - loser_output_4)\r\n",
        "regret_winner_4 = np.log(y_global_orig - winner_output_4)\r\n",
        "\r\n",
        "train_regret_loser_4 = min_max_array(regret_loser_4)\r\n",
        "train_regret_winner_4 = min_max_array(regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4 = min(train_regret_loser_4)\r\n",
        "min_train_regret_winner_4 = min(train_regret_winner_4)\r\n",
        "\r\n",
        "min_train_regret_loser_4, min_train_regret_winner_4"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.581305235262819, -9.483577610242781)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nb5NkfyuwKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b51c2cb5-281d-4a8d-9b20-332f0807cec7"
      },
      "source": [
        "### Training regret minimization: run number = 5\r\n",
        "\r\n",
        "loser_output_5 = np.append(np.max(loser_5.GP.y[0:n_init]),loser_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_5 = np.append(np.max(winner_5.GP.y[0:n_init]),winner_5.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_5 = np.log(y_global_orig - loser_output_5)\r\n",
        "regret_winner_5 = np.log(y_global_orig - winner_output_5)\r\n",
        "\r\n",
        "train_regret_loser_5 = min_max_array(regret_loser_5)\r\n",
        "train_regret_winner_5 = min_max_array(regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5 = min(train_regret_loser_5)\r\n",
        "min_train_regret_winner_5 = min(train_regret_winner_5)\r\n",
        "\r\n",
        "min_train_regret_loser_5, min_train_regret_winner_5"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.667298642056275, -2.295577923353327)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0Q-WfXbuwNg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e80de399-54d6-486c-94ac-6bb82991698f"
      },
      "source": [
        "### Training regret minimization: run number = 6\r\n",
        "\r\n",
        "loser_output_6 = np.append(np.max(loser_6.GP.y[0:n_init]),loser_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_6 = np.append(np.max(winner_6.GP.y[0:n_init]),winner_6.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_6 = np.log(y_global_orig - loser_output_6)\r\n",
        "regret_winner_6 = np.log(y_global_orig - winner_output_6)\r\n",
        "\r\n",
        "train_regret_loser_6 = min_max_array(regret_loser_6)\r\n",
        "train_regret_winner_6 = min_max_array(regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6 = min(train_regret_loser_6)\r\n",
        "min_train_regret_winner_6 = min(train_regret_winner_6)\r\n",
        "\r\n",
        "min_train_regret_loser_6, min_train_regret_winner_6"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-6.48362698855227, -6.764504030108557)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqqS7VLcuwPy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbde771c-0e67-4076-a565-99232b79ebec"
      },
      "source": [
        "### Training regret minimization: run number = 7\r\n",
        "\r\n",
        "loser_output_7 = np.append(np.max(loser_7.GP.y[0:n_init]),loser_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_7 = np.append(np.max(winner_7.GP.y[0:n_init]),winner_7.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_7 = np.log(y_global_orig - loser_output_7)\r\n",
        "regret_winner_7 = np.log(y_global_orig - winner_output_7)\r\n",
        "\r\n",
        "train_regret_loser_7 = min_max_array(regret_loser_7)\r\n",
        "train_regret_winner_7 = min_max_array(regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7 = min(train_regret_loser_7)\r\n",
        "min_train_regret_winner_7 = min(train_regret_winner_7)\r\n",
        "\r\n",
        "min_train_regret_loser_7, min_train_regret_winner_7"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.909225293887742, -3.014807637622294)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOi5iX8guwSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f695b432-e59c-40d6-a093-1769d0986e8f"
      },
      "source": [
        "### Training regret minimization: run number = 8\r\n",
        "\r\n",
        "loser_output_8 = np.append(np.max(loser_8.GP.y[0:n_init]),loser_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_8 = np.append(np.max(winner_8.GP.y[0:n_init]),winner_8.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_8 = np.log(y_global_orig - loser_output_8)\r\n",
        "regret_winner_8 = np.log(y_global_orig - winner_output_8)\r\n",
        "\r\n",
        "train_regret_loser_8 = min_max_array(regret_loser_8)\r\n",
        "train_regret_winner_8 = min_max_array(regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8 = min(train_regret_loser_8)\r\n",
        "min_train_regret_winner_8 = min(train_regret_winner_8)\r\n",
        "\r\n",
        "min_train_regret_loser_8, min_train_regret_winner_8"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.753182791297979, -1.9564601119117921)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFVApeazuwU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646b5654-bc56-4188-bb18-14d0eb34bea3"
      },
      "source": [
        "### Training regret minimization: run number = 9\r\n",
        "\r\n",
        "loser_output_9 = np.append(np.max(loser_9.GP.y[0:n_init]),loser_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_9 = np.append(np.max(winner_9.GP.y[0:n_init]),winner_9.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_9 = np.log(y_global_orig - loser_output_9)\r\n",
        "regret_winner_9 = np.log(y_global_orig - winner_output_9)\r\n",
        "\r\n",
        "train_regret_loser_9 = min_max_array(regret_loser_9)\r\n",
        "train_regret_winner_9 = min_max_array(regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9 = min(train_regret_loser_9)\r\n",
        "min_train_regret_winner_9 = min(train_regret_winner_9)\r\n",
        "\r\n",
        "min_train_regret_loser_9, min_train_regret_winner_9"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.363942292904479, -8.947387900136846)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g92jk9WJuwXb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420741ab-f7a3-47c9-8b19-2c401ce5bf12"
      },
      "source": [
        "### Training regret minimization: run number = 10\r\n",
        "\r\n",
        "loser_output_10 = np.append(np.max(loser_10.GP.y[0:n_init]),loser_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_10 = np.append(np.max(winner_10.GP.y[0:n_init]),winner_10.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_10 = np.log(y_global_orig - loser_output_10)\r\n",
        "regret_winner_10 = np.log(y_global_orig - winner_output_10)\r\n",
        "\r\n",
        "train_regret_loser_10 = min_max_array(regret_loser_10)\r\n",
        "train_regret_winner_10 = min_max_array(regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10 = min(train_regret_loser_10)\r\n",
        "min_train_regret_winner_10 = min(train_regret_winner_10)\r\n",
        "\r\n",
        "min_train_regret_loser_10, min_train_regret_winner_10"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.650906541123928, -2.530227553905053)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmcF1x-NuwZz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad3590d5-9d91-4b9b-ad3d-cf1182c1dbcd"
      },
      "source": [
        "### Training regret minimization: run number = 11\r\n",
        "\r\n",
        "loser_output_11 = np.append(np.max(loser_11.GP.y[0:n_init]),loser_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_11 = np.append(np.max(winner_11.GP.y[0:n_init]),winner_11.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_11 = np.log(y_global_orig - loser_output_11)\r\n",
        "regret_winner_11 = np.log(y_global_orig - winner_output_11)\r\n",
        "\r\n",
        "train_regret_loser_11 = min_max_array(regret_loser_11)\r\n",
        "train_regret_winner_11 = min_max_array(regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11 = min(train_regret_loser_11)\r\n",
        "min_train_regret_winner_11 = min(train_regret_winner_11)\r\n",
        "\r\n",
        "min_train_regret_loser_11, min_train_regret_winner_11"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.174643386313633, -0.7825473231511763)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8axVhb6uwcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "275d9daf-802b-4a12-df1e-a87d7d3d9903"
      },
      "source": [
        "### Training regret minimization: run number = 12\r\n",
        "\r\n",
        "loser_output_12 = np.append(np.max(loser_12.GP.y[0:n_init]),loser_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_12 = np.append(np.max(winner_12.GP.y[0:n_init]),winner_12.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_12 = np.log(y_global_orig - loser_output_12)\r\n",
        "regret_winner_12 = np.log(y_global_orig - winner_output_12)\r\n",
        "\r\n",
        "train_regret_loser_12 = min_max_array(regret_loser_12)\r\n",
        "train_regret_winner_12 = min_max_array(regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12 = min(train_regret_loser_12)\r\n",
        "min_train_regret_winner_12 = min(train_regret_winner_12)\r\n",
        "\r\n",
        "min_train_regret_loser_12, min_train_regret_winner_12"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-4.9975693090031, -1.1232257351332933)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzlrL8XFuwfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c3464f4-6d23-41e4-da22-a55ff0884d84"
      },
      "source": [
        "### Training regret minimization: run number = 13\r\n",
        "\r\n",
        "loser_output_13 = np.append(np.max(loser_13.GP.y[0:n_init]),loser_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_13 = np.append(np.max(winner_13.GP.y[0:n_init]),winner_13.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_13 = np.log(y_global_orig - loser_output_13)\r\n",
        "regret_winner_13 = np.log(y_global_orig - winner_output_13)\r\n",
        "\r\n",
        "train_regret_loser_13 = min_max_array(regret_loser_13)\r\n",
        "train_regret_winner_13 = min_max_array(regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13 = min(train_regret_loser_13)\r\n",
        "min_train_regret_winner_13 = min(train_regret_winner_13)\r\n",
        "\r\n",
        "min_train_regret_loser_13, min_train_regret_winner_13"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-8.708311010744215, -9.087390227248882)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uZlLJ1quwh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3a6e89f-0c7e-4316-ddfa-de7335008221"
      },
      "source": [
        "### Training regret minimization: run number = 14\r\n",
        "\r\n",
        "loser_output_14 = np.append(np.max(loser_14.GP.y[0:n_init]),loser_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_14 = np.append(np.max(winner_14.GP.y[0:n_init]),winner_14.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_14 = np.log(y_global_orig - loser_output_14)\r\n",
        "regret_winner_14 = np.log(y_global_orig - winner_output_14)\r\n",
        "\r\n",
        "train_regret_loser_14 = min_max_array(regret_loser_14)\r\n",
        "train_regret_winner_14 = min_max_array(regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14 = min(train_regret_loser_14)\r\n",
        "min_train_regret_winner_14 = min(train_regret_winner_14)\r\n",
        "\r\n",
        "min_train_regret_loser_14, min_train_regret_winner_14"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.310681494569531, -1.718860586130575)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVBcHRiMuwjx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4443915c-0168-442f-c3bc-b2c606483a14"
      },
      "source": [
        "### Training regret minimization: run number = 15\r\n",
        "\r\n",
        "loser_output_15 = np.append(np.max(loser_15.GP.y[0:n_init]),loser_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_15 = np.append(np.max(winner_15.GP.y[0:n_init]),winner_15.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_15 = np.log(y_global_orig - loser_output_15)\r\n",
        "regret_winner_15 = np.log(y_global_orig - winner_output_15)\r\n",
        "\r\n",
        "train_regret_loser_15 = min_max_array(regret_loser_15)\r\n",
        "train_regret_winner_15 = min_max_array(regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15 = min(train_regret_loser_15)\r\n",
        "min_train_regret_winner_15 = min(train_regret_winner_15)\r\n",
        "\r\n",
        "min_train_regret_loser_15, min_train_regret_winner_15"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-3.622138313873521, -3.003547910297042)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8nZ_DrKuwnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5cce97a-eb60-49fa-c19c-4b1e217d7ed0"
      },
      "source": [
        "### Training regret minimization: run number = 16\r\n",
        "\r\n",
        "loser_output_16 = np.append(np.max(loser_16.GP.y[0:n_init]),loser_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_16 = np.append(np.max(winner_16.GP.y[0:n_init]),winner_16.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_16 = np.log(y_global_orig - loser_output_16)\r\n",
        "regret_winner_16 = np.log(y_global_orig - winner_output_16)\r\n",
        "\r\n",
        "train_regret_loser_16 = min_max_array(regret_loser_16)\r\n",
        "train_regret_winner_16 = min_max_array(regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16 = min(train_regret_loser_16)\r\n",
        "min_train_regret_winner_16 = min(train_regret_winner_16)\r\n",
        "\r\n",
        "min_train_regret_loser_16, min_train_regret_winner_16"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-6.1095947512514375, -2.313822120725613)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg6qzzQFuwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "664da82a-a49f-4ccc-e989-fb7761c34b75"
      },
      "source": [
        "### Training regret minimization: run number = 17\r\n",
        "\r\n",
        "loser_output_17 = np.append(np.max(loser_17.GP.y[0:n_init]),loser_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_17 = np.append(np.max(winner_17.GP.y[0:n_init]),winner_17.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_17 = np.log(y_global_orig - loser_output_17)\r\n",
        "regret_winner_17 = np.log(y_global_orig - winner_output_17)\r\n",
        "\r\n",
        "train_regret_loser_17 = min_max_array(regret_loser_17)\r\n",
        "train_regret_winner_17 = min_max_array(regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17 = min(train_regret_loser_17)\r\n",
        "min_train_regret_winner_17 = min(train_regret_winner_17)\r\n",
        "\r\n",
        "min_train_regret_loser_17, min_train_regret_winner_17"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.893161531911135, -1.3296190252912352)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrRtDLMFuwsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65ac94c3-3634-46d6-e4e2-6b160b968998"
      },
      "source": [
        "### Training regret minimization: run number = 18\r\n",
        "\r\n",
        "loser_output_18 = np.append(np.max(loser_18.GP.y[0:n_init]),loser_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_18 = np.append(np.max(winner_18.GP.y[0:n_init]),winner_18.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_18 = np.log(y_global_orig - loser_output_18)\r\n",
        "regret_winner_18 = np.log(y_global_orig - winner_output_18)\r\n",
        "\r\n",
        "train_regret_loser_18 = min_max_array(regret_loser_18)\r\n",
        "train_regret_winner_18 = min_max_array(regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18 = min(train_regret_loser_18)\r\n",
        "min_train_regret_winner_18 = min(train_regret_winner_18)\r\n",
        "\r\n",
        "min_train_regret_loser_18, min_train_regret_winner_18"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-5.0344672575848755, -6.6748338337579245)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkmSu4CUuwuh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deda2ce6-9fd7-410f-d9a3-34585291251f"
      },
      "source": [
        "### Training regret minimization: run number = 19\r\n",
        "\r\n",
        "loser_output_19 = np.append(np.max(loser_19.GP.y[0:n_init]),loser_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_19 = np.append(np.max(winner_19.GP.y[0:n_init]),winner_19.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_19 = np.log(y_global_orig - loser_output_19)\r\n",
        "regret_winner_19 = np.log(y_global_orig - winner_output_19)\r\n",
        "\r\n",
        "train_regret_loser_19 = min_max_array(regret_loser_19)\r\n",
        "train_regret_winner_19 = min_max_array(regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19 = min(train_regret_loser_19)\r\n",
        "min_train_regret_winner_19 = min(train_regret_winner_19)\r\n",
        "\r\n",
        "min_train_regret_loser_19, min_train_regret_winner_19"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-7.204853323630892, -4.908326639479538)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eymecjwkuwxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e42ba98a-7e47-44a9-fced-5ccc0714f6eb"
      },
      "source": [
        "### Training regret minimization: run number = 20\r\n",
        "\r\n",
        "loser_output_20 = np.append(np.max(loser_20.GP.y[0:n_init]),loser_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "winner_output_20 = np.append(np.max(winner_20.GP.y[0:n_init]),winner_20.GP.y[n_init:(n_init+max_iter)]) \r\n",
        "\r\n",
        "regret_loser_20 = np.log(y_global_orig - loser_output_20)\r\n",
        "regret_winner_20 = np.log(y_global_orig - winner_output_20)\r\n",
        "\r\n",
        "train_regret_loser_20 = min_max_array(regret_loser_20)\r\n",
        "train_regret_winner_20 = min_max_array(regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20 = min(train_regret_loser_20)\r\n",
        "min_train_regret_winner_20 = min(train_regret_winner_20)\r\n",
        "\r\n",
        "min_train_regret_loser_20, min_train_regret_winner_20"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-6.378142012561249, -0.8036988725044196)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5P6kq2Kuw0d"
      },
      "source": [
        "# Iteration1 :\r\n",
        "\r\n",
        "slice1 = 0\r\n",
        "\r\n",
        "loser1 = [train_regret_loser_1[slice1],\r\n",
        "       train_regret_loser_2[slice1],\r\n",
        "       train_regret_loser_3[slice1],\r\n",
        "       train_regret_loser_4[slice1],\r\n",
        "       train_regret_loser_5[slice1],\r\n",
        "       train_regret_loser_6[slice1],\r\n",
        "       train_regret_loser_7[slice1],\r\n",
        "       train_regret_loser_8[slice1],\r\n",
        "       train_regret_loser_9[slice1],\r\n",
        "       train_regret_loser_10[slice1],\r\n",
        "       train_regret_loser_11[slice1],\r\n",
        "       train_regret_loser_12[slice1],\r\n",
        "       train_regret_loser_13[slice1],\r\n",
        "       train_regret_loser_14[slice1],\r\n",
        "       train_regret_loser_15[slice1],\r\n",
        "       train_regret_loser_16[slice1],\r\n",
        "       train_regret_loser_17[slice1],\r\n",
        "       train_regret_loser_18[slice1],\r\n",
        "       train_regret_loser_19[slice1],\r\n",
        "       train_regret_loser_20[slice1]]\r\n",
        "\r\n",
        "winner1 = [train_regret_winner_1[slice1],\r\n",
        "       train_regret_winner_2[slice1],\r\n",
        "       train_regret_winner_3[slice1],\r\n",
        "       train_regret_winner_4[slice1],\r\n",
        "       train_regret_winner_5[slice1],\r\n",
        "       train_regret_winner_6[slice1],\r\n",
        "       train_regret_winner_7[slice1],\r\n",
        "       train_regret_winner_8[slice1],\r\n",
        "       train_regret_winner_9[slice1],\r\n",
        "       train_regret_winner_10[slice1],\r\n",
        "       train_regret_winner_11[slice1],\r\n",
        "       train_regret_winner_12[slice1],\r\n",
        "       train_regret_winner_13[slice1],\r\n",
        "       train_regret_winner_14[slice1],\r\n",
        "       train_regret_winner_15[slice1],\r\n",
        "       train_regret_winner_16[slice1],\r\n",
        "       train_regret_winner_17[slice1],\r\n",
        "       train_regret_winner_18[slice1],\r\n",
        "       train_regret_winner_19[slice1],\r\n",
        "       train_regret_winner_20[slice1]]\r\n",
        "\r\n",
        "loser1_results = pd.DataFrame(loser1).sort_values(by=[0], ascending=False)\r\n",
        "winner1_results = pd.DataFrame(winner1).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser1 = np.asarray(loser1_results[4:5][0])[0]\r\n",
        "median_loser1 = np.asarray(loser1_results[9:10][0])[0]\r\n",
        "upper_loser1 = np.asarray(loser1_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner1 = np.asarray(winner1_results[4:5][0])[0]\r\n",
        "median_winner1 = np.asarray(winner1_results[9:10][0])[0]\r\n",
        "upper_winner1 = np.asarray(winner1_results[14:15][0])[0]"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXcnj2pNuw3X"
      },
      "source": [
        "# Iteration11 :\r\n",
        "\r\n",
        "slice11 = 10\r\n",
        "\r\n",
        "loser11 = [train_regret_loser_1[slice11],\r\n",
        "       train_regret_loser_2[slice11],\r\n",
        "       train_regret_loser_3[slice11],\r\n",
        "       train_regret_loser_4[slice11],\r\n",
        "       train_regret_loser_5[slice11],\r\n",
        "       train_regret_loser_6[slice11],\r\n",
        "       train_regret_loser_7[slice11],\r\n",
        "       train_regret_loser_8[slice11],\r\n",
        "       train_regret_loser_9[slice11],\r\n",
        "       train_regret_loser_10[slice11],\r\n",
        "       train_regret_loser_11[slice11],\r\n",
        "       train_regret_loser_12[slice11],\r\n",
        "       train_regret_loser_13[slice11],\r\n",
        "       train_regret_loser_14[slice11],\r\n",
        "       train_regret_loser_15[slice11],\r\n",
        "       train_regret_loser_16[slice11],\r\n",
        "       train_regret_loser_17[slice11],\r\n",
        "       train_regret_loser_18[slice11],\r\n",
        "       train_regret_loser_19[slice11],\r\n",
        "       train_regret_loser_20[slice11]]\r\n",
        "\r\n",
        "winner11 = [train_regret_winner_1[slice11],\r\n",
        "       train_regret_winner_2[slice11],\r\n",
        "       train_regret_winner_3[slice11],\r\n",
        "       train_regret_winner_4[slice11],\r\n",
        "       train_regret_winner_5[slice11],\r\n",
        "       train_regret_winner_6[slice11],\r\n",
        "       train_regret_winner_7[slice11],\r\n",
        "       train_regret_winner_8[slice11],\r\n",
        "       train_regret_winner_9[slice11],\r\n",
        "       train_regret_winner_10[slice11],\r\n",
        "       train_regret_winner_11[slice11],\r\n",
        "       train_regret_winner_12[slice11],\r\n",
        "       train_regret_winner_13[slice11],\r\n",
        "       train_regret_winner_14[slice11],\r\n",
        "       train_regret_winner_15[slice11],\r\n",
        "       train_regret_winner_16[slice11],\r\n",
        "       train_regret_winner_17[slice11],\r\n",
        "       train_regret_winner_18[slice11],\r\n",
        "       train_regret_winner_19[slice11],\r\n",
        "       train_regret_winner_20[slice11]]\r\n",
        "\r\n",
        "loser11_results = pd.DataFrame(loser11).sort_values(by=[0], ascending=False)\r\n",
        "winner11_results = pd.DataFrame(winner11).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser11 = np.asarray(loser11_results[4:5][0])[0]\r\n",
        "median_loser11 = np.asarray(loser11_results[9:10][0])[0]\r\n",
        "upper_loser11 = np.asarray(loser11_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner11 = np.asarray(winner11_results[4:5][0])[0]\r\n",
        "median_winner11 = np.asarray(winner11_results[9:10][0])[0]\r\n",
        "upper_winner11 = np.asarray(winner11_results[14:15][0])[0]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxQa6BAVuw6L"
      },
      "source": [
        "# Iteration21 :\r\n",
        "\r\n",
        "slice21 = 20\r\n",
        "\r\n",
        "loser21 = [train_regret_loser_1[slice21],\r\n",
        "       train_regret_loser_2[slice21],\r\n",
        "       train_regret_loser_3[slice21],\r\n",
        "       train_regret_loser_4[slice21],\r\n",
        "       train_regret_loser_5[slice21],\r\n",
        "       train_regret_loser_6[slice21],\r\n",
        "       train_regret_loser_7[slice21],\r\n",
        "       train_regret_loser_8[slice21],\r\n",
        "       train_regret_loser_9[slice21],\r\n",
        "       train_regret_loser_10[slice21],\r\n",
        "       train_regret_loser_11[slice21],\r\n",
        "       train_regret_loser_12[slice21],\r\n",
        "       train_regret_loser_13[slice21],\r\n",
        "       train_regret_loser_14[slice21],\r\n",
        "       train_regret_loser_15[slice21],\r\n",
        "       train_regret_loser_16[slice21],\r\n",
        "       train_regret_loser_17[slice21],\r\n",
        "       train_regret_loser_18[slice21],\r\n",
        "       train_regret_loser_19[slice21],\r\n",
        "       train_regret_loser_20[slice21]]\r\n",
        "\r\n",
        "winner21 = [train_regret_winner_1[slice21],\r\n",
        "       train_regret_winner_2[slice21],\r\n",
        "       train_regret_winner_3[slice21],\r\n",
        "       train_regret_winner_4[slice21],\r\n",
        "       train_regret_winner_5[slice21],\r\n",
        "       train_regret_winner_6[slice21],\r\n",
        "       train_regret_winner_7[slice21],\r\n",
        "       train_regret_winner_8[slice21],\r\n",
        "       train_regret_winner_9[slice21],\r\n",
        "       train_regret_winner_10[slice21],\r\n",
        "       train_regret_winner_11[slice21],\r\n",
        "       train_regret_winner_12[slice21],\r\n",
        "       train_regret_winner_13[slice21],\r\n",
        "       train_regret_winner_14[slice21],\r\n",
        "       train_regret_winner_15[slice21],\r\n",
        "       train_regret_winner_16[slice21],\r\n",
        "       train_regret_winner_17[slice21],\r\n",
        "       train_regret_winner_18[slice21],\r\n",
        "       train_regret_winner_19[slice21],\r\n",
        "       train_regret_winner_20[slice21]]\r\n",
        "\r\n",
        "loser21_results = pd.DataFrame(loser21).sort_values(by=[0], ascending=False)\r\n",
        "winner21_results = pd.DataFrame(winner21).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser21 = np.asarray(loser21_results[4:5][0])[0]\r\n",
        "median_loser21 = np.asarray(loser21_results[9:10][0])[0]\r\n",
        "upper_loser21 = np.asarray(loser21_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner21 = np.asarray(winner21_results[4:5][0])[0]\r\n",
        "median_winner21 = np.asarray(winner21_results[9:10][0])[0]\r\n",
        "upper_winner21 = np.asarray(winner21_results[14:15][0])[0]"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wdJdIsWuw8r"
      },
      "source": [
        "# Iteration31 :\r\n",
        "\r\n",
        "slice31 = 30\r\n",
        "\r\n",
        "loser31 = [train_regret_loser_1[slice31],\r\n",
        "       train_regret_loser_2[slice31],\r\n",
        "       train_regret_loser_3[slice31],\r\n",
        "       train_regret_loser_4[slice31],\r\n",
        "       train_regret_loser_5[slice31],\r\n",
        "       train_regret_loser_6[slice31],\r\n",
        "       train_regret_loser_7[slice31],\r\n",
        "       train_regret_loser_8[slice31],\r\n",
        "       train_regret_loser_9[slice31],\r\n",
        "       train_regret_loser_10[slice31],\r\n",
        "       train_regret_loser_11[slice31],\r\n",
        "       train_regret_loser_12[slice31],\r\n",
        "       train_regret_loser_13[slice31],\r\n",
        "       train_regret_loser_14[slice31],\r\n",
        "       train_regret_loser_15[slice31],\r\n",
        "       train_regret_loser_16[slice31],\r\n",
        "       train_regret_loser_17[slice31],\r\n",
        "       train_regret_loser_18[slice31],\r\n",
        "       train_regret_loser_19[slice31],\r\n",
        "       train_regret_loser_20[slice31]]\r\n",
        "\r\n",
        "winner31 = [train_regret_winner_1[slice31],\r\n",
        "       train_regret_winner_2[slice31],\r\n",
        "       train_regret_winner_3[slice31],\r\n",
        "       train_regret_winner_4[slice31],\r\n",
        "       train_regret_winner_5[slice31],\r\n",
        "       train_regret_winner_6[slice31],\r\n",
        "       train_regret_winner_7[slice31],\r\n",
        "       train_regret_winner_8[slice31],\r\n",
        "       train_regret_winner_9[slice31],\r\n",
        "       train_regret_winner_10[slice31],\r\n",
        "       train_regret_winner_11[slice31],\r\n",
        "       train_regret_winner_12[slice31],\r\n",
        "       train_regret_winner_13[slice31],\r\n",
        "       train_regret_winner_14[slice31],\r\n",
        "       train_regret_winner_15[slice31],\r\n",
        "       train_regret_winner_16[slice31],\r\n",
        "       train_regret_winner_17[slice31],\r\n",
        "       train_regret_winner_18[slice31],\r\n",
        "       train_regret_winner_19[slice31],\r\n",
        "       train_regret_winner_20[slice31]]\r\n",
        "\r\n",
        "loser31_results = pd.DataFrame(loser31).sort_values(by=[0], ascending=False)\r\n",
        "winner31_results = pd.DataFrame(winner31).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser31 = np.asarray(loser31_results[4:5][0])[0]\r\n",
        "median_loser31 = np.asarray(loser31_results[9:10][0])[0]\r\n",
        "upper_loser31 = np.asarray(loser31_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner31 = np.asarray(winner31_results[4:5][0])[0]\r\n",
        "median_winner31 = np.asarray(winner31_results[9:10][0])[0]\r\n",
        "upper_winner31 = np.asarray(winner31_results[14:15][0])[0]"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vWZBDufuw_Z"
      },
      "source": [
        "# Iteration41 :\r\n",
        "\r\n",
        "slice41 = 40\r\n",
        "\r\n",
        "loser41 = [train_regret_loser_1[slice41],\r\n",
        "       train_regret_loser_2[slice41],\r\n",
        "       train_regret_loser_3[slice41],\r\n",
        "       train_regret_loser_4[slice41],\r\n",
        "       train_regret_loser_5[slice41],\r\n",
        "       train_regret_loser_6[slice41],\r\n",
        "       train_regret_loser_7[slice41],\r\n",
        "       train_regret_loser_8[slice41],\r\n",
        "       train_regret_loser_9[slice41],\r\n",
        "       train_regret_loser_10[slice41],\r\n",
        "       train_regret_loser_11[slice41],\r\n",
        "       train_regret_loser_12[slice41],\r\n",
        "       train_regret_loser_13[slice41],\r\n",
        "       train_regret_loser_14[slice41],\r\n",
        "       train_regret_loser_15[slice41],\r\n",
        "       train_regret_loser_16[slice41],\r\n",
        "       train_regret_loser_17[slice41],\r\n",
        "       train_regret_loser_18[slice41],\r\n",
        "       train_regret_loser_19[slice41],\r\n",
        "       train_regret_loser_20[slice41]]\r\n",
        "\r\n",
        "winner41 = [train_regret_winner_1[slice41],\r\n",
        "       train_regret_winner_2[slice41],\r\n",
        "       train_regret_winner_3[slice41],\r\n",
        "       train_regret_winner_4[slice41],\r\n",
        "       train_regret_winner_5[slice41],\r\n",
        "       train_regret_winner_6[slice41],\r\n",
        "       train_regret_winner_7[slice41],\r\n",
        "       train_regret_winner_8[slice41],\r\n",
        "       train_regret_winner_9[slice41],\r\n",
        "       train_regret_winner_10[slice41],\r\n",
        "       train_regret_winner_11[slice41],\r\n",
        "       train_regret_winner_12[slice41],\r\n",
        "       train_regret_winner_13[slice41],\r\n",
        "       train_regret_winner_14[slice41],\r\n",
        "       train_regret_winner_15[slice41],\r\n",
        "       train_regret_winner_16[slice41],\r\n",
        "       train_regret_winner_17[slice41],\r\n",
        "       train_regret_winner_18[slice41],\r\n",
        "       train_regret_winner_19[slice41],\r\n",
        "       train_regret_winner_20[slice41]]\r\n",
        "\r\n",
        "loser41_results = pd.DataFrame(loser41).sort_values(by=[0], ascending=False)\r\n",
        "winner41_results = pd.DataFrame(winner41).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser41 = np.asarray(loser41_results[4:5][0])[0]\r\n",
        "median_loser41 = np.asarray(loser41_results[9:10][0])[0]\r\n",
        "upper_loser41 = np.asarray(loser41_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner41 = np.asarray(winner41_results[4:5][0])[0]\r\n",
        "median_winner41 = np.asarray(winner41_results[9:10][0])[0]\r\n",
        "upper_winner41 = np.asarray(winner41_results[14:15][0])[0]"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwi_GzjiuxB6"
      },
      "source": [
        "# Iteration51 :\r\n",
        "\r\n",
        "slice51 = 50\r\n",
        "\r\n",
        "loser51 = [train_regret_loser_1[slice51],\r\n",
        "       train_regret_loser_2[slice51],\r\n",
        "       train_regret_loser_3[slice51],\r\n",
        "       train_regret_loser_4[slice51],\r\n",
        "       train_regret_loser_5[slice51],\r\n",
        "       train_regret_loser_6[slice51],\r\n",
        "       train_regret_loser_7[slice51],\r\n",
        "       train_regret_loser_8[slice51],\r\n",
        "       train_regret_loser_9[slice51],\r\n",
        "       train_regret_loser_10[slice51],\r\n",
        "       train_regret_loser_11[slice51],\r\n",
        "       train_regret_loser_12[slice51],\r\n",
        "       train_regret_loser_13[slice51],\r\n",
        "       train_regret_loser_14[slice51],\r\n",
        "       train_regret_loser_15[slice51],\r\n",
        "       train_regret_loser_16[slice51],\r\n",
        "       train_regret_loser_17[slice51],\r\n",
        "       train_regret_loser_18[slice51],\r\n",
        "       train_regret_loser_19[slice51],\r\n",
        "       train_regret_loser_20[slice51]]\r\n",
        "\r\n",
        "winner51 = [train_regret_winner_1[slice51],\r\n",
        "       train_regret_winner_2[slice51],\r\n",
        "       train_regret_winner_3[slice51],\r\n",
        "       train_regret_winner_4[slice51],\r\n",
        "       train_regret_winner_5[slice51],\r\n",
        "       train_regret_winner_6[slice51],\r\n",
        "       train_regret_winner_7[slice51],\r\n",
        "       train_regret_winner_8[slice51],\r\n",
        "       train_regret_winner_9[slice51],\r\n",
        "       train_regret_winner_10[slice51],\r\n",
        "       train_regret_winner_11[slice51],\r\n",
        "       train_regret_winner_12[slice51],\r\n",
        "       train_regret_winner_13[slice51],\r\n",
        "       train_regret_winner_14[slice51],\r\n",
        "       train_regret_winner_15[slice51],\r\n",
        "       train_regret_winner_16[slice51],\r\n",
        "       train_regret_winner_17[slice51],\r\n",
        "       train_regret_winner_18[slice51],\r\n",
        "       train_regret_winner_19[slice51],\r\n",
        "       train_regret_winner_20[slice51]]\r\n",
        "\r\n",
        "loser51_results = pd.DataFrame(loser51).sort_values(by=[0], ascending=False)\r\n",
        "winner51_results = pd.DataFrame(winner51).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser51 = np.asarray(loser51_results[4:5][0])[0]\r\n",
        "median_loser51 = np.asarray(loser51_results[9:10][0])[0]\r\n",
        "upper_loser51 = np.asarray(loser51_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner51 = np.asarray(winner51_results[4:5][0])[0]\r\n",
        "median_winner51 = np.asarray(winner51_results[9:10][0])[0]\r\n",
        "upper_winner51 = np.asarray(winner51_results[14:15][0])[0]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk9nlS6puxFZ"
      },
      "source": [
        "# Iteration61 :\r\n",
        "\r\n",
        "slice61 = 60\r\n",
        "\r\n",
        "loser61 = [train_regret_loser_1[slice61],\r\n",
        "       train_regret_loser_2[slice61],\r\n",
        "       train_regret_loser_3[slice61],\r\n",
        "       train_regret_loser_4[slice61],\r\n",
        "       train_regret_loser_5[slice61],\r\n",
        "       train_regret_loser_6[slice61],\r\n",
        "       train_regret_loser_7[slice61],\r\n",
        "       train_regret_loser_8[slice61],\r\n",
        "       train_regret_loser_9[slice61],\r\n",
        "       train_regret_loser_10[slice61],\r\n",
        "       train_regret_loser_11[slice61],\r\n",
        "       train_regret_loser_12[slice61],\r\n",
        "       train_regret_loser_13[slice61],\r\n",
        "       train_regret_loser_14[slice61],\r\n",
        "       train_regret_loser_15[slice61],\r\n",
        "       train_regret_loser_16[slice61],\r\n",
        "       train_regret_loser_17[slice61],\r\n",
        "       train_regret_loser_18[slice61],\r\n",
        "       train_regret_loser_19[slice61],\r\n",
        "       train_regret_loser_20[slice61]]\r\n",
        "\r\n",
        "winner61 = [train_regret_winner_1[slice61],\r\n",
        "       train_regret_winner_2[slice61],\r\n",
        "       train_regret_winner_3[slice61],\r\n",
        "       train_regret_winner_4[slice61],\r\n",
        "       train_regret_winner_5[slice61],\r\n",
        "       train_regret_winner_6[slice61],\r\n",
        "       train_regret_winner_7[slice61],\r\n",
        "       train_regret_winner_8[slice61],\r\n",
        "       train_regret_winner_9[slice61],\r\n",
        "       train_regret_winner_10[slice61],\r\n",
        "       train_regret_winner_11[slice61],\r\n",
        "       train_regret_winner_12[slice61],\r\n",
        "       train_regret_winner_13[slice61],\r\n",
        "       train_regret_winner_14[slice61],\r\n",
        "       train_regret_winner_15[slice61],\r\n",
        "       train_regret_winner_16[slice61],\r\n",
        "       train_regret_winner_17[slice61],\r\n",
        "       train_regret_winner_18[slice61],\r\n",
        "       train_regret_winner_19[slice61],\r\n",
        "       train_regret_winner_20[slice61]]\r\n",
        "\r\n",
        "loser61_results = pd.DataFrame(loser61).sort_values(by=[0], ascending=False)\r\n",
        "winner61_results = pd.DataFrame(winner61).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser61 = np.asarray(loser61_results[4:5][0])[0]\r\n",
        "median_loser61 = np.asarray(loser61_results[9:10][0])[0]\r\n",
        "upper_loser61 = np.asarray(loser61_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner61 = np.asarray(winner61_results[4:5][0])[0]\r\n",
        "median_winner61 = np.asarray(winner61_results[9:10][0])[0]\r\n",
        "upper_winner61 = np.asarray(winner61_results[14:15][0])[0]"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMus9vDpuxHN"
      },
      "source": [
        "# Iteration71 :\r\n",
        "\r\n",
        "slice71 = 70\r\n",
        "\r\n",
        "loser71 = [train_regret_loser_1[slice71],\r\n",
        "       train_regret_loser_2[slice71],\r\n",
        "       train_regret_loser_3[slice71],\r\n",
        "       train_regret_loser_4[slice71],\r\n",
        "       train_regret_loser_5[slice71],\r\n",
        "       train_regret_loser_6[slice71],\r\n",
        "       train_regret_loser_7[slice71],\r\n",
        "       train_regret_loser_8[slice71],\r\n",
        "       train_regret_loser_9[slice71],\r\n",
        "       train_regret_loser_10[slice71],\r\n",
        "       train_regret_loser_11[slice71],\r\n",
        "       train_regret_loser_12[slice71],\r\n",
        "       train_regret_loser_13[slice71],\r\n",
        "       train_regret_loser_14[slice71],\r\n",
        "       train_regret_loser_15[slice71],\r\n",
        "       train_regret_loser_16[slice71],\r\n",
        "       train_regret_loser_17[slice71],\r\n",
        "       train_regret_loser_18[slice71],\r\n",
        "       train_regret_loser_19[slice71],\r\n",
        "       train_regret_loser_20[slice71]]\r\n",
        "\r\n",
        "winner71 = [train_regret_winner_1[slice71],\r\n",
        "       train_regret_winner_2[slice71],\r\n",
        "       train_regret_winner_3[slice71],\r\n",
        "       train_regret_winner_4[slice71],\r\n",
        "       train_regret_winner_5[slice71],\r\n",
        "       train_regret_winner_6[slice71],\r\n",
        "       train_regret_winner_7[slice71],\r\n",
        "       train_regret_winner_8[slice71],\r\n",
        "       train_regret_winner_9[slice71],\r\n",
        "       train_regret_winner_10[slice71],\r\n",
        "       train_regret_winner_11[slice71],\r\n",
        "       train_regret_winner_12[slice71],\r\n",
        "       train_regret_winner_13[slice71],\r\n",
        "       train_regret_winner_14[slice71],\r\n",
        "       train_regret_winner_15[slice71],\r\n",
        "       train_regret_winner_16[slice71],\r\n",
        "       train_regret_winner_17[slice71],\r\n",
        "       train_regret_winner_18[slice71],\r\n",
        "       train_regret_winner_19[slice71],\r\n",
        "       train_regret_winner_20[slice71]]\r\n",
        "\r\n",
        "loser71_results = pd.DataFrame(loser71).sort_values(by=[0], ascending=False)\r\n",
        "winner71_results = pd.DataFrame(winner71).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser71 = np.asarray(loser71_results[4:5][0])[0]\r\n",
        "median_loser71 = np.asarray(loser71_results[9:10][0])[0]\r\n",
        "upper_loser71 = np.asarray(loser71_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner71 = np.asarray(winner71_results[4:5][0])[0]\r\n",
        "median_winner71 = np.asarray(winner71_results[9:10][0])[0]\r\n",
        "upper_winner71 = np.asarray(winner71_results[14:15][0])[0]"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6Mg0x9vuxL4"
      },
      "source": [
        "# Iteration81 :\r\n",
        "\r\n",
        "slice81 = 80\r\n",
        "\r\n",
        "loser81 = [train_regret_loser_1[slice81],\r\n",
        "       train_regret_loser_2[slice81],\r\n",
        "       train_regret_loser_3[slice81],\r\n",
        "       train_regret_loser_4[slice81],\r\n",
        "       train_regret_loser_5[slice81],\r\n",
        "       train_regret_loser_6[slice81],\r\n",
        "       train_regret_loser_7[slice81],\r\n",
        "       train_regret_loser_8[slice81],\r\n",
        "       train_regret_loser_9[slice81],\r\n",
        "       train_regret_loser_10[slice81],\r\n",
        "       train_regret_loser_11[slice81],\r\n",
        "       train_regret_loser_12[slice81],\r\n",
        "       train_regret_loser_13[slice81],\r\n",
        "       train_regret_loser_14[slice81],\r\n",
        "       train_regret_loser_15[slice81],\r\n",
        "       train_regret_loser_16[slice81],\r\n",
        "       train_regret_loser_17[slice81],\r\n",
        "       train_regret_loser_18[slice81],\r\n",
        "       train_regret_loser_19[slice81],\r\n",
        "       train_regret_loser_20[slice81]]\r\n",
        "\r\n",
        "winner81 = [train_regret_winner_1[slice81],\r\n",
        "       train_regret_winner_2[slice81],\r\n",
        "       train_regret_winner_3[slice81],\r\n",
        "       train_regret_winner_4[slice81],\r\n",
        "       train_regret_winner_5[slice81],\r\n",
        "       train_regret_winner_6[slice81],\r\n",
        "       train_regret_winner_7[slice81],\r\n",
        "       train_regret_winner_8[slice81],\r\n",
        "       train_regret_winner_9[slice81],\r\n",
        "       train_regret_winner_10[slice81],\r\n",
        "       train_regret_winner_11[slice81],\r\n",
        "       train_regret_winner_12[slice81],\r\n",
        "       train_regret_winner_13[slice81],\r\n",
        "       train_regret_winner_14[slice81],\r\n",
        "       train_regret_winner_15[slice81],\r\n",
        "       train_regret_winner_16[slice81],\r\n",
        "       train_regret_winner_17[slice81],\r\n",
        "       train_regret_winner_18[slice81],\r\n",
        "       train_regret_winner_19[slice81],\r\n",
        "       train_regret_winner_20[slice81]]\r\n",
        "\r\n",
        "loser81_results = pd.DataFrame(loser81).sort_values(by=[0], ascending=False)\r\n",
        "winner81_results = pd.DataFrame(winner81).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser81 = np.asarray(loser81_results[4:5][0])[0]\r\n",
        "median_loser81 = np.asarray(loser81_results[9:10][0])[0]\r\n",
        "upper_loser81 = np.asarray(loser81_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner81 = np.asarray(winner81_results[4:5][0])[0]\r\n",
        "median_winner81 = np.asarray(winner81_results[9:10][0])[0]\r\n",
        "upper_winner81 = np.asarray(winner81_results[14:15][0])[0]"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gu9K8XlteV4"
      },
      "source": [
        "# Iteration91 :\r\n",
        "\r\n",
        "slice91 = 90\r\n",
        "\r\n",
        "loser91 = [train_regret_loser_1[slice91],\r\n",
        "       train_regret_loser_2[slice91],\r\n",
        "       train_regret_loser_3[slice91],\r\n",
        "       train_regret_loser_4[slice91],\r\n",
        "       train_regret_loser_5[slice91],\r\n",
        "       train_regret_loser_6[slice91],\r\n",
        "       train_regret_loser_7[slice91],\r\n",
        "       train_regret_loser_8[slice91],\r\n",
        "       train_regret_loser_9[slice91],\r\n",
        "       train_regret_loser_10[slice91],\r\n",
        "       train_regret_loser_11[slice91],\r\n",
        "       train_regret_loser_12[slice91],\r\n",
        "       train_regret_loser_13[slice91],\r\n",
        "       train_regret_loser_14[slice91],\r\n",
        "       train_regret_loser_15[slice91],\r\n",
        "       train_regret_loser_16[slice91],\r\n",
        "       train_regret_loser_17[slice91],\r\n",
        "       train_regret_loser_18[slice91],\r\n",
        "       train_regret_loser_19[slice91],\r\n",
        "       train_regret_loser_20[slice91]]\r\n",
        "\r\n",
        "winner91 = [train_regret_winner_1[slice91],\r\n",
        "       train_regret_winner_2[slice91],\r\n",
        "       train_regret_winner_3[slice91],\r\n",
        "       train_regret_winner_4[slice91],\r\n",
        "       train_regret_winner_5[slice91],\r\n",
        "       train_regret_winner_6[slice91],\r\n",
        "       train_regret_winner_7[slice91],\r\n",
        "       train_regret_winner_8[slice91],\r\n",
        "       train_regret_winner_9[slice91],\r\n",
        "       train_regret_winner_10[slice91],\r\n",
        "       train_regret_winner_11[slice91],\r\n",
        "       train_regret_winner_12[slice91],\r\n",
        "       train_regret_winner_13[slice91],\r\n",
        "       train_regret_winner_14[slice91],\r\n",
        "       train_regret_winner_15[slice91],\r\n",
        "       train_regret_winner_16[slice91],\r\n",
        "       train_regret_winner_17[slice91],\r\n",
        "       train_regret_winner_18[slice91],\r\n",
        "       train_regret_winner_19[slice91],\r\n",
        "       train_regret_winner_20[slice91]]\r\n",
        "\r\n",
        "loser91_results = pd.DataFrame(loser91).sort_values(by=[0], ascending=False)\r\n",
        "winner91_results = pd.DataFrame(winner91).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser91 = np.asarray(loser91_results[4:5][0])[0]\r\n",
        "median_loser91 = np.asarray(loser91_results[9:10][0])[0]\r\n",
        "upper_loser91 = np.asarray(loser91_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner91 = np.asarray(winner91_results[4:5][0])[0]\r\n",
        "median_winner91 = np.asarray(winner91_results[9:10][0])[0]\r\n",
        "upper_winner91 = np.asarray(winner91_results[14:15][0])[0]"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVVeft6steXv"
      },
      "source": [
        "# Iteration101 :\r\n",
        "\r\n",
        "slice101 = 100\r\n",
        "\r\n",
        "loser101 = [train_regret_loser_1[slice101],\r\n",
        "       train_regret_loser_2[slice101],\r\n",
        "       train_regret_loser_3[slice101],\r\n",
        "       train_regret_loser_4[slice101],\r\n",
        "       train_regret_loser_5[slice101],\r\n",
        "       train_regret_loser_6[slice101],\r\n",
        "       train_regret_loser_7[slice101],\r\n",
        "       train_regret_loser_8[slice101],\r\n",
        "       train_regret_loser_9[slice101],\r\n",
        "       train_regret_loser_10[slice101],\r\n",
        "       train_regret_loser_11[slice101],\r\n",
        "       train_regret_loser_12[slice101],\r\n",
        "       train_regret_loser_13[slice101],\r\n",
        "       train_regret_loser_14[slice101],\r\n",
        "       train_regret_loser_15[slice101],\r\n",
        "       train_regret_loser_16[slice101],\r\n",
        "       train_regret_loser_17[slice101],\r\n",
        "       train_regret_loser_18[slice101],\r\n",
        "       train_regret_loser_19[slice101],\r\n",
        "       train_regret_loser_20[slice101]]\r\n",
        "\r\n",
        "winner101 = [train_regret_winner_1[slice101],\r\n",
        "       train_regret_winner_2[slice101],\r\n",
        "       train_regret_winner_3[slice101],\r\n",
        "       train_regret_winner_4[slice101],\r\n",
        "       train_regret_winner_5[slice101],\r\n",
        "       train_regret_winner_6[slice101],\r\n",
        "       train_regret_winner_7[slice101],\r\n",
        "       train_regret_winner_8[slice101],\r\n",
        "       train_regret_winner_9[slice101],\r\n",
        "       train_regret_winner_10[slice101],\r\n",
        "       train_regret_winner_11[slice101],\r\n",
        "       train_regret_winner_12[slice101],\r\n",
        "       train_regret_winner_13[slice101],\r\n",
        "       train_regret_winner_14[slice101],\r\n",
        "       train_regret_winner_15[slice101],\r\n",
        "       train_regret_winner_16[slice101],\r\n",
        "       train_regret_winner_17[slice101],\r\n",
        "       train_regret_winner_18[slice101],\r\n",
        "       train_regret_winner_19[slice101],\r\n",
        "       train_regret_winner_20[slice101]]\r\n",
        "\r\n",
        "loser101_results = pd.DataFrame(loser101).sort_values(by=[0], ascending=False)\r\n",
        "winner101_results = pd.DataFrame(winner101).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser101 = np.asarray(loser101_results[4:5][0])[0]\r\n",
        "median_loser101 = np.asarray(loser101_results[9:10][0])[0]\r\n",
        "upper_loser101 = np.asarray(loser101_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner101 = np.asarray(winner101_results[4:5][0])[0]\r\n",
        "median_winner101 = np.asarray(winner101_results[9:10][0])[0]\r\n",
        "upper_winner101 = np.asarray(winner101_results[14:15][0])[0]"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maqbtTXXtebT"
      },
      "source": [
        "# Iteration2 :\r\n",
        "\r\n",
        "slice2 = 1\r\n",
        "\r\n",
        "loser2 = [train_regret_loser_1[slice2],\r\n",
        "       train_regret_loser_2[slice2],\r\n",
        "       train_regret_loser_3[slice2],\r\n",
        "       train_regret_loser_4[slice2],\r\n",
        "       train_regret_loser_5[slice2],\r\n",
        "       train_regret_loser_6[slice2],\r\n",
        "       train_regret_loser_7[slice2],\r\n",
        "       train_regret_loser_8[slice2],\r\n",
        "       train_regret_loser_9[slice2],\r\n",
        "       train_regret_loser_10[slice2],\r\n",
        "       train_regret_loser_11[slice2],\r\n",
        "       train_regret_loser_12[slice2],\r\n",
        "       train_regret_loser_13[slice2],\r\n",
        "       train_regret_loser_14[slice2],\r\n",
        "       train_regret_loser_15[slice2],\r\n",
        "       train_regret_loser_16[slice2],\r\n",
        "       train_regret_loser_17[slice2],\r\n",
        "       train_regret_loser_18[slice2],\r\n",
        "       train_regret_loser_19[slice2],\r\n",
        "       train_regret_loser_20[slice2]]\r\n",
        "\r\n",
        "winner2 = [train_regret_winner_1[slice2],\r\n",
        "       train_regret_winner_2[slice2],\r\n",
        "       train_regret_winner_3[slice2],\r\n",
        "       train_regret_winner_4[slice2],\r\n",
        "       train_regret_winner_5[slice2],\r\n",
        "       train_regret_winner_6[slice2],\r\n",
        "       train_regret_winner_7[slice2],\r\n",
        "       train_regret_winner_8[slice2],\r\n",
        "       train_regret_winner_9[slice2],\r\n",
        "       train_regret_winner_10[slice2],\r\n",
        "       train_regret_winner_11[slice2],\r\n",
        "       train_regret_winner_12[slice2],\r\n",
        "       train_regret_winner_13[slice2],\r\n",
        "       train_regret_winner_14[slice2],\r\n",
        "       train_regret_winner_15[slice2],\r\n",
        "       train_regret_winner_16[slice2],\r\n",
        "       train_regret_winner_17[slice2],\r\n",
        "       train_regret_winner_18[slice2],\r\n",
        "       train_regret_winner_19[slice2],\r\n",
        "       train_regret_winner_20[slice2]]\r\n",
        "\r\n",
        "loser2_results = pd.DataFrame(loser2).sort_values(by=[0], ascending=False)\r\n",
        "winner2_results = pd.DataFrame(winner2).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser2 = np.asarray(loser2_results[4:5][0])[0]\r\n",
        "median_loser2 = np.asarray(loser2_results[9:10][0])[0]\r\n",
        "upper_loser2 = np.asarray(loser2_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner2 = np.asarray(winner2_results[4:5][0])[0]\r\n",
        "median_winner2 = np.asarray(winner2_results[9:10][0])[0]\r\n",
        "upper_winner2 = np.asarray(winner2_results[14:15][0])[0]"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWIBh0dQw-ZM"
      },
      "source": [
        "# Iteration12 :\r\n",
        "\r\n",
        "slice12 = 11\r\n",
        "\r\n",
        "loser12 = [train_regret_loser_1[slice12],\r\n",
        "       train_regret_loser_2[slice12],\r\n",
        "       train_regret_loser_3[slice12],\r\n",
        "       train_regret_loser_4[slice12],\r\n",
        "       train_regret_loser_5[slice12],\r\n",
        "       train_regret_loser_6[slice12],\r\n",
        "       train_regret_loser_7[slice12],\r\n",
        "       train_regret_loser_8[slice12],\r\n",
        "       train_regret_loser_9[slice12],\r\n",
        "       train_regret_loser_10[slice12],\r\n",
        "       train_regret_loser_11[slice12],\r\n",
        "       train_regret_loser_12[slice12],\r\n",
        "       train_regret_loser_13[slice12],\r\n",
        "       train_regret_loser_14[slice12],\r\n",
        "       train_regret_loser_15[slice12],\r\n",
        "       train_regret_loser_16[slice12],\r\n",
        "       train_regret_loser_17[slice12],\r\n",
        "       train_regret_loser_18[slice12],\r\n",
        "       train_regret_loser_19[slice12],\r\n",
        "       train_regret_loser_20[slice12]]\r\n",
        "\r\n",
        "winner12 = [train_regret_winner_1[slice12],\r\n",
        "       train_regret_winner_2[slice12],\r\n",
        "       train_regret_winner_3[slice12],\r\n",
        "       train_regret_winner_4[slice12],\r\n",
        "       train_regret_winner_5[slice12],\r\n",
        "       train_regret_winner_6[slice12],\r\n",
        "       train_regret_winner_7[slice12],\r\n",
        "       train_regret_winner_8[slice12],\r\n",
        "       train_regret_winner_9[slice12],\r\n",
        "       train_regret_winner_10[slice12],\r\n",
        "       train_regret_winner_11[slice12],\r\n",
        "       train_regret_winner_12[slice12],\r\n",
        "       train_regret_winner_13[slice12],\r\n",
        "       train_regret_winner_14[slice12],\r\n",
        "       train_regret_winner_15[slice12],\r\n",
        "       train_regret_winner_16[slice12],\r\n",
        "       train_regret_winner_17[slice12],\r\n",
        "       train_regret_winner_18[slice12],\r\n",
        "       train_regret_winner_19[slice12],\r\n",
        "       train_regret_winner_20[slice12]]\r\n",
        "\r\n",
        "loser12_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner12_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser12 = np.asarray(loser12_results[4:5][0])[0]\r\n",
        "median_loser12 = np.asarray(loser12_results[9:10][0])[0]\r\n",
        "upper_loser12 = np.asarray(loser12_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner12 = np.asarray(winner12_results[4:5][0])[0]\r\n",
        "median_winner12 = np.asarray(winner12_results[9:10][0])[0]\r\n",
        "upper_winner12 = np.asarray(winner12_results[14:15][0])[0]"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjIhsei3w-cV"
      },
      "source": [
        "# Iteration22 :\r\n",
        "\r\n",
        "slice22 = 21\r\n",
        "\r\n",
        "loser22 = [train_regret_loser_1[slice22],\r\n",
        "       train_regret_loser_2[slice22],\r\n",
        "       train_regret_loser_3[slice22],\r\n",
        "       train_regret_loser_4[slice22],\r\n",
        "       train_regret_loser_5[slice22],\r\n",
        "       train_regret_loser_6[slice22],\r\n",
        "       train_regret_loser_7[slice22],\r\n",
        "       train_regret_loser_8[slice22],\r\n",
        "       train_regret_loser_9[slice22],\r\n",
        "       train_regret_loser_10[slice22],\r\n",
        "       train_regret_loser_11[slice22],\r\n",
        "       train_regret_loser_12[slice22],\r\n",
        "       train_regret_loser_13[slice22],\r\n",
        "       train_regret_loser_14[slice22],\r\n",
        "       train_regret_loser_15[slice22],\r\n",
        "       train_regret_loser_16[slice22],\r\n",
        "       train_regret_loser_17[slice22],\r\n",
        "       train_regret_loser_18[slice22],\r\n",
        "       train_regret_loser_19[slice22],\r\n",
        "       train_regret_loser_20[slice22]]\r\n",
        "\r\n",
        "winner22 = [train_regret_winner_1[slice22],\r\n",
        "       train_regret_winner_2[slice22],\r\n",
        "       train_regret_winner_3[slice22],\r\n",
        "       train_regret_winner_4[slice22],\r\n",
        "       train_regret_winner_5[slice22],\r\n",
        "       train_regret_winner_6[slice22],\r\n",
        "       train_regret_winner_7[slice22],\r\n",
        "       train_regret_winner_8[slice22],\r\n",
        "       train_regret_winner_9[slice22],\r\n",
        "       train_regret_winner_10[slice22],\r\n",
        "       train_regret_winner_11[slice22],\r\n",
        "       train_regret_winner_12[slice22],\r\n",
        "       train_regret_winner_13[slice22],\r\n",
        "       train_regret_winner_14[slice22],\r\n",
        "       train_regret_winner_15[slice22],\r\n",
        "       train_regret_winner_16[slice22],\r\n",
        "       train_regret_winner_17[slice22],\r\n",
        "       train_regret_winner_18[slice22],\r\n",
        "       train_regret_winner_19[slice22],\r\n",
        "       train_regret_winner_20[slice22]]\r\n",
        "\r\n",
        "loser22_results = pd.DataFrame(loser22).sort_values(by=[0], ascending=False)\r\n",
        "winner22_results = pd.DataFrame(winner22).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser22 = np.asarray(loser22_results[4:5][0])[0]\r\n",
        "median_loser22 = np.asarray(loser22_results[9:10][0])[0]\r\n",
        "upper_loser22 = np.asarray(loser22_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner22 = np.asarray(winner22_results[4:5][0])[0]\r\n",
        "median_winner22 = np.asarray(winner22_results[9:10][0])[0]\r\n",
        "upper_winner22 = np.asarray(winner22_results[14:15][0])[0]"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur9p9GkHw-fq"
      },
      "source": [
        "# Iteration32 :\r\n",
        "\r\n",
        "slice32 = 31\r\n",
        "\r\n",
        "loser32 = [train_regret_loser_1[slice32],\r\n",
        "       train_regret_loser_2[slice32],\r\n",
        "       train_regret_loser_3[slice32],\r\n",
        "       train_regret_loser_4[slice32],\r\n",
        "       train_regret_loser_5[slice32],\r\n",
        "       train_regret_loser_6[slice32],\r\n",
        "       train_regret_loser_7[slice32],\r\n",
        "       train_regret_loser_8[slice32],\r\n",
        "       train_regret_loser_9[slice32],\r\n",
        "       train_regret_loser_10[slice32],\r\n",
        "       train_regret_loser_11[slice32],\r\n",
        "       train_regret_loser_12[slice32],\r\n",
        "       train_regret_loser_13[slice32],\r\n",
        "       train_regret_loser_14[slice32],\r\n",
        "       train_regret_loser_15[slice32],\r\n",
        "       train_regret_loser_16[slice32],\r\n",
        "       train_regret_loser_17[slice32],\r\n",
        "       train_regret_loser_18[slice32],\r\n",
        "       train_regret_loser_19[slice32],\r\n",
        "       train_regret_loser_20[slice32]]\r\n",
        "\r\n",
        "winner32 = [train_regret_winner_1[slice32],\r\n",
        "       train_regret_winner_2[slice32],\r\n",
        "       train_regret_winner_3[slice32],\r\n",
        "       train_regret_winner_4[slice32],\r\n",
        "       train_regret_winner_5[slice32],\r\n",
        "       train_regret_winner_6[slice32],\r\n",
        "       train_regret_winner_7[slice32],\r\n",
        "       train_regret_winner_8[slice32],\r\n",
        "       train_regret_winner_9[slice32],\r\n",
        "       train_regret_winner_10[slice32],\r\n",
        "       train_regret_winner_11[slice32],\r\n",
        "       train_regret_winner_12[slice32],\r\n",
        "       train_regret_winner_13[slice32],\r\n",
        "       train_regret_winner_14[slice32],\r\n",
        "       train_regret_winner_15[slice32],\r\n",
        "       train_regret_winner_16[slice32],\r\n",
        "       train_regret_winner_17[slice32],\r\n",
        "       train_regret_winner_18[slice32],\r\n",
        "       train_regret_winner_19[slice32],\r\n",
        "       train_regret_winner_20[slice32]]\r\n",
        "\r\n",
        "loser32_results = pd.DataFrame(loser32).sort_values(by=[0], ascending=False)\r\n",
        "winner32_results = pd.DataFrame(winner32).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser32 = np.asarray(loser32_results[4:5][0])[0]\r\n",
        "median_loser32 = np.asarray(loser32_results[9:10][0])[0]\r\n",
        "upper_loser32 = np.asarray(loser32_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner32 = np.asarray(winner32_results[4:5][0])[0]\r\n",
        "median_winner32 = np.asarray(winner32_results[9:10][0])[0]\r\n",
        "upper_winner32 = np.asarray(winner32_results[14:15][0])[0]"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_rzHB8w-hy"
      },
      "source": [
        "# Iteration42 :\r\n",
        "\r\n",
        "slice42 = 41\r\n",
        "\r\n",
        "loser42 = [train_regret_loser_1[slice42],\r\n",
        "       train_regret_loser_2[slice42],\r\n",
        "       train_regret_loser_3[slice42],\r\n",
        "       train_regret_loser_4[slice42],\r\n",
        "       train_regret_loser_5[slice42],\r\n",
        "       train_regret_loser_6[slice42],\r\n",
        "       train_regret_loser_7[slice42],\r\n",
        "       train_regret_loser_8[slice42],\r\n",
        "       train_regret_loser_9[slice42],\r\n",
        "       train_regret_loser_10[slice42],\r\n",
        "       train_regret_loser_11[slice42],\r\n",
        "       train_regret_loser_12[slice42],\r\n",
        "       train_regret_loser_13[slice42],\r\n",
        "       train_regret_loser_14[slice42],\r\n",
        "       train_regret_loser_15[slice42],\r\n",
        "       train_regret_loser_16[slice42],\r\n",
        "       train_regret_loser_17[slice42],\r\n",
        "       train_regret_loser_18[slice42],\r\n",
        "       train_regret_loser_19[slice42],\r\n",
        "       train_regret_loser_20[slice42]]\r\n",
        "\r\n",
        "winner42 = [train_regret_winner_1[slice42],\r\n",
        "       train_regret_winner_2[slice42],\r\n",
        "       train_regret_winner_3[slice42],\r\n",
        "       train_regret_winner_4[slice42],\r\n",
        "       train_regret_winner_5[slice42],\r\n",
        "       train_regret_winner_6[slice42],\r\n",
        "       train_regret_winner_7[slice42],\r\n",
        "       train_regret_winner_8[slice42],\r\n",
        "       train_regret_winner_9[slice42],\r\n",
        "       train_regret_winner_10[slice42],\r\n",
        "       train_regret_winner_11[slice42],\r\n",
        "       train_regret_winner_12[slice42],\r\n",
        "       train_regret_winner_13[slice42],\r\n",
        "       train_regret_winner_14[slice42],\r\n",
        "       train_regret_winner_15[slice42],\r\n",
        "       train_regret_winner_16[slice42],\r\n",
        "       train_regret_winner_17[slice42],\r\n",
        "       train_regret_winner_18[slice42],\r\n",
        "       train_regret_winner_19[slice42],\r\n",
        "       train_regret_winner_20[slice42]]\r\n",
        "\r\n",
        "loser42_results = pd.DataFrame(loser42).sort_values(by=[0], ascending=False)\r\n",
        "winner42_results = pd.DataFrame(winner42).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser42 = np.asarray(loser42_results[4:5][0])[0]\r\n",
        "median_loser42 = np.asarray(loser42_results[9:10][0])[0]\r\n",
        "upper_loser42 = np.asarray(loser42_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner42 = np.asarray(winner42_results[4:5][0])[0]\r\n",
        "median_winner42 = np.asarray(winner42_results[9:10][0])[0]\r\n",
        "upper_winner42 = np.asarray(winner42_results[14:15][0])[0]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKUUG6vAw-kz"
      },
      "source": [
        "# Iteration52 :\r\n",
        "\r\n",
        "slice52 = 51\r\n",
        "\r\n",
        "loser52 = [train_regret_loser_1[slice52],\r\n",
        "       train_regret_loser_2[slice52],\r\n",
        "       train_regret_loser_3[slice52],\r\n",
        "       train_regret_loser_4[slice52],\r\n",
        "       train_regret_loser_5[slice52],\r\n",
        "       train_regret_loser_6[slice52],\r\n",
        "       train_regret_loser_7[slice52],\r\n",
        "       train_regret_loser_8[slice52],\r\n",
        "       train_regret_loser_9[slice52],\r\n",
        "       train_regret_loser_10[slice52],\r\n",
        "       train_regret_loser_11[slice52],\r\n",
        "       train_regret_loser_12[slice52],\r\n",
        "       train_regret_loser_13[slice52],\r\n",
        "       train_regret_loser_14[slice52],\r\n",
        "       train_regret_loser_15[slice52],\r\n",
        "       train_regret_loser_16[slice52],\r\n",
        "       train_regret_loser_17[slice52],\r\n",
        "       train_regret_loser_18[slice52],\r\n",
        "       train_regret_loser_19[slice52],\r\n",
        "       train_regret_loser_20[slice52]]\r\n",
        "\r\n",
        "winner52 = [train_regret_winner_1[slice52],\r\n",
        "       train_regret_winner_2[slice52],\r\n",
        "       train_regret_winner_3[slice52],\r\n",
        "       train_regret_winner_4[slice52],\r\n",
        "       train_regret_winner_5[slice52],\r\n",
        "       train_regret_winner_6[slice52],\r\n",
        "       train_regret_winner_7[slice52],\r\n",
        "       train_regret_winner_8[slice52],\r\n",
        "       train_regret_winner_9[slice52],\r\n",
        "       train_regret_winner_10[slice52],\r\n",
        "       train_regret_winner_11[slice52],\r\n",
        "       train_regret_winner_12[slice52],\r\n",
        "       train_regret_winner_13[slice52],\r\n",
        "       train_regret_winner_14[slice52],\r\n",
        "       train_regret_winner_15[slice52],\r\n",
        "       train_regret_winner_16[slice52],\r\n",
        "       train_regret_winner_17[slice52],\r\n",
        "       train_regret_winner_18[slice52],\r\n",
        "       train_regret_winner_19[slice52],\r\n",
        "       train_regret_winner_20[slice52]]\r\n",
        "\r\n",
        "loser52_results = pd.DataFrame(loser52).sort_values(by=[0], ascending=False)\r\n",
        "winner52_results = pd.DataFrame(winner52).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser52 = np.asarray(loser52_results[4:5][0])[0]\r\n",
        "median_loser52 = np.asarray(loser52_results[9:10][0])[0]\r\n",
        "upper_loser52 = np.asarray(loser52_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner52 = np.asarray(winner52_results[4:5][0])[0]\r\n",
        "median_winner52 = np.asarray(winner52_results[9:10][0])[0]\r\n",
        "upper_winner52 = np.asarray(winner52_results[14:15][0])[0]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9cJNkew-nY"
      },
      "source": [
        "# Iteration62 :\r\n",
        "\r\n",
        "slice62 = 61\r\n",
        "\r\n",
        "loser62 = [train_regret_loser_1[slice62],\r\n",
        "       train_regret_loser_2[slice62],\r\n",
        "       train_regret_loser_3[slice62],\r\n",
        "       train_regret_loser_4[slice62],\r\n",
        "       train_regret_loser_5[slice62],\r\n",
        "       train_regret_loser_6[slice62],\r\n",
        "       train_regret_loser_7[slice62],\r\n",
        "       train_regret_loser_8[slice62],\r\n",
        "       train_regret_loser_9[slice62],\r\n",
        "       train_regret_loser_10[slice62],\r\n",
        "       train_regret_loser_11[slice62],\r\n",
        "       train_regret_loser_12[slice62],\r\n",
        "       train_regret_loser_13[slice62],\r\n",
        "       train_regret_loser_14[slice62],\r\n",
        "       train_regret_loser_15[slice62],\r\n",
        "       train_regret_loser_16[slice62],\r\n",
        "       train_regret_loser_17[slice62],\r\n",
        "       train_regret_loser_18[slice62],\r\n",
        "       train_regret_loser_19[slice62],\r\n",
        "       train_regret_loser_20[slice62]]\r\n",
        "\r\n",
        "winner62 = [train_regret_winner_1[slice62],\r\n",
        "       train_regret_winner_2[slice62],\r\n",
        "       train_regret_winner_3[slice62],\r\n",
        "       train_regret_winner_4[slice62],\r\n",
        "       train_regret_winner_5[slice62],\r\n",
        "       train_regret_winner_6[slice62],\r\n",
        "       train_regret_winner_7[slice62],\r\n",
        "       train_regret_winner_8[slice62],\r\n",
        "       train_regret_winner_9[slice62],\r\n",
        "       train_regret_winner_10[slice62],\r\n",
        "       train_regret_winner_11[slice62],\r\n",
        "       train_regret_winner_12[slice62],\r\n",
        "       train_regret_winner_13[slice62],\r\n",
        "       train_regret_winner_14[slice62],\r\n",
        "       train_regret_winner_15[slice62],\r\n",
        "       train_regret_winner_16[slice62],\r\n",
        "       train_regret_winner_17[slice62],\r\n",
        "       train_regret_winner_18[slice62],\r\n",
        "       train_regret_winner_19[slice62],\r\n",
        "       train_regret_winner_20[slice62]]\r\n",
        "\r\n",
        "loser62_results = pd.DataFrame(loser62).sort_values(by=[0], ascending=False)\r\n",
        "winner62_results = pd.DataFrame(winner62).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser62 = np.asarray(loser62_results[4:5][0])[0]\r\n",
        "median_loser62 = np.asarray(loser62_results[9:10][0])[0]\r\n",
        "upper_loser62 = np.asarray(loser62_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner62 = np.asarray(winner62_results[4:5][0])[0]\r\n",
        "median_winner62 = np.asarray(winner62_results[9:10][0])[0]\r\n",
        "upper_winner62 = np.asarray(winner62_results[14:15][0])[0]"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DZ8kHHmw-qK"
      },
      "source": [
        "# Iteration72 :\r\n",
        "\r\n",
        "slice72 = 71\r\n",
        "\r\n",
        "loser72 = [train_regret_loser_1[slice72],\r\n",
        "       train_regret_loser_2[slice72],\r\n",
        "       train_regret_loser_3[slice72],\r\n",
        "       train_regret_loser_4[slice72],\r\n",
        "       train_regret_loser_5[slice72],\r\n",
        "       train_regret_loser_6[slice72],\r\n",
        "       train_regret_loser_7[slice72],\r\n",
        "       train_regret_loser_8[slice72],\r\n",
        "       train_regret_loser_9[slice72],\r\n",
        "       train_regret_loser_10[slice72],\r\n",
        "       train_regret_loser_11[slice72],\r\n",
        "       train_regret_loser_12[slice72],\r\n",
        "       train_regret_loser_13[slice72],\r\n",
        "       train_regret_loser_14[slice72],\r\n",
        "       train_regret_loser_15[slice72],\r\n",
        "       train_regret_loser_16[slice72],\r\n",
        "       train_regret_loser_17[slice72],\r\n",
        "       train_regret_loser_18[slice72],\r\n",
        "       train_regret_loser_19[slice72],\r\n",
        "       train_regret_loser_20[slice72]]\r\n",
        "\r\n",
        "winner72 = [train_regret_winner_1[slice72],\r\n",
        "       train_regret_winner_2[slice72],\r\n",
        "       train_regret_winner_3[slice72],\r\n",
        "       train_regret_winner_4[slice72],\r\n",
        "       train_regret_winner_5[slice72],\r\n",
        "       train_regret_winner_6[slice72],\r\n",
        "       train_regret_winner_7[slice72],\r\n",
        "       train_regret_winner_8[slice72],\r\n",
        "       train_regret_winner_9[slice72],\r\n",
        "       train_regret_winner_10[slice72],\r\n",
        "       train_regret_winner_11[slice72],\r\n",
        "       train_regret_winner_12[slice72],\r\n",
        "       train_regret_winner_13[slice72],\r\n",
        "       train_regret_winner_14[slice72],\r\n",
        "       train_regret_winner_15[slice72],\r\n",
        "       train_regret_winner_16[slice72],\r\n",
        "       train_regret_winner_17[slice72],\r\n",
        "       train_regret_winner_18[slice72],\r\n",
        "       train_regret_winner_19[slice72],\r\n",
        "       train_regret_winner_20[slice72]]\r\n",
        "\r\n",
        "loser72_results = pd.DataFrame(loser72).sort_values(by=[0], ascending=False)\r\n",
        "winner72_results = pd.DataFrame(winner72).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser72 = np.asarray(loser72_results[4:5][0])[0]\r\n",
        "median_loser72 = np.asarray(loser72_results[9:10][0])[0]\r\n",
        "upper_loser72 = np.asarray(loser72_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner72 = np.asarray(winner72_results[4:5][0])[0]\r\n",
        "median_winner72 = np.asarray(winner72_results[9:10][0])[0]\r\n",
        "upper_winner72 = np.asarray(winner72_results[14:15][0])[0]"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBzBvnVyw-sz"
      },
      "source": [
        "# Iteration82 :\r\n",
        "\r\n",
        "slice82 = 81\r\n",
        "\r\n",
        "loser82 = [train_regret_loser_1[slice82],\r\n",
        "       train_regret_loser_2[slice82],\r\n",
        "       train_regret_loser_3[slice82],\r\n",
        "       train_regret_loser_4[slice82],\r\n",
        "       train_regret_loser_5[slice82],\r\n",
        "       train_regret_loser_6[slice82],\r\n",
        "       train_regret_loser_7[slice82],\r\n",
        "       train_regret_loser_8[slice82],\r\n",
        "       train_regret_loser_9[slice82],\r\n",
        "       train_regret_loser_10[slice82],\r\n",
        "       train_regret_loser_11[slice82],\r\n",
        "       train_regret_loser_12[slice82],\r\n",
        "       train_regret_loser_13[slice82],\r\n",
        "       train_regret_loser_14[slice82],\r\n",
        "       train_regret_loser_15[slice82],\r\n",
        "       train_regret_loser_16[slice82],\r\n",
        "       train_regret_loser_17[slice82],\r\n",
        "       train_regret_loser_18[slice82],\r\n",
        "       train_regret_loser_19[slice82],\r\n",
        "       train_regret_loser_20[slice82]]\r\n",
        "\r\n",
        "winner82 = [train_regret_winner_1[slice82],\r\n",
        "       train_regret_winner_2[slice82],\r\n",
        "       train_regret_winner_3[slice82],\r\n",
        "       train_regret_winner_4[slice82],\r\n",
        "       train_regret_winner_5[slice82],\r\n",
        "       train_regret_winner_6[slice82],\r\n",
        "       train_regret_winner_7[slice82],\r\n",
        "       train_regret_winner_8[slice82],\r\n",
        "       train_regret_winner_9[slice82],\r\n",
        "       train_regret_winner_10[slice82],\r\n",
        "       train_regret_winner_11[slice82],\r\n",
        "       train_regret_winner_12[slice82],\r\n",
        "       train_regret_winner_13[slice82],\r\n",
        "       train_regret_winner_14[slice82],\r\n",
        "       train_regret_winner_15[slice82],\r\n",
        "       train_regret_winner_16[slice82],\r\n",
        "       train_regret_winner_17[slice82],\r\n",
        "       train_regret_winner_18[slice82],\r\n",
        "       train_regret_winner_19[slice82],\r\n",
        "       train_regret_winner_20[slice82]]\r\n",
        "\r\n",
        "loser82_results = pd.DataFrame(loser82).sort_values(by=[0], ascending=False)\r\n",
        "winner82_results = pd.DataFrame(winner82).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser82 = np.asarray(loser82_results[4:5][0])[0]\r\n",
        "median_loser82 = np.asarray(loser82_results[9:10][0])[0]\r\n",
        "upper_loser82 = np.asarray(loser82_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner82 = np.asarray(winner82_results[4:5][0])[0]\r\n",
        "median_winner82 = np.asarray(winner82_results[9:10][0])[0]\r\n",
        "upper_winner82 = np.asarray(winner82_results[14:15][0])[0]"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Qj--n6Bw-y2"
      },
      "source": [
        "# Iteration92 :\r\n",
        "\r\n",
        "slice92 = 91\r\n",
        "\r\n",
        "loser92 = [train_regret_loser_1[slice92],\r\n",
        "       train_regret_loser_2[slice92],\r\n",
        "       train_regret_loser_3[slice92],\r\n",
        "       train_regret_loser_4[slice92],\r\n",
        "       train_regret_loser_5[slice92],\r\n",
        "       train_regret_loser_6[slice92],\r\n",
        "       train_regret_loser_7[slice92],\r\n",
        "       train_regret_loser_8[slice92],\r\n",
        "       train_regret_loser_9[slice92],\r\n",
        "       train_regret_loser_10[slice92],\r\n",
        "       train_regret_loser_11[slice92],\r\n",
        "       train_regret_loser_12[slice92],\r\n",
        "       train_regret_loser_13[slice92],\r\n",
        "       train_regret_loser_14[slice92],\r\n",
        "       train_regret_loser_15[slice92],\r\n",
        "       train_regret_loser_16[slice92],\r\n",
        "       train_regret_loser_17[slice92],\r\n",
        "       train_regret_loser_18[slice92],\r\n",
        "       train_regret_loser_19[slice92],\r\n",
        "       train_regret_loser_20[slice92]]\r\n",
        "\r\n",
        "winner92 = [train_regret_winner_1[slice92],\r\n",
        "       train_regret_winner_2[slice92],\r\n",
        "       train_regret_winner_3[slice92],\r\n",
        "       train_regret_winner_4[slice92],\r\n",
        "       train_regret_winner_5[slice92],\r\n",
        "       train_regret_winner_6[slice92],\r\n",
        "       train_regret_winner_7[slice92],\r\n",
        "       train_regret_winner_8[slice92],\r\n",
        "       train_regret_winner_9[slice92],\r\n",
        "       train_regret_winner_10[slice92],\r\n",
        "       train_regret_winner_11[slice92],\r\n",
        "       train_regret_winner_12[slice92],\r\n",
        "       train_regret_winner_13[slice92],\r\n",
        "       train_regret_winner_14[slice92],\r\n",
        "       train_regret_winner_15[slice92],\r\n",
        "       train_regret_winner_16[slice92],\r\n",
        "       train_regret_winner_17[slice92],\r\n",
        "       train_regret_winner_18[slice92],\r\n",
        "       train_regret_winner_19[slice92],\r\n",
        "       train_regret_winner_20[slice92]]\r\n",
        "\r\n",
        "loser92_results = pd.DataFrame(loser92).sort_values(by=[0], ascending=False)\r\n",
        "winner92_results = pd.DataFrame(winner92).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser92 = np.asarray(loser92_results[4:5][0])[0]\r\n",
        "median_loser92 = np.asarray(loser92_results[9:10][0])[0]\r\n",
        "upper_loser92 = np.asarray(loser92_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner92 = np.asarray(winner92_results[4:5][0])[0]\r\n",
        "median_winner92 = np.asarray(winner92_results[9:10][0])[0]\r\n",
        "upper_winner92 = np.asarray(winner92_results[14:15][0])[0]"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaJ5KqLdw-0L"
      },
      "source": [
        "# Iteration3 :\r\n",
        "\r\n",
        "slice3 = 2\r\n",
        "\r\n",
        "loser3 = [train_regret_loser_1[slice3],\r\n",
        "       train_regret_loser_2[slice3],\r\n",
        "       train_regret_loser_3[slice3],\r\n",
        "       train_regret_loser_4[slice3],\r\n",
        "       train_regret_loser_5[slice3],\r\n",
        "       train_regret_loser_6[slice3],\r\n",
        "       train_regret_loser_7[slice3],\r\n",
        "       train_regret_loser_8[slice3],\r\n",
        "       train_regret_loser_9[slice3],\r\n",
        "       train_regret_loser_10[slice3],\r\n",
        "       train_regret_loser_11[slice3],\r\n",
        "       train_regret_loser_12[slice3],\r\n",
        "       train_regret_loser_13[slice3],\r\n",
        "       train_regret_loser_14[slice3],\r\n",
        "       train_regret_loser_15[slice3],\r\n",
        "       train_regret_loser_16[slice3],\r\n",
        "       train_regret_loser_17[slice3],\r\n",
        "       train_regret_loser_18[slice3],\r\n",
        "       train_regret_loser_19[slice3],\r\n",
        "       train_regret_loser_20[slice3]]\r\n",
        "\r\n",
        "winner3 = [train_regret_winner_1[slice3],\r\n",
        "       train_regret_winner_2[slice3],\r\n",
        "       train_regret_winner_3[slice3],\r\n",
        "       train_regret_winner_4[slice3],\r\n",
        "       train_regret_winner_5[slice3],\r\n",
        "       train_regret_winner_6[slice3],\r\n",
        "       train_regret_winner_7[slice3],\r\n",
        "       train_regret_winner_8[slice3],\r\n",
        "       train_regret_winner_9[slice3],\r\n",
        "       train_regret_winner_10[slice3],\r\n",
        "       train_regret_winner_11[slice3],\r\n",
        "       train_regret_winner_12[slice3],\r\n",
        "       train_regret_winner_13[slice3],\r\n",
        "       train_regret_winner_14[slice3],\r\n",
        "       train_regret_winner_15[slice3],\r\n",
        "       train_regret_winner_16[slice3],\r\n",
        "       train_regret_winner_17[slice3],\r\n",
        "       train_regret_winner_18[slice3],\r\n",
        "       train_regret_winner_19[slice3],\r\n",
        "       train_regret_winner_20[slice3]]\r\n",
        "\r\n",
        "loser3_results = pd.DataFrame(loser3).sort_values(by=[0], ascending=False)\r\n",
        "winner3_results = pd.DataFrame(winner3).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser3 = np.asarray(loser3_results[4:5][0])[0]\r\n",
        "median_loser3 = np.asarray(loser3_results[9:10][0])[0]\r\n",
        "upper_loser3 = np.asarray(loser3_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner3 = np.asarray(winner3_results[4:5][0])[0]\r\n",
        "median_winner3 = np.asarray(winner3_results[9:10][0])[0]\r\n",
        "upper_winner3 = np.asarray(winner3_results[14:15][0])[0]"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkaoT1dyw-16"
      },
      "source": [
        "# Iteration13 :\r\n",
        "\r\n",
        "slice13 = 12\r\n",
        "\r\n",
        "loser13 = [train_regret_loser_1[slice13],\r\n",
        "       train_regret_loser_2[slice13],\r\n",
        "       train_regret_loser_3[slice13],\r\n",
        "       train_regret_loser_4[slice13],\r\n",
        "       train_regret_loser_5[slice13],\r\n",
        "       train_regret_loser_6[slice13],\r\n",
        "       train_regret_loser_7[slice13],\r\n",
        "       train_regret_loser_8[slice13],\r\n",
        "       train_regret_loser_9[slice13],\r\n",
        "       train_regret_loser_10[slice13],\r\n",
        "       train_regret_loser_11[slice13],\r\n",
        "       train_regret_loser_12[slice13],\r\n",
        "       train_regret_loser_13[slice13],\r\n",
        "       train_regret_loser_14[slice13],\r\n",
        "       train_regret_loser_15[slice13],\r\n",
        "       train_regret_loser_16[slice13],\r\n",
        "       train_regret_loser_17[slice13],\r\n",
        "       train_regret_loser_18[slice13],\r\n",
        "       train_regret_loser_19[slice13],\r\n",
        "       train_regret_loser_20[slice13]]\r\n",
        "\r\n",
        "winner13 = [train_regret_winner_1[slice13],\r\n",
        "       train_regret_winner_2[slice13],\r\n",
        "       train_regret_winner_3[slice13],\r\n",
        "       train_regret_winner_4[slice13],\r\n",
        "       train_regret_winner_5[slice13],\r\n",
        "       train_regret_winner_6[slice13],\r\n",
        "       train_regret_winner_7[slice13],\r\n",
        "       train_regret_winner_8[slice13],\r\n",
        "       train_regret_winner_9[slice13],\r\n",
        "       train_regret_winner_10[slice13],\r\n",
        "       train_regret_winner_11[slice13],\r\n",
        "       train_regret_winner_12[slice13],\r\n",
        "       train_regret_winner_13[slice13],\r\n",
        "       train_regret_winner_14[slice13],\r\n",
        "       train_regret_winner_15[slice13],\r\n",
        "       train_regret_winner_16[slice13],\r\n",
        "       train_regret_winner_17[slice13],\r\n",
        "       train_regret_winner_18[slice13],\r\n",
        "       train_regret_winner_19[slice13],\r\n",
        "       train_regret_winner_20[slice13]]\r\n",
        "\r\n",
        "loser13_results = pd.DataFrame(loser12).sort_values(by=[0], ascending=False)\r\n",
        "winner13_results = pd.DataFrame(winner12).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser13 = np.asarray(loser13_results[4:5][0])[0]\r\n",
        "median_loser13 = np.asarray(loser13_results[9:10][0])[0]\r\n",
        "upper_loser13 = np.asarray(loser13_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner13 = np.asarray(winner13_results[4:5][0])[0]\r\n",
        "median_winner13 = np.asarray(winner13_results[9:10][0])[0]\r\n",
        "upper_winner13 = np.asarray(winner13_results[14:15][0])[0]"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZbVMrBaw-4B"
      },
      "source": [
        "# Iteration23 :\r\n",
        "\r\n",
        "slice23 = 22\r\n",
        "\r\n",
        "loser23 = [train_regret_loser_1[slice23],\r\n",
        "       train_regret_loser_2[slice23],\r\n",
        "       train_regret_loser_3[slice23],\r\n",
        "       train_regret_loser_4[slice23],\r\n",
        "       train_regret_loser_5[slice23],\r\n",
        "       train_regret_loser_6[slice23],\r\n",
        "       train_regret_loser_7[slice23],\r\n",
        "       train_regret_loser_8[slice23],\r\n",
        "       train_regret_loser_9[slice23],\r\n",
        "       train_regret_loser_10[slice23],\r\n",
        "       train_regret_loser_11[slice23],\r\n",
        "       train_regret_loser_12[slice23],\r\n",
        "       train_regret_loser_13[slice23],\r\n",
        "       train_regret_loser_14[slice23],\r\n",
        "       train_regret_loser_15[slice23],\r\n",
        "       train_regret_loser_16[slice23],\r\n",
        "       train_regret_loser_17[slice23],\r\n",
        "       train_regret_loser_18[slice23],\r\n",
        "       train_regret_loser_19[slice23],\r\n",
        "       train_regret_loser_20[slice23]]\r\n",
        "\r\n",
        "winner23 = [train_regret_winner_1[slice23],\r\n",
        "       train_regret_winner_2[slice23],\r\n",
        "       train_regret_winner_3[slice23],\r\n",
        "       train_regret_winner_4[slice23],\r\n",
        "       train_regret_winner_5[slice23],\r\n",
        "       train_regret_winner_6[slice23],\r\n",
        "       train_regret_winner_7[slice23],\r\n",
        "       train_regret_winner_8[slice23],\r\n",
        "       train_regret_winner_9[slice23],\r\n",
        "       train_regret_winner_10[slice23],\r\n",
        "       train_regret_winner_11[slice23],\r\n",
        "       train_regret_winner_12[slice23],\r\n",
        "       train_regret_winner_13[slice23],\r\n",
        "       train_regret_winner_14[slice23],\r\n",
        "       train_regret_winner_15[slice23],\r\n",
        "       train_regret_winner_16[slice23],\r\n",
        "       train_regret_winner_17[slice23],\r\n",
        "       train_regret_winner_18[slice23],\r\n",
        "       train_regret_winner_19[slice23],\r\n",
        "       train_regret_winner_20[slice23]]\r\n",
        "\r\n",
        "loser23_results = pd.DataFrame(loser23).sort_values(by=[0], ascending=False)\r\n",
        "winner23_results = pd.DataFrame(winner23).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser23 = np.asarray(loser23_results[4:5][0])[0]\r\n",
        "median_loser23 = np.asarray(loser23_results[9:10][0])[0]\r\n",
        "upper_loser23 = np.asarray(loser23_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner23 = np.asarray(winner23_results[4:5][0])[0]\r\n",
        "median_winner23 = np.asarray(winner23_results[9:10][0])[0]\r\n",
        "upper_winner23 = np.asarray(winner23_results[14:15][0])[0]"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUa6HRCUw-7G"
      },
      "source": [
        "# Iteration33 :\r\n",
        "\r\n",
        "slice33 = 32\r\n",
        "\r\n",
        "loser33 = [train_regret_loser_1[slice33],\r\n",
        "       train_regret_loser_2[slice33],\r\n",
        "       train_regret_loser_3[slice33],\r\n",
        "       train_regret_loser_4[slice33],\r\n",
        "       train_regret_loser_5[slice33],\r\n",
        "       train_regret_loser_6[slice33],\r\n",
        "       train_regret_loser_7[slice33],\r\n",
        "       train_regret_loser_8[slice33],\r\n",
        "       train_regret_loser_9[slice33],\r\n",
        "       train_regret_loser_10[slice33],\r\n",
        "       train_regret_loser_11[slice33],\r\n",
        "       train_regret_loser_12[slice33],\r\n",
        "       train_regret_loser_13[slice33],\r\n",
        "       train_regret_loser_14[slice33],\r\n",
        "       train_regret_loser_15[slice33],\r\n",
        "       train_regret_loser_16[slice33],\r\n",
        "       train_regret_loser_17[slice33],\r\n",
        "       train_regret_loser_18[slice33],\r\n",
        "       train_regret_loser_19[slice33],\r\n",
        "       train_regret_loser_20[slice33]]\r\n",
        "\r\n",
        "winner33 = [train_regret_winner_1[slice33],\r\n",
        "       train_regret_winner_2[slice33],\r\n",
        "       train_regret_winner_3[slice33],\r\n",
        "       train_regret_winner_4[slice33],\r\n",
        "       train_regret_winner_5[slice33],\r\n",
        "       train_regret_winner_6[slice33],\r\n",
        "       train_regret_winner_7[slice33],\r\n",
        "       train_regret_winner_8[slice33],\r\n",
        "       train_regret_winner_9[slice33],\r\n",
        "       train_regret_winner_10[slice33],\r\n",
        "       train_regret_winner_11[slice33],\r\n",
        "       train_regret_winner_12[slice33],\r\n",
        "       train_regret_winner_13[slice33],\r\n",
        "       train_regret_winner_14[slice33],\r\n",
        "       train_regret_winner_15[slice33],\r\n",
        "       train_regret_winner_16[slice33],\r\n",
        "       train_regret_winner_17[slice33],\r\n",
        "       train_regret_winner_18[slice33],\r\n",
        "       train_regret_winner_19[slice33],\r\n",
        "       train_regret_winner_20[slice33]]\r\n",
        "\r\n",
        "loser33_results = pd.DataFrame(loser33).sort_values(by=[0], ascending=False)\r\n",
        "winner33_results = pd.DataFrame(winner33).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser33 = np.asarray(loser33_results[4:5][0])[0]\r\n",
        "median_loser33 = np.asarray(loser33_results[9:10][0])[0]\r\n",
        "upper_loser33 = np.asarray(loser33_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner33 = np.asarray(winner33_results[4:5][0])[0]\r\n",
        "median_winner33 = np.asarray(winner33_results[9:10][0])[0]\r\n",
        "upper_winner33 = np.asarray(winner33_results[14:15][0])[0]"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EVxQwsxw-9n"
      },
      "source": [
        "# Iteration43 :\r\n",
        "\r\n",
        "slice43 = 42\r\n",
        "\r\n",
        "loser43 = [train_regret_loser_1[slice43],\r\n",
        "       train_regret_loser_2[slice43],\r\n",
        "       train_regret_loser_3[slice43],\r\n",
        "       train_regret_loser_4[slice43],\r\n",
        "       train_regret_loser_5[slice43],\r\n",
        "       train_regret_loser_6[slice43],\r\n",
        "       train_regret_loser_7[slice43],\r\n",
        "       train_regret_loser_8[slice43],\r\n",
        "       train_regret_loser_9[slice43],\r\n",
        "       train_regret_loser_10[slice43],\r\n",
        "       train_regret_loser_11[slice43],\r\n",
        "       train_regret_loser_12[slice43],\r\n",
        "       train_regret_loser_13[slice43],\r\n",
        "       train_regret_loser_14[slice43],\r\n",
        "       train_regret_loser_15[slice43],\r\n",
        "       train_regret_loser_16[slice43],\r\n",
        "       train_regret_loser_17[slice43],\r\n",
        "       train_regret_loser_18[slice43],\r\n",
        "       train_regret_loser_19[slice43],\r\n",
        "       train_regret_loser_20[slice43]]\r\n",
        "\r\n",
        "winner43 = [train_regret_winner_1[slice43],\r\n",
        "       train_regret_winner_2[slice43],\r\n",
        "       train_regret_winner_3[slice43],\r\n",
        "       train_regret_winner_4[slice43],\r\n",
        "       train_regret_winner_5[slice43],\r\n",
        "       train_regret_winner_6[slice43],\r\n",
        "       train_regret_winner_7[slice43],\r\n",
        "       train_regret_winner_8[slice43],\r\n",
        "       train_regret_winner_9[slice43],\r\n",
        "       train_regret_winner_10[slice43],\r\n",
        "       train_regret_winner_11[slice43],\r\n",
        "       train_regret_winner_12[slice43],\r\n",
        "       train_regret_winner_13[slice43],\r\n",
        "       train_regret_winner_14[slice43],\r\n",
        "       train_regret_winner_15[slice43],\r\n",
        "       train_regret_winner_16[slice43],\r\n",
        "       train_regret_winner_17[slice43],\r\n",
        "       train_regret_winner_18[slice43],\r\n",
        "       train_regret_winner_19[slice43],\r\n",
        "       train_regret_winner_20[slice43]]\r\n",
        "\r\n",
        "loser43_results = pd.DataFrame(loser43).sort_values(by=[0], ascending=False)\r\n",
        "winner43_results = pd.DataFrame(winner43).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser43 = np.asarray(loser43_results[4:5][0])[0]\r\n",
        "median_loser43 = np.asarray(loser43_results[9:10][0])[0]\r\n",
        "upper_loser43 = np.asarray(loser43_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner43 = np.asarray(winner43_results[4:5][0])[0]\r\n",
        "median_winner43 = np.asarray(winner43_results[9:10][0])[0]\r\n",
        "upper_winner43 = np.asarray(winner43_results[14:15][0])[0]"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQtRyMv-w_AZ"
      },
      "source": [
        "# Iteration53 :\r\n",
        "\r\n",
        "slice53 = 52\r\n",
        "\r\n",
        "loser53 = [train_regret_loser_1[slice53],\r\n",
        "       train_regret_loser_2[slice53],\r\n",
        "       train_regret_loser_3[slice53],\r\n",
        "       train_regret_loser_4[slice53],\r\n",
        "       train_regret_loser_5[slice53],\r\n",
        "       train_regret_loser_6[slice53],\r\n",
        "       train_regret_loser_7[slice53],\r\n",
        "       train_regret_loser_8[slice53],\r\n",
        "       train_regret_loser_9[slice53],\r\n",
        "       train_regret_loser_10[slice53],\r\n",
        "       train_regret_loser_11[slice53],\r\n",
        "       train_regret_loser_12[slice53],\r\n",
        "       train_regret_loser_13[slice53],\r\n",
        "       train_regret_loser_14[slice53],\r\n",
        "       train_regret_loser_15[slice53],\r\n",
        "       train_regret_loser_16[slice53],\r\n",
        "       train_regret_loser_17[slice53],\r\n",
        "       train_regret_loser_18[slice53],\r\n",
        "       train_regret_loser_19[slice53],\r\n",
        "       train_regret_loser_20[slice53]]\r\n",
        "\r\n",
        "winner53 = [train_regret_winner_1[slice53],\r\n",
        "       train_regret_winner_2[slice53],\r\n",
        "       train_regret_winner_3[slice53],\r\n",
        "       train_regret_winner_4[slice53],\r\n",
        "       train_regret_winner_5[slice53],\r\n",
        "       train_regret_winner_6[slice53],\r\n",
        "       train_regret_winner_7[slice53],\r\n",
        "       train_regret_winner_8[slice53],\r\n",
        "       train_regret_winner_9[slice53],\r\n",
        "       train_regret_winner_10[slice53],\r\n",
        "       train_regret_winner_11[slice53],\r\n",
        "       train_regret_winner_12[slice53],\r\n",
        "       train_regret_winner_13[slice53],\r\n",
        "       train_regret_winner_14[slice53],\r\n",
        "       train_regret_winner_15[slice53],\r\n",
        "       train_regret_winner_16[slice53],\r\n",
        "       train_regret_winner_17[slice53],\r\n",
        "       train_regret_winner_18[slice53],\r\n",
        "       train_regret_winner_19[slice53],\r\n",
        "       train_regret_winner_20[slice53]]\r\n",
        "\r\n",
        "loser53_results = pd.DataFrame(loser53).sort_values(by=[0], ascending=False)\r\n",
        "winner53_results = pd.DataFrame(winner53).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser53 = np.asarray(loser53_results[4:5][0])[0]\r\n",
        "median_loser53 = np.asarray(loser53_results[9:10][0])[0]\r\n",
        "upper_loser53 = np.asarray(loser53_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner53 = np.asarray(winner53_results[4:5][0])[0]\r\n",
        "median_winner53 = np.asarray(winner53_results[9:10][0])[0]\r\n",
        "upper_winner53 = np.asarray(winner53_results[14:15][0])[0]"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CIMEXXYw_DR"
      },
      "source": [
        "# Iteration63 :\r\n",
        "\r\n",
        "slice63 = 62\r\n",
        "\r\n",
        "loser63 = [train_regret_loser_1[slice63],\r\n",
        "       train_regret_loser_2[slice63],\r\n",
        "       train_regret_loser_3[slice63],\r\n",
        "       train_regret_loser_4[slice63],\r\n",
        "       train_regret_loser_5[slice63],\r\n",
        "       train_regret_loser_6[slice63],\r\n",
        "       train_regret_loser_7[slice63],\r\n",
        "       train_regret_loser_8[slice63],\r\n",
        "       train_regret_loser_9[slice63],\r\n",
        "       train_regret_loser_10[slice63],\r\n",
        "       train_regret_loser_11[slice63],\r\n",
        "       train_regret_loser_12[slice63],\r\n",
        "       train_regret_loser_13[slice63],\r\n",
        "       train_regret_loser_14[slice63],\r\n",
        "       train_regret_loser_15[slice63],\r\n",
        "       train_regret_loser_16[slice63],\r\n",
        "       train_regret_loser_17[slice63],\r\n",
        "       train_regret_loser_18[slice63],\r\n",
        "       train_regret_loser_19[slice63],\r\n",
        "       train_regret_loser_20[slice63]]\r\n",
        "\r\n",
        "winner63 = [train_regret_winner_1[slice63],\r\n",
        "       train_regret_winner_2[slice63],\r\n",
        "       train_regret_winner_3[slice63],\r\n",
        "       train_regret_winner_4[slice63],\r\n",
        "       train_regret_winner_5[slice63],\r\n",
        "       train_regret_winner_6[slice63],\r\n",
        "       train_regret_winner_7[slice63],\r\n",
        "       train_regret_winner_8[slice63],\r\n",
        "       train_regret_winner_9[slice63],\r\n",
        "       train_regret_winner_10[slice63],\r\n",
        "       train_regret_winner_11[slice63],\r\n",
        "       train_regret_winner_12[slice63],\r\n",
        "       train_regret_winner_13[slice63],\r\n",
        "       train_regret_winner_14[slice63],\r\n",
        "       train_regret_winner_15[slice63],\r\n",
        "       train_regret_winner_16[slice63],\r\n",
        "       train_regret_winner_17[slice63],\r\n",
        "       train_regret_winner_18[slice63],\r\n",
        "       train_regret_winner_19[slice63],\r\n",
        "       train_regret_winner_20[slice63]]\r\n",
        "\r\n",
        "loser63_results = pd.DataFrame(loser63).sort_values(by=[0], ascending=False)\r\n",
        "winner63_results = pd.DataFrame(winner63).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser63 = np.asarray(loser63_results[4:5][0])[0]\r\n",
        "median_loser63 = np.asarray(loser63_results[9:10][0])[0]\r\n",
        "upper_loser63 = np.asarray(loser63_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner63 = np.asarray(winner63_results[4:5][0])[0]\r\n",
        "median_winner63 = np.asarray(winner63_results[9:10][0])[0]\r\n",
        "upper_winner63 = np.asarray(winner63_results[14:15][0])[0]"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg4GfwU_w_GF"
      },
      "source": [
        "# Iteration73 :\r\n",
        "\r\n",
        "slice73 = 72\r\n",
        "\r\n",
        "loser73 = [train_regret_loser_1[slice73],\r\n",
        "       train_regret_loser_2[slice73],\r\n",
        "       train_regret_loser_3[slice73],\r\n",
        "       train_regret_loser_4[slice73],\r\n",
        "       train_regret_loser_5[slice73],\r\n",
        "       train_regret_loser_6[slice73],\r\n",
        "       train_regret_loser_7[slice73],\r\n",
        "       train_regret_loser_8[slice73],\r\n",
        "       train_regret_loser_9[slice73],\r\n",
        "       train_regret_loser_10[slice73],\r\n",
        "       train_regret_loser_11[slice73],\r\n",
        "       train_regret_loser_12[slice73],\r\n",
        "       train_regret_loser_13[slice73],\r\n",
        "       train_regret_loser_14[slice73],\r\n",
        "       train_regret_loser_15[slice73],\r\n",
        "       train_regret_loser_16[slice73],\r\n",
        "       train_regret_loser_17[slice73],\r\n",
        "       train_regret_loser_18[slice73],\r\n",
        "       train_regret_loser_19[slice73],\r\n",
        "       train_regret_loser_20[slice73]]\r\n",
        "\r\n",
        "winner73 = [train_regret_winner_1[slice73],\r\n",
        "       train_regret_winner_2[slice73],\r\n",
        "       train_regret_winner_3[slice73],\r\n",
        "       train_regret_winner_4[slice73],\r\n",
        "       train_regret_winner_5[slice73],\r\n",
        "       train_regret_winner_6[slice73],\r\n",
        "       train_regret_winner_7[slice73],\r\n",
        "       train_regret_winner_8[slice73],\r\n",
        "       train_regret_winner_9[slice73],\r\n",
        "       train_regret_winner_10[slice73],\r\n",
        "       train_regret_winner_11[slice73],\r\n",
        "       train_regret_winner_12[slice73],\r\n",
        "       train_regret_winner_13[slice73],\r\n",
        "       train_regret_winner_14[slice73],\r\n",
        "       train_regret_winner_15[slice73],\r\n",
        "       train_regret_winner_16[slice73],\r\n",
        "       train_regret_winner_17[slice73],\r\n",
        "       train_regret_winner_18[slice73],\r\n",
        "       train_regret_winner_19[slice73],\r\n",
        "       train_regret_winner_20[slice73]]\r\n",
        "\r\n",
        "loser73_results = pd.DataFrame(loser73).sort_values(by=[0], ascending=False)\r\n",
        "winner73_results = pd.DataFrame(winner73).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser73 = np.asarray(loser73_results[4:5][0])[0]\r\n",
        "median_loser73 = np.asarray(loser73_results[9:10][0])[0]\r\n",
        "upper_loser73 = np.asarray(loser73_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner73 = np.asarray(winner73_results[4:5][0])[0]\r\n",
        "median_winner73 = np.asarray(winner73_results[9:10][0])[0]\r\n",
        "upper_winner73 = np.asarray(winner73_results[14:15][0])[0]"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WmpcEV4w_I9"
      },
      "source": [
        "# Iteration83 :\r\n",
        "\r\n",
        "slice83 = 82\r\n",
        "\r\n",
        "loser83 = [train_regret_loser_1[slice83],\r\n",
        "       train_regret_loser_2[slice83],\r\n",
        "       train_regret_loser_3[slice83],\r\n",
        "       train_regret_loser_4[slice83],\r\n",
        "       train_regret_loser_5[slice83],\r\n",
        "       train_regret_loser_6[slice83],\r\n",
        "       train_regret_loser_7[slice83],\r\n",
        "       train_regret_loser_8[slice83],\r\n",
        "       train_regret_loser_9[slice83],\r\n",
        "       train_regret_loser_10[slice83],\r\n",
        "       train_regret_loser_11[slice83],\r\n",
        "       train_regret_loser_12[slice83],\r\n",
        "       train_regret_loser_13[slice83],\r\n",
        "       train_regret_loser_14[slice83],\r\n",
        "       train_regret_loser_15[slice83],\r\n",
        "       train_regret_loser_16[slice83],\r\n",
        "       train_regret_loser_17[slice83],\r\n",
        "       train_regret_loser_18[slice83],\r\n",
        "       train_regret_loser_19[slice83],\r\n",
        "       train_regret_loser_20[slice83]]\r\n",
        "\r\n",
        "winner83 = [train_regret_winner_1[slice83],\r\n",
        "       train_regret_winner_2[slice83],\r\n",
        "       train_regret_winner_3[slice83],\r\n",
        "       train_regret_winner_4[slice83],\r\n",
        "       train_regret_winner_5[slice83],\r\n",
        "       train_regret_winner_6[slice83],\r\n",
        "       train_regret_winner_7[slice83],\r\n",
        "       train_regret_winner_8[slice83],\r\n",
        "       train_regret_winner_9[slice83],\r\n",
        "       train_regret_winner_10[slice83],\r\n",
        "       train_regret_winner_11[slice83],\r\n",
        "       train_regret_winner_12[slice83],\r\n",
        "       train_regret_winner_13[slice83],\r\n",
        "       train_regret_winner_14[slice83],\r\n",
        "       train_regret_winner_15[slice83],\r\n",
        "       train_regret_winner_16[slice83],\r\n",
        "       train_regret_winner_17[slice83],\r\n",
        "       train_regret_winner_18[slice83],\r\n",
        "       train_regret_winner_19[slice83],\r\n",
        "       train_regret_winner_20[slice83]]\r\n",
        "\r\n",
        "loser83_results = pd.DataFrame(loser83).sort_values(by=[0], ascending=False)\r\n",
        "winner83_results = pd.DataFrame(winner83).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser83 = np.asarray(loser83_results[4:5][0])[0]\r\n",
        "median_loser83 = np.asarray(loser83_results[9:10][0])[0]\r\n",
        "upper_loser83 = np.asarray(loser83_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner83 = np.asarray(winner83_results[4:5][0])[0]\r\n",
        "median_winner83 = np.asarray(winner83_results[9:10][0])[0]\r\n",
        "upper_winner83 = np.asarray(winner83_results[14:15][0])[0]"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGPwoBZyw_MF"
      },
      "source": [
        "# Iteration93 :\r\n",
        "\r\n",
        "slice93 = 92\r\n",
        "\r\n",
        "loser93 = [train_regret_loser_1[slice93],\r\n",
        "       train_regret_loser_2[slice93],\r\n",
        "       train_regret_loser_3[slice93],\r\n",
        "       train_regret_loser_4[slice93],\r\n",
        "       train_regret_loser_5[slice93],\r\n",
        "       train_regret_loser_6[slice93],\r\n",
        "       train_regret_loser_7[slice93],\r\n",
        "       train_regret_loser_8[slice93],\r\n",
        "       train_regret_loser_9[slice93],\r\n",
        "       train_regret_loser_10[slice93],\r\n",
        "       train_regret_loser_11[slice93],\r\n",
        "       train_regret_loser_12[slice93],\r\n",
        "       train_regret_loser_13[slice93],\r\n",
        "       train_regret_loser_14[slice93],\r\n",
        "       train_regret_loser_15[slice93],\r\n",
        "       train_regret_loser_16[slice93],\r\n",
        "       train_regret_loser_17[slice93],\r\n",
        "       train_regret_loser_18[slice93],\r\n",
        "       train_regret_loser_19[slice93],\r\n",
        "       train_regret_loser_20[slice93]]\r\n",
        "\r\n",
        "winner93 = [train_regret_winner_1[slice93],\r\n",
        "       train_regret_winner_2[slice93],\r\n",
        "       train_regret_winner_3[slice93],\r\n",
        "       train_regret_winner_4[slice93],\r\n",
        "       train_regret_winner_5[slice93],\r\n",
        "       train_regret_winner_6[slice93],\r\n",
        "       train_regret_winner_7[slice93],\r\n",
        "       train_regret_winner_8[slice93],\r\n",
        "       train_regret_winner_9[slice93],\r\n",
        "       train_regret_winner_10[slice93],\r\n",
        "       train_regret_winner_11[slice93],\r\n",
        "       train_regret_winner_12[slice93],\r\n",
        "       train_regret_winner_13[slice93],\r\n",
        "       train_regret_winner_14[slice93],\r\n",
        "       train_regret_winner_15[slice93],\r\n",
        "       train_regret_winner_16[slice93],\r\n",
        "       train_regret_winner_17[slice93],\r\n",
        "       train_regret_winner_18[slice93],\r\n",
        "       train_regret_winner_19[slice93],\r\n",
        "       train_regret_winner_20[slice93]]\r\n",
        "\r\n",
        "loser93_results = pd.DataFrame(loser93).sort_values(by=[0], ascending=False)\r\n",
        "winner93_results = pd.DataFrame(winner93).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser93 = np.asarray(loser93_results[4:5][0])[0]\r\n",
        "median_loser93 = np.asarray(loser93_results[9:10][0])[0]\r\n",
        "upper_loser93 = np.asarray(loser93_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner93 = np.asarray(winner93_results[4:5][0])[0]\r\n",
        "median_winner93 = np.asarray(winner93_results[9:10][0])[0]\r\n",
        "upper_winner93 = np.asarray(winner93_results[14:15][0])[0]"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_ghdGzPw_Oq"
      },
      "source": [
        "# Iteration4 :\r\n",
        "\r\n",
        "slice4 = 3\r\n",
        "\r\n",
        "loser4 = [train_regret_loser_1[slice4],\r\n",
        "       train_regret_loser_2[slice4],\r\n",
        "       train_regret_loser_3[slice4],\r\n",
        "       train_regret_loser_4[slice4],\r\n",
        "       train_regret_loser_5[slice4],\r\n",
        "       train_regret_loser_6[slice4],\r\n",
        "       train_regret_loser_7[slice4],\r\n",
        "       train_regret_loser_8[slice4],\r\n",
        "       train_regret_loser_9[slice4],\r\n",
        "       train_regret_loser_10[slice4],\r\n",
        "       train_regret_loser_11[slice4],\r\n",
        "       train_regret_loser_12[slice4],\r\n",
        "       train_regret_loser_13[slice4],\r\n",
        "       train_regret_loser_14[slice4],\r\n",
        "       train_regret_loser_15[slice4],\r\n",
        "       train_regret_loser_16[slice4],\r\n",
        "       train_regret_loser_17[slice4],\r\n",
        "       train_regret_loser_18[slice4],\r\n",
        "       train_regret_loser_19[slice4],\r\n",
        "       train_regret_loser_20[slice4]]\r\n",
        "\r\n",
        "winner4 = [train_regret_winner_1[slice4],\r\n",
        "       train_regret_winner_2[slice4],\r\n",
        "       train_regret_winner_3[slice4],\r\n",
        "       train_regret_winner_4[slice4],\r\n",
        "       train_regret_winner_5[slice4],\r\n",
        "       train_regret_winner_6[slice4],\r\n",
        "       train_regret_winner_7[slice4],\r\n",
        "       train_regret_winner_8[slice4],\r\n",
        "       train_regret_winner_9[slice4],\r\n",
        "       train_regret_winner_10[slice4],\r\n",
        "       train_regret_winner_11[slice4],\r\n",
        "       train_regret_winner_12[slice4],\r\n",
        "       train_regret_winner_13[slice4],\r\n",
        "       train_regret_winner_14[slice4],\r\n",
        "       train_regret_winner_15[slice4],\r\n",
        "       train_regret_winner_16[slice4],\r\n",
        "       train_regret_winner_17[slice4],\r\n",
        "       train_regret_winner_18[slice4],\r\n",
        "       train_regret_winner_19[slice4],\r\n",
        "       train_regret_winner_20[slice4]]\r\n",
        "\r\n",
        "loser4_results = pd.DataFrame(loser4).sort_values(by=[0], ascending=False)\r\n",
        "winner4_results = pd.DataFrame(winner4).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser4 = np.asarray(loser4_results[4:5][0])[0]\r\n",
        "median_loser4 = np.asarray(loser4_results[9:10][0])[0]\r\n",
        "upper_loser4 = np.asarray(loser4_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner4 = np.asarray(winner4_results[4:5][0])[0]\r\n",
        "median_winner4 = np.asarray(winner4_results[9:10][0])[0]\r\n",
        "upper_winner4 = np.asarray(winner4_results[14:15][0])[0]"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNvDt4QMw_Rz"
      },
      "source": [
        "# Iteration14 :\r\n",
        "\r\n",
        "slice14 = 13\r\n",
        "\r\n",
        "loser14 = [train_regret_loser_1[slice14],\r\n",
        "       train_regret_loser_2[slice14],\r\n",
        "       train_regret_loser_3[slice14],\r\n",
        "       train_regret_loser_4[slice14],\r\n",
        "       train_regret_loser_5[slice14],\r\n",
        "       train_regret_loser_6[slice14],\r\n",
        "       train_regret_loser_7[slice14],\r\n",
        "       train_regret_loser_8[slice14],\r\n",
        "       train_regret_loser_9[slice14],\r\n",
        "       train_regret_loser_10[slice14],\r\n",
        "       train_regret_loser_11[slice14],\r\n",
        "       train_regret_loser_12[slice14],\r\n",
        "       train_regret_loser_13[slice14],\r\n",
        "       train_regret_loser_14[slice14],\r\n",
        "       train_regret_loser_15[slice14],\r\n",
        "       train_regret_loser_16[slice14],\r\n",
        "       train_regret_loser_17[slice14],\r\n",
        "       train_regret_loser_18[slice14],\r\n",
        "       train_regret_loser_19[slice14],\r\n",
        "       train_regret_loser_20[slice14]]\r\n",
        "\r\n",
        "winner14 = [train_regret_winner_1[slice14],\r\n",
        "       train_regret_winner_2[slice14],\r\n",
        "       train_regret_winner_3[slice14],\r\n",
        "       train_regret_winner_4[slice14],\r\n",
        "       train_regret_winner_5[slice14],\r\n",
        "       train_regret_winner_6[slice14],\r\n",
        "       train_regret_winner_7[slice14],\r\n",
        "       train_regret_winner_8[slice14],\r\n",
        "       train_regret_winner_9[slice14],\r\n",
        "       train_regret_winner_10[slice14],\r\n",
        "       train_regret_winner_11[slice14],\r\n",
        "       train_regret_winner_12[slice14],\r\n",
        "       train_regret_winner_13[slice14],\r\n",
        "       train_regret_winner_14[slice14],\r\n",
        "       train_regret_winner_15[slice14],\r\n",
        "       train_regret_winner_16[slice14],\r\n",
        "       train_regret_winner_17[slice14],\r\n",
        "       train_regret_winner_18[slice14],\r\n",
        "       train_regret_winner_19[slice14],\r\n",
        "       train_regret_winner_20[slice14]]\r\n",
        "\r\n",
        "loser14_results = pd.DataFrame(loser14).sort_values(by=[0], ascending=False)\r\n",
        "winner14_results = pd.DataFrame(winner14).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser14 = np.asarray(loser14_results[4:5][0])[0]\r\n",
        "median_loser14 = np.asarray(loser14_results[9:10][0])[0]\r\n",
        "upper_loser14 = np.asarray(loser14_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner14 = np.asarray(winner14_results[4:5][0])[0]\r\n",
        "median_winner14 = np.asarray(winner14_results[9:10][0])[0]\r\n",
        "upper_winner14 = np.asarray(winner14_results[14:15][0])[0]"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx3kMQV0w_Ua"
      },
      "source": [
        "# Iteration24 :\r\n",
        "\r\n",
        "slice24 = 23\r\n",
        "\r\n",
        "loser24 = [train_regret_loser_1[slice24],\r\n",
        "       train_regret_loser_2[slice24],\r\n",
        "       train_regret_loser_3[slice24],\r\n",
        "       train_regret_loser_4[slice24],\r\n",
        "       train_regret_loser_5[slice24],\r\n",
        "       train_regret_loser_6[slice24],\r\n",
        "       train_regret_loser_7[slice24],\r\n",
        "       train_regret_loser_8[slice24],\r\n",
        "       train_regret_loser_9[slice24],\r\n",
        "       train_regret_loser_10[slice24],\r\n",
        "       train_regret_loser_11[slice24],\r\n",
        "       train_regret_loser_12[slice24],\r\n",
        "       train_regret_loser_13[slice24],\r\n",
        "       train_regret_loser_14[slice24],\r\n",
        "       train_regret_loser_15[slice24],\r\n",
        "       train_regret_loser_16[slice24],\r\n",
        "       train_regret_loser_17[slice24],\r\n",
        "       train_regret_loser_18[slice24],\r\n",
        "       train_regret_loser_19[slice24],\r\n",
        "       train_regret_loser_20[slice24]]\r\n",
        "\r\n",
        "winner24 = [train_regret_winner_1[slice24],\r\n",
        "       train_regret_winner_2[slice24],\r\n",
        "       train_regret_winner_3[slice24],\r\n",
        "       train_regret_winner_4[slice24],\r\n",
        "       train_regret_winner_5[slice24],\r\n",
        "       train_regret_winner_6[slice24],\r\n",
        "       train_regret_winner_7[slice24],\r\n",
        "       train_regret_winner_8[slice24],\r\n",
        "       train_regret_winner_9[slice24],\r\n",
        "       train_regret_winner_10[slice24],\r\n",
        "       train_regret_winner_11[slice24],\r\n",
        "       train_regret_winner_12[slice24],\r\n",
        "       train_regret_winner_13[slice24],\r\n",
        "       train_regret_winner_14[slice24],\r\n",
        "       train_regret_winner_15[slice24],\r\n",
        "       train_regret_winner_16[slice24],\r\n",
        "       train_regret_winner_17[slice24],\r\n",
        "       train_regret_winner_18[slice24],\r\n",
        "       train_regret_winner_19[slice24],\r\n",
        "       train_regret_winner_20[slice24]]\r\n",
        "\r\n",
        "loser24_results = pd.DataFrame(loser24).sort_values(by=[0], ascending=False)\r\n",
        "winner24_results = pd.DataFrame(winner24).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser24 = np.asarray(loser24_results[4:5][0])[0]\r\n",
        "median_loser24 = np.asarray(loser24_results[9:10][0])[0]\r\n",
        "upper_loser24 = np.asarray(loser24_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner24 = np.asarray(winner24_results[4:5][0])[0]\r\n",
        "median_winner24 = np.asarray(winner24_results[9:10][0])[0]\r\n",
        "upper_winner24 = np.asarray(winner24_results[14:15][0])[0]"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4ljkyMfw_Xj"
      },
      "source": [
        "# Iteration34 :\r\n",
        "\r\n",
        "slice34 = 33\r\n",
        "\r\n",
        "loser34 = [train_regret_loser_1[slice34],\r\n",
        "       train_regret_loser_2[slice34],\r\n",
        "       train_regret_loser_3[slice34],\r\n",
        "       train_regret_loser_4[slice34],\r\n",
        "       train_regret_loser_5[slice34],\r\n",
        "       train_regret_loser_6[slice34],\r\n",
        "       train_regret_loser_7[slice34],\r\n",
        "       train_regret_loser_8[slice34],\r\n",
        "       train_regret_loser_9[slice34],\r\n",
        "       train_regret_loser_10[slice34],\r\n",
        "       train_regret_loser_11[slice34],\r\n",
        "       train_regret_loser_12[slice34],\r\n",
        "       train_regret_loser_13[slice34],\r\n",
        "       train_regret_loser_14[slice34],\r\n",
        "       train_regret_loser_15[slice34],\r\n",
        "       train_regret_loser_16[slice34],\r\n",
        "       train_regret_loser_17[slice34],\r\n",
        "       train_regret_loser_18[slice34],\r\n",
        "       train_regret_loser_19[slice34],\r\n",
        "       train_regret_loser_20[slice34]]\r\n",
        "\r\n",
        "winner34 = [train_regret_winner_1[slice34],\r\n",
        "       train_regret_winner_2[slice34],\r\n",
        "       train_regret_winner_3[slice34],\r\n",
        "       train_regret_winner_4[slice34],\r\n",
        "       train_regret_winner_5[slice34],\r\n",
        "       train_regret_winner_6[slice34],\r\n",
        "       train_regret_winner_7[slice34],\r\n",
        "       train_regret_winner_8[slice34],\r\n",
        "       train_regret_winner_9[slice34],\r\n",
        "       train_regret_winner_10[slice34],\r\n",
        "       train_regret_winner_11[slice34],\r\n",
        "       train_regret_winner_12[slice34],\r\n",
        "       train_regret_winner_13[slice34],\r\n",
        "       train_regret_winner_14[slice34],\r\n",
        "       train_regret_winner_15[slice34],\r\n",
        "       train_regret_winner_16[slice34],\r\n",
        "       train_regret_winner_17[slice34],\r\n",
        "       train_regret_winner_18[slice34],\r\n",
        "       train_regret_winner_19[slice34],\r\n",
        "       train_regret_winner_20[slice34]]\r\n",
        "\r\n",
        "loser34_results = pd.DataFrame(loser34).sort_values(by=[0], ascending=False)\r\n",
        "winner34_results = pd.DataFrame(winner34).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser34 = np.asarray(loser34_results[4:5][0])[0]\r\n",
        "median_loser34 = np.asarray(loser34_results[9:10][0])[0]\r\n",
        "upper_loser34 = np.asarray(loser34_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner34 = np.asarray(winner34_results[4:5][0])[0]\r\n",
        "median_winner34 = np.asarray(winner34_results[9:10][0])[0]\r\n",
        "upper_winner34 = np.asarray(winner34_results[14:15][0])[0]"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGKj-NR8w_ae"
      },
      "source": [
        "# Iteration44 :\r\n",
        "\r\n",
        "slice44 = 43\r\n",
        "\r\n",
        "loser44 = [train_regret_loser_1[slice44],\r\n",
        "       train_regret_loser_2[slice44],\r\n",
        "       train_regret_loser_3[slice44],\r\n",
        "       train_regret_loser_4[slice44],\r\n",
        "       train_regret_loser_5[slice44],\r\n",
        "       train_regret_loser_6[slice44],\r\n",
        "       train_regret_loser_7[slice44],\r\n",
        "       train_regret_loser_8[slice44],\r\n",
        "       train_regret_loser_9[slice44],\r\n",
        "       train_regret_loser_10[slice44],\r\n",
        "       train_regret_loser_11[slice44],\r\n",
        "       train_regret_loser_12[slice44],\r\n",
        "       train_regret_loser_13[slice44],\r\n",
        "       train_regret_loser_14[slice44],\r\n",
        "       train_regret_loser_15[slice44],\r\n",
        "       train_regret_loser_16[slice44],\r\n",
        "       train_regret_loser_17[slice44],\r\n",
        "       train_regret_loser_18[slice44],\r\n",
        "       train_regret_loser_19[slice44],\r\n",
        "       train_regret_loser_20[slice44]]\r\n",
        "\r\n",
        "winner44 = [train_regret_winner_1[slice44],\r\n",
        "       train_regret_winner_2[slice44],\r\n",
        "       train_regret_winner_3[slice44],\r\n",
        "       train_regret_winner_4[slice44],\r\n",
        "       train_regret_winner_5[slice44],\r\n",
        "       train_regret_winner_6[slice44],\r\n",
        "       train_regret_winner_7[slice44],\r\n",
        "       train_regret_winner_8[slice44],\r\n",
        "       train_regret_winner_9[slice44],\r\n",
        "       train_regret_winner_10[slice44],\r\n",
        "       train_regret_winner_11[slice44],\r\n",
        "       train_regret_winner_12[slice44],\r\n",
        "       train_regret_winner_13[slice44],\r\n",
        "       train_regret_winner_14[slice44],\r\n",
        "       train_regret_winner_15[slice44],\r\n",
        "       train_regret_winner_16[slice44],\r\n",
        "       train_regret_winner_17[slice44],\r\n",
        "       train_regret_winner_18[slice44],\r\n",
        "       train_regret_winner_19[slice44],\r\n",
        "       train_regret_winner_20[slice44]]\r\n",
        "\r\n",
        "loser44_results = pd.DataFrame(loser44).sort_values(by=[0], ascending=False)\r\n",
        "winner44_results = pd.DataFrame(winner44).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser44 = np.asarray(loser44_results[4:5][0])[0]\r\n",
        "median_loser44 = np.asarray(loser44_results[9:10][0])[0]\r\n",
        "upper_loser44 = np.asarray(loser44_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner44 = np.asarray(winner44_results[4:5][0])[0]\r\n",
        "median_winner44 = np.asarray(winner44_results[9:10][0])[0]\r\n",
        "upper_winner44 = np.asarray(winner44_results[14:15][0])[0]"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJzx094Fw_c_"
      },
      "source": [
        "# Iteration54 :\r\n",
        "\r\n",
        "slice54 = 53\r\n",
        "\r\n",
        "loser54 = [train_regret_loser_1[slice54],\r\n",
        "       train_regret_loser_2[slice54],\r\n",
        "       train_regret_loser_3[slice54],\r\n",
        "       train_regret_loser_4[slice54],\r\n",
        "       train_regret_loser_5[slice54],\r\n",
        "       train_regret_loser_6[slice54],\r\n",
        "       train_regret_loser_7[slice54],\r\n",
        "       train_regret_loser_8[slice54],\r\n",
        "       train_regret_loser_9[slice54],\r\n",
        "       train_regret_loser_10[slice54],\r\n",
        "       train_regret_loser_11[slice54],\r\n",
        "       train_regret_loser_12[slice54],\r\n",
        "       train_regret_loser_13[slice54],\r\n",
        "       train_regret_loser_14[slice54],\r\n",
        "       train_regret_loser_15[slice54],\r\n",
        "       train_regret_loser_16[slice54],\r\n",
        "       train_regret_loser_17[slice54],\r\n",
        "       train_regret_loser_18[slice54],\r\n",
        "       train_regret_loser_19[slice54],\r\n",
        "       train_regret_loser_20[slice54]]\r\n",
        "\r\n",
        "winner54 = [train_regret_winner_1[slice54],\r\n",
        "       train_regret_winner_2[slice54],\r\n",
        "       train_regret_winner_3[slice54],\r\n",
        "       train_regret_winner_4[slice54],\r\n",
        "       train_regret_winner_5[slice54],\r\n",
        "       train_regret_winner_6[slice54],\r\n",
        "       train_regret_winner_7[slice54],\r\n",
        "       train_regret_winner_8[slice54],\r\n",
        "       train_regret_winner_9[slice54],\r\n",
        "       train_regret_winner_10[slice54],\r\n",
        "       train_regret_winner_11[slice54],\r\n",
        "       train_regret_winner_12[slice54],\r\n",
        "       train_regret_winner_13[slice54],\r\n",
        "       train_regret_winner_14[slice54],\r\n",
        "       train_regret_winner_15[slice54],\r\n",
        "       train_regret_winner_16[slice54],\r\n",
        "       train_regret_winner_17[slice54],\r\n",
        "       train_regret_winner_18[slice54],\r\n",
        "       train_regret_winner_19[slice54],\r\n",
        "       train_regret_winner_20[slice54]]\r\n",
        "\r\n",
        "loser54_results = pd.DataFrame(loser54).sort_values(by=[0], ascending=False)\r\n",
        "winner54_results = pd.DataFrame(winner54).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser54 = np.asarray(loser54_results[4:5][0])[0]\r\n",
        "median_loser54 = np.asarray(loser54_results[9:10][0])[0]\r\n",
        "upper_loser54 = np.asarray(loser54_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner54 = np.asarray(winner54_results[4:5][0])[0]\r\n",
        "median_winner54 = np.asarray(winner54_results[9:10][0])[0]\r\n",
        "upper_winner54 = np.asarray(winner54_results[14:15][0])[0]"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEVYMqj_w_fr"
      },
      "source": [
        "# Iteration64 :\r\n",
        "\r\n",
        "slice64 = 63\r\n",
        "\r\n",
        "loser64 = [train_regret_loser_1[slice64],\r\n",
        "       train_regret_loser_2[slice64],\r\n",
        "       train_regret_loser_3[slice64],\r\n",
        "       train_regret_loser_4[slice64],\r\n",
        "       train_regret_loser_5[slice64],\r\n",
        "       train_regret_loser_6[slice64],\r\n",
        "       train_regret_loser_7[slice64],\r\n",
        "       train_regret_loser_8[slice64],\r\n",
        "       train_regret_loser_9[slice64],\r\n",
        "       train_regret_loser_10[slice64],\r\n",
        "       train_regret_loser_11[slice64],\r\n",
        "       train_regret_loser_12[slice64],\r\n",
        "       train_regret_loser_13[slice64],\r\n",
        "       train_regret_loser_14[slice64],\r\n",
        "       train_regret_loser_15[slice64],\r\n",
        "       train_regret_loser_16[slice64],\r\n",
        "       train_regret_loser_17[slice64],\r\n",
        "       train_regret_loser_18[slice64],\r\n",
        "       train_regret_loser_19[slice64],\r\n",
        "       train_regret_loser_20[slice64]]\r\n",
        "\r\n",
        "winner64 = [train_regret_winner_1[slice64],\r\n",
        "       train_regret_winner_2[slice64],\r\n",
        "       train_regret_winner_3[slice64],\r\n",
        "       train_regret_winner_4[slice64],\r\n",
        "       train_regret_winner_5[slice64],\r\n",
        "       train_regret_winner_6[slice64],\r\n",
        "       train_regret_winner_7[slice64],\r\n",
        "       train_regret_winner_8[slice64],\r\n",
        "       train_regret_winner_9[slice64],\r\n",
        "       train_regret_winner_10[slice64],\r\n",
        "       train_regret_winner_11[slice64],\r\n",
        "       train_regret_winner_12[slice64],\r\n",
        "       train_regret_winner_13[slice64],\r\n",
        "       train_regret_winner_14[slice64],\r\n",
        "       train_regret_winner_15[slice64],\r\n",
        "       train_regret_winner_16[slice64],\r\n",
        "       train_regret_winner_17[slice64],\r\n",
        "       train_regret_winner_18[slice64],\r\n",
        "       train_regret_winner_19[slice64],\r\n",
        "       train_regret_winner_20[slice64]]\r\n",
        "\r\n",
        "loser64_results = pd.DataFrame(loser64).sort_values(by=[0], ascending=False)\r\n",
        "winner64_results = pd.DataFrame(winner64).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser64 = np.asarray(loser64_results[4:5][0])[0]\r\n",
        "median_loser64 = np.asarray(loser64_results[9:10][0])[0]\r\n",
        "upper_loser64 = np.asarray(loser64_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner64 = np.asarray(winner64_results[4:5][0])[0]\r\n",
        "median_winner64 = np.asarray(winner64_results[9:10][0])[0]\r\n",
        "upper_winner64 = np.asarray(winner64_results[14:15][0])[0]"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvr3S_p0w_ii"
      },
      "source": [
        "# Iteration74 :\r\n",
        "\r\n",
        "slice74 = 73\r\n",
        "\r\n",
        "loser74 = [train_regret_loser_1[slice74],\r\n",
        "       train_regret_loser_2[slice74],\r\n",
        "       train_regret_loser_3[slice74],\r\n",
        "       train_regret_loser_4[slice74],\r\n",
        "       train_regret_loser_5[slice74],\r\n",
        "       train_regret_loser_6[slice74],\r\n",
        "       train_regret_loser_7[slice74],\r\n",
        "       train_regret_loser_8[slice74],\r\n",
        "       train_regret_loser_9[slice74],\r\n",
        "       train_regret_loser_10[slice74],\r\n",
        "       train_regret_loser_11[slice74],\r\n",
        "       train_regret_loser_12[slice74],\r\n",
        "       train_regret_loser_13[slice74],\r\n",
        "       train_regret_loser_14[slice74],\r\n",
        "       train_regret_loser_15[slice74],\r\n",
        "       train_regret_loser_16[slice74],\r\n",
        "       train_regret_loser_17[slice74],\r\n",
        "       train_regret_loser_18[slice74],\r\n",
        "       train_regret_loser_19[slice74],\r\n",
        "       train_regret_loser_20[slice74]]\r\n",
        "\r\n",
        "winner74 = [train_regret_winner_1[slice74],\r\n",
        "       train_regret_winner_2[slice74],\r\n",
        "       train_regret_winner_3[slice74],\r\n",
        "       train_regret_winner_4[slice74],\r\n",
        "       train_regret_winner_5[slice74],\r\n",
        "       train_regret_winner_6[slice74],\r\n",
        "       train_regret_winner_7[slice74],\r\n",
        "       train_regret_winner_8[slice74],\r\n",
        "       train_regret_winner_9[slice74],\r\n",
        "       train_regret_winner_10[slice74],\r\n",
        "       train_regret_winner_11[slice74],\r\n",
        "       train_regret_winner_12[slice74],\r\n",
        "       train_regret_winner_13[slice74],\r\n",
        "       train_regret_winner_14[slice74],\r\n",
        "       train_regret_winner_15[slice74],\r\n",
        "       train_regret_winner_16[slice74],\r\n",
        "       train_regret_winner_17[slice74],\r\n",
        "       train_regret_winner_18[slice74],\r\n",
        "       train_regret_winner_19[slice74],\r\n",
        "       train_regret_winner_20[slice74]]\r\n",
        "\r\n",
        "loser74_results = pd.DataFrame(loser74).sort_values(by=[0], ascending=False)\r\n",
        "winner74_results = pd.DataFrame(winner74).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser74 = np.asarray(loser74_results[4:5][0])[0]\r\n",
        "median_loser74 = np.asarray(loser74_results[9:10][0])[0]\r\n",
        "upper_loser74 = np.asarray(loser74_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner74 = np.asarray(winner74_results[4:5][0])[0]\r\n",
        "median_winner74 = np.asarray(winner74_results[9:10][0])[0]\r\n",
        "upper_winner74 = np.asarray(winner74_results[14:15][0])[0]"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEAvUVK7w_lO"
      },
      "source": [
        "# Iteration84 :\r\n",
        "\r\n",
        "slice84 = 83\r\n",
        "\r\n",
        "loser84 = [train_regret_loser_1[slice84],\r\n",
        "       train_regret_loser_2[slice84],\r\n",
        "       train_regret_loser_3[slice84],\r\n",
        "       train_regret_loser_4[slice84],\r\n",
        "       train_regret_loser_5[slice84],\r\n",
        "       train_regret_loser_6[slice84],\r\n",
        "       train_regret_loser_7[slice84],\r\n",
        "       train_regret_loser_8[slice84],\r\n",
        "       train_regret_loser_9[slice84],\r\n",
        "       train_regret_loser_10[slice84],\r\n",
        "       train_regret_loser_11[slice84],\r\n",
        "       train_regret_loser_12[slice84],\r\n",
        "       train_regret_loser_13[slice84],\r\n",
        "       train_regret_loser_14[slice84],\r\n",
        "       train_regret_loser_15[slice84],\r\n",
        "       train_regret_loser_16[slice84],\r\n",
        "       train_regret_loser_17[slice84],\r\n",
        "       train_regret_loser_18[slice84],\r\n",
        "       train_regret_loser_19[slice84],\r\n",
        "       train_regret_loser_20[slice84]]\r\n",
        "\r\n",
        "winner84 = [train_regret_winner_1[slice84],\r\n",
        "       train_regret_winner_2[slice84],\r\n",
        "       train_regret_winner_3[slice84],\r\n",
        "       train_regret_winner_4[slice84],\r\n",
        "       train_regret_winner_5[slice84],\r\n",
        "       train_regret_winner_6[slice84],\r\n",
        "       train_regret_winner_7[slice84],\r\n",
        "       train_regret_winner_8[slice84],\r\n",
        "       train_regret_winner_9[slice84],\r\n",
        "       train_regret_winner_10[slice84],\r\n",
        "       train_regret_winner_11[slice84],\r\n",
        "       train_regret_winner_12[slice84],\r\n",
        "       train_regret_winner_13[slice84],\r\n",
        "       train_regret_winner_14[slice84],\r\n",
        "       train_regret_winner_15[slice84],\r\n",
        "       train_regret_winner_16[slice84],\r\n",
        "       train_regret_winner_17[slice84],\r\n",
        "       train_regret_winner_18[slice84],\r\n",
        "       train_regret_winner_19[slice84],\r\n",
        "       train_regret_winner_20[slice84]]\r\n",
        "\r\n",
        "loser84_results = pd.DataFrame(loser84).sort_values(by=[0], ascending=False)\r\n",
        "winner84_results = pd.DataFrame(winner84).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser84 = np.asarray(loser84_results[4:5][0])[0]\r\n",
        "median_loser84 = np.asarray(loser84_results[9:10][0])[0]\r\n",
        "upper_loser84 = np.asarray(loser84_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner84 = np.asarray(winner84_results[4:5][0])[0]\r\n",
        "median_winner84 = np.asarray(winner84_results[9:10][0])[0]\r\n",
        "upper_winner84 = np.asarray(winner84_results[14:15][0])[0]"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TMZzhdGw_oC"
      },
      "source": [
        "# Iteration94 :\r\n",
        "\r\n",
        "slice94 = 93\r\n",
        "\r\n",
        "loser94 = [train_regret_loser_1[slice94],\r\n",
        "       train_regret_loser_2[slice94],\r\n",
        "       train_regret_loser_3[slice94],\r\n",
        "       train_regret_loser_4[slice94],\r\n",
        "       train_regret_loser_5[slice94],\r\n",
        "       train_regret_loser_6[slice94],\r\n",
        "       train_regret_loser_7[slice94],\r\n",
        "       train_regret_loser_8[slice94],\r\n",
        "       train_regret_loser_9[slice94],\r\n",
        "       train_regret_loser_10[slice94],\r\n",
        "       train_regret_loser_11[slice94],\r\n",
        "       train_regret_loser_12[slice94],\r\n",
        "       train_regret_loser_13[slice94],\r\n",
        "       train_regret_loser_14[slice94],\r\n",
        "       train_regret_loser_15[slice94],\r\n",
        "       train_regret_loser_16[slice94],\r\n",
        "       train_regret_loser_17[slice94],\r\n",
        "       train_regret_loser_18[slice94],\r\n",
        "       train_regret_loser_19[slice94],\r\n",
        "       train_regret_loser_20[slice94]]\r\n",
        "\r\n",
        "winner94 = [train_regret_winner_1[slice94],\r\n",
        "       train_regret_winner_2[slice94],\r\n",
        "       train_regret_winner_3[slice94],\r\n",
        "       train_regret_winner_4[slice94],\r\n",
        "       train_regret_winner_5[slice94],\r\n",
        "       train_regret_winner_6[slice94],\r\n",
        "       train_regret_winner_7[slice94],\r\n",
        "       train_regret_winner_8[slice94],\r\n",
        "       train_regret_winner_9[slice94],\r\n",
        "       train_regret_winner_10[slice94],\r\n",
        "       train_regret_winner_11[slice94],\r\n",
        "       train_regret_winner_12[slice94],\r\n",
        "       train_regret_winner_13[slice94],\r\n",
        "       train_regret_winner_14[slice94],\r\n",
        "       train_regret_winner_15[slice94],\r\n",
        "       train_regret_winner_16[slice94],\r\n",
        "       train_regret_winner_17[slice94],\r\n",
        "       train_regret_winner_18[slice94],\r\n",
        "       train_regret_winner_19[slice94],\r\n",
        "       train_regret_winner_20[slice94]]\r\n",
        "\r\n",
        "loser94_results = pd.DataFrame(loser94).sort_values(by=[0], ascending=False)\r\n",
        "winner94_results = pd.DataFrame(winner94).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser94 = np.asarray(loser94_results[4:5][0])[0]\r\n",
        "median_loser94 = np.asarray(loser94_results[9:10][0])[0]\r\n",
        "upper_loser94 = np.asarray(loser94_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner94 = np.asarray(winner94_results[4:5][0])[0]\r\n",
        "median_winner94 = np.asarray(winner94_results[9:10][0])[0]\r\n",
        "upper_winner94 = np.asarray(winner94_results[14:15][0])[0]"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMxHQOgaw_qu"
      },
      "source": [
        "# Iteration5 :\r\n",
        "\r\n",
        "slice5 = 4\r\n",
        "\r\n",
        "loser5 = [train_regret_loser_1[slice5],\r\n",
        "       train_regret_loser_2[slice5],\r\n",
        "       train_regret_loser_3[slice5],\r\n",
        "       train_regret_loser_4[slice5],\r\n",
        "       train_regret_loser_5[slice5],\r\n",
        "       train_regret_loser_6[slice5],\r\n",
        "       train_regret_loser_7[slice5],\r\n",
        "       train_regret_loser_8[slice5],\r\n",
        "       train_regret_loser_9[slice5],\r\n",
        "       train_regret_loser_10[slice5],\r\n",
        "       train_regret_loser_11[slice5],\r\n",
        "       train_regret_loser_12[slice5],\r\n",
        "       train_regret_loser_13[slice5],\r\n",
        "       train_regret_loser_14[slice5],\r\n",
        "       train_regret_loser_15[slice5],\r\n",
        "       train_regret_loser_16[slice5],\r\n",
        "       train_regret_loser_17[slice5],\r\n",
        "       train_regret_loser_18[slice5],\r\n",
        "       train_regret_loser_19[slice5],\r\n",
        "       train_regret_loser_20[slice5]]\r\n",
        "\r\n",
        "winner5 = [train_regret_winner_1[slice5],\r\n",
        "       train_regret_winner_2[slice5],\r\n",
        "       train_regret_winner_3[slice5],\r\n",
        "       train_regret_winner_4[slice5],\r\n",
        "       train_regret_winner_5[slice5],\r\n",
        "       train_regret_winner_6[slice5],\r\n",
        "       train_regret_winner_7[slice5],\r\n",
        "       train_regret_winner_8[slice5],\r\n",
        "       train_regret_winner_9[slice5],\r\n",
        "       train_regret_winner_10[slice5],\r\n",
        "       train_regret_winner_11[slice5],\r\n",
        "       train_regret_winner_12[slice5],\r\n",
        "       train_regret_winner_13[slice5],\r\n",
        "       train_regret_winner_14[slice5],\r\n",
        "       train_regret_winner_15[slice5],\r\n",
        "       train_regret_winner_16[slice5],\r\n",
        "       train_regret_winner_17[slice5],\r\n",
        "       train_regret_winner_18[slice5],\r\n",
        "       train_regret_winner_19[slice5],\r\n",
        "       train_regret_winner_20[slice5]]\r\n",
        "\r\n",
        "loser5_results = pd.DataFrame(loser5).sort_values(by=[0], ascending=False)\r\n",
        "winner5_results = pd.DataFrame(winner5).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser5 = np.asarray(loser5_results[4:5][0])[0]\r\n",
        "median_loser5 = np.asarray(loser5_results[9:10][0])[0]\r\n",
        "upper_loser5 = np.asarray(loser5_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner5 = np.asarray(winner5_results[4:5][0])[0]\r\n",
        "median_winner5 = np.asarray(winner5_results[9:10][0])[0]\r\n",
        "upper_winner5 = np.asarray(winner5_results[14:15][0])[0]"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPVFQTfNw_wF"
      },
      "source": [
        "# Iteration15 :\r\n",
        "\r\n",
        "slice15 = 14\r\n",
        "\r\n",
        "loser15 = [train_regret_loser_1[slice15],\r\n",
        "       train_regret_loser_2[slice15],\r\n",
        "       train_regret_loser_3[slice15],\r\n",
        "       train_regret_loser_4[slice15],\r\n",
        "       train_regret_loser_5[slice15],\r\n",
        "       train_regret_loser_6[slice15],\r\n",
        "       train_regret_loser_7[slice15],\r\n",
        "       train_regret_loser_8[slice15],\r\n",
        "       train_regret_loser_9[slice15],\r\n",
        "       train_regret_loser_10[slice15],\r\n",
        "       train_regret_loser_11[slice15],\r\n",
        "       train_regret_loser_12[slice15],\r\n",
        "       train_regret_loser_13[slice15],\r\n",
        "       train_regret_loser_14[slice15],\r\n",
        "       train_regret_loser_15[slice15],\r\n",
        "       train_regret_loser_16[slice15],\r\n",
        "       train_regret_loser_17[slice15],\r\n",
        "       train_regret_loser_18[slice15],\r\n",
        "       train_regret_loser_19[slice15],\r\n",
        "       train_regret_loser_20[slice15]]\r\n",
        "\r\n",
        "winner15 = [train_regret_winner_1[slice15],\r\n",
        "       train_regret_winner_2[slice15],\r\n",
        "       train_regret_winner_3[slice15],\r\n",
        "       train_regret_winner_4[slice15],\r\n",
        "       train_regret_winner_5[slice15],\r\n",
        "       train_regret_winner_6[slice15],\r\n",
        "       train_regret_winner_7[slice15],\r\n",
        "       train_regret_winner_8[slice15],\r\n",
        "       train_regret_winner_9[slice15],\r\n",
        "       train_regret_winner_10[slice15],\r\n",
        "       train_regret_winner_11[slice15],\r\n",
        "       train_regret_winner_12[slice15],\r\n",
        "       train_regret_winner_13[slice15],\r\n",
        "       train_regret_winner_14[slice15],\r\n",
        "       train_regret_winner_15[slice15],\r\n",
        "       train_regret_winner_16[slice15],\r\n",
        "       train_regret_winner_17[slice15],\r\n",
        "       train_regret_winner_18[slice15],\r\n",
        "       train_regret_winner_19[slice15],\r\n",
        "       train_regret_winner_20[slice15]]\r\n",
        "\r\n",
        "loser15_results = pd.DataFrame(loser15).sort_values(by=[0], ascending=False)\r\n",
        "winner15_results = pd.DataFrame(winner15).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser15 = np.asarray(loser15_results[4:5][0])[0]\r\n",
        "median_loser15 = np.asarray(loser15_results[9:10][0])[0]\r\n",
        "upper_loser15 = np.asarray(loser15_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner15 = np.asarray(winner15_results[4:5][0])[0]\r\n",
        "median_winner15 = np.asarray(winner15_results[9:10][0])[0]\r\n",
        "upper_winner15 = np.asarray(winner15_results[14:15][0])[0]"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADBmBA3ew_zK"
      },
      "source": [
        "# Iteration25 :\r\n",
        "\r\n",
        "slice25 = 24\r\n",
        "\r\n",
        "loser25 = [train_regret_loser_1[slice25],\r\n",
        "       train_regret_loser_2[slice25],\r\n",
        "       train_regret_loser_3[slice25],\r\n",
        "       train_regret_loser_4[slice25],\r\n",
        "       train_regret_loser_5[slice25],\r\n",
        "       train_regret_loser_6[slice25],\r\n",
        "       train_regret_loser_7[slice25],\r\n",
        "       train_regret_loser_8[slice25],\r\n",
        "       train_regret_loser_9[slice25],\r\n",
        "       train_regret_loser_10[slice25],\r\n",
        "       train_regret_loser_11[slice25],\r\n",
        "       train_regret_loser_12[slice25],\r\n",
        "       train_regret_loser_13[slice25],\r\n",
        "       train_regret_loser_14[slice25],\r\n",
        "       train_regret_loser_15[slice25],\r\n",
        "       train_regret_loser_16[slice25],\r\n",
        "       train_regret_loser_17[slice25],\r\n",
        "       train_regret_loser_18[slice25],\r\n",
        "       train_regret_loser_19[slice25],\r\n",
        "       train_regret_loser_20[slice25]]\r\n",
        "\r\n",
        "winner25 = [train_regret_winner_1[slice25],\r\n",
        "       train_regret_winner_2[slice25],\r\n",
        "       train_regret_winner_3[slice25],\r\n",
        "       train_regret_winner_4[slice25],\r\n",
        "       train_regret_winner_5[slice25],\r\n",
        "       train_regret_winner_6[slice25],\r\n",
        "       train_regret_winner_7[slice25],\r\n",
        "       train_regret_winner_8[slice25],\r\n",
        "       train_regret_winner_9[slice25],\r\n",
        "       train_regret_winner_10[slice25],\r\n",
        "       train_regret_winner_11[slice25],\r\n",
        "       train_regret_winner_12[slice25],\r\n",
        "       train_regret_winner_13[slice25],\r\n",
        "       train_regret_winner_14[slice25],\r\n",
        "       train_regret_winner_15[slice25],\r\n",
        "       train_regret_winner_16[slice25],\r\n",
        "       train_regret_winner_17[slice25],\r\n",
        "       train_regret_winner_18[slice25],\r\n",
        "       train_regret_winner_19[slice25],\r\n",
        "       train_regret_winner_20[slice25]]\r\n",
        "\r\n",
        "loser25_results = pd.DataFrame(loser25).sort_values(by=[0], ascending=False)\r\n",
        "winner25_results = pd.DataFrame(winner25).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser25 = np.asarray(loser25_results[4:5][0])[0]\r\n",
        "median_loser25 = np.asarray(loser25_results[9:10][0])[0]\r\n",
        "upper_loser25 = np.asarray(loser25_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner25 = np.asarray(winner25_results[4:5][0])[0]\r\n",
        "median_winner25 = np.asarray(winner25_results[9:10][0])[0]\r\n",
        "upper_winner25 = np.asarray(winner25_results[14:15][0])[0]"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUk9UtSTw_2N"
      },
      "source": [
        "# Iteration35 :\r\n",
        "\r\n",
        "slice35 = 34\r\n",
        "\r\n",
        "loser35 = [train_regret_loser_1[slice35],\r\n",
        "       train_regret_loser_2[slice35],\r\n",
        "       train_regret_loser_3[slice35],\r\n",
        "       train_regret_loser_4[slice35],\r\n",
        "       train_regret_loser_5[slice35],\r\n",
        "       train_regret_loser_6[slice35],\r\n",
        "       train_regret_loser_7[slice35],\r\n",
        "       train_regret_loser_8[slice35],\r\n",
        "       train_regret_loser_9[slice35],\r\n",
        "       train_regret_loser_10[slice35],\r\n",
        "       train_regret_loser_11[slice35],\r\n",
        "       train_regret_loser_12[slice35],\r\n",
        "       train_regret_loser_13[slice35],\r\n",
        "       train_regret_loser_14[slice35],\r\n",
        "       train_regret_loser_15[slice35],\r\n",
        "       train_regret_loser_16[slice35],\r\n",
        "       train_regret_loser_17[slice35],\r\n",
        "       train_regret_loser_18[slice35],\r\n",
        "       train_regret_loser_19[slice35],\r\n",
        "       train_regret_loser_20[slice35]]\r\n",
        "\r\n",
        "winner35 = [train_regret_winner_1[slice35],\r\n",
        "       train_regret_winner_2[slice35],\r\n",
        "       train_regret_winner_3[slice35],\r\n",
        "       train_regret_winner_4[slice35],\r\n",
        "       train_regret_winner_5[slice35],\r\n",
        "       train_regret_winner_6[slice35],\r\n",
        "       train_regret_winner_7[slice35],\r\n",
        "       train_regret_winner_8[slice35],\r\n",
        "       train_regret_winner_9[slice35],\r\n",
        "       train_regret_winner_10[slice35],\r\n",
        "       train_regret_winner_11[slice35],\r\n",
        "       train_regret_winner_12[slice35],\r\n",
        "       train_regret_winner_13[slice35],\r\n",
        "       train_regret_winner_14[slice35],\r\n",
        "       train_regret_winner_15[slice35],\r\n",
        "       train_regret_winner_16[slice35],\r\n",
        "       train_regret_winner_17[slice35],\r\n",
        "       train_regret_winner_18[slice35],\r\n",
        "       train_regret_winner_19[slice35],\r\n",
        "       train_regret_winner_20[slice35]]\r\n",
        "\r\n",
        "loser35_results = pd.DataFrame(loser35).sort_values(by=[0], ascending=False)\r\n",
        "winner35_results = pd.DataFrame(winner35).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser35 = np.asarray(loser35_results[4:5][0])[0]\r\n",
        "median_loser35 = np.asarray(loser35_results[9:10][0])[0]\r\n",
        "upper_loser35 = np.asarray(loser35_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner35 = np.asarray(winner35_results[4:5][0])[0]\r\n",
        "median_winner35 = np.asarray(winner35_results[9:10][0])[0]\r\n",
        "upper_winner35 = np.asarray(winner35_results[14:15][0])[0]"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX3VgqD_w_4z"
      },
      "source": [
        "# Iteration45 :\r\n",
        "\r\n",
        "slice45 = 44\r\n",
        "\r\n",
        "loser45 = [train_regret_loser_1[slice45],\r\n",
        "       train_regret_loser_2[slice45],\r\n",
        "       train_regret_loser_3[slice45],\r\n",
        "       train_regret_loser_4[slice45],\r\n",
        "       train_regret_loser_5[slice45],\r\n",
        "       train_regret_loser_6[slice45],\r\n",
        "       train_regret_loser_7[slice45],\r\n",
        "       train_regret_loser_8[slice45],\r\n",
        "       train_regret_loser_9[slice45],\r\n",
        "       train_regret_loser_10[slice45],\r\n",
        "       train_regret_loser_11[slice45],\r\n",
        "       train_regret_loser_12[slice45],\r\n",
        "       train_regret_loser_13[slice45],\r\n",
        "       train_regret_loser_14[slice45],\r\n",
        "       train_regret_loser_15[slice45],\r\n",
        "       train_regret_loser_16[slice45],\r\n",
        "       train_regret_loser_17[slice45],\r\n",
        "       train_regret_loser_18[slice45],\r\n",
        "       train_regret_loser_19[slice45],\r\n",
        "       train_regret_loser_20[slice45]]\r\n",
        "\r\n",
        "winner45 = [train_regret_winner_1[slice45],\r\n",
        "       train_regret_winner_2[slice45],\r\n",
        "       train_regret_winner_3[slice45],\r\n",
        "       train_regret_winner_4[slice45],\r\n",
        "       train_regret_winner_5[slice45],\r\n",
        "       train_regret_winner_6[slice45],\r\n",
        "       train_regret_winner_7[slice45],\r\n",
        "       train_regret_winner_8[slice45],\r\n",
        "       train_regret_winner_9[slice45],\r\n",
        "       train_regret_winner_10[slice45],\r\n",
        "       train_regret_winner_11[slice45],\r\n",
        "       train_regret_winner_12[slice45],\r\n",
        "       train_regret_winner_13[slice45],\r\n",
        "       train_regret_winner_14[slice45],\r\n",
        "       train_regret_winner_15[slice45],\r\n",
        "       train_regret_winner_16[slice45],\r\n",
        "       train_regret_winner_17[slice45],\r\n",
        "       train_regret_winner_18[slice45],\r\n",
        "       train_regret_winner_19[slice45],\r\n",
        "       train_regret_winner_20[slice45]]\r\n",
        "\r\n",
        "loser45_results = pd.DataFrame(loser45).sort_values(by=[0], ascending=False)\r\n",
        "winner45_results = pd.DataFrame(winner45).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser45 = np.asarray(loser45_results[4:5][0])[0]\r\n",
        "median_loser45 = np.asarray(loser45_results[9:10][0])[0]\r\n",
        "upper_loser45 = np.asarray(loser45_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner45 = np.asarray(winner45_results[4:5][0])[0]\r\n",
        "median_winner45 = np.asarray(winner45_results[9:10][0])[0]\r\n",
        "upper_winner45 = np.asarray(winner45_results[14:15][0])[0]"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o35W-AOzw_8F"
      },
      "source": [
        "# Iteration55 :\r\n",
        "\r\n",
        "slice55 = 54\r\n",
        "\r\n",
        "loser55 = [train_regret_loser_1[slice55],\r\n",
        "       train_regret_loser_2[slice55],\r\n",
        "       train_regret_loser_3[slice55],\r\n",
        "       train_regret_loser_4[slice55],\r\n",
        "       train_regret_loser_5[slice55],\r\n",
        "       train_regret_loser_6[slice55],\r\n",
        "       train_regret_loser_7[slice55],\r\n",
        "       train_regret_loser_8[slice55],\r\n",
        "       train_regret_loser_9[slice55],\r\n",
        "       train_regret_loser_10[slice55],\r\n",
        "       train_regret_loser_11[slice55],\r\n",
        "       train_regret_loser_12[slice55],\r\n",
        "       train_regret_loser_13[slice55],\r\n",
        "       train_regret_loser_14[slice55],\r\n",
        "       train_regret_loser_15[slice55],\r\n",
        "       train_regret_loser_16[slice55],\r\n",
        "       train_regret_loser_17[slice55],\r\n",
        "       train_regret_loser_18[slice55],\r\n",
        "       train_regret_loser_19[slice55],\r\n",
        "       train_regret_loser_20[slice55]]\r\n",
        "\r\n",
        "winner55 = [train_regret_winner_1[slice55],\r\n",
        "       train_regret_winner_2[slice55],\r\n",
        "       train_regret_winner_3[slice55],\r\n",
        "       train_regret_winner_4[slice55],\r\n",
        "       train_regret_winner_5[slice55],\r\n",
        "       train_regret_winner_6[slice55],\r\n",
        "       train_regret_winner_7[slice55],\r\n",
        "       train_regret_winner_8[slice55],\r\n",
        "       train_regret_winner_9[slice55],\r\n",
        "       train_regret_winner_10[slice55],\r\n",
        "       train_regret_winner_11[slice55],\r\n",
        "       train_regret_winner_12[slice55],\r\n",
        "       train_regret_winner_13[slice55],\r\n",
        "       train_regret_winner_14[slice55],\r\n",
        "       train_regret_winner_15[slice55],\r\n",
        "       train_regret_winner_16[slice55],\r\n",
        "       train_regret_winner_17[slice55],\r\n",
        "       train_regret_winner_18[slice55],\r\n",
        "       train_regret_winner_19[slice55],\r\n",
        "       train_regret_winner_20[slice55]]\r\n",
        "\r\n",
        "loser55_results = pd.DataFrame(loser55).sort_values(by=[0], ascending=False)\r\n",
        "winner55_results = pd.DataFrame(winner55).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser55 = np.asarray(loser55_results[4:5][0])[0]\r\n",
        "median_loser55 = np.asarray(loser55_results[9:10][0])[0]\r\n",
        "upper_loser55 = np.asarray(loser55_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner55 = np.asarray(winner55_results[4:5][0])[0]\r\n",
        "median_winner55 = np.asarray(winner55_results[9:10][0])[0]\r\n",
        "upper_winner55 = np.asarray(winner55_results[14:15][0])[0]"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUAjKXWdw__J"
      },
      "source": [
        "# Iteration65 :\r\n",
        "\r\n",
        "slice65 = 64\r\n",
        "\r\n",
        "loser65 = [train_regret_loser_1[slice65],\r\n",
        "       train_regret_loser_2[slice65],\r\n",
        "       train_regret_loser_3[slice65],\r\n",
        "       train_regret_loser_4[slice65],\r\n",
        "       train_regret_loser_5[slice65],\r\n",
        "       train_regret_loser_6[slice65],\r\n",
        "       train_regret_loser_7[slice65],\r\n",
        "       train_regret_loser_8[slice65],\r\n",
        "       train_regret_loser_9[slice65],\r\n",
        "       train_regret_loser_10[slice65],\r\n",
        "       train_regret_loser_11[slice65],\r\n",
        "       train_regret_loser_12[slice65],\r\n",
        "       train_regret_loser_13[slice65],\r\n",
        "       train_regret_loser_14[slice65],\r\n",
        "       train_regret_loser_15[slice65],\r\n",
        "       train_regret_loser_16[slice65],\r\n",
        "       train_regret_loser_17[slice65],\r\n",
        "       train_regret_loser_18[slice65],\r\n",
        "       train_regret_loser_19[slice65],\r\n",
        "       train_regret_loser_20[slice65]]\r\n",
        "\r\n",
        "winner65 = [train_regret_winner_1[slice65],\r\n",
        "       train_regret_winner_2[slice65],\r\n",
        "       train_regret_winner_3[slice65],\r\n",
        "       train_regret_winner_4[slice65],\r\n",
        "       train_regret_winner_5[slice65],\r\n",
        "       train_regret_winner_6[slice65],\r\n",
        "       train_regret_winner_7[slice65],\r\n",
        "       train_regret_winner_8[slice65],\r\n",
        "       train_regret_winner_9[slice65],\r\n",
        "       train_regret_winner_10[slice65],\r\n",
        "       train_regret_winner_11[slice65],\r\n",
        "       train_regret_winner_12[slice65],\r\n",
        "       train_regret_winner_13[slice65],\r\n",
        "       train_regret_winner_14[slice65],\r\n",
        "       train_regret_winner_15[slice65],\r\n",
        "       train_regret_winner_16[slice65],\r\n",
        "       train_regret_winner_17[slice65],\r\n",
        "       train_regret_winner_18[slice65],\r\n",
        "       train_regret_winner_19[slice65],\r\n",
        "       train_regret_winner_20[slice65]]\r\n",
        "\r\n",
        "loser65_results = pd.DataFrame(loser65).sort_values(by=[0], ascending=False)\r\n",
        "winner65_results = pd.DataFrame(winner65).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser65 = np.asarray(loser65_results[4:5][0])[0]\r\n",
        "median_loser65 = np.asarray(loser65_results[9:10][0])[0]\r\n",
        "upper_loser65 = np.asarray(loser65_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner65 = np.asarray(winner65_results[4:5][0])[0]\r\n",
        "median_winner65 = np.asarray(winner65_results[9:10][0])[0]\r\n",
        "upper_winner65 = np.asarray(winner65_results[14:15][0])[0]"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D00FcmZwxAB9"
      },
      "source": [
        "# Iteration75 :\r\n",
        "\r\n",
        "slice75 = 74\r\n",
        "\r\n",
        "loser75 = [train_regret_loser_1[slice75],\r\n",
        "       train_regret_loser_2[slice75],\r\n",
        "       train_regret_loser_3[slice75],\r\n",
        "       train_regret_loser_4[slice75],\r\n",
        "       train_regret_loser_5[slice75],\r\n",
        "       train_regret_loser_6[slice75],\r\n",
        "       train_regret_loser_7[slice75],\r\n",
        "       train_regret_loser_8[slice75],\r\n",
        "       train_regret_loser_9[slice75],\r\n",
        "       train_regret_loser_10[slice75],\r\n",
        "       train_regret_loser_11[slice75],\r\n",
        "       train_regret_loser_12[slice75],\r\n",
        "       train_regret_loser_13[slice75],\r\n",
        "       train_regret_loser_14[slice75],\r\n",
        "       train_regret_loser_15[slice75],\r\n",
        "       train_regret_loser_16[slice75],\r\n",
        "       train_regret_loser_17[slice75],\r\n",
        "       train_regret_loser_18[slice75],\r\n",
        "       train_regret_loser_19[slice75],\r\n",
        "       train_regret_loser_20[slice75]]\r\n",
        "\r\n",
        "winner75 = [train_regret_winner_1[slice75],\r\n",
        "       train_regret_winner_2[slice75],\r\n",
        "       train_regret_winner_3[slice75],\r\n",
        "       train_regret_winner_4[slice75],\r\n",
        "       train_regret_winner_5[slice75],\r\n",
        "       train_regret_winner_6[slice75],\r\n",
        "       train_regret_winner_7[slice75],\r\n",
        "       train_regret_winner_8[slice75],\r\n",
        "       train_regret_winner_9[slice75],\r\n",
        "       train_regret_winner_10[slice75],\r\n",
        "       train_regret_winner_11[slice75],\r\n",
        "       train_regret_winner_12[slice75],\r\n",
        "       train_regret_winner_13[slice75],\r\n",
        "       train_regret_winner_14[slice75],\r\n",
        "       train_regret_winner_15[slice75],\r\n",
        "       train_regret_winner_16[slice75],\r\n",
        "       train_regret_winner_17[slice75],\r\n",
        "       train_regret_winner_18[slice75],\r\n",
        "       train_regret_winner_19[slice75],\r\n",
        "       train_regret_winner_20[slice75]]\r\n",
        "\r\n",
        "loser75_results = pd.DataFrame(loser75).sort_values(by=[0], ascending=False)\r\n",
        "winner75_results = pd.DataFrame(winner75).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser75 = np.asarray(loser75_results[4:5][0])[0]\r\n",
        "median_loser75 = np.asarray(loser75_results[9:10][0])[0]\r\n",
        "upper_loser75 = np.asarray(loser75_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner75 = np.asarray(winner75_results[4:5][0])[0]\r\n",
        "median_winner75 = np.asarray(winner75_results[9:10][0])[0]\r\n",
        "upper_winner75 = np.asarray(winner75_results[14:15][0])[0]"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvnzmprxxAFB"
      },
      "source": [
        "# Iteration85 :\r\n",
        "\r\n",
        "slice85 = 84\r\n",
        "\r\n",
        "loser85 = [train_regret_loser_1[slice85],\r\n",
        "       train_regret_loser_2[slice85],\r\n",
        "       train_regret_loser_3[slice85],\r\n",
        "       train_regret_loser_4[slice85],\r\n",
        "       train_regret_loser_5[slice85],\r\n",
        "       train_regret_loser_6[slice85],\r\n",
        "       train_regret_loser_7[slice85],\r\n",
        "       train_regret_loser_8[slice85],\r\n",
        "       train_regret_loser_9[slice85],\r\n",
        "       train_regret_loser_10[slice85],\r\n",
        "       train_regret_loser_11[slice85],\r\n",
        "       train_regret_loser_12[slice85],\r\n",
        "       train_regret_loser_13[slice85],\r\n",
        "       train_regret_loser_14[slice85],\r\n",
        "       train_regret_loser_15[slice85],\r\n",
        "       train_regret_loser_16[slice85],\r\n",
        "       train_regret_loser_17[slice85],\r\n",
        "       train_regret_loser_18[slice85],\r\n",
        "       train_regret_loser_19[slice85],\r\n",
        "       train_regret_loser_20[slice85]]\r\n",
        "\r\n",
        "winner85 = [train_regret_winner_1[slice85],\r\n",
        "       train_regret_winner_2[slice85],\r\n",
        "       train_regret_winner_3[slice85],\r\n",
        "       train_regret_winner_4[slice85],\r\n",
        "       train_regret_winner_5[slice85],\r\n",
        "       train_regret_winner_6[slice85],\r\n",
        "       train_regret_winner_7[slice85],\r\n",
        "       train_regret_winner_8[slice85],\r\n",
        "       train_regret_winner_9[slice85],\r\n",
        "       train_regret_winner_10[slice85],\r\n",
        "       train_regret_winner_11[slice85],\r\n",
        "       train_regret_winner_12[slice85],\r\n",
        "       train_regret_winner_13[slice85],\r\n",
        "       train_regret_winner_14[slice85],\r\n",
        "       train_regret_winner_15[slice85],\r\n",
        "       train_regret_winner_16[slice85],\r\n",
        "       train_regret_winner_17[slice85],\r\n",
        "       train_regret_winner_18[slice85],\r\n",
        "       train_regret_winner_19[slice85],\r\n",
        "       train_regret_winner_20[slice85]]\r\n",
        "\r\n",
        "loser85_results = pd.DataFrame(loser85).sort_values(by=[0], ascending=False)\r\n",
        "winner85_results = pd.DataFrame(winner85).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser85 = np.asarray(loser85_results[4:5][0])[0]\r\n",
        "median_loser85 = np.asarray(loser85_results[9:10][0])[0]\r\n",
        "upper_loser85 = np.asarray(loser85_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner85 = np.asarray(winner85_results[4:5][0])[0]\r\n",
        "median_winner85 = np.asarray(winner85_results[9:10][0])[0]\r\n",
        "upper_winner85 = np.asarray(winner85_results[14:15][0])[0]"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztvJhrN6xAH1"
      },
      "source": [
        "# Iteration95 :\r\n",
        "\r\n",
        "slice95 = 94\r\n",
        "\r\n",
        "loser95 = [train_regret_loser_1[slice95],\r\n",
        "       train_regret_loser_2[slice95],\r\n",
        "       train_regret_loser_3[slice95],\r\n",
        "       train_regret_loser_4[slice95],\r\n",
        "       train_regret_loser_5[slice95],\r\n",
        "       train_regret_loser_6[slice95],\r\n",
        "       train_regret_loser_7[slice95],\r\n",
        "       train_regret_loser_8[slice95],\r\n",
        "       train_regret_loser_9[slice95],\r\n",
        "       train_regret_loser_10[slice95],\r\n",
        "       train_regret_loser_11[slice95],\r\n",
        "       train_regret_loser_12[slice95],\r\n",
        "       train_regret_loser_13[slice95],\r\n",
        "       train_regret_loser_14[slice95],\r\n",
        "       train_regret_loser_15[slice95],\r\n",
        "       train_regret_loser_16[slice95],\r\n",
        "       train_regret_loser_17[slice95],\r\n",
        "       train_regret_loser_18[slice95],\r\n",
        "       train_regret_loser_19[slice95],\r\n",
        "       train_regret_loser_20[slice95]]\r\n",
        "\r\n",
        "winner95 = [train_regret_winner_1[slice95],\r\n",
        "       train_regret_winner_2[slice95],\r\n",
        "       train_regret_winner_3[slice95],\r\n",
        "       train_regret_winner_4[slice95],\r\n",
        "       train_regret_winner_5[slice95],\r\n",
        "       train_regret_winner_6[slice95],\r\n",
        "       train_regret_winner_7[slice95],\r\n",
        "       train_regret_winner_8[slice95],\r\n",
        "       train_regret_winner_9[slice95],\r\n",
        "       train_regret_winner_10[slice95],\r\n",
        "       train_regret_winner_11[slice95],\r\n",
        "       train_regret_winner_12[slice95],\r\n",
        "       train_regret_winner_13[slice95],\r\n",
        "       train_regret_winner_14[slice95],\r\n",
        "       train_regret_winner_15[slice95],\r\n",
        "       train_regret_winner_16[slice95],\r\n",
        "       train_regret_winner_17[slice95],\r\n",
        "       train_regret_winner_18[slice95],\r\n",
        "       train_regret_winner_19[slice95],\r\n",
        "       train_regret_winner_20[slice95]]\r\n",
        "\r\n",
        "loser95_results = pd.DataFrame(loser95).sort_values(by=[0], ascending=False)\r\n",
        "winner95_results = pd.DataFrame(winner95).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser95 = np.asarray(loser95_results[4:5][0])[0]\r\n",
        "median_loser95 = np.asarray(loser95_results[9:10][0])[0]\r\n",
        "upper_loser95 = np.asarray(loser95_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner95 = np.asarray(winner95_results[4:5][0])[0]\r\n",
        "median_winner95 = np.asarray(winner95_results[9:10][0])[0]\r\n",
        "upper_winner95 = np.asarray(winner95_results[14:15][0])[0]"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK-4nr-OxALA"
      },
      "source": [
        "# Iteration6 :\r\n",
        "\r\n",
        "slice6 = 5\r\n",
        "\r\n",
        "loser6 = [train_regret_loser_1[slice6],\r\n",
        "       train_regret_loser_2[slice6],\r\n",
        "       train_regret_loser_3[slice6],\r\n",
        "       train_regret_loser_4[slice6],\r\n",
        "       train_regret_loser_5[slice6],\r\n",
        "       train_regret_loser_6[slice6],\r\n",
        "       train_regret_loser_7[slice6],\r\n",
        "       train_regret_loser_8[slice6],\r\n",
        "       train_regret_loser_9[slice6],\r\n",
        "       train_regret_loser_10[slice6],\r\n",
        "       train_regret_loser_11[slice6],\r\n",
        "       train_regret_loser_12[slice6],\r\n",
        "       train_regret_loser_13[slice6],\r\n",
        "       train_regret_loser_14[slice6],\r\n",
        "       train_regret_loser_15[slice6],\r\n",
        "       train_regret_loser_16[slice6],\r\n",
        "       train_regret_loser_17[slice6],\r\n",
        "       train_regret_loser_18[slice6],\r\n",
        "       train_regret_loser_19[slice6],\r\n",
        "       train_regret_loser_20[slice6]]\r\n",
        "\r\n",
        "winner6 = [train_regret_winner_1[slice6],\r\n",
        "       train_regret_winner_2[slice6],\r\n",
        "       train_regret_winner_3[slice6],\r\n",
        "       train_regret_winner_4[slice6],\r\n",
        "       train_regret_winner_5[slice6],\r\n",
        "       train_regret_winner_6[slice6],\r\n",
        "       train_regret_winner_7[slice6],\r\n",
        "       train_regret_winner_8[slice6],\r\n",
        "       train_regret_winner_9[slice6],\r\n",
        "       train_regret_winner_10[slice6],\r\n",
        "       train_regret_winner_11[slice6],\r\n",
        "       train_regret_winner_12[slice6],\r\n",
        "       train_regret_winner_13[slice6],\r\n",
        "       train_regret_winner_14[slice6],\r\n",
        "       train_regret_winner_15[slice6],\r\n",
        "       train_regret_winner_16[slice6],\r\n",
        "       train_regret_winner_17[slice6],\r\n",
        "       train_regret_winner_18[slice6],\r\n",
        "       train_regret_winner_19[slice6],\r\n",
        "       train_regret_winner_20[slice6]]\r\n",
        "\r\n",
        "loser6_results = pd.DataFrame(loser6).sort_values(by=[0], ascending=False)\r\n",
        "winner6_results = pd.DataFrame(winner6).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser6 = np.asarray(loser6_results[4:5][0])[0]\r\n",
        "median_loser6 = np.asarray(loser6_results[9:10][0])[0]\r\n",
        "upper_loser6 = np.asarray(loser6_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner6 = np.asarray(winner6_results[4:5][0])[0]\r\n",
        "median_winner6 = np.asarray(winner6_results[9:10][0])[0]\r\n",
        "upper_winner6 = np.asarray(winner6_results[14:15][0])[0]"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSwfNX5ExANt"
      },
      "source": [
        "# Iteration16 :\r\n",
        "\r\n",
        "slice16 = 15\r\n",
        "\r\n",
        "loser16 = [train_regret_loser_1[slice16],\r\n",
        "       train_regret_loser_2[slice16],\r\n",
        "       train_regret_loser_3[slice16],\r\n",
        "       train_regret_loser_4[slice16],\r\n",
        "       train_regret_loser_5[slice16],\r\n",
        "       train_regret_loser_6[slice16],\r\n",
        "       train_regret_loser_7[slice16],\r\n",
        "       train_regret_loser_8[slice16],\r\n",
        "       train_regret_loser_9[slice16],\r\n",
        "       train_regret_loser_10[slice16],\r\n",
        "       train_regret_loser_11[slice16],\r\n",
        "       train_regret_loser_12[slice16],\r\n",
        "       train_regret_loser_13[slice16],\r\n",
        "       train_regret_loser_14[slice16],\r\n",
        "       train_regret_loser_15[slice16],\r\n",
        "       train_regret_loser_16[slice16],\r\n",
        "       train_regret_loser_17[slice16],\r\n",
        "       train_regret_loser_18[slice16],\r\n",
        "       train_regret_loser_19[slice16],\r\n",
        "       train_regret_loser_20[slice16]]\r\n",
        "\r\n",
        "winner16 = [train_regret_winner_1[slice16],\r\n",
        "       train_regret_winner_2[slice16],\r\n",
        "       train_regret_winner_3[slice16],\r\n",
        "       train_regret_winner_4[slice16],\r\n",
        "       train_regret_winner_5[slice16],\r\n",
        "       train_regret_winner_6[slice16],\r\n",
        "       train_regret_winner_7[slice16],\r\n",
        "       train_regret_winner_8[slice16],\r\n",
        "       train_regret_winner_9[slice16],\r\n",
        "       train_regret_winner_10[slice16],\r\n",
        "       train_regret_winner_11[slice16],\r\n",
        "       train_regret_winner_12[slice16],\r\n",
        "       train_regret_winner_13[slice16],\r\n",
        "       train_regret_winner_14[slice16],\r\n",
        "       train_regret_winner_15[slice16],\r\n",
        "       train_regret_winner_16[slice16],\r\n",
        "       train_regret_winner_17[slice16],\r\n",
        "       train_regret_winner_18[slice16],\r\n",
        "       train_regret_winner_19[slice16],\r\n",
        "       train_regret_winner_20[slice16]]\r\n",
        "\r\n",
        "loser16_results = pd.DataFrame(loser16).sort_values(by=[0], ascending=False)\r\n",
        "winner16_results = pd.DataFrame(winner16).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser16 = np.asarray(loser16_results[4:5][0])[0]\r\n",
        "median_loser16 = np.asarray(loser16_results[9:10][0])[0]\r\n",
        "upper_loser16 = np.asarray(loser16_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner16 = np.asarray(winner16_results[4:5][0])[0]\r\n",
        "median_winner16 = np.asarray(winner16_results[9:10][0])[0]\r\n",
        "upper_winner16 = np.asarray(winner16_results[14:15][0])[0]"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOB3hOLIxAQy"
      },
      "source": [
        "# Iteration26 :\r\n",
        "\r\n",
        "slice26 = 25\r\n",
        "\r\n",
        "loser26 = [train_regret_loser_1[slice26],\r\n",
        "       train_regret_loser_2[slice26],\r\n",
        "       train_regret_loser_3[slice26],\r\n",
        "       train_regret_loser_4[slice26],\r\n",
        "       train_regret_loser_5[slice26],\r\n",
        "       train_regret_loser_6[slice26],\r\n",
        "       train_regret_loser_7[slice26],\r\n",
        "       train_regret_loser_8[slice26],\r\n",
        "       train_regret_loser_9[slice26],\r\n",
        "       train_regret_loser_10[slice26],\r\n",
        "       train_regret_loser_11[slice26],\r\n",
        "       train_regret_loser_12[slice26],\r\n",
        "       train_regret_loser_13[slice26],\r\n",
        "       train_regret_loser_14[slice26],\r\n",
        "       train_regret_loser_15[slice26],\r\n",
        "       train_regret_loser_16[slice26],\r\n",
        "       train_regret_loser_17[slice26],\r\n",
        "       train_regret_loser_18[slice26],\r\n",
        "       train_regret_loser_19[slice26],\r\n",
        "       train_regret_loser_20[slice26]]\r\n",
        "\r\n",
        "winner26 = [train_regret_winner_1[slice26],\r\n",
        "       train_regret_winner_2[slice26],\r\n",
        "       train_regret_winner_3[slice26],\r\n",
        "       train_regret_winner_4[slice26],\r\n",
        "       train_regret_winner_5[slice26],\r\n",
        "       train_regret_winner_6[slice26],\r\n",
        "       train_regret_winner_7[slice26],\r\n",
        "       train_regret_winner_8[slice26],\r\n",
        "       train_regret_winner_9[slice26],\r\n",
        "       train_regret_winner_10[slice26],\r\n",
        "       train_regret_winner_11[slice26],\r\n",
        "       train_regret_winner_12[slice26],\r\n",
        "       train_regret_winner_13[slice26],\r\n",
        "       train_regret_winner_14[slice26],\r\n",
        "       train_regret_winner_15[slice26],\r\n",
        "       train_regret_winner_16[slice26],\r\n",
        "       train_regret_winner_17[slice26],\r\n",
        "       train_regret_winner_18[slice26],\r\n",
        "       train_regret_winner_19[slice26],\r\n",
        "       train_regret_winner_20[slice26]]\r\n",
        "\r\n",
        "loser26_results = pd.DataFrame(loser26).sort_values(by=[0], ascending=False)\r\n",
        "winner26_results = pd.DataFrame(winner26).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser26 = np.asarray(loser26_results[4:5][0])[0]\r\n",
        "median_loser26 = np.asarray(loser26_results[9:10][0])[0]\r\n",
        "upper_loser26 = np.asarray(loser26_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner26 = np.asarray(winner26_results[4:5][0])[0]\r\n",
        "median_winner26 = np.asarray(winner26_results[9:10][0])[0]\r\n",
        "upper_winner26 = np.asarray(winner26_results[14:15][0])[0]"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-0hQMtOxATh"
      },
      "source": [
        "# Iteration36 :\r\n",
        "\r\n",
        "slice36 = 35\r\n",
        "\r\n",
        "loser36 = [train_regret_loser_1[slice36],\r\n",
        "       train_regret_loser_2[slice36],\r\n",
        "       train_regret_loser_3[slice36],\r\n",
        "       train_regret_loser_4[slice36],\r\n",
        "       train_regret_loser_5[slice36],\r\n",
        "       train_regret_loser_6[slice36],\r\n",
        "       train_regret_loser_7[slice36],\r\n",
        "       train_regret_loser_8[slice36],\r\n",
        "       train_regret_loser_9[slice36],\r\n",
        "       train_regret_loser_10[slice36],\r\n",
        "       train_regret_loser_11[slice36],\r\n",
        "       train_regret_loser_12[slice36],\r\n",
        "       train_regret_loser_13[slice36],\r\n",
        "       train_regret_loser_14[slice36],\r\n",
        "       train_regret_loser_15[slice36],\r\n",
        "       train_regret_loser_16[slice36],\r\n",
        "       train_regret_loser_17[slice36],\r\n",
        "       train_regret_loser_18[slice36],\r\n",
        "       train_regret_loser_19[slice36],\r\n",
        "       train_regret_loser_20[slice36]]\r\n",
        "\r\n",
        "winner36 = [train_regret_winner_1[slice36],\r\n",
        "       train_regret_winner_2[slice36],\r\n",
        "       train_regret_winner_3[slice36],\r\n",
        "       train_regret_winner_4[slice36],\r\n",
        "       train_regret_winner_5[slice36],\r\n",
        "       train_regret_winner_6[slice36],\r\n",
        "       train_regret_winner_7[slice36],\r\n",
        "       train_regret_winner_8[slice36],\r\n",
        "       train_regret_winner_9[slice36],\r\n",
        "       train_regret_winner_10[slice36],\r\n",
        "       train_regret_winner_11[slice36],\r\n",
        "       train_regret_winner_12[slice36],\r\n",
        "       train_regret_winner_13[slice36],\r\n",
        "       train_regret_winner_14[slice36],\r\n",
        "       train_regret_winner_15[slice36],\r\n",
        "       train_regret_winner_16[slice36],\r\n",
        "       train_regret_winner_17[slice36],\r\n",
        "       train_regret_winner_18[slice36],\r\n",
        "       train_regret_winner_19[slice36],\r\n",
        "       train_regret_winner_20[slice36]]\r\n",
        "\r\n",
        "loser36_results = pd.DataFrame(loser36).sort_values(by=[0], ascending=False)\r\n",
        "winner36_results = pd.DataFrame(winner36).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser36 = np.asarray(loser36_results[4:5][0])[0]\r\n",
        "median_loser36 = np.asarray(loser36_results[9:10][0])[0]\r\n",
        "upper_loser36 = np.asarray(loser36_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner36 = np.asarray(winner36_results[4:5][0])[0]\r\n",
        "median_winner36 = np.asarray(winner36_results[9:10][0])[0]\r\n",
        "upper_winner36 = np.asarray(winner36_results[14:15][0])[0]"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSqBLqTgxAWs"
      },
      "source": [
        "# Iteration46 :\r\n",
        "\r\n",
        "slice46 = 45\r\n",
        "\r\n",
        "loser46 = [train_regret_loser_1[slice46],\r\n",
        "       train_regret_loser_2[slice46],\r\n",
        "       train_regret_loser_3[slice46],\r\n",
        "       train_regret_loser_4[slice46],\r\n",
        "       train_regret_loser_5[slice46],\r\n",
        "       train_regret_loser_6[slice46],\r\n",
        "       train_regret_loser_7[slice46],\r\n",
        "       train_regret_loser_8[slice46],\r\n",
        "       train_regret_loser_9[slice46],\r\n",
        "       train_regret_loser_10[slice46],\r\n",
        "       train_regret_loser_11[slice46],\r\n",
        "       train_regret_loser_12[slice46],\r\n",
        "       train_regret_loser_13[slice46],\r\n",
        "       train_regret_loser_14[slice46],\r\n",
        "       train_regret_loser_15[slice46],\r\n",
        "       train_regret_loser_16[slice46],\r\n",
        "       train_regret_loser_17[slice46],\r\n",
        "       train_regret_loser_18[slice46],\r\n",
        "       train_regret_loser_19[slice46],\r\n",
        "       train_regret_loser_20[slice46]]\r\n",
        "\r\n",
        "winner46 = [train_regret_winner_1[slice46],\r\n",
        "       train_regret_winner_2[slice46],\r\n",
        "       train_regret_winner_3[slice46],\r\n",
        "       train_regret_winner_4[slice46],\r\n",
        "       train_regret_winner_5[slice46],\r\n",
        "       train_regret_winner_6[slice46],\r\n",
        "       train_regret_winner_7[slice46],\r\n",
        "       train_regret_winner_8[slice46],\r\n",
        "       train_regret_winner_9[slice46],\r\n",
        "       train_regret_winner_10[slice46],\r\n",
        "       train_regret_winner_11[slice46],\r\n",
        "       train_regret_winner_12[slice46],\r\n",
        "       train_regret_winner_13[slice46],\r\n",
        "       train_regret_winner_14[slice46],\r\n",
        "       train_regret_winner_15[slice46],\r\n",
        "       train_regret_winner_16[slice46],\r\n",
        "       train_regret_winner_17[slice46],\r\n",
        "       train_regret_winner_18[slice46],\r\n",
        "       train_regret_winner_19[slice46],\r\n",
        "       train_regret_winner_20[slice46]]\r\n",
        "\r\n",
        "loser46_results = pd.DataFrame(loser46).sort_values(by=[0], ascending=False)\r\n",
        "winner46_results = pd.DataFrame(winner46).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser46 = np.asarray(loser46_results[4:5][0])[0]\r\n",
        "median_loser46 = np.asarray(loser46_results[9:10][0])[0]\r\n",
        "upper_loser46 = np.asarray(loser46_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner46 = np.asarray(winner46_results[4:5][0])[0]\r\n",
        "median_winner46 = np.asarray(winner46_results[9:10][0])[0]\r\n",
        "upper_winner46 = np.asarray(winner46_results[14:15][0])[0]"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFwO80w6xAZZ"
      },
      "source": [
        "# Iteration56 :\r\n",
        "\r\n",
        "slice56 = 55\r\n",
        "\r\n",
        "loser56 = [train_regret_loser_1[slice56],\r\n",
        "       train_regret_loser_2[slice56],\r\n",
        "       train_regret_loser_3[slice56],\r\n",
        "       train_regret_loser_4[slice56],\r\n",
        "       train_regret_loser_5[slice56],\r\n",
        "       train_regret_loser_6[slice56],\r\n",
        "       train_regret_loser_7[slice56],\r\n",
        "       train_regret_loser_8[slice56],\r\n",
        "       train_regret_loser_9[slice56],\r\n",
        "       train_regret_loser_10[slice56],\r\n",
        "       train_regret_loser_11[slice56],\r\n",
        "       train_regret_loser_12[slice56],\r\n",
        "       train_regret_loser_13[slice56],\r\n",
        "       train_regret_loser_14[slice56],\r\n",
        "       train_regret_loser_15[slice56],\r\n",
        "       train_regret_loser_16[slice56],\r\n",
        "       train_regret_loser_17[slice56],\r\n",
        "       train_regret_loser_18[slice56],\r\n",
        "       train_regret_loser_19[slice56],\r\n",
        "       train_regret_loser_20[slice56]]\r\n",
        "\r\n",
        "winner56 = [train_regret_winner_1[slice56],\r\n",
        "       train_regret_winner_2[slice56],\r\n",
        "       train_regret_winner_3[slice56],\r\n",
        "       train_regret_winner_4[slice56],\r\n",
        "       train_regret_winner_5[slice56],\r\n",
        "       train_regret_winner_6[slice56],\r\n",
        "       train_regret_winner_7[slice56],\r\n",
        "       train_regret_winner_8[slice56],\r\n",
        "       train_regret_winner_9[slice56],\r\n",
        "       train_regret_winner_10[slice56],\r\n",
        "       train_regret_winner_11[slice56],\r\n",
        "       train_regret_winner_12[slice56],\r\n",
        "       train_regret_winner_13[slice56],\r\n",
        "       train_regret_winner_14[slice56],\r\n",
        "       train_regret_winner_15[slice56],\r\n",
        "       train_regret_winner_16[slice56],\r\n",
        "       train_regret_winner_17[slice56],\r\n",
        "       train_regret_winner_18[slice56],\r\n",
        "       train_regret_winner_19[slice56],\r\n",
        "       train_regret_winner_20[slice56]]\r\n",
        "\r\n",
        "loser56_results = pd.DataFrame(loser56).sort_values(by=[0], ascending=False)\r\n",
        "winner56_results = pd.DataFrame(winner56).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser56 = np.asarray(loser56_results[4:5][0])[0]\r\n",
        "median_loser56 = np.asarray(loser56_results[9:10][0])[0]\r\n",
        "upper_loser56 = np.asarray(loser56_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner56 = np.asarray(winner56_results[4:5][0])[0]\r\n",
        "median_winner56 = np.asarray(winner56_results[9:10][0])[0]\r\n",
        "upper_winner56 = np.asarray(winner56_results[14:15][0])[0]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfVZBDP8xAcC"
      },
      "source": [
        "# Iteration66 :\r\n",
        "\r\n",
        "slice66 = 65\r\n",
        "\r\n",
        "loser66 = [train_regret_loser_1[slice66],\r\n",
        "       train_regret_loser_2[slice66],\r\n",
        "       train_regret_loser_3[slice66],\r\n",
        "       train_regret_loser_4[slice66],\r\n",
        "       train_regret_loser_5[slice66],\r\n",
        "       train_regret_loser_6[slice66],\r\n",
        "       train_regret_loser_7[slice66],\r\n",
        "       train_regret_loser_8[slice66],\r\n",
        "       train_regret_loser_9[slice66],\r\n",
        "       train_regret_loser_10[slice66],\r\n",
        "       train_regret_loser_11[slice66],\r\n",
        "       train_regret_loser_12[slice66],\r\n",
        "       train_regret_loser_13[slice66],\r\n",
        "       train_regret_loser_14[slice66],\r\n",
        "       train_regret_loser_15[slice66],\r\n",
        "       train_regret_loser_16[slice66],\r\n",
        "       train_regret_loser_17[slice66],\r\n",
        "       train_regret_loser_18[slice66],\r\n",
        "       train_regret_loser_19[slice66],\r\n",
        "       train_regret_loser_20[slice66]]\r\n",
        "\r\n",
        "winner66 = [train_regret_winner_1[slice66],\r\n",
        "       train_regret_winner_2[slice66],\r\n",
        "       train_regret_winner_3[slice66],\r\n",
        "       train_regret_winner_4[slice66],\r\n",
        "       train_regret_winner_5[slice66],\r\n",
        "       train_regret_winner_6[slice66],\r\n",
        "       train_regret_winner_7[slice66],\r\n",
        "       train_regret_winner_8[slice66],\r\n",
        "       train_regret_winner_9[slice66],\r\n",
        "       train_regret_winner_10[slice66],\r\n",
        "       train_regret_winner_11[slice66],\r\n",
        "       train_regret_winner_12[slice66],\r\n",
        "       train_regret_winner_13[slice66],\r\n",
        "       train_regret_winner_14[slice66],\r\n",
        "       train_regret_winner_15[slice66],\r\n",
        "       train_regret_winner_16[slice66],\r\n",
        "       train_regret_winner_17[slice66],\r\n",
        "       train_regret_winner_18[slice66],\r\n",
        "       train_regret_winner_19[slice66],\r\n",
        "       train_regret_winner_20[slice66]]\r\n",
        "\r\n",
        "loser66_results = pd.DataFrame(loser66).sort_values(by=[0], ascending=False)\r\n",
        "winner66_results = pd.DataFrame(winner66).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser66 = np.asarray(loser66_results[4:5][0])[0]\r\n",
        "median_loser66 = np.asarray(loser66_results[9:10][0])[0]\r\n",
        "upper_loser66 = np.asarray(loser66_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner66 = np.asarray(winner66_results[4:5][0])[0]\r\n",
        "median_winner66 = np.asarray(winner66_results[9:10][0])[0]\r\n",
        "upper_winner66 = np.asarray(winner66_results[14:15][0])[0]"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnSYoG7bxAfL"
      },
      "source": [
        "# Iteration76 :\r\n",
        "\r\n",
        "slice76 = 75\r\n",
        "\r\n",
        "loser76 = [train_regret_loser_1[slice76],\r\n",
        "       train_regret_loser_2[slice76],\r\n",
        "       train_regret_loser_3[slice76],\r\n",
        "       train_regret_loser_4[slice76],\r\n",
        "       train_regret_loser_5[slice76],\r\n",
        "       train_regret_loser_6[slice76],\r\n",
        "       train_regret_loser_7[slice76],\r\n",
        "       train_regret_loser_8[slice76],\r\n",
        "       train_regret_loser_9[slice76],\r\n",
        "       train_regret_loser_10[slice76],\r\n",
        "       train_regret_loser_11[slice76],\r\n",
        "       train_regret_loser_12[slice76],\r\n",
        "       train_regret_loser_13[slice76],\r\n",
        "       train_regret_loser_14[slice76],\r\n",
        "       train_regret_loser_15[slice76],\r\n",
        "       train_regret_loser_16[slice76],\r\n",
        "       train_regret_loser_17[slice76],\r\n",
        "       train_regret_loser_18[slice76],\r\n",
        "       train_regret_loser_19[slice76],\r\n",
        "       train_regret_loser_20[slice76]]\r\n",
        "\r\n",
        "winner76 = [train_regret_winner_1[slice76],\r\n",
        "       train_regret_winner_2[slice76],\r\n",
        "       train_regret_winner_3[slice76],\r\n",
        "       train_regret_winner_4[slice76],\r\n",
        "       train_regret_winner_5[slice76],\r\n",
        "       train_regret_winner_6[slice76],\r\n",
        "       train_regret_winner_7[slice76],\r\n",
        "       train_regret_winner_8[slice76],\r\n",
        "       train_regret_winner_9[slice76],\r\n",
        "       train_regret_winner_10[slice76],\r\n",
        "       train_regret_winner_11[slice76],\r\n",
        "       train_regret_winner_12[slice76],\r\n",
        "       train_regret_winner_13[slice76],\r\n",
        "       train_regret_winner_14[slice76],\r\n",
        "       train_regret_winner_15[slice76],\r\n",
        "       train_regret_winner_16[slice76],\r\n",
        "       train_regret_winner_17[slice76],\r\n",
        "       train_regret_winner_18[slice76],\r\n",
        "       train_regret_winner_19[slice76],\r\n",
        "       train_regret_winner_20[slice76]]\r\n",
        "\r\n",
        "loser76_results = pd.DataFrame(loser76).sort_values(by=[0], ascending=False)\r\n",
        "winner76_results = pd.DataFrame(winner76).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser76 = np.asarray(loser76_results[4:5][0])[0]\r\n",
        "median_loser76 = np.asarray(loser76_results[9:10][0])[0]\r\n",
        "upper_loser76 = np.asarray(loser76_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner76 = np.asarray(winner76_results[4:5][0])[0]\r\n",
        "median_winner76 = np.asarray(winner76_results[9:10][0])[0]\r\n",
        "upper_winner76 = np.asarray(winner76_results[14:15][0])[0]"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ze_Oy9mxAiO"
      },
      "source": [
        "# Iteration86 :\r\n",
        "\r\n",
        "slice86 = 85\r\n",
        "\r\n",
        "loser86 = [train_regret_loser_1[slice86],\r\n",
        "       train_regret_loser_2[slice86],\r\n",
        "       train_regret_loser_3[slice86],\r\n",
        "       train_regret_loser_4[slice86],\r\n",
        "       train_regret_loser_5[slice86],\r\n",
        "       train_regret_loser_6[slice86],\r\n",
        "       train_regret_loser_7[slice86],\r\n",
        "       train_regret_loser_8[slice86],\r\n",
        "       train_regret_loser_9[slice86],\r\n",
        "       train_regret_loser_10[slice86],\r\n",
        "       train_regret_loser_11[slice86],\r\n",
        "       train_regret_loser_12[slice86],\r\n",
        "       train_regret_loser_13[slice86],\r\n",
        "       train_regret_loser_14[slice86],\r\n",
        "       train_regret_loser_15[slice86],\r\n",
        "       train_regret_loser_16[slice86],\r\n",
        "       train_regret_loser_17[slice86],\r\n",
        "       train_regret_loser_18[slice86],\r\n",
        "       train_regret_loser_19[slice86],\r\n",
        "       train_regret_loser_20[slice86]]\r\n",
        "\r\n",
        "winner86 = [train_regret_winner_1[slice86],\r\n",
        "       train_regret_winner_2[slice86],\r\n",
        "       train_regret_winner_3[slice86],\r\n",
        "       train_regret_winner_4[slice86],\r\n",
        "       train_regret_winner_5[slice86],\r\n",
        "       train_regret_winner_6[slice86],\r\n",
        "       train_regret_winner_7[slice86],\r\n",
        "       train_regret_winner_8[slice86],\r\n",
        "       train_regret_winner_9[slice86],\r\n",
        "       train_regret_winner_10[slice86],\r\n",
        "       train_regret_winner_11[slice86],\r\n",
        "       train_regret_winner_12[slice86],\r\n",
        "       train_regret_winner_13[slice86],\r\n",
        "       train_regret_winner_14[slice86],\r\n",
        "       train_regret_winner_15[slice86],\r\n",
        "       train_regret_winner_16[slice86],\r\n",
        "       train_regret_winner_17[slice86],\r\n",
        "       train_regret_winner_18[slice86],\r\n",
        "       train_regret_winner_19[slice86],\r\n",
        "       train_regret_winner_20[slice86]]\r\n",
        "\r\n",
        "loser86_results = pd.DataFrame(loser86).sort_values(by=[0], ascending=False)\r\n",
        "winner86_results = pd.DataFrame(winner86).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser86 = np.asarray(loser86_results[4:5][0])[0]\r\n",
        "median_loser86 = np.asarray(loser86_results[9:10][0])[0]\r\n",
        "upper_loser86 = np.asarray(loser86_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner86 = np.asarray(winner86_results[4:5][0])[0]\r\n",
        "median_winner86 = np.asarray(winner86_results[9:10][0])[0]\r\n",
        "upper_winner86 = np.asarray(winner86_results[14:15][0])[0]"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glsE9_RPxAlS"
      },
      "source": [
        "# Iteration96 :\r\n",
        "\r\n",
        "slice96 = 95\r\n",
        "\r\n",
        "loser96 = [train_regret_loser_1[slice96],\r\n",
        "       train_regret_loser_2[slice96],\r\n",
        "       train_regret_loser_3[slice96],\r\n",
        "       train_regret_loser_4[slice96],\r\n",
        "       train_regret_loser_5[slice96],\r\n",
        "       train_regret_loser_6[slice96],\r\n",
        "       train_regret_loser_7[slice96],\r\n",
        "       train_regret_loser_8[slice96],\r\n",
        "       train_regret_loser_9[slice96],\r\n",
        "       train_regret_loser_10[slice96],\r\n",
        "       train_regret_loser_11[slice96],\r\n",
        "       train_regret_loser_12[slice96],\r\n",
        "       train_regret_loser_13[slice96],\r\n",
        "       train_regret_loser_14[slice96],\r\n",
        "       train_regret_loser_15[slice96],\r\n",
        "       train_regret_loser_16[slice96],\r\n",
        "       train_regret_loser_17[slice96],\r\n",
        "       train_regret_loser_18[slice96],\r\n",
        "       train_regret_loser_19[slice96],\r\n",
        "       train_regret_loser_20[slice96]]\r\n",
        "\r\n",
        "winner96 = [train_regret_winner_1[slice96],\r\n",
        "       train_regret_winner_2[slice96],\r\n",
        "       train_regret_winner_3[slice96],\r\n",
        "       train_regret_winner_4[slice96],\r\n",
        "       train_regret_winner_5[slice96],\r\n",
        "       train_regret_winner_6[slice96],\r\n",
        "       train_regret_winner_7[slice96],\r\n",
        "       train_regret_winner_8[slice96],\r\n",
        "       train_regret_winner_9[slice96],\r\n",
        "       train_regret_winner_10[slice96],\r\n",
        "       train_regret_winner_11[slice96],\r\n",
        "       train_regret_winner_12[slice96],\r\n",
        "       train_regret_winner_13[slice96],\r\n",
        "       train_regret_winner_14[slice96],\r\n",
        "       train_regret_winner_15[slice96],\r\n",
        "       train_regret_winner_16[slice96],\r\n",
        "       train_regret_winner_17[slice96],\r\n",
        "       train_regret_winner_18[slice96],\r\n",
        "       train_regret_winner_19[slice96],\r\n",
        "       train_regret_winner_20[slice96]]\r\n",
        "\r\n",
        "loser96_results = pd.DataFrame(loser96).sort_values(by=[0], ascending=False)\r\n",
        "winner96_results = pd.DataFrame(winner96).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser96 = np.asarray(loser96_results[4:5][0])[0]\r\n",
        "median_loser96 = np.asarray(loser96_results[9:10][0])[0]\r\n",
        "upper_loser96 = np.asarray(loser96_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner96 = np.asarray(winner96_results[4:5][0])[0]\r\n",
        "median_winner96 = np.asarray(winner96_results[9:10][0])[0]\r\n",
        "upper_winner96 = np.asarray(winner96_results[14:15][0])[0]"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhsEfl1RxAoA"
      },
      "source": [
        "# Iteration7 :\r\n",
        "\r\n",
        "slice7 = 6\r\n",
        "\r\n",
        "loser7 = [train_regret_loser_1[slice7],\r\n",
        "       train_regret_loser_2[slice7],\r\n",
        "       train_regret_loser_3[slice7],\r\n",
        "       train_regret_loser_4[slice7],\r\n",
        "       train_regret_loser_5[slice7],\r\n",
        "       train_regret_loser_6[slice7],\r\n",
        "       train_regret_loser_7[slice7],\r\n",
        "       train_regret_loser_8[slice7],\r\n",
        "       train_regret_loser_9[slice7],\r\n",
        "       train_regret_loser_10[slice7],\r\n",
        "       train_regret_loser_11[slice7],\r\n",
        "       train_regret_loser_12[slice7],\r\n",
        "       train_regret_loser_13[slice7],\r\n",
        "       train_regret_loser_14[slice7],\r\n",
        "       train_regret_loser_15[slice7],\r\n",
        "       train_regret_loser_16[slice7],\r\n",
        "       train_regret_loser_17[slice7],\r\n",
        "       train_regret_loser_18[slice7],\r\n",
        "       train_regret_loser_19[slice7],\r\n",
        "       train_regret_loser_20[slice7]]\r\n",
        "\r\n",
        "winner7 = [train_regret_winner_1[slice7],\r\n",
        "       train_regret_winner_2[slice7],\r\n",
        "       train_regret_winner_3[slice7],\r\n",
        "       train_regret_winner_4[slice7],\r\n",
        "       train_regret_winner_5[slice7],\r\n",
        "       train_regret_winner_6[slice7],\r\n",
        "       train_regret_winner_7[slice7],\r\n",
        "       train_regret_winner_8[slice7],\r\n",
        "       train_regret_winner_9[slice7],\r\n",
        "       train_regret_winner_10[slice7],\r\n",
        "       train_regret_winner_11[slice7],\r\n",
        "       train_regret_winner_12[slice7],\r\n",
        "       train_regret_winner_13[slice7],\r\n",
        "       train_regret_winner_14[slice7],\r\n",
        "       train_regret_winner_15[slice7],\r\n",
        "       train_regret_winner_16[slice7],\r\n",
        "       train_regret_winner_17[slice7],\r\n",
        "       train_regret_winner_18[slice7],\r\n",
        "       train_regret_winner_19[slice7],\r\n",
        "       train_regret_winner_20[slice7]]\r\n",
        "\r\n",
        "loser7_results = pd.DataFrame(loser7).sort_values(by=[0], ascending=False)\r\n",
        "winner7_results = pd.DataFrame(winner7).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser7 = np.asarray(loser7_results[4:5][0])[0]\r\n",
        "median_loser7 = np.asarray(loser7_results[9:10][0])[0]\r\n",
        "upper_loser7 = np.asarray(loser7_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner7 = np.asarray(winner7_results[4:5][0])[0]\r\n",
        "median_winner7 = np.asarray(winner7_results[9:10][0])[0]\r\n",
        "upper_winner7 = np.asarray(winner7_results[14:15][0])[0]"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUCMheLKxArL"
      },
      "source": [
        "# Iteration17 :\r\n",
        "\r\n",
        "slice17 = 16\r\n",
        "\r\n",
        "loser17 = [train_regret_loser_1[slice17],\r\n",
        "       train_regret_loser_2[slice17],\r\n",
        "       train_regret_loser_3[slice17],\r\n",
        "       train_regret_loser_4[slice17],\r\n",
        "       train_regret_loser_5[slice17],\r\n",
        "       train_regret_loser_6[slice17],\r\n",
        "       train_regret_loser_7[slice17],\r\n",
        "       train_regret_loser_8[slice17],\r\n",
        "       train_regret_loser_9[slice17],\r\n",
        "       train_regret_loser_10[slice17],\r\n",
        "       train_regret_loser_11[slice17],\r\n",
        "       train_regret_loser_12[slice17],\r\n",
        "       train_regret_loser_13[slice17],\r\n",
        "       train_regret_loser_14[slice17],\r\n",
        "       train_regret_loser_15[slice17],\r\n",
        "       train_regret_loser_16[slice17],\r\n",
        "       train_regret_loser_17[slice17],\r\n",
        "       train_regret_loser_18[slice17],\r\n",
        "       train_regret_loser_19[slice17],\r\n",
        "       train_regret_loser_20[slice17]]\r\n",
        "\r\n",
        "winner17 = [train_regret_winner_1[slice17],\r\n",
        "       train_regret_winner_2[slice17],\r\n",
        "       train_regret_winner_3[slice17],\r\n",
        "       train_regret_winner_4[slice17],\r\n",
        "       train_regret_winner_5[slice17],\r\n",
        "       train_regret_winner_6[slice17],\r\n",
        "       train_regret_winner_7[slice17],\r\n",
        "       train_regret_winner_8[slice17],\r\n",
        "       train_regret_winner_9[slice17],\r\n",
        "       train_regret_winner_10[slice17],\r\n",
        "       train_regret_winner_11[slice17],\r\n",
        "       train_regret_winner_12[slice17],\r\n",
        "       train_regret_winner_13[slice17],\r\n",
        "       train_regret_winner_14[slice17],\r\n",
        "       train_regret_winner_15[slice17],\r\n",
        "       train_regret_winner_16[slice17],\r\n",
        "       train_regret_winner_17[slice17],\r\n",
        "       train_regret_winner_18[slice17],\r\n",
        "       train_regret_winner_19[slice17],\r\n",
        "       train_regret_winner_20[slice17]]\r\n",
        "\r\n",
        "loser17_results = pd.DataFrame(loser17).sort_values(by=[0], ascending=False)\r\n",
        "winner17_results = pd.DataFrame(winner17).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser17 = np.asarray(loser17_results[4:5][0])[0]\r\n",
        "median_loser17 = np.asarray(loser17_results[9:10][0])[0]\r\n",
        "upper_loser17 = np.asarray(loser17_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner17 = np.asarray(winner17_results[4:5][0])[0]\r\n",
        "median_winner17 = np.asarray(winner17_results[9:10][0])[0]\r\n",
        "upper_winner17 = np.asarray(winner17_results[14:15][0])[0]"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYnBmq7_xAuB"
      },
      "source": [
        "# Iteration27 :\r\n",
        "\r\n",
        "slice27 = 26\r\n",
        "\r\n",
        "loser27 = [train_regret_loser_1[slice27],\r\n",
        "       train_regret_loser_2[slice27],\r\n",
        "       train_regret_loser_3[slice27],\r\n",
        "       train_regret_loser_4[slice27],\r\n",
        "       train_regret_loser_5[slice27],\r\n",
        "       train_regret_loser_6[slice27],\r\n",
        "       train_regret_loser_7[slice27],\r\n",
        "       train_regret_loser_8[slice27],\r\n",
        "       train_regret_loser_9[slice27],\r\n",
        "       train_regret_loser_10[slice27],\r\n",
        "       train_regret_loser_11[slice27],\r\n",
        "       train_regret_loser_12[slice27],\r\n",
        "       train_regret_loser_13[slice27],\r\n",
        "       train_regret_loser_14[slice27],\r\n",
        "       train_regret_loser_15[slice27],\r\n",
        "       train_regret_loser_16[slice27],\r\n",
        "       train_regret_loser_17[slice27],\r\n",
        "       train_regret_loser_18[slice27],\r\n",
        "       train_regret_loser_19[slice27],\r\n",
        "       train_regret_loser_20[slice27]]\r\n",
        "\r\n",
        "winner27 = [train_regret_winner_1[slice27],\r\n",
        "       train_regret_winner_2[slice27],\r\n",
        "       train_regret_winner_3[slice27],\r\n",
        "       train_regret_winner_4[slice27],\r\n",
        "       train_regret_winner_5[slice27],\r\n",
        "       train_regret_winner_6[slice27],\r\n",
        "       train_regret_winner_7[slice27],\r\n",
        "       train_regret_winner_8[slice27],\r\n",
        "       train_regret_winner_9[slice27],\r\n",
        "       train_regret_winner_10[slice27],\r\n",
        "       train_regret_winner_11[slice27],\r\n",
        "       train_regret_winner_12[slice27],\r\n",
        "       train_regret_winner_13[slice27],\r\n",
        "       train_regret_winner_14[slice27],\r\n",
        "       train_regret_winner_15[slice27],\r\n",
        "       train_regret_winner_16[slice27],\r\n",
        "       train_regret_winner_17[slice27],\r\n",
        "       train_regret_winner_18[slice27],\r\n",
        "       train_regret_winner_19[slice27],\r\n",
        "       train_regret_winner_20[slice27]]\r\n",
        "\r\n",
        "loser27_results = pd.DataFrame(loser27).sort_values(by=[0], ascending=False)\r\n",
        "winner27_results = pd.DataFrame(winner27).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser27 = np.asarray(loser27_results[4:5][0])[0]\r\n",
        "median_loser27 = np.asarray(loser27_results[9:10][0])[0]\r\n",
        "upper_loser27 = np.asarray(loser27_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner27 = np.asarray(winner27_results[4:5][0])[0]\r\n",
        "median_winner27 = np.asarray(winner27_results[9:10][0])[0]\r\n",
        "upper_winner27 = np.asarray(winner27_results[14:15][0])[0]"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ChA8KOUxAw6"
      },
      "source": [
        "# Iteration37 :\r\n",
        "\r\n",
        "slice37 = 36\r\n",
        "\r\n",
        "loser37 = [train_regret_loser_1[slice37],\r\n",
        "       train_regret_loser_2[slice37],\r\n",
        "       train_regret_loser_3[slice37],\r\n",
        "       train_regret_loser_4[slice37],\r\n",
        "       train_regret_loser_5[slice37],\r\n",
        "       train_regret_loser_6[slice37],\r\n",
        "       train_regret_loser_7[slice37],\r\n",
        "       train_regret_loser_8[slice37],\r\n",
        "       train_regret_loser_9[slice37],\r\n",
        "       train_regret_loser_10[slice37],\r\n",
        "       train_regret_loser_11[slice37],\r\n",
        "       train_regret_loser_12[slice37],\r\n",
        "       train_regret_loser_13[slice37],\r\n",
        "       train_regret_loser_14[slice37],\r\n",
        "       train_regret_loser_15[slice37],\r\n",
        "       train_regret_loser_16[slice37],\r\n",
        "       train_regret_loser_17[slice37],\r\n",
        "       train_regret_loser_18[slice37],\r\n",
        "       train_regret_loser_19[slice37],\r\n",
        "       train_regret_loser_20[slice37]]\r\n",
        "\r\n",
        "winner37 = [train_regret_winner_1[slice37],\r\n",
        "       train_regret_winner_2[slice37],\r\n",
        "       train_regret_winner_3[slice37],\r\n",
        "       train_regret_winner_4[slice37],\r\n",
        "       train_regret_winner_5[slice37],\r\n",
        "       train_regret_winner_6[slice37],\r\n",
        "       train_regret_winner_7[slice37],\r\n",
        "       train_regret_winner_8[slice37],\r\n",
        "       train_regret_winner_9[slice37],\r\n",
        "       train_regret_winner_10[slice37],\r\n",
        "       train_regret_winner_11[slice37],\r\n",
        "       train_regret_winner_12[slice37],\r\n",
        "       train_regret_winner_13[slice37],\r\n",
        "       train_regret_winner_14[slice37],\r\n",
        "       train_regret_winner_15[slice37],\r\n",
        "       train_regret_winner_16[slice37],\r\n",
        "       train_regret_winner_17[slice37],\r\n",
        "       train_regret_winner_18[slice37],\r\n",
        "       train_regret_winner_19[slice37],\r\n",
        "       train_regret_winner_20[slice37]]\r\n",
        "\r\n",
        "loser37_results = pd.DataFrame(loser37).sort_values(by=[0], ascending=False)\r\n",
        "winner37_results = pd.DataFrame(winner37).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser37 = np.asarray(loser37_results[4:5][0])[0]\r\n",
        "median_loser37 = np.asarray(loser37_results[9:10][0])[0]\r\n",
        "upper_loser37 = np.asarray(loser37_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner37 = np.asarray(winner37_results[4:5][0])[0]\r\n",
        "median_winner37 = np.asarray(winner37_results[9:10][0])[0]\r\n",
        "upper_winner37 = np.asarray(winner37_results[14:15][0])[0]"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVsY1buDxAzx"
      },
      "source": [
        "# Iteration47 :\r\n",
        "\r\n",
        "slice47 = 46\r\n",
        "\r\n",
        "loser47 = [train_regret_loser_1[slice47],\r\n",
        "       train_regret_loser_2[slice47],\r\n",
        "       train_regret_loser_3[slice47],\r\n",
        "       train_regret_loser_4[slice47],\r\n",
        "       train_regret_loser_5[slice47],\r\n",
        "       train_regret_loser_6[slice47],\r\n",
        "       train_regret_loser_7[slice47],\r\n",
        "       train_regret_loser_8[slice47],\r\n",
        "       train_regret_loser_9[slice47],\r\n",
        "       train_regret_loser_10[slice47],\r\n",
        "       train_regret_loser_11[slice47],\r\n",
        "       train_regret_loser_12[slice47],\r\n",
        "       train_regret_loser_13[slice47],\r\n",
        "       train_regret_loser_14[slice47],\r\n",
        "       train_regret_loser_15[slice47],\r\n",
        "       train_regret_loser_16[slice47],\r\n",
        "       train_regret_loser_17[slice47],\r\n",
        "       train_regret_loser_18[slice47],\r\n",
        "       train_regret_loser_19[slice47],\r\n",
        "       train_regret_loser_20[slice47]]\r\n",
        "\r\n",
        "winner47 = [train_regret_winner_1[slice47],\r\n",
        "       train_regret_winner_2[slice47],\r\n",
        "       train_regret_winner_3[slice47],\r\n",
        "       train_regret_winner_4[slice47],\r\n",
        "       train_regret_winner_5[slice47],\r\n",
        "       train_regret_winner_6[slice47],\r\n",
        "       train_regret_winner_7[slice47],\r\n",
        "       train_regret_winner_8[slice47],\r\n",
        "       train_regret_winner_9[slice47],\r\n",
        "       train_regret_winner_10[slice47],\r\n",
        "       train_regret_winner_11[slice47],\r\n",
        "       train_regret_winner_12[slice47],\r\n",
        "       train_regret_winner_13[slice47],\r\n",
        "       train_regret_winner_14[slice47],\r\n",
        "       train_regret_winner_15[slice47],\r\n",
        "       train_regret_winner_16[slice47],\r\n",
        "       train_regret_winner_17[slice47],\r\n",
        "       train_regret_winner_18[slice47],\r\n",
        "       train_regret_winner_19[slice47],\r\n",
        "       train_regret_winner_20[slice47]]\r\n",
        "\r\n",
        "loser47_results = pd.DataFrame(loser47).sort_values(by=[0], ascending=False)\r\n",
        "winner47_results = pd.DataFrame(winner47).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser47 = np.asarray(loser47_results[4:5][0])[0]\r\n",
        "median_loser47 = np.asarray(loser47_results[9:10][0])[0]\r\n",
        "upper_loser47 = np.asarray(loser47_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner47 = np.asarray(winner47_results[4:5][0])[0]\r\n",
        "median_winner47 = np.asarray(winner47_results[9:10][0])[0]\r\n",
        "upper_winner47 = np.asarray(winner47_results[14:15][0])[0]"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fIy_EyTxA20"
      },
      "source": [
        "# Iteration57 :\r\n",
        "\r\n",
        "slice57 = 56\r\n",
        "\r\n",
        "loser57 = [train_regret_loser_1[slice57],\r\n",
        "       train_regret_loser_2[slice57],\r\n",
        "       train_regret_loser_3[slice57],\r\n",
        "       train_regret_loser_4[slice57],\r\n",
        "       train_regret_loser_5[slice57],\r\n",
        "       train_regret_loser_6[slice57],\r\n",
        "       train_regret_loser_7[slice57],\r\n",
        "       train_regret_loser_8[slice57],\r\n",
        "       train_regret_loser_9[slice57],\r\n",
        "       train_regret_loser_10[slice57],\r\n",
        "       train_regret_loser_11[slice57],\r\n",
        "       train_regret_loser_12[slice57],\r\n",
        "       train_regret_loser_13[slice57],\r\n",
        "       train_regret_loser_14[slice57],\r\n",
        "       train_regret_loser_15[slice57],\r\n",
        "       train_regret_loser_16[slice57],\r\n",
        "       train_regret_loser_17[slice57],\r\n",
        "       train_regret_loser_18[slice57],\r\n",
        "       train_regret_loser_19[slice57],\r\n",
        "       train_regret_loser_20[slice57]]\r\n",
        "\r\n",
        "winner57 = [train_regret_winner_1[slice57],\r\n",
        "       train_regret_winner_2[slice57],\r\n",
        "       train_regret_winner_3[slice57],\r\n",
        "       train_regret_winner_4[slice57],\r\n",
        "       train_regret_winner_5[slice57],\r\n",
        "       train_regret_winner_6[slice57],\r\n",
        "       train_regret_winner_7[slice57],\r\n",
        "       train_regret_winner_8[slice57],\r\n",
        "       train_regret_winner_9[slice57],\r\n",
        "       train_regret_winner_10[slice57],\r\n",
        "       train_regret_winner_11[slice57],\r\n",
        "       train_regret_winner_12[slice57],\r\n",
        "       train_regret_winner_13[slice57],\r\n",
        "       train_regret_winner_14[slice57],\r\n",
        "       train_regret_winner_15[slice57],\r\n",
        "       train_regret_winner_16[slice57],\r\n",
        "       train_regret_winner_17[slice57],\r\n",
        "       train_regret_winner_18[slice57],\r\n",
        "       train_regret_winner_19[slice57],\r\n",
        "       train_regret_winner_20[slice57]]\r\n",
        "\r\n",
        "loser57_results = pd.DataFrame(loser57).sort_values(by=[0], ascending=False)\r\n",
        "winner57_results = pd.DataFrame(winner57).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser57 = np.asarray(loser57_results[4:5][0])[0]\r\n",
        "median_loser57 = np.asarray(loser57_results[9:10][0])[0]\r\n",
        "upper_loser57 = np.asarray(loser57_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner57 = np.asarray(winner57_results[4:5][0])[0]\r\n",
        "median_winner57 = np.asarray(winner57_results[9:10][0])[0]\r\n",
        "upper_winner57 = np.asarray(winner57_results[14:15][0])[0]"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umwdBl8XxA5r"
      },
      "source": [
        "# Iteration67 :\r\n",
        "\r\n",
        "slice67 = 66\r\n",
        "\r\n",
        "loser67 = [train_regret_loser_1[slice67],\r\n",
        "       train_regret_loser_2[slice67],\r\n",
        "       train_regret_loser_3[slice67],\r\n",
        "       train_regret_loser_4[slice67],\r\n",
        "       train_regret_loser_5[slice67],\r\n",
        "       train_regret_loser_6[slice67],\r\n",
        "       train_regret_loser_7[slice67],\r\n",
        "       train_regret_loser_8[slice67],\r\n",
        "       train_regret_loser_9[slice67],\r\n",
        "       train_regret_loser_10[slice67],\r\n",
        "       train_regret_loser_11[slice67],\r\n",
        "       train_regret_loser_12[slice67],\r\n",
        "       train_regret_loser_13[slice67],\r\n",
        "       train_regret_loser_14[slice67],\r\n",
        "       train_regret_loser_15[slice67],\r\n",
        "       train_regret_loser_16[slice67],\r\n",
        "       train_regret_loser_17[slice67],\r\n",
        "       train_regret_loser_18[slice67],\r\n",
        "       train_regret_loser_19[slice67],\r\n",
        "       train_regret_loser_20[slice67]]\r\n",
        "\r\n",
        "winner67 = [train_regret_winner_1[slice67],\r\n",
        "       train_regret_winner_2[slice67],\r\n",
        "       train_regret_winner_3[slice67],\r\n",
        "       train_regret_winner_4[slice67],\r\n",
        "       train_regret_winner_5[slice67],\r\n",
        "       train_regret_winner_6[slice67],\r\n",
        "       train_regret_winner_7[slice67],\r\n",
        "       train_regret_winner_8[slice67],\r\n",
        "       train_regret_winner_9[slice67],\r\n",
        "       train_regret_winner_10[slice67],\r\n",
        "       train_regret_winner_11[slice67],\r\n",
        "       train_regret_winner_12[slice67],\r\n",
        "       train_regret_winner_13[slice67],\r\n",
        "       train_regret_winner_14[slice67],\r\n",
        "       train_regret_winner_15[slice67],\r\n",
        "       train_regret_winner_16[slice67],\r\n",
        "       train_regret_winner_17[slice67],\r\n",
        "       train_regret_winner_18[slice67],\r\n",
        "       train_regret_winner_19[slice67],\r\n",
        "       train_regret_winner_20[slice67]]\r\n",
        "\r\n",
        "loser67_results = pd.DataFrame(loser67).sort_values(by=[0], ascending=False)\r\n",
        "winner67_results = pd.DataFrame(winner67).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser67 = np.asarray(loser67_results[4:5][0])[0]\r\n",
        "median_loser67 = np.asarray(loser67_results[9:10][0])[0]\r\n",
        "upper_loser67 = np.asarray(loser67_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner67 = np.asarray(winner67_results[4:5][0])[0]\r\n",
        "median_winner67 = np.asarray(winner67_results[9:10][0])[0]\r\n",
        "upper_winner67 = np.asarray(winner67_results[14:15][0])[0]"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF3KFX-ZxA8q"
      },
      "source": [
        "# Iteration77 :\r\n",
        "\r\n",
        "slice77 = 76\r\n",
        "\r\n",
        "loser77 = [train_regret_loser_1[slice77],\r\n",
        "       train_regret_loser_2[slice77],\r\n",
        "       train_regret_loser_3[slice77],\r\n",
        "       train_regret_loser_4[slice77],\r\n",
        "       train_regret_loser_5[slice77],\r\n",
        "       train_regret_loser_6[slice77],\r\n",
        "       train_regret_loser_7[slice77],\r\n",
        "       train_regret_loser_8[slice77],\r\n",
        "       train_regret_loser_9[slice77],\r\n",
        "       train_regret_loser_10[slice77],\r\n",
        "       train_regret_loser_11[slice77],\r\n",
        "       train_regret_loser_12[slice77],\r\n",
        "       train_regret_loser_13[slice77],\r\n",
        "       train_regret_loser_14[slice77],\r\n",
        "       train_regret_loser_15[slice77],\r\n",
        "       train_regret_loser_16[slice77],\r\n",
        "       train_regret_loser_17[slice77],\r\n",
        "       train_regret_loser_18[slice77],\r\n",
        "       train_regret_loser_19[slice77],\r\n",
        "       train_regret_loser_20[slice77]]\r\n",
        "\r\n",
        "winner77 = [train_regret_winner_1[slice77],\r\n",
        "       train_regret_winner_2[slice77],\r\n",
        "       train_regret_winner_3[slice77],\r\n",
        "       train_regret_winner_4[slice77],\r\n",
        "       train_regret_winner_5[slice77],\r\n",
        "       train_regret_winner_6[slice77],\r\n",
        "       train_regret_winner_7[slice77],\r\n",
        "       train_regret_winner_8[slice77],\r\n",
        "       train_regret_winner_9[slice77],\r\n",
        "       train_regret_winner_10[slice77],\r\n",
        "       train_regret_winner_11[slice77],\r\n",
        "       train_regret_winner_12[slice77],\r\n",
        "       train_regret_winner_13[slice77],\r\n",
        "       train_regret_winner_14[slice77],\r\n",
        "       train_regret_winner_15[slice77],\r\n",
        "       train_regret_winner_16[slice77],\r\n",
        "       train_regret_winner_17[slice77],\r\n",
        "       train_regret_winner_18[slice77],\r\n",
        "       train_regret_winner_19[slice77],\r\n",
        "       train_regret_winner_20[slice77]]\r\n",
        "\r\n",
        "loser77_results = pd.DataFrame(loser77).sort_values(by=[0], ascending=False)\r\n",
        "winner77_results = pd.DataFrame(winner77).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser77 = np.asarray(loser77_results[4:5][0])[0]\r\n",
        "median_loser77 = np.asarray(loser77_results[9:10][0])[0]\r\n",
        "upper_loser77 = np.asarray(loser77_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner77 = np.asarray(winner77_results[4:5][0])[0]\r\n",
        "median_winner77 = np.asarray(winner77_results[9:10][0])[0]\r\n",
        "upper_winner77 = np.asarray(winner77_results[14:15][0])[0]"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CVYft39xA_t"
      },
      "source": [
        "# Iteration87 :\r\n",
        "\r\n",
        "slice87 = 86\r\n",
        "\r\n",
        "loser87 = [train_regret_loser_1[slice87],\r\n",
        "       train_regret_loser_2[slice87],\r\n",
        "       train_regret_loser_3[slice87],\r\n",
        "       train_regret_loser_4[slice87],\r\n",
        "       train_regret_loser_5[slice87],\r\n",
        "       train_regret_loser_6[slice87],\r\n",
        "       train_regret_loser_7[slice87],\r\n",
        "       train_regret_loser_8[slice87],\r\n",
        "       train_regret_loser_9[slice87],\r\n",
        "       train_regret_loser_10[slice87],\r\n",
        "       train_regret_loser_11[slice87],\r\n",
        "       train_regret_loser_12[slice87],\r\n",
        "       train_regret_loser_13[slice87],\r\n",
        "       train_regret_loser_14[slice87],\r\n",
        "       train_regret_loser_15[slice87],\r\n",
        "       train_regret_loser_16[slice87],\r\n",
        "       train_regret_loser_17[slice87],\r\n",
        "       train_regret_loser_18[slice87],\r\n",
        "       train_regret_loser_19[slice87],\r\n",
        "       train_regret_loser_20[slice87]]\r\n",
        "\r\n",
        "winner87 = [train_regret_winner_1[slice87],\r\n",
        "       train_regret_winner_2[slice87],\r\n",
        "       train_regret_winner_3[slice87],\r\n",
        "       train_regret_winner_4[slice87],\r\n",
        "       train_regret_winner_5[slice87],\r\n",
        "       train_regret_winner_6[slice87],\r\n",
        "       train_regret_winner_7[slice87],\r\n",
        "       train_regret_winner_8[slice87],\r\n",
        "       train_regret_winner_9[slice87],\r\n",
        "       train_regret_winner_10[slice87],\r\n",
        "       train_regret_winner_11[slice87],\r\n",
        "       train_regret_winner_12[slice87],\r\n",
        "       train_regret_winner_13[slice87],\r\n",
        "       train_regret_winner_14[slice87],\r\n",
        "       train_regret_winner_15[slice87],\r\n",
        "       train_regret_winner_16[slice87],\r\n",
        "       train_regret_winner_17[slice87],\r\n",
        "       train_regret_winner_18[slice87],\r\n",
        "       train_regret_winner_19[slice87],\r\n",
        "       train_regret_winner_20[slice87]]\r\n",
        "\r\n",
        "loser87_results = pd.DataFrame(loser87).sort_values(by=[0], ascending=False)\r\n",
        "winner87_results = pd.DataFrame(winner87).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser87 = np.asarray(loser87_results[4:5][0])[0]\r\n",
        "median_loser87 = np.asarray(loser87_results[9:10][0])[0]\r\n",
        "upper_loser87 = np.asarray(loser87_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner87 = np.asarray(winner87_results[4:5][0])[0]\r\n",
        "median_winner87 = np.asarray(winner87_results[9:10][0])[0]\r\n",
        "upper_winner87 = np.asarray(winner87_results[14:15][0])[0]"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gPkLC0yxBCs"
      },
      "source": [
        "# Iteration97 :\r\n",
        "\r\n",
        "slice97 = 96\r\n",
        "\r\n",
        "loser97 = [train_regret_loser_1[slice97],\r\n",
        "       train_regret_loser_2[slice97],\r\n",
        "       train_regret_loser_3[slice97],\r\n",
        "       train_regret_loser_4[slice97],\r\n",
        "       train_regret_loser_5[slice97],\r\n",
        "       train_regret_loser_6[slice97],\r\n",
        "       train_regret_loser_7[slice97],\r\n",
        "       train_regret_loser_8[slice97],\r\n",
        "       train_regret_loser_9[slice97],\r\n",
        "       train_regret_loser_10[slice97],\r\n",
        "       train_regret_loser_11[slice97],\r\n",
        "       train_regret_loser_12[slice97],\r\n",
        "       train_regret_loser_13[slice97],\r\n",
        "       train_regret_loser_14[slice97],\r\n",
        "       train_regret_loser_15[slice97],\r\n",
        "       train_regret_loser_16[slice97],\r\n",
        "       train_regret_loser_17[slice97],\r\n",
        "       train_regret_loser_18[slice97],\r\n",
        "       train_regret_loser_19[slice97],\r\n",
        "       train_regret_loser_20[slice97]]\r\n",
        "\r\n",
        "winner97 = [train_regret_winner_1[slice97],\r\n",
        "       train_regret_winner_2[slice97],\r\n",
        "       train_regret_winner_3[slice97],\r\n",
        "       train_regret_winner_4[slice97],\r\n",
        "       train_regret_winner_5[slice97],\r\n",
        "       train_regret_winner_6[slice97],\r\n",
        "       train_regret_winner_7[slice97],\r\n",
        "       train_regret_winner_8[slice97],\r\n",
        "       train_regret_winner_9[slice97],\r\n",
        "       train_regret_winner_10[slice97],\r\n",
        "       train_regret_winner_11[slice97],\r\n",
        "       train_regret_winner_12[slice97],\r\n",
        "       train_regret_winner_13[slice97],\r\n",
        "       train_regret_winner_14[slice97],\r\n",
        "       train_regret_winner_15[slice97],\r\n",
        "       train_regret_winner_16[slice97],\r\n",
        "       train_regret_winner_17[slice97],\r\n",
        "       train_regret_winner_18[slice97],\r\n",
        "       train_regret_winner_19[slice97],\r\n",
        "       train_regret_winner_20[slice97]]\r\n",
        "\r\n",
        "loser97_results = pd.DataFrame(loser97).sort_values(by=[0], ascending=False)\r\n",
        "winner97_results = pd.DataFrame(winner97).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser97 = np.asarray(loser97_results[4:5][0])[0]\r\n",
        "median_loser97 = np.asarray(loser97_results[9:10][0])[0]\r\n",
        "upper_loser97 = np.asarray(loser97_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner97 = np.asarray(winner97_results[4:5][0])[0]\r\n",
        "median_winner97 = np.asarray(winner97_results[9:10][0])[0]\r\n",
        "upper_winner97 = np.asarray(winner97_results[14:15][0])[0]"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbgYEpYUxBFr"
      },
      "source": [
        "# Iteration8 :\r\n",
        "\r\n",
        "slice8 = 7\r\n",
        "\r\n",
        "loser8 = [train_regret_loser_1[slice8],\r\n",
        "       train_regret_loser_2[slice8],\r\n",
        "       train_regret_loser_3[slice8],\r\n",
        "       train_regret_loser_4[slice8],\r\n",
        "       train_regret_loser_5[slice8],\r\n",
        "       train_regret_loser_6[slice8],\r\n",
        "       train_regret_loser_7[slice8],\r\n",
        "       train_regret_loser_8[slice8],\r\n",
        "       train_regret_loser_9[slice8],\r\n",
        "       train_regret_loser_10[slice8],\r\n",
        "       train_regret_loser_11[slice8],\r\n",
        "       train_regret_loser_12[slice8],\r\n",
        "       train_regret_loser_13[slice8],\r\n",
        "       train_regret_loser_14[slice8],\r\n",
        "       train_regret_loser_15[slice8],\r\n",
        "       train_regret_loser_16[slice8],\r\n",
        "       train_regret_loser_17[slice8],\r\n",
        "       train_regret_loser_18[slice8],\r\n",
        "       train_regret_loser_19[slice8],\r\n",
        "       train_regret_loser_20[slice8]]\r\n",
        "\r\n",
        "winner8 = [train_regret_winner_1[slice8],\r\n",
        "       train_regret_winner_2[slice8],\r\n",
        "       train_regret_winner_3[slice8],\r\n",
        "       train_regret_winner_4[slice8],\r\n",
        "       train_regret_winner_5[slice8],\r\n",
        "       train_regret_winner_6[slice8],\r\n",
        "       train_regret_winner_7[slice8],\r\n",
        "       train_regret_winner_8[slice8],\r\n",
        "       train_regret_winner_9[slice8],\r\n",
        "       train_regret_winner_10[slice8],\r\n",
        "       train_regret_winner_11[slice8],\r\n",
        "       train_regret_winner_12[slice8],\r\n",
        "       train_regret_winner_13[slice8],\r\n",
        "       train_regret_winner_14[slice8],\r\n",
        "       train_regret_winner_15[slice8],\r\n",
        "       train_regret_winner_16[slice8],\r\n",
        "       train_regret_winner_17[slice8],\r\n",
        "       train_regret_winner_18[slice8],\r\n",
        "       train_regret_winner_19[slice8],\r\n",
        "       train_regret_winner_20[slice8]]\r\n",
        "\r\n",
        "loser8_results = pd.DataFrame(loser8).sort_values(by=[0], ascending=False)\r\n",
        "winner8_results = pd.DataFrame(winner8).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser8 = np.asarray(loser8_results[4:5][0])[0]\r\n",
        "median_loser8 = np.asarray(loser8_results[9:10][0])[0]\r\n",
        "upper_loser8 = np.asarray(loser8_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner8 = np.asarray(winner8_results[4:5][0])[0]\r\n",
        "median_winner8 = np.asarray(winner8_results[9:10][0])[0]\r\n",
        "upper_winner8 = np.asarray(winner8_results[14:15][0])[0]"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im9dHUqvxBH9"
      },
      "source": [
        "# Iteration18 :\r\n",
        "\r\n",
        "slice18 = 17\r\n",
        "\r\n",
        "loser18 = [train_regret_loser_1[slice18],\r\n",
        "       train_regret_loser_2[slice18],\r\n",
        "       train_regret_loser_3[slice18],\r\n",
        "       train_regret_loser_4[slice18],\r\n",
        "       train_regret_loser_5[slice18],\r\n",
        "       train_regret_loser_6[slice18],\r\n",
        "       train_regret_loser_7[slice18],\r\n",
        "       train_regret_loser_8[slice18],\r\n",
        "       train_regret_loser_9[slice18],\r\n",
        "       train_regret_loser_10[slice18],\r\n",
        "       train_regret_loser_11[slice18],\r\n",
        "       train_regret_loser_12[slice18],\r\n",
        "       train_regret_loser_13[slice18],\r\n",
        "       train_regret_loser_14[slice18],\r\n",
        "       train_regret_loser_15[slice18],\r\n",
        "       train_regret_loser_16[slice18],\r\n",
        "       train_regret_loser_17[slice18],\r\n",
        "       train_regret_loser_18[slice18],\r\n",
        "       train_regret_loser_19[slice18],\r\n",
        "       train_regret_loser_20[slice18]]\r\n",
        "\r\n",
        "winner18 = [train_regret_winner_1[slice18],\r\n",
        "       train_regret_winner_2[slice18],\r\n",
        "       train_regret_winner_3[slice18],\r\n",
        "       train_regret_winner_4[slice18],\r\n",
        "       train_regret_winner_5[slice18],\r\n",
        "       train_regret_winner_6[slice18],\r\n",
        "       train_regret_winner_7[slice18],\r\n",
        "       train_regret_winner_8[slice18],\r\n",
        "       train_regret_winner_9[slice18],\r\n",
        "       train_regret_winner_10[slice18],\r\n",
        "       train_regret_winner_11[slice18],\r\n",
        "       train_regret_winner_12[slice18],\r\n",
        "       train_regret_winner_13[slice18],\r\n",
        "       train_regret_winner_14[slice18],\r\n",
        "       train_regret_winner_15[slice18],\r\n",
        "       train_regret_winner_16[slice18],\r\n",
        "       train_regret_winner_17[slice18],\r\n",
        "       train_regret_winner_18[slice18],\r\n",
        "       train_regret_winner_19[slice18],\r\n",
        "       train_regret_winner_20[slice18]]\r\n",
        "\r\n",
        "loser18_results = pd.DataFrame(loser18).sort_values(by=[0], ascending=False)\r\n",
        "winner18_results = pd.DataFrame(winner18).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser18 = np.asarray(loser18_results[4:5][0])[0]\r\n",
        "median_loser18 = np.asarray(loser18_results[9:10][0])[0]\r\n",
        "upper_loser18 = np.asarray(loser18_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner18 = np.asarray(winner18_results[4:5][0])[0]\r\n",
        "median_winner18 = np.asarray(winner18_results[9:10][0])[0]\r\n",
        "upper_winner18 = np.asarray(winner18_results[14:15][0])[0]"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvqFjlBDxBK4"
      },
      "source": [
        "# Iteration28 :\r\n",
        "\r\n",
        "slice28 = 27\r\n",
        "\r\n",
        "loser28 = [train_regret_loser_1[slice28],\r\n",
        "       train_regret_loser_2[slice28],\r\n",
        "       train_regret_loser_3[slice28],\r\n",
        "       train_regret_loser_4[slice28],\r\n",
        "       train_regret_loser_5[slice28],\r\n",
        "       train_regret_loser_6[slice28],\r\n",
        "       train_regret_loser_7[slice28],\r\n",
        "       train_regret_loser_8[slice28],\r\n",
        "       train_regret_loser_9[slice28],\r\n",
        "       train_regret_loser_10[slice28],\r\n",
        "       train_regret_loser_11[slice28],\r\n",
        "       train_regret_loser_12[slice28],\r\n",
        "       train_regret_loser_13[slice28],\r\n",
        "       train_regret_loser_14[slice28],\r\n",
        "       train_regret_loser_15[slice28],\r\n",
        "       train_regret_loser_16[slice28],\r\n",
        "       train_regret_loser_17[slice28],\r\n",
        "       train_regret_loser_18[slice28],\r\n",
        "       train_regret_loser_19[slice28],\r\n",
        "       train_regret_loser_20[slice28]]\r\n",
        "\r\n",
        "winner28 = [train_regret_winner_1[slice28],\r\n",
        "       train_regret_winner_2[slice28],\r\n",
        "       train_regret_winner_3[slice28],\r\n",
        "       train_regret_winner_4[slice28],\r\n",
        "       train_regret_winner_5[slice28],\r\n",
        "       train_regret_winner_6[slice28],\r\n",
        "       train_regret_winner_7[slice28],\r\n",
        "       train_regret_winner_8[slice28],\r\n",
        "       train_regret_winner_9[slice28],\r\n",
        "       train_regret_winner_10[slice28],\r\n",
        "       train_regret_winner_11[slice28],\r\n",
        "       train_regret_winner_12[slice28],\r\n",
        "       train_regret_winner_13[slice28],\r\n",
        "       train_regret_winner_14[slice28],\r\n",
        "       train_regret_winner_15[slice28],\r\n",
        "       train_regret_winner_16[slice28],\r\n",
        "       train_regret_winner_17[slice28],\r\n",
        "       train_regret_winner_18[slice28],\r\n",
        "       train_regret_winner_19[slice28],\r\n",
        "       train_regret_winner_20[slice28]]\r\n",
        "\r\n",
        "loser28_results = pd.DataFrame(loser28).sort_values(by=[0], ascending=False)\r\n",
        "winner28_results = pd.DataFrame(winner28).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser28 = np.asarray(loser28_results[4:5][0])[0]\r\n",
        "median_loser28 = np.asarray(loser28_results[9:10][0])[0]\r\n",
        "upper_loser28 = np.asarray(loser28_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner28 = np.asarray(winner28_results[4:5][0])[0]\r\n",
        "median_winner28 = np.asarray(winner28_results[9:10][0])[0]\r\n",
        "upper_winner28 = np.asarray(winner28_results[14:15][0])[0]"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It5daJCEted3"
      },
      "source": [
        "# Iteration38 :\r\n",
        "\r\n",
        "slice38 = 37\r\n",
        "\r\n",
        "loser38 = [train_regret_loser_1[slice38],\r\n",
        "       train_regret_loser_2[slice38],\r\n",
        "       train_regret_loser_3[slice38],\r\n",
        "       train_regret_loser_4[slice38],\r\n",
        "       train_regret_loser_5[slice38],\r\n",
        "       train_regret_loser_6[slice38],\r\n",
        "       train_regret_loser_7[slice38],\r\n",
        "       train_regret_loser_8[slice38],\r\n",
        "       train_regret_loser_9[slice38],\r\n",
        "       train_regret_loser_10[slice38],\r\n",
        "       train_regret_loser_11[slice38],\r\n",
        "       train_regret_loser_12[slice38],\r\n",
        "       train_regret_loser_13[slice38],\r\n",
        "       train_regret_loser_14[slice38],\r\n",
        "       train_regret_loser_15[slice38],\r\n",
        "       train_regret_loser_16[slice38],\r\n",
        "       train_regret_loser_17[slice38],\r\n",
        "       train_regret_loser_18[slice38],\r\n",
        "       train_regret_loser_19[slice38],\r\n",
        "       train_regret_loser_20[slice38]]\r\n",
        "\r\n",
        "winner38 = [train_regret_winner_1[slice38],\r\n",
        "       train_regret_winner_2[slice38],\r\n",
        "       train_regret_winner_3[slice38],\r\n",
        "       train_regret_winner_4[slice38],\r\n",
        "       train_regret_winner_5[slice38],\r\n",
        "       train_regret_winner_6[slice38],\r\n",
        "       train_regret_winner_7[slice38],\r\n",
        "       train_regret_winner_8[slice38],\r\n",
        "       train_regret_winner_9[slice38],\r\n",
        "       train_regret_winner_10[slice38],\r\n",
        "       train_regret_winner_11[slice38],\r\n",
        "       train_regret_winner_12[slice38],\r\n",
        "       train_regret_winner_13[slice38],\r\n",
        "       train_regret_winner_14[slice38],\r\n",
        "       train_regret_winner_15[slice38],\r\n",
        "       train_regret_winner_16[slice38],\r\n",
        "       train_regret_winner_17[slice38],\r\n",
        "       train_regret_winner_18[slice38],\r\n",
        "       train_regret_winner_19[slice38],\r\n",
        "       train_regret_winner_20[slice38]]\r\n",
        "\r\n",
        "loser38_results = pd.DataFrame(loser38).sort_values(by=[0], ascending=False)\r\n",
        "winner38_results = pd.DataFrame(winner38).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser38 = np.asarray(loser38_results[4:5][0])[0]\r\n",
        "median_loser38 = np.asarray(loser38_results[9:10][0])[0]\r\n",
        "upper_loser38 = np.asarray(loser38_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner38 = np.asarray(winner38_results[4:5][0])[0]\r\n",
        "median_winner38 = np.asarray(winner38_results[9:10][0])[0]\r\n",
        "upper_winner38 = np.asarray(winner38_results[14:15][0])[0]"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GekXOf_gtegw"
      },
      "source": [
        "# Iteration48 :\r\n",
        "\r\n",
        "slice48 = 47\r\n",
        "\r\n",
        "loser48 = [train_regret_loser_1[slice48],\r\n",
        "       train_regret_loser_2[slice48],\r\n",
        "       train_regret_loser_3[slice48],\r\n",
        "       train_regret_loser_4[slice48],\r\n",
        "       train_regret_loser_5[slice48],\r\n",
        "       train_regret_loser_6[slice48],\r\n",
        "       train_regret_loser_7[slice48],\r\n",
        "       train_regret_loser_8[slice48],\r\n",
        "       train_regret_loser_9[slice48],\r\n",
        "       train_regret_loser_10[slice48],\r\n",
        "       train_regret_loser_11[slice48],\r\n",
        "       train_regret_loser_12[slice48],\r\n",
        "       train_regret_loser_13[slice48],\r\n",
        "       train_regret_loser_14[slice48],\r\n",
        "       train_regret_loser_15[slice48],\r\n",
        "       train_regret_loser_16[slice48],\r\n",
        "       train_regret_loser_17[slice48],\r\n",
        "       train_regret_loser_18[slice48],\r\n",
        "       train_regret_loser_19[slice48],\r\n",
        "       train_regret_loser_20[slice48]]\r\n",
        "\r\n",
        "winner48 = [train_regret_winner_1[slice48],\r\n",
        "       train_regret_winner_2[slice48],\r\n",
        "       train_regret_winner_3[slice48],\r\n",
        "       train_regret_winner_4[slice48],\r\n",
        "       train_regret_winner_5[slice48],\r\n",
        "       train_regret_winner_6[slice48],\r\n",
        "       train_regret_winner_7[slice48],\r\n",
        "       train_regret_winner_8[slice48],\r\n",
        "       train_regret_winner_9[slice48],\r\n",
        "       train_regret_winner_10[slice48],\r\n",
        "       train_regret_winner_11[slice48],\r\n",
        "       train_regret_winner_12[slice48],\r\n",
        "       train_regret_winner_13[slice48],\r\n",
        "       train_regret_winner_14[slice48],\r\n",
        "       train_regret_winner_15[slice48],\r\n",
        "       train_regret_winner_16[slice48],\r\n",
        "       train_regret_winner_17[slice48],\r\n",
        "       train_regret_winner_18[slice48],\r\n",
        "       train_regret_winner_19[slice48],\r\n",
        "       train_regret_winner_20[slice48]]\r\n",
        "\r\n",
        "loser48_results = pd.DataFrame(loser48).sort_values(by=[0], ascending=False)\r\n",
        "winner48_results = pd.DataFrame(winner48).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser48 = np.asarray(loser48_results[4:5][0])[0]\r\n",
        "median_loser48 = np.asarray(loser48_results[9:10][0])[0]\r\n",
        "upper_loser48 = np.asarray(loser48_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner48 = np.asarray(winner48_results[4:5][0])[0]\r\n",
        "median_winner48 = np.asarray(winner48_results[9:10][0])[0]\r\n",
        "upper_winner48 = np.asarray(winner48_results[14:15][0])[0]"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNpV6knOtejv"
      },
      "source": [
        "# Iteration58 :\r\n",
        "\r\n",
        "slice58 = 57\r\n",
        "\r\n",
        "loser58 = [train_regret_loser_1[slice58],\r\n",
        "       train_regret_loser_2[slice58],\r\n",
        "       train_regret_loser_3[slice58],\r\n",
        "       train_regret_loser_4[slice58],\r\n",
        "       train_regret_loser_5[slice58],\r\n",
        "       train_regret_loser_6[slice58],\r\n",
        "       train_regret_loser_7[slice58],\r\n",
        "       train_regret_loser_8[slice58],\r\n",
        "       train_regret_loser_9[slice58],\r\n",
        "       train_regret_loser_10[slice58],\r\n",
        "       train_regret_loser_11[slice58],\r\n",
        "       train_regret_loser_12[slice58],\r\n",
        "       train_regret_loser_13[slice58],\r\n",
        "       train_regret_loser_14[slice58],\r\n",
        "       train_regret_loser_15[slice58],\r\n",
        "       train_regret_loser_16[slice58],\r\n",
        "       train_regret_loser_17[slice58],\r\n",
        "       train_regret_loser_18[slice58],\r\n",
        "       train_regret_loser_19[slice58],\r\n",
        "       train_regret_loser_20[slice58]]\r\n",
        "\r\n",
        "winner58 = [train_regret_winner_1[slice58],\r\n",
        "       train_regret_winner_2[slice58],\r\n",
        "       train_regret_winner_3[slice58],\r\n",
        "       train_regret_winner_4[slice58],\r\n",
        "       train_regret_winner_5[slice58],\r\n",
        "       train_regret_winner_6[slice58],\r\n",
        "       train_regret_winner_7[slice58],\r\n",
        "       train_regret_winner_8[slice58],\r\n",
        "       train_regret_winner_9[slice58],\r\n",
        "       train_regret_winner_10[slice58],\r\n",
        "       train_regret_winner_11[slice58],\r\n",
        "       train_regret_winner_12[slice58],\r\n",
        "       train_regret_winner_13[slice58],\r\n",
        "       train_regret_winner_14[slice58],\r\n",
        "       train_regret_winner_15[slice58],\r\n",
        "       train_regret_winner_16[slice58],\r\n",
        "       train_regret_winner_17[slice58],\r\n",
        "       train_regret_winner_18[slice58],\r\n",
        "       train_regret_winner_19[slice58],\r\n",
        "       train_regret_winner_20[slice58]]\r\n",
        "\r\n",
        "loser58_results = pd.DataFrame(loser58).sort_values(by=[0], ascending=False)\r\n",
        "winner58_results = pd.DataFrame(winner58).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser58 = np.asarray(loser58_results[4:5][0])[0]\r\n",
        "median_loser58 = np.asarray(loser58_results[9:10][0])[0]\r\n",
        "upper_loser58 = np.asarray(loser58_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner58 = np.asarray(winner58_results[4:5][0])[0]\r\n",
        "median_winner58 = np.asarray(winner58_results[9:10][0])[0]\r\n",
        "upper_winner58 = np.asarray(winner58_results[14:15][0])[0]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VFsqaBqtemn"
      },
      "source": [
        "# Iteration68 :\r\n",
        "\r\n",
        "slice68 = 67\r\n",
        "\r\n",
        "loser68 = [train_regret_loser_1[slice68],\r\n",
        "       train_regret_loser_2[slice68],\r\n",
        "       train_regret_loser_3[slice68],\r\n",
        "       train_regret_loser_4[slice68],\r\n",
        "       train_regret_loser_5[slice68],\r\n",
        "       train_regret_loser_6[slice68],\r\n",
        "       train_regret_loser_7[slice68],\r\n",
        "       train_regret_loser_8[slice68],\r\n",
        "       train_regret_loser_9[slice68],\r\n",
        "       train_regret_loser_10[slice68],\r\n",
        "       train_regret_loser_11[slice68],\r\n",
        "       train_regret_loser_12[slice68],\r\n",
        "       train_regret_loser_13[slice68],\r\n",
        "       train_regret_loser_14[slice68],\r\n",
        "       train_regret_loser_15[slice68],\r\n",
        "       train_regret_loser_16[slice68],\r\n",
        "       train_regret_loser_17[slice68],\r\n",
        "       train_regret_loser_18[slice68],\r\n",
        "       train_regret_loser_19[slice68],\r\n",
        "       train_regret_loser_20[slice68]]\r\n",
        "\r\n",
        "winner68 = [train_regret_winner_1[slice68],\r\n",
        "       train_regret_winner_2[slice68],\r\n",
        "       train_regret_winner_3[slice68],\r\n",
        "       train_regret_winner_4[slice68],\r\n",
        "       train_regret_winner_5[slice68],\r\n",
        "       train_regret_winner_6[slice68],\r\n",
        "       train_regret_winner_7[slice68],\r\n",
        "       train_regret_winner_8[slice68],\r\n",
        "       train_regret_winner_9[slice68],\r\n",
        "       train_regret_winner_10[slice68],\r\n",
        "       train_regret_winner_11[slice68],\r\n",
        "       train_regret_winner_12[slice68],\r\n",
        "       train_regret_winner_13[slice68],\r\n",
        "       train_regret_winner_14[slice68],\r\n",
        "       train_regret_winner_15[slice68],\r\n",
        "       train_regret_winner_16[slice68],\r\n",
        "       train_regret_winner_17[slice68],\r\n",
        "       train_regret_winner_18[slice68],\r\n",
        "       train_regret_winner_19[slice68],\r\n",
        "       train_regret_winner_20[slice68]]\r\n",
        "\r\n",
        "loser68_results = pd.DataFrame(loser68).sort_values(by=[0], ascending=False)\r\n",
        "winner68_results = pd.DataFrame(winner68).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser68 = np.asarray(loser68_results[4:5][0])[0]\r\n",
        "median_loser68 = np.asarray(loser68_results[9:10][0])[0]\r\n",
        "upper_loser68 = np.asarray(loser68_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner68 = np.asarray(winner68_results[4:5][0])[0]\r\n",
        "median_winner68 = np.asarray(winner68_results[9:10][0])[0]\r\n",
        "upper_winner68 = np.asarray(winner68_results[14:15][0])[0]"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XlFH0Wxtepo"
      },
      "source": [
        "# Iteration78 :\r\n",
        "\r\n",
        "slice78 = 77\r\n",
        "\r\n",
        "loser78 = [train_regret_loser_1[slice78],\r\n",
        "       train_regret_loser_2[slice78],\r\n",
        "       train_regret_loser_3[slice78],\r\n",
        "       train_regret_loser_4[slice78],\r\n",
        "       train_regret_loser_5[slice78],\r\n",
        "       train_regret_loser_6[slice78],\r\n",
        "       train_regret_loser_7[slice78],\r\n",
        "       train_regret_loser_8[slice78],\r\n",
        "       train_regret_loser_9[slice78],\r\n",
        "       train_regret_loser_10[slice78],\r\n",
        "       train_regret_loser_11[slice78],\r\n",
        "       train_regret_loser_12[slice78],\r\n",
        "       train_regret_loser_13[slice78],\r\n",
        "       train_regret_loser_14[slice78],\r\n",
        "       train_regret_loser_15[slice78],\r\n",
        "       train_regret_loser_16[slice78],\r\n",
        "       train_regret_loser_17[slice78],\r\n",
        "       train_regret_loser_18[slice78],\r\n",
        "       train_regret_loser_19[slice78],\r\n",
        "       train_regret_loser_20[slice78]]\r\n",
        "\r\n",
        "winner78 = [train_regret_winner_1[slice78],\r\n",
        "       train_regret_winner_2[slice78],\r\n",
        "       train_regret_winner_3[slice78],\r\n",
        "       train_regret_winner_4[slice78],\r\n",
        "       train_regret_winner_5[slice78],\r\n",
        "       train_regret_winner_6[slice78],\r\n",
        "       train_regret_winner_7[slice78],\r\n",
        "       train_regret_winner_8[slice78],\r\n",
        "       train_regret_winner_9[slice78],\r\n",
        "       train_regret_winner_10[slice78],\r\n",
        "       train_regret_winner_11[slice78],\r\n",
        "       train_regret_winner_12[slice78],\r\n",
        "       train_regret_winner_13[slice78],\r\n",
        "       train_regret_winner_14[slice78],\r\n",
        "       train_regret_winner_15[slice78],\r\n",
        "       train_regret_winner_16[slice78],\r\n",
        "       train_regret_winner_17[slice78],\r\n",
        "       train_regret_winner_18[slice78],\r\n",
        "       train_regret_winner_19[slice78],\r\n",
        "       train_regret_winner_20[slice78]]\r\n",
        "\r\n",
        "loser78_results = pd.DataFrame(loser78).sort_values(by=[0], ascending=False)\r\n",
        "winner78_results = pd.DataFrame(winner78).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser78 = np.asarray(loser78_results[4:5][0])[0]\r\n",
        "median_loser78 = np.asarray(loser78_results[9:10][0])[0]\r\n",
        "upper_loser78 = np.asarray(loser78_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner78 = np.asarray(winner78_results[4:5][0])[0]\r\n",
        "median_winner78 = np.asarray(winner78_results[9:10][0])[0]\r\n",
        "upper_winner78 = np.asarray(winner78_results[14:15][0])[0]"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsL0PJVetesC"
      },
      "source": [
        "# Iteration88 :\r\n",
        "\r\n",
        "slice88 = 87\r\n",
        "\r\n",
        "loser88 = [train_regret_loser_1[slice88],\r\n",
        "       train_regret_loser_2[slice88],\r\n",
        "       train_regret_loser_3[slice88],\r\n",
        "       train_regret_loser_4[slice88],\r\n",
        "       train_regret_loser_5[slice88],\r\n",
        "       train_regret_loser_6[slice88],\r\n",
        "       train_regret_loser_7[slice88],\r\n",
        "       train_regret_loser_8[slice88],\r\n",
        "       train_regret_loser_9[slice88],\r\n",
        "       train_regret_loser_10[slice88],\r\n",
        "       train_regret_loser_11[slice88],\r\n",
        "       train_regret_loser_12[slice88],\r\n",
        "       train_regret_loser_13[slice88],\r\n",
        "       train_regret_loser_14[slice88],\r\n",
        "       train_regret_loser_15[slice88],\r\n",
        "       train_regret_loser_16[slice88],\r\n",
        "       train_regret_loser_17[slice88],\r\n",
        "       train_regret_loser_18[slice88],\r\n",
        "       train_regret_loser_19[slice88],\r\n",
        "       train_regret_loser_20[slice88]]\r\n",
        "\r\n",
        "winner88 = [train_regret_winner_1[slice88],\r\n",
        "       train_regret_winner_2[slice88],\r\n",
        "       train_regret_winner_3[slice88],\r\n",
        "       train_regret_winner_4[slice88],\r\n",
        "       train_regret_winner_5[slice88],\r\n",
        "       train_regret_winner_6[slice88],\r\n",
        "       train_regret_winner_7[slice88],\r\n",
        "       train_regret_winner_8[slice88],\r\n",
        "       train_regret_winner_9[slice88],\r\n",
        "       train_regret_winner_10[slice88],\r\n",
        "       train_regret_winner_11[slice88],\r\n",
        "       train_regret_winner_12[slice88],\r\n",
        "       train_regret_winner_13[slice88],\r\n",
        "       train_regret_winner_14[slice88],\r\n",
        "       train_regret_winner_15[slice88],\r\n",
        "       train_regret_winner_16[slice88],\r\n",
        "       train_regret_winner_17[slice88],\r\n",
        "       train_regret_winner_18[slice88],\r\n",
        "       train_regret_winner_19[slice88],\r\n",
        "       train_regret_winner_20[slice88]]\r\n",
        "\r\n",
        "loser88_results = pd.DataFrame(loser88).sort_values(by=[0], ascending=False)\r\n",
        "winner88_results = pd.DataFrame(winner88).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser88 = np.asarray(loser88_results[4:5][0])[0]\r\n",
        "median_loser88 = np.asarray(loser88_results[9:10][0])[0]\r\n",
        "upper_loser88 = np.asarray(loser88_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner88 = np.asarray(winner88_results[4:5][0])[0]\r\n",
        "median_winner88 = np.asarray(winner88_results[9:10][0])[0]\r\n",
        "upper_winner88 = np.asarray(winner88_results[14:15][0])[0]"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYWEWGku1o_9"
      },
      "source": [
        "# Iteration98 :\r\n",
        "\r\n",
        "slice98 = 97\r\n",
        "\r\n",
        "loser98 = [train_regret_loser_1[slice98],\r\n",
        "       train_regret_loser_2[slice98],\r\n",
        "       train_regret_loser_3[slice98],\r\n",
        "       train_regret_loser_4[slice98],\r\n",
        "       train_regret_loser_5[slice98],\r\n",
        "       train_regret_loser_6[slice98],\r\n",
        "       train_regret_loser_7[slice98],\r\n",
        "       train_regret_loser_8[slice98],\r\n",
        "       train_regret_loser_9[slice98],\r\n",
        "       train_regret_loser_10[slice98],\r\n",
        "       train_regret_loser_11[slice98],\r\n",
        "       train_regret_loser_12[slice98],\r\n",
        "       train_regret_loser_13[slice98],\r\n",
        "       train_regret_loser_14[slice98],\r\n",
        "       train_regret_loser_15[slice98],\r\n",
        "       train_regret_loser_16[slice98],\r\n",
        "       train_regret_loser_17[slice98],\r\n",
        "       train_regret_loser_18[slice98],\r\n",
        "       train_regret_loser_19[slice98],\r\n",
        "       train_regret_loser_20[slice98]]\r\n",
        "\r\n",
        "winner98 = [train_regret_winner_1[slice98],\r\n",
        "       train_regret_winner_2[slice98],\r\n",
        "       train_regret_winner_3[slice98],\r\n",
        "       train_regret_winner_4[slice98],\r\n",
        "       train_regret_winner_5[slice98],\r\n",
        "       train_regret_winner_6[slice98],\r\n",
        "       train_regret_winner_7[slice98],\r\n",
        "       train_regret_winner_8[slice98],\r\n",
        "       train_regret_winner_9[slice98],\r\n",
        "       train_regret_winner_10[slice98],\r\n",
        "       train_regret_winner_11[slice98],\r\n",
        "       train_regret_winner_12[slice98],\r\n",
        "       train_regret_winner_13[slice98],\r\n",
        "       train_regret_winner_14[slice98],\r\n",
        "       train_regret_winner_15[slice98],\r\n",
        "       train_regret_winner_16[slice98],\r\n",
        "       train_regret_winner_17[slice98],\r\n",
        "       train_regret_winner_18[slice98],\r\n",
        "       train_regret_winner_19[slice98],\r\n",
        "       train_regret_winner_20[slice98]]\r\n",
        "\r\n",
        "loser98_results = pd.DataFrame(loser98).sort_values(by=[0], ascending=False)\r\n",
        "winner98_results = pd.DataFrame(winner98).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser98 = np.asarray(loser98_results[4:5][0])[0]\r\n",
        "median_loser98 = np.asarray(loser98_results[9:10][0])[0]\r\n",
        "upper_loser98 = np.asarray(loser98_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner98 = np.asarray(winner98_results[4:5][0])[0]\r\n",
        "median_winner98 = np.asarray(winner98_results[9:10][0])[0]\r\n",
        "upper_winner98 = np.asarray(winner98_results[14:15][0])[0]"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guLGgcX91pDK"
      },
      "source": [
        "# Iteration9 :\r\n",
        "\r\n",
        "slice9 = 8\r\n",
        "\r\n",
        "loser9 = [train_regret_loser_1[slice9],\r\n",
        "       train_regret_loser_2[slice9],\r\n",
        "       train_regret_loser_3[slice9],\r\n",
        "       train_regret_loser_4[slice9],\r\n",
        "       train_regret_loser_5[slice9],\r\n",
        "       train_regret_loser_6[slice9],\r\n",
        "       train_regret_loser_7[slice9],\r\n",
        "       train_regret_loser_8[slice9],\r\n",
        "       train_regret_loser_9[slice9],\r\n",
        "       train_regret_loser_10[slice9],\r\n",
        "       train_regret_loser_11[slice9],\r\n",
        "       train_regret_loser_12[slice9],\r\n",
        "       train_regret_loser_13[slice9],\r\n",
        "       train_regret_loser_14[slice9],\r\n",
        "       train_regret_loser_15[slice9],\r\n",
        "       train_regret_loser_16[slice9],\r\n",
        "       train_regret_loser_17[slice9],\r\n",
        "       train_regret_loser_18[slice9],\r\n",
        "       train_regret_loser_19[slice9],\r\n",
        "       train_regret_loser_20[slice9]]\r\n",
        "\r\n",
        "winner9 = [train_regret_winner_1[slice9],\r\n",
        "       train_regret_winner_2[slice9],\r\n",
        "       train_regret_winner_3[slice9],\r\n",
        "       train_regret_winner_4[slice9],\r\n",
        "       train_regret_winner_5[slice9],\r\n",
        "       train_regret_winner_6[slice9],\r\n",
        "       train_regret_winner_7[slice9],\r\n",
        "       train_regret_winner_8[slice9],\r\n",
        "       train_regret_winner_9[slice9],\r\n",
        "       train_regret_winner_10[slice9],\r\n",
        "       train_regret_winner_11[slice9],\r\n",
        "       train_regret_winner_12[slice9],\r\n",
        "       train_regret_winner_13[slice9],\r\n",
        "       train_regret_winner_14[slice9],\r\n",
        "       train_regret_winner_15[slice9],\r\n",
        "       train_regret_winner_16[slice9],\r\n",
        "       train_regret_winner_17[slice9],\r\n",
        "       train_regret_winner_18[slice9],\r\n",
        "       train_regret_winner_19[slice9],\r\n",
        "       train_regret_winner_20[slice9]]\r\n",
        "\r\n",
        "loser9_results = pd.DataFrame(loser9).sort_values(by=[0], ascending=False)\r\n",
        "winner9_results = pd.DataFrame(winner9).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser9 = np.asarray(loser9_results[4:5][0])[0]\r\n",
        "median_loser9 = np.asarray(loser9_results[9:10][0])[0]\r\n",
        "upper_loser9 = np.asarray(loser9_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner9 = np.asarray(winner9_results[4:5][0])[0]\r\n",
        "median_winner9 = np.asarray(winner9_results[9:10][0])[0]\r\n",
        "upper_winner9 = np.asarray(winner9_results[14:15][0])[0]"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3_9-eoT1pGd"
      },
      "source": [
        "# Iteration19 :\r\n",
        "\r\n",
        "slice19 = 18\r\n",
        "\r\n",
        "loser19 = [train_regret_loser_1[slice19],\r\n",
        "       train_regret_loser_2[slice19],\r\n",
        "       train_regret_loser_3[slice19],\r\n",
        "       train_regret_loser_4[slice19],\r\n",
        "       train_regret_loser_5[slice19],\r\n",
        "       train_regret_loser_6[slice19],\r\n",
        "       train_regret_loser_7[slice19],\r\n",
        "       train_regret_loser_8[slice19],\r\n",
        "       train_regret_loser_9[slice19],\r\n",
        "       train_regret_loser_10[slice19],\r\n",
        "       train_regret_loser_11[slice19],\r\n",
        "       train_regret_loser_12[slice19],\r\n",
        "       train_regret_loser_13[slice19],\r\n",
        "       train_regret_loser_14[slice19],\r\n",
        "       train_regret_loser_15[slice19],\r\n",
        "       train_regret_loser_16[slice19],\r\n",
        "       train_regret_loser_17[slice19],\r\n",
        "       train_regret_loser_18[slice19],\r\n",
        "       train_regret_loser_19[slice19],\r\n",
        "       train_regret_loser_20[slice19]]\r\n",
        "\r\n",
        "winner19 = [train_regret_winner_1[slice19],\r\n",
        "       train_regret_winner_2[slice19],\r\n",
        "       train_regret_winner_3[slice19],\r\n",
        "       train_regret_winner_4[slice19],\r\n",
        "       train_regret_winner_5[slice19],\r\n",
        "       train_regret_winner_6[slice19],\r\n",
        "       train_regret_winner_7[slice19],\r\n",
        "       train_regret_winner_8[slice19],\r\n",
        "       train_regret_winner_9[slice19],\r\n",
        "       train_regret_winner_10[slice19],\r\n",
        "       train_regret_winner_11[slice19],\r\n",
        "       train_regret_winner_12[slice19],\r\n",
        "       train_regret_winner_13[slice19],\r\n",
        "       train_regret_winner_14[slice19],\r\n",
        "       train_regret_winner_15[slice19],\r\n",
        "       train_regret_winner_16[slice19],\r\n",
        "       train_regret_winner_17[slice19],\r\n",
        "       train_regret_winner_18[slice19],\r\n",
        "       train_regret_winner_19[slice19],\r\n",
        "       train_regret_winner_20[slice19]]\r\n",
        "\r\n",
        "loser19_results = pd.DataFrame(loser19).sort_values(by=[0], ascending=False)\r\n",
        "winner19_results = pd.DataFrame(winner19).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser19 = np.asarray(loser19_results[4:5][0])[0]\r\n",
        "median_loser19 = np.asarray(loser19_results[9:10][0])[0]\r\n",
        "upper_loser19 = np.asarray(loser19_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner19 = np.asarray(winner19_results[4:5][0])[0]\r\n",
        "median_winner19 = np.asarray(winner19_results[9:10][0])[0]\r\n",
        "upper_winner19 = np.asarray(winner19_results[14:15][0])[0]"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ae5x13M1pRp"
      },
      "source": [
        "# Iteration29 :\r\n",
        "\r\n",
        "slice29 = 28\r\n",
        "\r\n",
        "loser29 = [train_regret_loser_1[slice29],\r\n",
        "       train_regret_loser_2[slice29],\r\n",
        "       train_regret_loser_3[slice29],\r\n",
        "       train_regret_loser_4[slice29],\r\n",
        "       train_regret_loser_5[slice29],\r\n",
        "       train_regret_loser_6[slice29],\r\n",
        "       train_regret_loser_7[slice29],\r\n",
        "       train_regret_loser_8[slice29],\r\n",
        "       train_regret_loser_9[slice29],\r\n",
        "       train_regret_loser_10[slice29],\r\n",
        "       train_regret_loser_11[slice29],\r\n",
        "       train_regret_loser_12[slice29],\r\n",
        "       train_regret_loser_13[slice29],\r\n",
        "       train_regret_loser_14[slice29],\r\n",
        "       train_regret_loser_15[slice29],\r\n",
        "       train_regret_loser_16[slice29],\r\n",
        "       train_regret_loser_17[slice29],\r\n",
        "       train_regret_loser_18[slice29],\r\n",
        "       train_regret_loser_19[slice29],\r\n",
        "       train_regret_loser_20[slice29]]\r\n",
        "\r\n",
        "winner29 = [train_regret_winner_1[slice29],\r\n",
        "       train_regret_winner_2[slice29],\r\n",
        "       train_regret_winner_3[slice29],\r\n",
        "       train_regret_winner_4[slice29],\r\n",
        "       train_regret_winner_5[slice29],\r\n",
        "       train_regret_winner_6[slice29],\r\n",
        "       train_regret_winner_7[slice29],\r\n",
        "       train_regret_winner_8[slice29],\r\n",
        "       train_regret_winner_9[slice29],\r\n",
        "       train_regret_winner_10[slice29],\r\n",
        "       train_regret_winner_11[slice29],\r\n",
        "       train_regret_winner_12[slice29],\r\n",
        "       train_regret_winner_13[slice29],\r\n",
        "       train_regret_winner_14[slice29],\r\n",
        "       train_regret_winner_15[slice29],\r\n",
        "       train_regret_winner_16[slice29],\r\n",
        "       train_regret_winner_17[slice29],\r\n",
        "       train_regret_winner_18[slice29],\r\n",
        "       train_regret_winner_19[slice29],\r\n",
        "       train_regret_winner_20[slice29]]\r\n",
        "\r\n",
        "loser29_results = pd.DataFrame(loser29).sort_values(by=[0], ascending=False)\r\n",
        "winner29_results = pd.DataFrame(winner29).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser29 = np.asarray(loser29_results[4:5][0])[0]\r\n",
        "median_loser29 = np.asarray(loser29_results[9:10][0])[0]\r\n",
        "upper_loser29 = np.asarray(loser29_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner29 = np.asarray(winner29_results[4:5][0])[0]\r\n",
        "median_winner29 = np.asarray(winner29_results[9:10][0])[0]\r\n",
        "upper_winner29 = np.asarray(winner29_results[14:15][0])[0]"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR9aPvBy1pUc"
      },
      "source": [
        "# Iteration39 :\r\n",
        "\r\n",
        "slice39 = 38\r\n",
        "\r\n",
        "loser39 = [train_regret_loser_1[slice39],\r\n",
        "       train_regret_loser_2[slice39],\r\n",
        "       train_regret_loser_3[slice39],\r\n",
        "       train_regret_loser_4[slice39],\r\n",
        "       train_regret_loser_5[slice39],\r\n",
        "       train_regret_loser_6[slice39],\r\n",
        "       train_regret_loser_7[slice39],\r\n",
        "       train_regret_loser_8[slice39],\r\n",
        "       train_regret_loser_9[slice39],\r\n",
        "       train_regret_loser_10[slice39],\r\n",
        "       train_regret_loser_11[slice39],\r\n",
        "       train_regret_loser_12[slice39],\r\n",
        "       train_regret_loser_13[slice39],\r\n",
        "       train_regret_loser_14[slice39],\r\n",
        "       train_regret_loser_15[slice39],\r\n",
        "       train_regret_loser_16[slice39],\r\n",
        "       train_regret_loser_17[slice39],\r\n",
        "       train_regret_loser_18[slice39],\r\n",
        "       train_regret_loser_19[slice39],\r\n",
        "       train_regret_loser_20[slice39]]\r\n",
        "\r\n",
        "winner39 = [train_regret_winner_1[slice39],\r\n",
        "       train_regret_winner_2[slice39],\r\n",
        "       train_regret_winner_3[slice39],\r\n",
        "       train_regret_winner_4[slice39],\r\n",
        "       train_regret_winner_5[slice39],\r\n",
        "       train_regret_winner_6[slice39],\r\n",
        "       train_regret_winner_7[slice39],\r\n",
        "       train_regret_winner_8[slice39],\r\n",
        "       train_regret_winner_9[slice39],\r\n",
        "       train_regret_winner_10[slice39],\r\n",
        "       train_regret_winner_11[slice39],\r\n",
        "       train_regret_winner_12[slice39],\r\n",
        "       train_regret_winner_13[slice39],\r\n",
        "       train_regret_winner_14[slice39],\r\n",
        "       train_regret_winner_15[slice39],\r\n",
        "       train_regret_winner_16[slice39],\r\n",
        "       train_regret_winner_17[slice39],\r\n",
        "       train_regret_winner_18[slice39],\r\n",
        "       train_regret_winner_19[slice39],\r\n",
        "       train_regret_winner_20[slice39]]\r\n",
        "\r\n",
        "loser39_results = pd.DataFrame(loser39).sort_values(by=[0], ascending=False)\r\n",
        "winner39_results = pd.DataFrame(winner39).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser39 = np.asarray(loser39_results[4:5][0])[0]\r\n",
        "median_loser39 = np.asarray(loser39_results[9:10][0])[0]\r\n",
        "upper_loser39 = np.asarray(loser39_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner39 = np.asarray(winner39_results[4:5][0])[0]\r\n",
        "median_winner39 = np.asarray(winner39_results[9:10][0])[0]\r\n",
        "upper_winner39 = np.asarray(winner39_results[14:15][0])[0]"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vb7vh2K1pXp"
      },
      "source": [
        "# Iteration49 :\r\n",
        "\r\n",
        "slice49 = 48\r\n",
        "\r\n",
        "loser49 = [train_regret_loser_1[slice49],\r\n",
        "       train_regret_loser_2[slice49],\r\n",
        "       train_regret_loser_3[slice49],\r\n",
        "       train_regret_loser_4[slice49],\r\n",
        "       train_regret_loser_5[slice49],\r\n",
        "       train_regret_loser_6[slice49],\r\n",
        "       train_regret_loser_7[slice49],\r\n",
        "       train_regret_loser_8[slice49],\r\n",
        "       train_regret_loser_9[slice49],\r\n",
        "       train_regret_loser_10[slice49],\r\n",
        "       train_regret_loser_11[slice49],\r\n",
        "       train_regret_loser_12[slice49],\r\n",
        "       train_regret_loser_13[slice49],\r\n",
        "       train_regret_loser_14[slice49],\r\n",
        "       train_regret_loser_15[slice49],\r\n",
        "       train_regret_loser_16[slice49],\r\n",
        "       train_regret_loser_17[slice49],\r\n",
        "       train_regret_loser_18[slice49],\r\n",
        "       train_regret_loser_19[slice49],\r\n",
        "       train_regret_loser_20[slice49]]\r\n",
        "\r\n",
        "winner49 = [train_regret_winner_1[slice49],\r\n",
        "       train_regret_winner_2[slice49],\r\n",
        "       train_regret_winner_3[slice49],\r\n",
        "       train_regret_winner_4[slice49],\r\n",
        "       train_regret_winner_5[slice49],\r\n",
        "       train_regret_winner_6[slice49],\r\n",
        "       train_regret_winner_7[slice49],\r\n",
        "       train_regret_winner_8[slice49],\r\n",
        "       train_regret_winner_9[slice49],\r\n",
        "       train_regret_winner_10[slice49],\r\n",
        "       train_regret_winner_11[slice49],\r\n",
        "       train_regret_winner_12[slice49],\r\n",
        "       train_regret_winner_13[slice49],\r\n",
        "       train_regret_winner_14[slice49],\r\n",
        "       train_regret_winner_15[slice49],\r\n",
        "       train_regret_winner_16[slice49],\r\n",
        "       train_regret_winner_17[slice49],\r\n",
        "       train_regret_winner_18[slice49],\r\n",
        "       train_regret_winner_19[slice49],\r\n",
        "       train_regret_winner_20[slice49]]\r\n",
        "\r\n",
        "loser49_results = pd.DataFrame(loser49).sort_values(by=[0], ascending=False)\r\n",
        "winner49_results = pd.DataFrame(winner49).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser49 = np.asarray(loser49_results[4:5][0])[0]\r\n",
        "median_loser49 = np.asarray(loser49_results[9:10][0])[0]\r\n",
        "upper_loser49 = np.asarray(loser49_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner49 = np.asarray(winner49_results[4:5][0])[0]\r\n",
        "median_winner49 = np.asarray(winner49_results[9:10][0])[0]\r\n",
        "upper_winner49 = np.asarray(winner49_results[14:15][0])[0]"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wN7DWwt1paM"
      },
      "source": [
        "# Iteration59 :\r\n",
        "\r\n",
        "slice59 = 58\r\n",
        "\r\n",
        "loser59 = [train_regret_loser_1[slice59],\r\n",
        "       train_regret_loser_2[slice59],\r\n",
        "       train_regret_loser_3[slice59],\r\n",
        "       train_regret_loser_4[slice59],\r\n",
        "       train_regret_loser_5[slice59],\r\n",
        "       train_regret_loser_6[slice59],\r\n",
        "       train_regret_loser_7[slice59],\r\n",
        "       train_regret_loser_8[slice59],\r\n",
        "       train_regret_loser_9[slice59],\r\n",
        "       train_regret_loser_10[slice59],\r\n",
        "       train_regret_loser_11[slice59],\r\n",
        "       train_regret_loser_12[slice59],\r\n",
        "       train_regret_loser_13[slice59],\r\n",
        "       train_regret_loser_14[slice59],\r\n",
        "       train_regret_loser_15[slice59],\r\n",
        "       train_regret_loser_16[slice59],\r\n",
        "       train_regret_loser_17[slice59],\r\n",
        "       train_regret_loser_18[slice59],\r\n",
        "       train_regret_loser_19[slice59],\r\n",
        "       train_regret_loser_20[slice59]]\r\n",
        "\r\n",
        "winner59 = [train_regret_winner_1[slice59],\r\n",
        "       train_regret_winner_2[slice59],\r\n",
        "       train_regret_winner_3[slice59],\r\n",
        "       train_regret_winner_4[slice59],\r\n",
        "       train_regret_winner_5[slice59],\r\n",
        "       train_regret_winner_6[slice59],\r\n",
        "       train_regret_winner_7[slice59],\r\n",
        "       train_regret_winner_8[slice59],\r\n",
        "       train_regret_winner_9[slice59],\r\n",
        "       train_regret_winner_10[slice59],\r\n",
        "       train_regret_winner_11[slice59],\r\n",
        "       train_regret_winner_12[slice59],\r\n",
        "       train_regret_winner_13[slice59],\r\n",
        "       train_regret_winner_14[slice59],\r\n",
        "       train_regret_winner_15[slice59],\r\n",
        "       train_regret_winner_16[slice59],\r\n",
        "       train_regret_winner_17[slice59],\r\n",
        "       train_regret_winner_18[slice59],\r\n",
        "       train_regret_winner_19[slice59],\r\n",
        "       train_regret_winner_20[slice59]]\r\n",
        "\r\n",
        "loser59_results = pd.DataFrame(loser59).sort_values(by=[0], ascending=False)\r\n",
        "winner59_results = pd.DataFrame(winner59).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser59 = np.asarray(loser59_results[4:5][0])[0]\r\n",
        "median_loser59 = np.asarray(loser59_results[9:10][0])[0]\r\n",
        "upper_loser59 = np.asarray(loser59_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner59 = np.asarray(winner59_results[4:5][0])[0]\r\n",
        "median_winner59 = np.asarray(winner59_results[9:10][0])[0]\r\n",
        "upper_winner59 = np.asarray(winner59_results[14:15][0])[0]"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqtgekml1pde"
      },
      "source": [
        "# Iteration69 :\r\n",
        "\r\n",
        "slice69 = 68\r\n",
        "\r\n",
        "loser69 = [train_regret_loser_1[slice69],\r\n",
        "       train_regret_loser_2[slice69],\r\n",
        "       train_regret_loser_3[slice69],\r\n",
        "       train_regret_loser_4[slice69],\r\n",
        "       train_regret_loser_5[slice69],\r\n",
        "       train_regret_loser_6[slice69],\r\n",
        "       train_regret_loser_7[slice69],\r\n",
        "       train_regret_loser_8[slice69],\r\n",
        "       train_regret_loser_9[slice69],\r\n",
        "       train_regret_loser_10[slice69],\r\n",
        "       train_regret_loser_11[slice69],\r\n",
        "       train_regret_loser_12[slice69],\r\n",
        "       train_regret_loser_13[slice69],\r\n",
        "       train_regret_loser_14[slice69],\r\n",
        "       train_regret_loser_15[slice69],\r\n",
        "       train_regret_loser_16[slice69],\r\n",
        "       train_regret_loser_17[slice69],\r\n",
        "       train_regret_loser_18[slice69],\r\n",
        "       train_regret_loser_19[slice69],\r\n",
        "       train_regret_loser_20[slice69]]\r\n",
        "\r\n",
        "winner69 = [train_regret_winner_1[slice69],\r\n",
        "       train_regret_winner_2[slice69],\r\n",
        "       train_regret_winner_3[slice69],\r\n",
        "       train_regret_winner_4[slice69],\r\n",
        "       train_regret_winner_5[slice69],\r\n",
        "       train_regret_winner_6[slice69],\r\n",
        "       train_regret_winner_7[slice69],\r\n",
        "       train_regret_winner_8[slice69],\r\n",
        "       train_regret_winner_9[slice69],\r\n",
        "       train_regret_winner_10[slice69],\r\n",
        "       train_regret_winner_11[slice69],\r\n",
        "       train_regret_winner_12[slice69],\r\n",
        "       train_regret_winner_13[slice69],\r\n",
        "       train_regret_winner_14[slice69],\r\n",
        "       train_regret_winner_15[slice69],\r\n",
        "       train_regret_winner_16[slice69],\r\n",
        "       train_regret_winner_17[slice69],\r\n",
        "       train_regret_winner_18[slice69],\r\n",
        "       train_regret_winner_19[slice69],\r\n",
        "       train_regret_winner_20[slice69]]\r\n",
        "\r\n",
        "loser69_results = pd.DataFrame(loser69).sort_values(by=[0], ascending=False)\r\n",
        "winner69_results = pd.DataFrame(winner69).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser69 = np.asarray(loser69_results[4:5][0])[0]\r\n",
        "median_loser69 = np.asarray(loser69_results[9:10][0])[0]\r\n",
        "upper_loser69 = np.asarray(loser69_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner69 = np.asarray(winner69_results[4:5][0])[0]\r\n",
        "median_winner69 = np.asarray(winner69_results[9:10][0])[0]\r\n",
        "upper_winner69 = np.asarray(winner69_results[14:15][0])[0]"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPFTa1171pgF"
      },
      "source": [
        "# Iteration79 :\r\n",
        "\r\n",
        "slice79 = 78\r\n",
        "\r\n",
        "loser79 = [train_regret_loser_1[slice79],\r\n",
        "       train_regret_loser_2[slice79],\r\n",
        "       train_regret_loser_3[slice79],\r\n",
        "       train_regret_loser_4[slice79],\r\n",
        "       train_regret_loser_5[slice79],\r\n",
        "       train_regret_loser_6[slice79],\r\n",
        "       train_regret_loser_7[slice79],\r\n",
        "       train_regret_loser_8[slice79],\r\n",
        "       train_regret_loser_9[slice79],\r\n",
        "       train_regret_loser_10[slice79],\r\n",
        "       train_regret_loser_11[slice79],\r\n",
        "       train_regret_loser_12[slice79],\r\n",
        "       train_regret_loser_13[slice79],\r\n",
        "       train_regret_loser_14[slice79],\r\n",
        "       train_regret_loser_15[slice79],\r\n",
        "       train_regret_loser_16[slice79],\r\n",
        "       train_regret_loser_17[slice79],\r\n",
        "       train_regret_loser_18[slice79],\r\n",
        "       train_regret_loser_19[slice79],\r\n",
        "       train_regret_loser_20[slice79]]\r\n",
        "\r\n",
        "winner79 = [train_regret_winner_1[slice79],\r\n",
        "       train_regret_winner_2[slice79],\r\n",
        "       train_regret_winner_3[slice79],\r\n",
        "       train_regret_winner_4[slice79],\r\n",
        "       train_regret_winner_5[slice79],\r\n",
        "       train_regret_winner_6[slice79],\r\n",
        "       train_regret_winner_7[slice79],\r\n",
        "       train_regret_winner_8[slice79],\r\n",
        "       train_regret_winner_9[slice79],\r\n",
        "       train_regret_winner_10[slice79],\r\n",
        "       train_regret_winner_11[slice79],\r\n",
        "       train_regret_winner_12[slice79],\r\n",
        "       train_regret_winner_13[slice79],\r\n",
        "       train_regret_winner_14[slice79],\r\n",
        "       train_regret_winner_15[slice79],\r\n",
        "       train_regret_winner_16[slice79],\r\n",
        "       train_regret_winner_17[slice79],\r\n",
        "       train_regret_winner_18[slice79],\r\n",
        "       train_regret_winner_19[slice79],\r\n",
        "       train_regret_winner_20[slice79]]\r\n",
        "\r\n",
        "loser79_results = pd.DataFrame(loser79).sort_values(by=[0], ascending=False)\r\n",
        "winner79_results = pd.DataFrame(winner79).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser79 = np.asarray(loser79_results[4:5][0])[0]\r\n",
        "median_loser79 = np.asarray(loser79_results[9:10][0])[0]\r\n",
        "upper_loser79 = np.asarray(loser79_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner79 = np.asarray(winner79_results[4:5][0])[0]\r\n",
        "median_winner79 = np.asarray(winner79_results[9:10][0])[0]\r\n",
        "upper_winner79 = np.asarray(winner79_results[14:15][0])[0]"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTnRF4lx1pio"
      },
      "source": [
        "# Iteration89 :\r\n",
        "\r\n",
        "slice89 = 88\r\n",
        "\r\n",
        "loser89 = [train_regret_loser_1[slice89],\r\n",
        "       train_regret_loser_2[slice89],\r\n",
        "       train_regret_loser_3[slice89],\r\n",
        "       train_regret_loser_4[slice89],\r\n",
        "       train_regret_loser_5[slice89],\r\n",
        "       train_regret_loser_6[slice89],\r\n",
        "       train_regret_loser_7[slice89],\r\n",
        "       train_regret_loser_8[slice89],\r\n",
        "       train_regret_loser_9[slice89],\r\n",
        "       train_regret_loser_10[slice89],\r\n",
        "       train_regret_loser_11[slice89],\r\n",
        "       train_regret_loser_12[slice89],\r\n",
        "       train_regret_loser_13[slice89],\r\n",
        "       train_regret_loser_14[slice89],\r\n",
        "       train_regret_loser_15[slice89],\r\n",
        "       train_regret_loser_16[slice89],\r\n",
        "       train_regret_loser_17[slice89],\r\n",
        "       train_regret_loser_18[slice89],\r\n",
        "       train_regret_loser_19[slice89],\r\n",
        "       train_regret_loser_20[slice89]]\r\n",
        "\r\n",
        "winner89 = [train_regret_winner_1[slice89],\r\n",
        "       train_regret_winner_2[slice89],\r\n",
        "       train_regret_winner_3[slice89],\r\n",
        "       train_regret_winner_4[slice89],\r\n",
        "       train_regret_winner_5[slice89],\r\n",
        "       train_regret_winner_6[slice89],\r\n",
        "       train_regret_winner_7[slice89],\r\n",
        "       train_regret_winner_8[slice89],\r\n",
        "       train_regret_winner_9[slice89],\r\n",
        "       train_regret_winner_10[slice89],\r\n",
        "       train_regret_winner_11[slice89],\r\n",
        "       train_regret_winner_12[slice89],\r\n",
        "       train_regret_winner_13[slice89],\r\n",
        "       train_regret_winner_14[slice89],\r\n",
        "       train_regret_winner_15[slice89],\r\n",
        "       train_regret_winner_16[slice89],\r\n",
        "       train_regret_winner_17[slice89],\r\n",
        "       train_regret_winner_18[slice89],\r\n",
        "       train_regret_winner_19[slice89],\r\n",
        "       train_regret_winner_20[slice89]]\r\n",
        "\r\n",
        "loser89_results = pd.DataFrame(loser89).sort_values(by=[0], ascending=False)\r\n",
        "winner89_results = pd.DataFrame(winner89).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser89 = np.asarray(loser89_results[4:5][0])[0]\r\n",
        "median_loser89 = np.asarray(loser89_results[9:10][0])[0]\r\n",
        "upper_loser89 = np.asarray(loser89_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner89 = np.asarray(winner89_results[4:5][0])[0]\r\n",
        "median_winner89 = np.asarray(winner89_results[9:10][0])[0]\r\n",
        "upper_winner89 = np.asarray(winner89_results[14:15][0])[0]"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiLAF38i1plO"
      },
      "source": [
        "# Iteration99 :\r\n",
        "\r\n",
        "slice99 = 98\r\n",
        "\r\n",
        "loser99 = [train_regret_loser_1[slice99],\r\n",
        "       train_regret_loser_2[slice99],\r\n",
        "       train_regret_loser_3[slice99],\r\n",
        "       train_regret_loser_4[slice99],\r\n",
        "       train_regret_loser_5[slice99],\r\n",
        "       train_regret_loser_6[slice99],\r\n",
        "       train_regret_loser_7[slice99],\r\n",
        "       train_regret_loser_8[slice99],\r\n",
        "       train_regret_loser_9[slice99],\r\n",
        "       train_regret_loser_10[slice99],\r\n",
        "       train_regret_loser_11[slice99],\r\n",
        "       train_regret_loser_12[slice99],\r\n",
        "       train_regret_loser_13[slice99],\r\n",
        "       train_regret_loser_14[slice99],\r\n",
        "       train_regret_loser_15[slice99],\r\n",
        "       train_regret_loser_16[slice99],\r\n",
        "       train_regret_loser_17[slice99],\r\n",
        "       train_regret_loser_18[slice99],\r\n",
        "       train_regret_loser_19[slice99],\r\n",
        "       train_regret_loser_20[slice99]]\r\n",
        "\r\n",
        "winner99 = [train_regret_winner_1[slice99],\r\n",
        "       train_regret_winner_2[slice99],\r\n",
        "       train_regret_winner_3[slice99],\r\n",
        "       train_regret_winner_4[slice99],\r\n",
        "       train_regret_winner_5[slice99],\r\n",
        "       train_regret_winner_6[slice99],\r\n",
        "       train_regret_winner_7[slice99],\r\n",
        "       train_regret_winner_8[slice99],\r\n",
        "       train_regret_winner_9[slice99],\r\n",
        "       train_regret_winner_10[slice99],\r\n",
        "       train_regret_winner_11[slice99],\r\n",
        "       train_regret_winner_12[slice99],\r\n",
        "       train_regret_winner_13[slice99],\r\n",
        "       train_regret_winner_14[slice99],\r\n",
        "       train_regret_winner_15[slice99],\r\n",
        "       train_regret_winner_16[slice99],\r\n",
        "       train_regret_winner_17[slice99],\r\n",
        "       train_regret_winner_18[slice99],\r\n",
        "       train_regret_winner_19[slice99],\r\n",
        "       train_regret_winner_20[slice99]]\r\n",
        "\r\n",
        "loser99_results = pd.DataFrame(loser99).sort_values(by=[0], ascending=False)\r\n",
        "winner99_results = pd.DataFrame(winner99).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser99 = np.asarray(loser99_results[4:5][0])[0]\r\n",
        "median_loser99 = np.asarray(loser99_results[9:10][0])[0]\r\n",
        "upper_loser99 = np.asarray(loser99_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner99 = np.asarray(winner99_results[4:5][0])[0]\r\n",
        "median_winner99 = np.asarray(winner99_results[9:10][0])[0]\r\n",
        "upper_winner99 = np.asarray(winner99_results[14:15][0])[0]"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBd_N0F21pn0"
      },
      "source": [
        "# Iteration10 :\r\n",
        "\r\n",
        "slice10 = 9\r\n",
        "\r\n",
        "loser10 = [train_regret_loser_1[slice10],\r\n",
        "       train_regret_loser_2[slice10],\r\n",
        "       train_regret_loser_3[slice10],\r\n",
        "       train_regret_loser_4[slice10],\r\n",
        "       train_regret_loser_5[slice10],\r\n",
        "       train_regret_loser_6[slice10],\r\n",
        "       train_regret_loser_7[slice10],\r\n",
        "       train_regret_loser_8[slice10],\r\n",
        "       train_regret_loser_9[slice10],\r\n",
        "       train_regret_loser_10[slice10],\r\n",
        "       train_regret_loser_11[slice10],\r\n",
        "       train_regret_loser_12[slice10],\r\n",
        "       train_regret_loser_13[slice10],\r\n",
        "       train_regret_loser_14[slice10],\r\n",
        "       train_regret_loser_15[slice10],\r\n",
        "       train_regret_loser_16[slice10],\r\n",
        "       train_regret_loser_17[slice10],\r\n",
        "       train_regret_loser_18[slice10],\r\n",
        "       train_regret_loser_19[slice10],\r\n",
        "       train_regret_loser_20[slice10]]\r\n",
        "\r\n",
        "winner10 = [train_regret_winner_1[slice10],\r\n",
        "       train_regret_winner_2[slice10],\r\n",
        "       train_regret_winner_3[slice10],\r\n",
        "       train_regret_winner_4[slice10],\r\n",
        "       train_regret_winner_5[slice10],\r\n",
        "       train_regret_winner_6[slice10],\r\n",
        "       train_regret_winner_7[slice10],\r\n",
        "       train_regret_winner_8[slice10],\r\n",
        "       train_regret_winner_9[slice10],\r\n",
        "       train_regret_winner_10[slice10],\r\n",
        "       train_regret_winner_11[slice10],\r\n",
        "       train_regret_winner_12[slice10],\r\n",
        "       train_regret_winner_13[slice10],\r\n",
        "       train_regret_winner_14[slice10],\r\n",
        "       train_regret_winner_15[slice10],\r\n",
        "       train_regret_winner_16[slice10],\r\n",
        "       train_regret_winner_17[slice10],\r\n",
        "       train_regret_winner_18[slice10],\r\n",
        "       train_regret_winner_19[slice10],\r\n",
        "       train_regret_winner_20[slice10]]\r\n",
        "\r\n",
        "loser10_results = pd.DataFrame(loser10).sort_values(by=[0], ascending=False)\r\n",
        "winner10_results = pd.DataFrame(winner10).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser10 = np.asarray(loser10_results[4:5][0])[0]\r\n",
        "median_loser10 = np.asarray(loser10_results[9:10][0])[0]\r\n",
        "upper_loser10 = np.asarray(loser10_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner10 = np.asarray(winner10_results[4:5][0])[0]\r\n",
        "median_winner10 = np.asarray(winner10_results[9:10][0])[0]\r\n",
        "upper_winner10 = np.asarray(winner10_results[14:15][0])[0]"
      ],
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXjdqc4s1pqb"
      },
      "source": [
        "# Iteration20 :\r\n",
        "\r\n",
        "slice20 = 19\r\n",
        "\r\n",
        "loser20 = [train_regret_loser_1[slice20],\r\n",
        "       train_regret_loser_2[slice20],\r\n",
        "       train_regret_loser_3[slice20],\r\n",
        "       train_regret_loser_4[slice20],\r\n",
        "       train_regret_loser_5[slice20],\r\n",
        "       train_regret_loser_6[slice20],\r\n",
        "       train_regret_loser_7[slice20],\r\n",
        "       train_regret_loser_8[slice20],\r\n",
        "       train_regret_loser_9[slice20],\r\n",
        "       train_regret_loser_10[slice20],\r\n",
        "       train_regret_loser_11[slice20],\r\n",
        "       train_regret_loser_12[slice20],\r\n",
        "       train_regret_loser_13[slice20],\r\n",
        "       train_regret_loser_14[slice20],\r\n",
        "       train_regret_loser_15[slice20],\r\n",
        "       train_regret_loser_16[slice20],\r\n",
        "       train_regret_loser_17[slice20],\r\n",
        "       train_regret_loser_18[slice20],\r\n",
        "       train_regret_loser_19[slice20],\r\n",
        "       train_regret_loser_20[slice20]]\r\n",
        "\r\n",
        "winner20 = [train_regret_winner_1[slice20],\r\n",
        "       train_regret_winner_2[slice20],\r\n",
        "       train_regret_winner_3[slice20],\r\n",
        "       train_regret_winner_4[slice20],\r\n",
        "       train_regret_winner_5[slice20],\r\n",
        "       train_regret_winner_6[slice20],\r\n",
        "       train_regret_winner_7[slice20],\r\n",
        "       train_regret_winner_8[slice20],\r\n",
        "       train_regret_winner_9[slice20],\r\n",
        "       train_regret_winner_10[slice20],\r\n",
        "       train_regret_winner_11[slice20],\r\n",
        "       train_regret_winner_12[slice20],\r\n",
        "       train_regret_winner_13[slice20],\r\n",
        "       train_regret_winner_14[slice20],\r\n",
        "       train_regret_winner_15[slice20],\r\n",
        "       train_regret_winner_16[slice20],\r\n",
        "       train_regret_winner_17[slice20],\r\n",
        "       train_regret_winner_18[slice20],\r\n",
        "       train_regret_winner_19[slice20],\r\n",
        "       train_regret_winner_20[slice20]]\r\n",
        "\r\n",
        "loser20_results = pd.DataFrame(loser20).sort_values(by=[0], ascending=False)\r\n",
        "winner20_results = pd.DataFrame(winner20).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser20 = np.asarray(loser20_results[4:5][0])[0]\r\n",
        "median_loser20 = np.asarray(loser20_results[9:10][0])[0]\r\n",
        "upper_loser20 = np.asarray(loser20_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner20 = np.asarray(winner20_results[4:5][0])[0]\r\n",
        "median_winner20 = np.asarray(winner20_results[9:10][0])[0]\r\n",
        "upper_winner20 = np.asarray(winner20_results[14:15][0])[0]"
      ],
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQBVS6dx1ptB"
      },
      "source": [
        "# Iteration30 :\r\n",
        "\r\n",
        "slice30 = 29\r\n",
        "\r\n",
        "loser30 = [train_regret_loser_1[slice30],\r\n",
        "       train_regret_loser_2[slice30],\r\n",
        "       train_regret_loser_3[slice30],\r\n",
        "       train_regret_loser_4[slice30],\r\n",
        "       train_regret_loser_5[slice30],\r\n",
        "       train_regret_loser_6[slice30],\r\n",
        "       train_regret_loser_7[slice30],\r\n",
        "       train_regret_loser_8[slice30],\r\n",
        "       train_regret_loser_9[slice30],\r\n",
        "       train_regret_loser_10[slice30],\r\n",
        "       train_regret_loser_11[slice30],\r\n",
        "       train_regret_loser_12[slice30],\r\n",
        "       train_regret_loser_13[slice30],\r\n",
        "       train_regret_loser_14[slice30],\r\n",
        "       train_regret_loser_15[slice30],\r\n",
        "       train_regret_loser_16[slice30],\r\n",
        "       train_regret_loser_17[slice30],\r\n",
        "       train_regret_loser_18[slice30],\r\n",
        "       train_regret_loser_19[slice30],\r\n",
        "       train_regret_loser_20[slice30]]\r\n",
        "\r\n",
        "winner30 = [train_regret_winner_1[slice30],\r\n",
        "       train_regret_winner_2[slice30],\r\n",
        "       train_regret_winner_3[slice30],\r\n",
        "       train_regret_winner_4[slice30],\r\n",
        "       train_regret_winner_5[slice30],\r\n",
        "       train_regret_winner_6[slice30],\r\n",
        "       train_regret_winner_7[slice30],\r\n",
        "       train_regret_winner_8[slice30],\r\n",
        "       train_regret_winner_9[slice30],\r\n",
        "       train_regret_winner_10[slice30],\r\n",
        "       train_regret_winner_11[slice30],\r\n",
        "       train_regret_winner_12[slice30],\r\n",
        "       train_regret_winner_13[slice30],\r\n",
        "       train_regret_winner_14[slice30],\r\n",
        "       train_regret_winner_15[slice30],\r\n",
        "       train_regret_winner_16[slice30],\r\n",
        "       train_regret_winner_17[slice30],\r\n",
        "       train_regret_winner_18[slice30],\r\n",
        "       train_regret_winner_19[slice30],\r\n",
        "       train_regret_winner_20[slice30]]\r\n",
        "\r\n",
        "loser30_results = pd.DataFrame(loser30).sort_values(by=[0], ascending=False)\r\n",
        "winner30_results = pd.DataFrame(winner30).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser30 = np.asarray(loser30_results[4:5][0])[0]\r\n",
        "median_loser30 = np.asarray(loser30_results[9:10][0])[0]\r\n",
        "upper_loser30 = np.asarray(loser30_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner30 = np.asarray(winner30_results[4:5][0])[0]\r\n",
        "median_winner30 = np.asarray(winner30_results[9:10][0])[0]\r\n",
        "upper_winner30 = np.asarray(winner30_results[14:15][0])[0]"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-iWfLkj1pvn"
      },
      "source": [
        "# Iteration40 :\r\n",
        "\r\n",
        "slice40 = 39\r\n",
        "\r\n",
        "loser40 = [train_regret_loser_1[slice40],\r\n",
        "       train_regret_loser_2[slice40],\r\n",
        "       train_regret_loser_3[slice40],\r\n",
        "       train_regret_loser_4[slice40],\r\n",
        "       train_regret_loser_5[slice40],\r\n",
        "       train_regret_loser_6[slice40],\r\n",
        "       train_regret_loser_7[slice40],\r\n",
        "       train_regret_loser_8[slice40],\r\n",
        "       train_regret_loser_9[slice40],\r\n",
        "       train_regret_loser_10[slice40],\r\n",
        "       train_regret_loser_11[slice40],\r\n",
        "       train_regret_loser_12[slice40],\r\n",
        "       train_regret_loser_13[slice40],\r\n",
        "       train_regret_loser_14[slice40],\r\n",
        "       train_regret_loser_15[slice40],\r\n",
        "       train_regret_loser_16[slice40],\r\n",
        "       train_regret_loser_17[slice40],\r\n",
        "       train_regret_loser_18[slice40],\r\n",
        "       train_regret_loser_19[slice40],\r\n",
        "       train_regret_loser_20[slice40]]\r\n",
        "\r\n",
        "winner40 = [train_regret_winner_1[slice40],\r\n",
        "       train_regret_winner_2[slice40],\r\n",
        "       train_regret_winner_3[slice40],\r\n",
        "       train_regret_winner_4[slice40],\r\n",
        "       train_regret_winner_5[slice40],\r\n",
        "       train_regret_winner_6[slice40],\r\n",
        "       train_regret_winner_7[slice40],\r\n",
        "       train_regret_winner_8[slice40],\r\n",
        "       train_regret_winner_9[slice40],\r\n",
        "       train_regret_winner_10[slice40],\r\n",
        "       train_regret_winner_11[slice40],\r\n",
        "       train_regret_winner_12[slice40],\r\n",
        "       train_regret_winner_13[slice40],\r\n",
        "       train_regret_winner_14[slice40],\r\n",
        "       train_regret_winner_15[slice40],\r\n",
        "       train_regret_winner_16[slice40],\r\n",
        "       train_regret_winner_17[slice40],\r\n",
        "       train_regret_winner_18[slice40],\r\n",
        "       train_regret_winner_19[slice40],\r\n",
        "       train_regret_winner_20[slice40]]\r\n",
        "\r\n",
        "loser40_results = pd.DataFrame(loser40).sort_values(by=[0], ascending=False)\r\n",
        "winner40_results = pd.DataFrame(winner40).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser40 = np.asarray(loser40_results[4:5][0])[0]\r\n",
        "median_loser40 = np.asarray(loser40_results[9:10][0])[0]\r\n",
        "upper_loser40 = np.asarray(loser40_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner40 = np.asarray(winner40_results[4:5][0])[0]\r\n",
        "median_winner40 = np.asarray(winner40_results[9:10][0])[0]\r\n",
        "upper_winner40 = np.asarray(winner40_results[14:15][0])[0]"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-G5i-Cjw2U7K"
      },
      "source": [
        "# Iteration50 :\r\n",
        "\r\n",
        "slice50 = 49\r\n",
        "\r\n",
        "loser50 = [train_regret_loser_1[slice50],\r\n",
        "       train_regret_loser_2[slice50],\r\n",
        "       train_regret_loser_3[slice50],\r\n",
        "       train_regret_loser_4[slice50],\r\n",
        "       train_regret_loser_5[slice50],\r\n",
        "       train_regret_loser_6[slice50],\r\n",
        "       train_regret_loser_7[slice50],\r\n",
        "       train_regret_loser_8[slice50],\r\n",
        "       train_regret_loser_9[slice50],\r\n",
        "       train_regret_loser_10[slice50],\r\n",
        "       train_regret_loser_11[slice50],\r\n",
        "       train_regret_loser_12[slice50],\r\n",
        "       train_regret_loser_13[slice50],\r\n",
        "       train_regret_loser_14[slice50],\r\n",
        "       train_regret_loser_15[slice50],\r\n",
        "       train_regret_loser_16[slice50],\r\n",
        "       train_regret_loser_17[slice50],\r\n",
        "       train_regret_loser_18[slice50],\r\n",
        "       train_regret_loser_19[slice50],\r\n",
        "       train_regret_loser_20[slice50]]\r\n",
        "\r\n",
        "winner50 = [train_regret_winner_1[slice50],\r\n",
        "       train_regret_winner_2[slice50],\r\n",
        "       train_regret_winner_3[slice50],\r\n",
        "       train_regret_winner_4[slice50],\r\n",
        "       train_regret_winner_5[slice50],\r\n",
        "       train_regret_winner_6[slice50],\r\n",
        "       train_regret_winner_7[slice50],\r\n",
        "       train_regret_winner_8[slice50],\r\n",
        "       train_regret_winner_9[slice50],\r\n",
        "       train_regret_winner_10[slice50],\r\n",
        "       train_regret_winner_11[slice50],\r\n",
        "       train_regret_winner_12[slice50],\r\n",
        "       train_regret_winner_13[slice50],\r\n",
        "       train_regret_winner_14[slice50],\r\n",
        "       train_regret_winner_15[slice50],\r\n",
        "       train_regret_winner_16[slice50],\r\n",
        "       train_regret_winner_17[slice50],\r\n",
        "       train_regret_winner_18[slice50],\r\n",
        "       train_regret_winner_19[slice50],\r\n",
        "       train_regret_winner_20[slice50]]\r\n",
        "\r\n",
        "loser50_results = pd.DataFrame(loser50).sort_values(by=[0], ascending=False)\r\n",
        "winner50_results = pd.DataFrame(winner50).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser50 = np.asarray(loser50_results[4:5][0])[0]\r\n",
        "median_loser50 = np.asarray(loser50_results[9:10][0])[0]\r\n",
        "upper_loser50 = np.asarray(loser50_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner50 = np.asarray(winner50_results[4:5][0])[0]\r\n",
        "median_winner50 = np.asarray(winner50_results[9:10][0])[0]\r\n",
        "upper_winner50 = np.asarray(winner50_results[14:15][0])[0]"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY6qxZQA2U-r"
      },
      "source": [
        "# Iteration60 :\r\n",
        "\r\n",
        "slice60 = 59\r\n",
        "\r\n",
        "loser60 = [train_regret_loser_1[slice60],\r\n",
        "       train_regret_loser_2[slice60],\r\n",
        "       train_regret_loser_3[slice60],\r\n",
        "       train_regret_loser_4[slice60],\r\n",
        "       train_regret_loser_5[slice60],\r\n",
        "       train_regret_loser_6[slice60],\r\n",
        "       train_regret_loser_7[slice60],\r\n",
        "       train_regret_loser_8[slice60],\r\n",
        "       train_regret_loser_9[slice60],\r\n",
        "       train_regret_loser_10[slice60],\r\n",
        "       train_regret_loser_11[slice60],\r\n",
        "       train_regret_loser_12[slice60],\r\n",
        "       train_regret_loser_13[slice60],\r\n",
        "       train_regret_loser_14[slice60],\r\n",
        "       train_regret_loser_15[slice60],\r\n",
        "       train_regret_loser_16[slice60],\r\n",
        "       train_regret_loser_17[slice60],\r\n",
        "       train_regret_loser_18[slice60],\r\n",
        "       train_regret_loser_19[slice60],\r\n",
        "       train_regret_loser_20[slice60]]\r\n",
        "\r\n",
        "winner60 = [train_regret_winner_1[slice60],\r\n",
        "       train_regret_winner_2[slice60],\r\n",
        "       train_regret_winner_3[slice60],\r\n",
        "       train_regret_winner_4[slice60],\r\n",
        "       train_regret_winner_5[slice60],\r\n",
        "       train_regret_winner_6[slice60],\r\n",
        "       train_regret_winner_7[slice60],\r\n",
        "       train_regret_winner_8[slice60],\r\n",
        "       train_regret_winner_9[slice60],\r\n",
        "       train_regret_winner_10[slice60],\r\n",
        "       train_regret_winner_11[slice60],\r\n",
        "       train_regret_winner_12[slice60],\r\n",
        "       train_regret_winner_13[slice60],\r\n",
        "       train_regret_winner_14[slice60],\r\n",
        "       train_regret_winner_15[slice60],\r\n",
        "       train_regret_winner_16[slice60],\r\n",
        "       train_regret_winner_17[slice60],\r\n",
        "       train_regret_winner_18[slice60],\r\n",
        "       train_regret_winner_19[slice60],\r\n",
        "       train_regret_winner_20[slice60]]\r\n",
        "\r\n",
        "loser60_results = pd.DataFrame(loser60).sort_values(by=[0], ascending=False)\r\n",
        "winner60_results = pd.DataFrame(winner60).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser60 = np.asarray(loser60_results[4:5][0])[0]\r\n",
        "median_loser60 = np.asarray(loser60_results[9:10][0])[0]\r\n",
        "upper_loser60 = np.asarray(loser60_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner60 = np.asarray(winner60_results[4:5][0])[0]\r\n",
        "median_winner60 = np.asarray(winner60_results[9:10][0])[0]\r\n",
        "upper_winner60 = np.asarray(winner60_results[14:15][0])[0]"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBqgVxVH2VCU"
      },
      "source": [
        "# Iteration70 :\r\n",
        "\r\n",
        "slice70 = 69\r\n",
        "\r\n",
        "loser70 = [train_regret_loser_1[slice70],\r\n",
        "       train_regret_loser_2[slice70],\r\n",
        "       train_regret_loser_3[slice70],\r\n",
        "       train_regret_loser_4[slice70],\r\n",
        "       train_regret_loser_5[slice70],\r\n",
        "       train_regret_loser_6[slice70],\r\n",
        "       train_regret_loser_7[slice70],\r\n",
        "       train_regret_loser_8[slice70],\r\n",
        "       train_regret_loser_9[slice70],\r\n",
        "       train_regret_loser_10[slice70],\r\n",
        "       train_regret_loser_11[slice70],\r\n",
        "       train_regret_loser_12[slice70],\r\n",
        "       train_regret_loser_13[slice70],\r\n",
        "       train_regret_loser_14[slice70],\r\n",
        "       train_regret_loser_15[slice70],\r\n",
        "       train_regret_loser_16[slice70],\r\n",
        "       train_regret_loser_17[slice70],\r\n",
        "       train_regret_loser_18[slice70],\r\n",
        "       train_regret_loser_19[slice70],\r\n",
        "       train_regret_loser_20[slice70]]\r\n",
        "\r\n",
        "winner70 = [train_regret_winner_1[slice70],\r\n",
        "       train_regret_winner_2[slice70],\r\n",
        "       train_regret_winner_3[slice70],\r\n",
        "       train_regret_winner_4[slice70],\r\n",
        "       train_regret_winner_5[slice70],\r\n",
        "       train_regret_winner_6[slice70],\r\n",
        "       train_regret_winner_7[slice70],\r\n",
        "       train_regret_winner_8[slice70],\r\n",
        "       train_regret_winner_9[slice70],\r\n",
        "       train_regret_winner_10[slice70],\r\n",
        "       train_regret_winner_11[slice70],\r\n",
        "       train_regret_winner_12[slice70],\r\n",
        "       train_regret_winner_13[slice70],\r\n",
        "       train_regret_winner_14[slice70],\r\n",
        "       train_regret_winner_15[slice70],\r\n",
        "       train_regret_winner_16[slice70],\r\n",
        "       train_regret_winner_17[slice70],\r\n",
        "       train_regret_winner_18[slice70],\r\n",
        "       train_regret_winner_19[slice70],\r\n",
        "       train_regret_winner_20[slice70]]\r\n",
        "\r\n",
        "loser70_results = pd.DataFrame(loser70).sort_values(by=[0], ascending=False)\r\n",
        "winner70_results = pd.DataFrame(winner70).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser70 = np.asarray(loser70_results[4:5][0])[0]\r\n",
        "median_loser70 = np.asarray(loser70_results[9:10][0])[0]\r\n",
        "upper_loser70 = np.asarray(loser70_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner70 = np.asarray(winner70_results[4:5][0])[0]\r\n",
        "median_winner70 = np.asarray(winner70_results[9:10][0])[0]\r\n",
        "upper_winner70 = np.asarray(winner70_results[14:15][0])[0]"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF7mlDZL1pyN"
      },
      "source": [
        "# Iteration80 :\r\n",
        "\r\n",
        "slice80 = 79\r\n",
        "\r\n",
        "loser80 = [train_regret_loser_1[slice80],\r\n",
        "       train_regret_loser_2[slice80],\r\n",
        "       train_regret_loser_3[slice80],\r\n",
        "       train_regret_loser_4[slice80],\r\n",
        "       train_regret_loser_5[slice80],\r\n",
        "       train_regret_loser_6[slice80],\r\n",
        "       train_regret_loser_7[slice80],\r\n",
        "       train_regret_loser_8[slice80],\r\n",
        "       train_regret_loser_9[slice80],\r\n",
        "       train_regret_loser_10[slice80],\r\n",
        "       train_regret_loser_11[slice80],\r\n",
        "       train_regret_loser_12[slice80],\r\n",
        "       train_regret_loser_13[slice80],\r\n",
        "       train_regret_loser_14[slice80],\r\n",
        "       train_regret_loser_15[slice80],\r\n",
        "       train_regret_loser_16[slice80],\r\n",
        "       train_regret_loser_17[slice80],\r\n",
        "       train_regret_loser_18[slice80],\r\n",
        "       train_regret_loser_19[slice80],\r\n",
        "       train_regret_loser_20[slice80]]\r\n",
        "\r\n",
        "winner80 = [train_regret_winner_1[slice80],\r\n",
        "       train_regret_winner_2[slice80],\r\n",
        "       train_regret_winner_3[slice80],\r\n",
        "       train_regret_winner_4[slice80],\r\n",
        "       train_regret_winner_5[slice80],\r\n",
        "       train_regret_winner_6[slice80],\r\n",
        "       train_regret_winner_7[slice80],\r\n",
        "       train_regret_winner_8[slice80],\r\n",
        "       train_regret_winner_9[slice80],\r\n",
        "       train_regret_winner_10[slice80],\r\n",
        "       train_regret_winner_11[slice80],\r\n",
        "       train_regret_winner_12[slice80],\r\n",
        "       train_regret_winner_13[slice80],\r\n",
        "       train_regret_winner_14[slice80],\r\n",
        "       train_regret_winner_15[slice80],\r\n",
        "       train_regret_winner_16[slice80],\r\n",
        "       train_regret_winner_17[slice80],\r\n",
        "       train_regret_winner_18[slice80],\r\n",
        "       train_regret_winner_19[slice80],\r\n",
        "       train_regret_winner_20[slice80]]\r\n",
        "\r\n",
        "loser80_results = pd.DataFrame(loser80).sort_values(by=[0], ascending=False)\r\n",
        "winner80_results = pd.DataFrame(winner80).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser80 = np.asarray(loser80_results[4:5][0])[0]\r\n",
        "median_loser80 = np.asarray(loser80_results[9:10][0])[0]\r\n",
        "upper_loser80 = np.asarray(loser80_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner80 = np.asarray(winner80_results[4:5][0])[0]\r\n",
        "median_winner80 = np.asarray(winner80_results[9:10][0])[0]\r\n",
        "upper_winner80 = np.asarray(winner80_results[14:15][0])[0]"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1Rw8IkD1p0z"
      },
      "source": [
        "# Iteration90 :\r\n",
        "\r\n",
        "slice90 = 89\r\n",
        "\r\n",
        "loser90 = [train_regret_loser_1[slice90],\r\n",
        "       train_regret_loser_2[slice90],\r\n",
        "       train_regret_loser_3[slice90],\r\n",
        "       train_regret_loser_4[slice90],\r\n",
        "       train_regret_loser_5[slice90],\r\n",
        "       train_regret_loser_6[slice90],\r\n",
        "       train_regret_loser_7[slice90],\r\n",
        "       train_regret_loser_8[slice90],\r\n",
        "       train_regret_loser_9[slice90],\r\n",
        "       train_regret_loser_10[slice90],\r\n",
        "       train_regret_loser_11[slice90],\r\n",
        "       train_regret_loser_12[slice90],\r\n",
        "       train_regret_loser_13[slice90],\r\n",
        "       train_regret_loser_14[slice90],\r\n",
        "       train_regret_loser_15[slice90],\r\n",
        "       train_regret_loser_16[slice90],\r\n",
        "       train_regret_loser_17[slice90],\r\n",
        "       train_regret_loser_18[slice90],\r\n",
        "       train_regret_loser_19[slice90],\r\n",
        "       train_regret_loser_20[slice90]]\r\n",
        "\r\n",
        "winner90 = [train_regret_winner_1[slice90],\r\n",
        "       train_regret_winner_2[slice90],\r\n",
        "       train_regret_winner_3[slice90],\r\n",
        "       train_regret_winner_4[slice90],\r\n",
        "       train_regret_winner_5[slice90],\r\n",
        "       train_regret_winner_6[slice90],\r\n",
        "       train_regret_winner_7[slice90],\r\n",
        "       train_regret_winner_8[slice90],\r\n",
        "       train_regret_winner_9[slice90],\r\n",
        "       train_regret_winner_10[slice90],\r\n",
        "       train_regret_winner_11[slice90],\r\n",
        "       train_regret_winner_12[slice90],\r\n",
        "       train_regret_winner_13[slice90],\r\n",
        "       train_regret_winner_14[slice90],\r\n",
        "       train_regret_winner_15[slice90],\r\n",
        "       train_regret_winner_16[slice90],\r\n",
        "       train_regret_winner_17[slice90],\r\n",
        "       train_regret_winner_18[slice90],\r\n",
        "       train_regret_winner_19[slice90],\r\n",
        "       train_regret_winner_20[slice90]]\r\n",
        "\r\n",
        "loser90_results = pd.DataFrame(loser90).sort_values(by=[0], ascending=False)\r\n",
        "winner90_results = pd.DataFrame(winner90).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser90 = np.asarray(loser90_results[4:5][0])[0]\r\n",
        "median_loser90 = np.asarray(loser90_results[9:10][0])[0]\r\n",
        "upper_loser90 = np.asarray(loser90_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner90 = np.asarray(winner90_results[4:5][0])[0]\r\n",
        "median_winner90 = np.asarray(winner90_results[9:10][0])[0]\r\n",
        "upper_winner90 = np.asarray(winner90_results[14:15][0])[0]"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiljCrq21p3a"
      },
      "source": [
        "# Iteration100 :\r\n",
        "\r\n",
        "slice100 = 99\r\n",
        "\r\n",
        "loser100 = [train_regret_loser_1[slice100],\r\n",
        "       train_regret_loser_2[slice100],\r\n",
        "       train_regret_loser_3[slice100],\r\n",
        "       train_regret_loser_4[slice100],\r\n",
        "       train_regret_loser_5[slice100],\r\n",
        "       train_regret_loser_6[slice100],\r\n",
        "       train_regret_loser_7[slice100],\r\n",
        "       train_regret_loser_8[slice100],\r\n",
        "       train_regret_loser_9[slice100],\r\n",
        "       train_regret_loser_10[slice100],\r\n",
        "       train_regret_loser_11[slice100],\r\n",
        "       train_regret_loser_12[slice100],\r\n",
        "       train_regret_loser_13[slice100],\r\n",
        "       train_regret_loser_14[slice100],\r\n",
        "       train_regret_loser_15[slice100],\r\n",
        "       train_regret_loser_16[slice100],\r\n",
        "       train_regret_loser_17[slice100],\r\n",
        "       train_regret_loser_18[slice100],\r\n",
        "       train_regret_loser_19[slice100],\r\n",
        "       train_regret_loser_20[slice100]]\r\n",
        "\r\n",
        "winner100 = [train_regret_winner_1[slice100],\r\n",
        "       train_regret_winner_2[slice100],\r\n",
        "       train_regret_winner_3[slice100],\r\n",
        "       train_regret_winner_4[slice100],\r\n",
        "       train_regret_winner_5[slice100],\r\n",
        "       train_regret_winner_6[slice100],\r\n",
        "       train_regret_winner_7[slice100],\r\n",
        "       train_regret_winner_8[slice100],\r\n",
        "       train_regret_winner_9[slice100],\r\n",
        "       train_regret_winner_10[slice100],\r\n",
        "       train_regret_winner_11[slice100],\r\n",
        "       train_regret_winner_12[slice100],\r\n",
        "       train_regret_winner_13[slice100],\r\n",
        "       train_regret_winner_14[slice100],\r\n",
        "       train_regret_winner_15[slice100],\r\n",
        "       train_regret_winner_16[slice100],\r\n",
        "       train_regret_winner_17[slice100],\r\n",
        "       train_regret_winner_18[slice100],\r\n",
        "       train_regret_winner_19[slice100],\r\n",
        "       train_regret_winner_20[slice100]]\r\n",
        "\r\n",
        "loser100_results = pd.DataFrame(loser100).sort_values(by=[0], ascending=False)\r\n",
        "winner100_results = pd.DataFrame(winner100).sort_values(by=[0], ascending=False)\r\n",
        "\r\n",
        "### Best training regret minimization IQR - loser:\r\n",
        "lower_loser100 = np.asarray(loser100_results[4:5][0])[0]\r\n",
        "median_loser100 = np.asarray(loser100_results[9:10][0])[0]\r\n",
        "upper_loser100 = np.asarray(loser100_results[14:15][0])[0]\r\n",
        "\r\n",
        "lower_winner100 = np.asarray(winner100_results[4:5][0])[0]\r\n",
        "median_winner100 = np.asarray(winner100_results[9:10][0])[0]\r\n",
        "upper_winner100 = np.asarray(winner100_results[14:15][0])[0]"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OItJgqUT2oNb"
      },
      "source": [
        "### Summarize arrays: 'Loser'\r\n",
        "\r\n",
        "lower_loser = [lower_loser1,\r\n",
        "            lower_loser2,\r\n",
        "            lower_loser3,\r\n",
        "            lower_loser4,\r\n",
        "            lower_loser5,\r\n",
        "            lower_loser6,\r\n",
        "            lower_loser7,\r\n",
        "            lower_loser8,\r\n",
        "            lower_loser9,\r\n",
        "            lower_loser10,\r\n",
        "            lower_loser11,\r\n",
        "            lower_loser12,\r\n",
        "            lower_loser13,\r\n",
        "            lower_loser14,\r\n",
        "            lower_loser15,\r\n",
        "            lower_loser16,\r\n",
        "            lower_loser17,\r\n",
        "            lower_loser18,\r\n",
        "            lower_loser19,\r\n",
        "            lower_loser20,\r\n",
        "            lower_loser21,\r\n",
        "            lower_loser22,\r\n",
        "            lower_loser23,\r\n",
        "            lower_loser24,\r\n",
        "            lower_loser25,\r\n",
        "            lower_loser26,\r\n",
        "            lower_loser27,\r\n",
        "            lower_loser28,\r\n",
        "            lower_loser29,\r\n",
        "            lower_loser30,\r\n",
        "            lower_loser31,\r\n",
        "            lower_loser32,\r\n",
        "            lower_loser33,\r\n",
        "            lower_loser34,\r\n",
        "            lower_loser35,\r\n",
        "            lower_loser36,\r\n",
        "            lower_loser37,\r\n",
        "            lower_loser38,\r\n",
        "            lower_loser39,\r\n",
        "            lower_loser40,\r\n",
        "            lower_loser41,\r\n",
        "            lower_loser42,\r\n",
        "            lower_loser43,\r\n",
        "            lower_loser44,\r\n",
        "            lower_loser45,\r\n",
        "            lower_loser46,\r\n",
        "            lower_loser47,\r\n",
        "            lower_loser48,\r\n",
        "            lower_loser49,\r\n",
        "            lower_loser50,\r\n",
        "            lower_loser51,\r\n",
        "            lower_loser52,\r\n",
        "            lower_loser53,\r\n",
        "            lower_loser54,\r\n",
        "            lower_loser55,\r\n",
        "            lower_loser56,\r\n",
        "            lower_loser57,\r\n",
        "            lower_loser58,\r\n",
        "            lower_loser59,\r\n",
        "            lower_loser60,\r\n",
        "            lower_loser61,\r\n",
        "            lower_loser62,\r\n",
        "            lower_loser63,\r\n",
        "            lower_loser64,\r\n",
        "            lower_loser65,\r\n",
        "            lower_loser66,\r\n",
        "            lower_loser67,\r\n",
        "            lower_loser68,\r\n",
        "            lower_loser69,\r\n",
        "            lower_loser70,\r\n",
        "            lower_loser71,\r\n",
        "            lower_loser72,\r\n",
        "            lower_loser73,\r\n",
        "            lower_loser74,\r\n",
        "            lower_loser75,\r\n",
        "            lower_loser76,\r\n",
        "            lower_loser77,\r\n",
        "            lower_loser78,\r\n",
        "            lower_loser79,\r\n",
        "            lower_loser80,\r\n",
        "            lower_loser81,\r\n",
        "            lower_loser82,\r\n",
        "            lower_loser83,\r\n",
        "            lower_loser84,\r\n",
        "            lower_loser85,\r\n",
        "            lower_loser86,\r\n",
        "            lower_loser87,\r\n",
        "            lower_loser88,\r\n",
        "            lower_loser89,\r\n",
        "            lower_loser90,\r\n",
        "            lower_loser91,\r\n",
        "            lower_loser92,\r\n",
        "            lower_loser93,\r\n",
        "            lower_loser94,\r\n",
        "            lower_loser95,\r\n",
        "            lower_loser96,\r\n",
        "            lower_loser97,\r\n",
        "            lower_loser98,\r\n",
        "            lower_loser99,\r\n",
        "            lower_loser100,\r\n",
        "            lower_loser101]\r\n",
        "\r\n",
        "median_loser = [median_loser1,\r\n",
        "            median_loser2,\r\n",
        "            median_loser3,\r\n",
        "            median_loser4,\r\n",
        "            median_loser5,\r\n",
        "            median_loser6,\r\n",
        "            median_loser7,\r\n",
        "            median_loser8,\r\n",
        "            median_loser9,\r\n",
        "            median_loser10,\r\n",
        "            median_loser11,\r\n",
        "            median_loser12,\r\n",
        "            median_loser13,\r\n",
        "            median_loser14,\r\n",
        "            median_loser15,\r\n",
        "            median_loser16,\r\n",
        "            median_loser17,\r\n",
        "            median_loser18,\r\n",
        "            median_loser19,\r\n",
        "            median_loser20,\r\n",
        "            median_loser21,\r\n",
        "            median_loser22,\r\n",
        "            median_loser23,\r\n",
        "            median_loser24,\r\n",
        "            median_loser25,\r\n",
        "            median_loser26,\r\n",
        "            median_loser27,\r\n",
        "            median_loser28,\r\n",
        "            median_loser29,\r\n",
        "            median_loser30,\r\n",
        "            median_loser31,\r\n",
        "            median_loser32,\r\n",
        "            median_loser33,\r\n",
        "            median_loser34,\r\n",
        "            median_loser35,\r\n",
        "            median_loser36,\r\n",
        "            median_loser37,\r\n",
        "            median_loser38,\r\n",
        "            median_loser39,\r\n",
        "            median_loser40,\r\n",
        "            median_loser41,\r\n",
        "            median_loser42,\r\n",
        "            median_loser43,\r\n",
        "            median_loser44,\r\n",
        "            median_loser45,\r\n",
        "            median_loser46,\r\n",
        "            median_loser47,\r\n",
        "            median_loser48,\r\n",
        "            median_loser49,\r\n",
        "            median_loser50,\r\n",
        "            median_loser51,\r\n",
        "            median_loser52,\r\n",
        "            median_loser53,\r\n",
        "            median_loser54,\r\n",
        "            median_loser55,\r\n",
        "            median_loser56,\r\n",
        "            median_loser57,\r\n",
        "            median_loser58,\r\n",
        "            median_loser59,\r\n",
        "            median_loser60,\r\n",
        "            median_loser61,\r\n",
        "            median_loser62,\r\n",
        "            median_loser63,\r\n",
        "            median_loser64,\r\n",
        "            median_loser65,\r\n",
        "            median_loser66,\r\n",
        "            median_loser67,\r\n",
        "            median_loser68,\r\n",
        "            median_loser69,\r\n",
        "            median_loser70,\r\n",
        "            median_loser71,\r\n",
        "            median_loser72,\r\n",
        "            median_loser73,\r\n",
        "            median_loser74,\r\n",
        "            median_loser75,\r\n",
        "            median_loser76,\r\n",
        "            median_loser77,\r\n",
        "            median_loser78,\r\n",
        "            median_loser79,\r\n",
        "            median_loser80,\r\n",
        "            median_loser81,\r\n",
        "            median_loser82,\r\n",
        "            median_loser83,\r\n",
        "            median_loser84,\r\n",
        "            median_loser85,\r\n",
        "            median_loser86,\r\n",
        "            median_loser87,\r\n",
        "            median_loser88,\r\n",
        "            median_loser89,\r\n",
        "            median_loser90,\r\n",
        "            median_loser91,\r\n",
        "            median_loser92,\r\n",
        "            median_loser93,\r\n",
        "            median_loser94,\r\n",
        "            median_loser95,\r\n",
        "            median_loser96,\r\n",
        "            median_loser97,\r\n",
        "            median_loser98,\r\n",
        "            median_loser99,\r\n",
        "            median_loser100,\r\n",
        "            median_loser101]\r\n",
        "\r\n",
        "upper_loser = [upper_loser1,\r\n",
        "            upper_loser2,\r\n",
        "            upper_loser3,\r\n",
        "            upper_loser4,\r\n",
        "            upper_loser5,\r\n",
        "            upper_loser6,\r\n",
        "            upper_loser7,\r\n",
        "            upper_loser8,\r\n",
        "            upper_loser9,\r\n",
        "            upper_loser10,\r\n",
        "            upper_loser11,\r\n",
        "            upper_loser12,\r\n",
        "            upper_loser13,\r\n",
        "            upper_loser14,\r\n",
        "            upper_loser15,\r\n",
        "            upper_loser16,\r\n",
        "            upper_loser17,\r\n",
        "            upper_loser18,\r\n",
        "            upper_loser19,\r\n",
        "            upper_loser20,\r\n",
        "            upper_loser21,\r\n",
        "            upper_loser22,\r\n",
        "            upper_loser23,\r\n",
        "            upper_loser24,\r\n",
        "            upper_loser25,\r\n",
        "            upper_loser26,\r\n",
        "            upper_loser27,\r\n",
        "            upper_loser28,\r\n",
        "            upper_loser29,\r\n",
        "            upper_loser30,\r\n",
        "            upper_loser31,\r\n",
        "            upper_loser32,\r\n",
        "            upper_loser33,\r\n",
        "            upper_loser34,\r\n",
        "            upper_loser35,\r\n",
        "            upper_loser36,\r\n",
        "            upper_loser37,\r\n",
        "            upper_loser38,\r\n",
        "            upper_loser39,\r\n",
        "            upper_loser40,\r\n",
        "            upper_loser41,\r\n",
        "            upper_loser42,\r\n",
        "            upper_loser43,\r\n",
        "            upper_loser44,\r\n",
        "            upper_loser45,\r\n",
        "            upper_loser46,\r\n",
        "            upper_loser47,\r\n",
        "            upper_loser48,\r\n",
        "            upper_loser49,\r\n",
        "            upper_loser50,\r\n",
        "            upper_loser51,\r\n",
        "            upper_loser52,\r\n",
        "            upper_loser53,\r\n",
        "            upper_loser54,\r\n",
        "            upper_loser55,\r\n",
        "            upper_loser56,\r\n",
        "            upper_loser57,\r\n",
        "            upper_loser58,\r\n",
        "            upper_loser59,\r\n",
        "            upper_loser60,\r\n",
        "            upper_loser61,\r\n",
        "            upper_loser62,\r\n",
        "            upper_loser63,\r\n",
        "            upper_loser64,\r\n",
        "            upper_loser65,\r\n",
        "            upper_loser66,\r\n",
        "            upper_loser67,\r\n",
        "            upper_loser68,\r\n",
        "            upper_loser69,\r\n",
        "            upper_loser70,\r\n",
        "            upper_loser71,\r\n",
        "            upper_loser72,\r\n",
        "            upper_loser73,\r\n",
        "            upper_loser74,\r\n",
        "            upper_loser75,\r\n",
        "            upper_loser76,\r\n",
        "            upper_loser77,\r\n",
        "            upper_loser78,\r\n",
        "            upper_loser79,\r\n",
        "            upper_loser80,\r\n",
        "            upper_loser81,\r\n",
        "            upper_loser82,\r\n",
        "            upper_loser83,\r\n",
        "            upper_loser84,\r\n",
        "            upper_loser85,\r\n",
        "            upper_loser86,\r\n",
        "            upper_loser87,\r\n",
        "            upper_loser88,\r\n",
        "            upper_loser89,\r\n",
        "            upper_loser90,\r\n",
        "            upper_loser91,\r\n",
        "            upper_loser92,\r\n",
        "            upper_loser93,\r\n",
        "            upper_loser94,\r\n",
        "            upper_loser95,\r\n",
        "            upper_loser96,\r\n",
        "            upper_loser97,\r\n",
        "            upper_loser98,\r\n",
        "            upper_loser99,\r\n",
        "            upper_loser100,\r\n",
        "            upper_loser101]"
      ],
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u19FndLT2oU3"
      },
      "source": [
        "### Summarize arrays: 'Winner'\r\n",
        "\r\n",
        "lower_winner = [lower_winner1,\r\n",
        "            lower_winner2,\r\n",
        "            lower_winner3,\r\n",
        "            lower_winner4,\r\n",
        "            lower_winner5,\r\n",
        "            lower_winner6,\r\n",
        "            lower_winner7,\r\n",
        "            lower_winner8,\r\n",
        "            lower_winner9,\r\n",
        "            lower_winner10,\r\n",
        "            lower_winner11,\r\n",
        "            lower_winner12,\r\n",
        "            lower_winner13,\r\n",
        "            lower_winner14,\r\n",
        "            lower_winner15,\r\n",
        "            lower_winner16,\r\n",
        "            lower_winner17,\r\n",
        "            lower_winner18,\r\n",
        "            lower_winner19,\r\n",
        "            lower_winner20,\r\n",
        "            lower_winner21,\r\n",
        "            lower_winner22,\r\n",
        "            lower_winner23,\r\n",
        "            lower_winner24,\r\n",
        "            lower_winner25,\r\n",
        "            lower_winner26,\r\n",
        "            lower_winner27,\r\n",
        "            lower_winner28,\r\n",
        "            lower_winner29,\r\n",
        "            lower_winner30,\r\n",
        "            lower_winner31,\r\n",
        "            lower_winner32,\r\n",
        "            lower_winner33,\r\n",
        "            lower_winner34,\r\n",
        "            lower_winner35,\r\n",
        "            lower_winner36,\r\n",
        "            lower_winner37,\r\n",
        "            lower_winner38,\r\n",
        "            lower_winner39,\r\n",
        "            lower_winner40,\r\n",
        "            lower_winner41,\r\n",
        "            lower_winner42,\r\n",
        "            lower_winner43,\r\n",
        "            lower_winner44,\r\n",
        "            lower_winner45,\r\n",
        "            lower_winner46,\r\n",
        "            lower_winner47,\r\n",
        "            lower_winner48,\r\n",
        "            lower_winner49,\r\n",
        "            lower_winner50,\r\n",
        "            lower_winner51,\r\n",
        "            lower_winner52,\r\n",
        "            lower_winner53,\r\n",
        "            lower_winner54,\r\n",
        "            lower_winner55,\r\n",
        "            lower_winner56,\r\n",
        "            lower_winner57,\r\n",
        "            lower_winner58,\r\n",
        "            lower_winner59,\r\n",
        "            lower_winner60,\r\n",
        "            lower_winner61,\r\n",
        "            lower_winner62,\r\n",
        "            lower_winner63,\r\n",
        "            lower_winner64,\r\n",
        "            lower_winner65,\r\n",
        "            lower_winner66,\r\n",
        "            lower_winner67,\r\n",
        "            lower_winner68,\r\n",
        "            lower_winner69,\r\n",
        "            lower_winner70,\r\n",
        "            lower_winner71,\r\n",
        "            lower_winner72,\r\n",
        "            lower_winner73,\r\n",
        "            lower_winner74,\r\n",
        "            lower_winner75,\r\n",
        "            lower_winner76,\r\n",
        "            lower_winner77,\r\n",
        "            lower_winner78,\r\n",
        "            lower_winner79,\r\n",
        "            lower_winner80,\r\n",
        "            lower_winner81,\r\n",
        "            lower_winner82,\r\n",
        "            lower_winner83,\r\n",
        "            lower_winner84,\r\n",
        "            lower_winner85,\r\n",
        "            lower_winner86,\r\n",
        "            lower_winner87,\r\n",
        "            lower_winner88,\r\n",
        "            lower_winner89,\r\n",
        "            lower_winner90,\r\n",
        "            lower_winner91,\r\n",
        "            lower_winner92,\r\n",
        "            lower_winner93,\r\n",
        "            lower_winner94,\r\n",
        "            lower_winner95,\r\n",
        "            lower_winner96,\r\n",
        "            lower_winner97,\r\n",
        "            lower_winner98,\r\n",
        "            lower_winner99,\r\n",
        "            lower_winner100,\r\n",
        "            lower_winner101]\r\n",
        "\r\n",
        "median_winner = [median_winner1,\r\n",
        "            median_winner2,\r\n",
        "            median_winner3,\r\n",
        "            median_winner4,\r\n",
        "            median_winner5,\r\n",
        "            median_winner6,\r\n",
        "            median_winner7,\r\n",
        "            median_winner8,\r\n",
        "            median_winner9,\r\n",
        "            median_winner10,\r\n",
        "            median_winner11,\r\n",
        "            median_winner12,\r\n",
        "            median_winner13,\r\n",
        "            median_winner14,\r\n",
        "            median_winner15,\r\n",
        "            median_winner16,\r\n",
        "            median_winner17,\r\n",
        "            median_winner18,\r\n",
        "            median_winner19,\r\n",
        "            median_winner20,\r\n",
        "            median_winner21,\r\n",
        "            median_winner22,\r\n",
        "            median_winner23,\r\n",
        "            median_winner24,\r\n",
        "            median_winner25,\r\n",
        "            median_winner26,\r\n",
        "            median_winner27,\r\n",
        "            median_winner28,\r\n",
        "            median_winner29,\r\n",
        "            median_winner30,\r\n",
        "            median_winner31,\r\n",
        "            median_winner32,\r\n",
        "            median_winner33,\r\n",
        "            median_winner34,\r\n",
        "            median_winner35,\r\n",
        "            median_winner36,\r\n",
        "            median_winner37,\r\n",
        "            median_winner38,\r\n",
        "            median_winner39,\r\n",
        "            median_winner40,\r\n",
        "            median_winner41,\r\n",
        "            median_winner42,\r\n",
        "            median_winner43,\r\n",
        "            median_winner44,\r\n",
        "            median_winner45,\r\n",
        "            median_winner46,\r\n",
        "            median_winner47,\r\n",
        "            median_winner48,\r\n",
        "            median_winner49,\r\n",
        "            median_winner50,\r\n",
        "            median_winner51,\r\n",
        "            median_winner52,\r\n",
        "            median_winner53,\r\n",
        "            median_winner54,\r\n",
        "            median_winner55,\r\n",
        "            median_winner56,\r\n",
        "            median_winner57,\r\n",
        "            median_winner58,\r\n",
        "            median_winner59,\r\n",
        "            median_winner60,\r\n",
        "            median_winner61,\r\n",
        "            median_winner62,\r\n",
        "            median_winner63,\r\n",
        "            median_winner64,\r\n",
        "            median_winner65,\r\n",
        "            median_winner66,\r\n",
        "            median_winner67,\r\n",
        "            median_winner68,\r\n",
        "            median_winner69,\r\n",
        "            median_winner70,\r\n",
        "            median_winner71,\r\n",
        "            median_winner72,\r\n",
        "            median_winner73,\r\n",
        "            median_winner74,\r\n",
        "            median_winner75,\r\n",
        "            median_winner76,\r\n",
        "            median_winner77,\r\n",
        "            median_winner78,\r\n",
        "            median_winner79,\r\n",
        "            median_winner80,\r\n",
        "            median_winner81,\r\n",
        "            median_winner82,\r\n",
        "            median_winner83,\r\n",
        "            median_winner84,\r\n",
        "            median_winner85,\r\n",
        "            median_winner86,\r\n",
        "            median_winner87,\r\n",
        "            median_winner88,\r\n",
        "            median_winner89,\r\n",
        "            median_winner90,\r\n",
        "            median_winner91,\r\n",
        "            median_winner92,\r\n",
        "            median_winner93,\r\n",
        "            median_winner94,\r\n",
        "            median_winner95,\r\n",
        "            median_winner96,\r\n",
        "            median_winner97,\r\n",
        "            median_winner98,\r\n",
        "            median_winner99,\r\n",
        "            median_winner100,\r\n",
        "            median_winner101]\r\n",
        "\r\n",
        "upper_winner = [upper_winner1,\r\n",
        "            upper_winner2,\r\n",
        "            upper_winner3,\r\n",
        "            upper_winner4,\r\n",
        "            upper_winner5,\r\n",
        "            upper_winner6,\r\n",
        "            upper_winner7,\r\n",
        "            upper_winner8,\r\n",
        "            upper_winner9,\r\n",
        "            upper_winner10,\r\n",
        "            upper_winner11,\r\n",
        "            upper_winner12,\r\n",
        "            upper_winner13,\r\n",
        "            upper_winner14,\r\n",
        "            upper_winner15,\r\n",
        "            upper_winner16,\r\n",
        "            upper_winner17,\r\n",
        "            upper_winner18,\r\n",
        "            upper_winner19,\r\n",
        "            upper_winner20,\r\n",
        "            upper_winner21,\r\n",
        "            upper_winner22,\r\n",
        "            upper_winner23,\r\n",
        "            upper_winner24,\r\n",
        "            upper_winner25,\r\n",
        "            upper_winner26,\r\n",
        "            upper_winner27,\r\n",
        "            upper_winner28,\r\n",
        "            upper_winner29,\r\n",
        "            upper_winner30,\r\n",
        "            upper_winner31,\r\n",
        "            upper_winner32,\r\n",
        "            upper_winner33,\r\n",
        "            upper_winner34,\r\n",
        "            upper_winner35,\r\n",
        "            upper_winner36,\r\n",
        "            upper_winner37,\r\n",
        "            upper_winner38,\r\n",
        "            upper_winner39,\r\n",
        "            upper_winner40,\r\n",
        "            upper_winner41,\r\n",
        "            upper_winner42,\r\n",
        "            upper_winner43,\r\n",
        "            upper_winner44,\r\n",
        "            upper_winner45,\r\n",
        "            upper_winner46,\r\n",
        "            upper_winner47,\r\n",
        "            upper_winner48,\r\n",
        "            upper_winner49,\r\n",
        "            upper_winner50,\r\n",
        "            upper_winner51,\r\n",
        "            upper_winner52,\r\n",
        "            upper_winner53,\r\n",
        "            upper_winner54,\r\n",
        "            upper_winner55,\r\n",
        "            upper_winner56,\r\n",
        "            upper_winner57,\r\n",
        "            upper_winner58,\r\n",
        "            upper_winner59,\r\n",
        "            upper_winner60,\r\n",
        "            upper_winner61,\r\n",
        "            upper_winner62,\r\n",
        "            upper_winner63,\r\n",
        "            upper_winner64,\r\n",
        "            upper_winner65,\r\n",
        "            upper_winner66,\r\n",
        "            upper_winner67,\r\n",
        "            upper_winner68,\r\n",
        "            upper_winner69,\r\n",
        "            upper_winner70,\r\n",
        "            upper_winner71,\r\n",
        "            upper_winner72,\r\n",
        "            upper_winner73,\r\n",
        "            upper_winner74,\r\n",
        "            upper_winner75,\r\n",
        "            upper_winner76,\r\n",
        "            upper_winner77,\r\n",
        "            upper_winner78,\r\n",
        "            upper_winner79,\r\n",
        "            upper_winner80,\r\n",
        "            upper_winner81,\r\n",
        "            upper_winner82,\r\n",
        "            upper_winner83,\r\n",
        "            upper_winner84,\r\n",
        "            upper_winner85,\r\n",
        "            upper_winner86,\r\n",
        "            upper_winner87,\r\n",
        "            upper_winner88,\r\n",
        "            upper_winner89,\r\n",
        "            upper_winner90,\r\n",
        "            upper_winner91,\r\n",
        "            upper_winner92,\r\n",
        "            upper_winner93,\r\n",
        "            upper_winner94,\r\n",
        "            upper_winner95,\r\n",
        "            upper_winner96,\r\n",
        "            upper_winner97,\r\n",
        "            upper_winner98,\r\n",
        "            upper_winner99,\r\n",
        "            upper_winner100,\r\n",
        "            upper_winner101]"
      ],
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84XIzmWD2oba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "outputId": "1676c2ac-e907-4ebd-f9f5-69c8bde3f09e"
      },
      "source": [
        "### Visualize!\r\n",
        "\r\n",
        "title = obj_func\r\n",
        "\r\n",
        "plt.figure()\r\n",
        "\r\n",
        "plt.plot(median_loser, color = 'Green')\r\n",
        "plt.plot(median_winner, color = 'Brown')\r\n",
        "\r\n",
        "xstar = np.arange(0, max_iter+1, step=1)\r\n",
        "plt.fill_between(xstar, lower_loser, upper_loser, facecolor = 'Green', alpha=0.4, label='GP EI Regret IQR: dEI')\r\n",
        "plt.fill_between(xstar, lower_winner, upper_winner, facecolor = 'Brown', alpha=0.4, label='GP CBM Regret IQR: dCBM')\r\n",
        "\r\n",
        "plt.title(title, weight = 'bold', family = 'Arial')\r\n",
        "plt.xlabel('(Post-initialization) iteration $\\it{k}$', weight = 'bold', family = 'Arial') \r\n",
        "plt.ylabel('log(Regret)', weight = 'bold', family = 'Arial') \r\n",
        "plt.legend(loc=1) # add plot legend\r\n",
        "\r\n",
        "plt.show()"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
            "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5dkR2Id2oiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23f3ba6b-c650-4700-fac0-cd75a294c778"
      },
      "source": [
        "time_lose, time_win"
      ],
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3535.0361382961273, 19491.90955233574)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNLBF57p2opi"
      },
      "source": [
        ""
      ],
      "execution_count": 183,
      "outputs": []
    }
  ]
}